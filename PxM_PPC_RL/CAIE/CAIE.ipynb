{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process noise = 0.0, Measurement noise = 0.0\n",
      "Process noise = 0.1, Measurement noise = 0.0\n",
      "Process noise = 0.2, Measurement noise = 0.0\n",
      "Process noise = 0.30000000000000004, Measurement noise = 0.0\n",
      "Process noise = 0.4, Measurement noise = 0.0\n",
      "Process noise = 0.5, Measurement noise = 0.0\n",
      "Process noise = 0.6000000000000001, Measurement noise = 0.0\n",
      "Process noise = 0.7000000000000001, Measurement noise = 0.0\n",
      "Process noise = 0.8, Measurement noise = 0.0\n",
      "Process noise = 0.9, Measurement noise = 0.0\n",
      "Process noise = 1.0, Measurement noise = 0.0\n"
     ]
    }
   ],
   "source": [
    "### DATA-DRIVEN DIAGNOSTICS I ###\n",
    "### GENERATE DATA FOR DATA-DRIVEN MODEL ###\n",
    "\n",
    "import random\n",
    "from prog_models.models import BatteryCircuit\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "# Ignore warnings when machine exceeds its end of life\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\"\"\" Method that uses a physical machine model from the prog_models package and a current (health) state of the model and\n",
    "an action (i.e., intensity), which is performed for 100 time steps\n",
    "    Parameter:\n",
    "        machine             machine model from the prog_models package\n",
    "        state               current (health) state of the model\n",
    "        action              loading of the machine for the next 100 time steps\n",
    "    Return:\n",
    "        health                               \n",
    "    \"\"\"\n",
    "def produce_model(machine, states, action):\n",
    "        \n",
    "        # Define load of battery\n",
    "        def future_loading(t, x=None):\n",
    "            return {'i': action}\n",
    "\n",
    "        # Set current state of machine\n",
    "        machine.parameters['x0'] = states\n",
    "        # Simulate 100 steps\n",
    "        options = {\n",
    "            'save_freq': 100,  # Frequency at which results are saved\n",
    "            'dt': 2  # Timestep\n",
    "        }\n",
    "        (_, _, states, outputs, event_states) = machine.simulate_to(100, future_loading, **options)\n",
    "        health = event_states[-1]['EOD']\n",
    "        return(round(health, 2), states[-1], outputs[-1]['t'], outputs[-1]['v'])\n",
    "def reset_states(machine):\n",
    "    # Returns initial states of machine, e.g., {'tb': 18.95, 'qb': 7856.3254, 'qcp': 0, 'qcs': 0} for Battery\n",
    "    return(machine.default_parameters['x0'])\n",
    "   \n",
    "for pn in [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]: \n",
    "    for mn in [0]:\n",
    "        print('Process noise = ' + str(0.1*pn) + ', Measurement noise = ' + str(0.1*mn))\n",
    "        battery = BatteryCircuit(process_noise = 0.1*pn, measurement_noise = 0.1*mn)\n",
    "        states = reset_states(battery)\n",
    "        reset_counter = 0\n",
    "        dataset = []\n",
    "        for i in range(int(1e4)):\n",
    "            # If asset failed last period, reset all historical values\n",
    "            if reset_counter == 0: t = v = t_1 = v_1 = t_2 = v_2 = t_3 = v_3 = 0 \n",
    "            # Shift history by one time period\n",
    "            v_3 = v_2\n",
    "            t_3 = t_2\n",
    "            v_2 = v_1\n",
    "            t_2 = t_1\n",
    "            v_1 = v\n",
    "            t_1 = t\n",
    "\n",
    "            # Increment reset_counter\n",
    "            reset_counter = reset_counter + 1\n",
    "            # Compute new health, states, t, and v using last battery state and a random new action\n",
    "            health, states, t, v = produce_model(machine=battery, states=states, action=random.sample((0, 1, 2, 3, 4), 1)[0])\n",
    "            \n",
    "            if health <= 0: \n",
    "                # Reset battery states to initialize battery for next produce_model call\n",
    "                states = reset_states(battery)\n",
    "                # Initialize reset_counter\n",
    "                reset_counter = 0\n",
    "                # Sometimes produce_model returns weird or negative values as the end of life is exceeded\n",
    "                # Here, we just simply set it to zero to not confuse a later learner \n",
    "                health = 0\n",
    "\n",
    "            # append to two-dimensional list\n",
    "            dataset.append([t, v, t_1, v_1, t_2, v_2, t_3, v_3, health])\n",
    "\n",
    "        # Transform two-dim list to dataframe\n",
    "        dataset = pd.DataFrame(dataset, columns=['t', 'v', 't_1', 'v_1', 't_2', 'v_2', 't_3', 'v_3', 'health'])\n",
    "        # Denote machine runs to-failure with incrementing id\n",
    "        k = 0\n",
    "        for i in range(dataset.shape[0]):\n",
    "            if (dataset.iloc[i]['t_1'] == 0 and dataset.iloc[i]['v_1'] == 0):\n",
    "                j = 1\n",
    "                k = k + 1\n",
    "            dataset.loc[i, 'time'] = j\n",
    "            j = j + 1\n",
    "            dataset.loc[i, 'ID'] = k\n",
    "        dataset = dataset.sort_values(['ID', 'time'], ascending=[True, False])\n",
    "        # Assign RUL by counting upwards per serial number in the descended data frame\n",
    "        dataset['RUL'] = dataset.groupby((dataset['ID'] != dataset['ID'].shift(1)).cumsum()).cumcount()\n",
    "        dataset = dataset.sort_values(['ID', 'time'], ascending=True)\n",
    "\n",
    "        # Save it as pickle\n",
    "        dataset.to_pickle('diagnostics/data_' + 'pn' + str(pn) + '_mn' + str(mn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model based on RUL (prognostics = True) or health (diagnostics, prognostics = False)\n",
    "prognostics = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process noise = 0.0, Measurement noise = 0.0\n",
      "R2: [0.98346924 0.97869884 0.98271119 0.98084096 0.98029931]\n",
      "mae: [-0.0186088  -0.02097155 -0.01949765 -0.0201391  -0.0208858 ]\n",
      "mape: [-7.86658635e-02 -3.23583633e+11 -9.00719925e+09 -9.20286148e-02\n",
      " -1.71587146e+11]\n",
      "rmse: [-0.03669848 -0.04135376 -0.03768967 -0.03960828 -0.04014499]\n",
      "Process noise = 0.1, Measurement noise = 0.0\n",
      "R2: [0.94511725 0.94338909 0.94308702 0.95269309 0.92781427]\n",
      "mae: [-0.04355035 -0.0430732  -0.0434886  -0.0409953  -0.04892035]\n",
      "mape: [-1.57625987e+09 -6.75539944e+09 -2.31935381e+10 -3.03992975e+10\n",
      " -3.69295169e+11]\n",
      "rmse: [-0.06585241 -0.06812947 -0.06765209 -0.06181947 -0.07649472]\n",
      "Process noise = 0.2, Measurement noise = 0.0\n",
      "R2: [0.89160295 0.90719555 0.90106071 0.89986382 0.8983238 ]\n",
      "mae: [-0.06726765 -0.0624331  -0.0637716  -0.0658622  -0.0643069 ]\n",
      "mape: [-3.90912448e+11 -3.67493730e+11 -1.03605309e+12 -7.60657977e+11\n",
      " -1.41413028e+11]\n",
      "rmse: [-0.09406841 -0.08643758 -0.08901846 -0.09017457 -0.0908437 ]\n",
      "Process noise = 0.30000000000000004, Measurement noise = 0.0\n",
      "R2: [0.87037827 0.84902257 0.84585148 0.85446821 0.85283189]\n",
      "mae: [-0.07610885 -0.0816426  -0.0808236  -0.0798498  -0.0796984 ]\n",
      "mape: [-8.21906932e+10 -2.44320280e+11 -3.61413870e+11 -2.58281439e+11\n",
      " -2.56705179e+10]\n",
      "rmse: [-0.10279177 -0.11007041 -0.11067439 -0.10907084 -0.10878756]\n",
      "Process noise = 0.4, Measurement noise = 0.0\n",
      "R2: [0.83456832 0.83012227 0.80835482 0.83903277 0.81974217]\n",
      "mae: [-0.0846392  -0.0875222  -0.09059015 -0.08526315 -0.090239  ]\n",
      "mape: [-1.02456892e+11 -4.00820367e+11 -2.39816680e+11 -2.95661316e+11\n",
      " -6.73288144e+10]\n",
      "rmse: [-0.11565337 -0.11667704 -0.12263368 -0.11445298 -0.12049927]\n",
      "Process noise = 0.5, Measurement noise = 0.0\n",
      "R2: [0.82839126 0.80480565 0.82500873 0.8179684  0.817267  ]\n",
      "mae: [-0.09095045 -0.096345   -0.0892272  -0.09109585 -0.09089715]\n",
      "mape: [-4.76480841e+11 -2.50850499e+11 -2.85978576e+11 -7.34086739e+10\n",
      " -4.02171447e+11]\n",
      "rmse: [-0.11899326 -0.12706747 -0.11847659 -0.12179301 -0.12175318]\n",
      "Process noise = 0.6000000000000001, Measurement noise = 0.0\n",
      "R2: [0.78807567 0.79536683 0.79253112 0.78549367 0.7681572 ]\n",
      "mae: [-0.09885245 -0.09946715 -0.09709435 -0.1021844  -0.1052449 ]\n",
      "mape: [-3.47452711e+11 -4.54638382e+11 -8.87209127e+11 -2.08516663e+11\n",
      " -3.28762773e+11]\n",
      "rmse: [-0.13075345 -0.13061024 -0.12871161 -0.13304715 -0.13793793]\n",
      "Process noise = 0.7000000000000001, Measurement noise = 0.0\n",
      "R2: [0.77073703 0.76979583 0.75657293 0.74444239 0.7908058 ]\n",
      "mae: [-0.1031984  -0.1025792  -0.10822995 -0.11202985 -0.09902865]\n",
      "mape: [-2.76746197e+11 -4.12529726e+11 -1.64156206e+11 -5.88395291e+11\n",
      " -3.12099454e+11]\n",
      "rmse: [-0.13438963 -0.1351357  -0.14146253 -0.14412065 -0.1310333 ]\n",
      "Process noise = 0.8, Measurement noise = 0.0\n",
      "R2: [0.75441041 0.73836646 0.72956504 0.7436836  0.74272255]\n",
      "mae: [-0.1075912  -0.1102882  -0.11299515 -0.10926115 -0.1102889 ]\n",
      "mape: [-3.29888673e+11 -1.32315757e+12 -3.39571412e+11 -9.10177485e+11\n",
      " -5.11158558e+11]\n",
      "rmse: [-0.14059851 -0.14411492 -0.14701622 -0.1434903  -0.14296051]\n",
      "Process noise = 0.9, Measurement noise = 0.0\n",
      "R2: [0.7459057  0.72225387 0.71386912 0.73079549 0.71229312]\n",
      "mae: [-0.1088923  -0.115251   -0.11461225 -0.1123348  -0.1146464 ]\n",
      "mape: [-2.86203756e+11 -1.43439648e+11 -3.62990130e+11 -8.26410532e+10\n",
      " -6.39285967e+11]\n",
      "rmse: [-0.14232725 -0.14946078 -0.14982241 -0.14679103 -0.14974282]\n",
      "Process noise = 1.0, Measurement noise = 0.0\n",
      "R2: [0.70404433 0.7015158  0.69136757 0.70469365 0.71885872]\n",
      "mae: [-0.1154981  -0.1165566  -0.11575255 -0.11541465 -0.11638255]\n",
      "mape: [-1.49744688e+11 -5.21967197e+11 -5.96051411e+11 -2.15925084e+12\n",
      " -3.37769972e+11]\n",
      "rmse: [-0.15095345 -0.15119491 -0.15424514 -0.15225114 -0.15056938]\n"
     ]
    }
   ],
   "source": [
    "### DATA-DRIVEN DIAGNOSTICS II ###\n",
    "### FIT AND TEST MODEL ###\n",
    "from sklearn import tree, linear_model, kernel_ridge, svm, neighbors, gaussian_process, ensemble, neural_network\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_validate\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "score_data = []\n",
    "for pn in [0, 1, 2, 3, 4, 5, 6, 7 , 8, 9, 10]: \n",
    "    for mn in [0]:\n",
    "        print('Process noise = ' + str(0.1*pn) + ', Measurement noise = ' + str(0.1*mn))\n",
    "        dataset = pd.read_pickle('diagnostics/data_' + 'pn' + str(pn) + '_mn' + str(mn))\n",
    "        X = dataset[['t', 'v', 't_1', 'v_1', 't_2', 'v_2', 't_3', 'v_3']]\n",
    "        if prognostics:\n",
    "            y = dataset['RUL']\n",
    "        else:\n",
    "            y = dataset['health']\n",
    "        learner = ensemble.RandomForestRegressor()\n",
    "        scoring = {'r2': 'r2',\n",
    "                    'mae': 'neg_mean_absolute_error',\n",
    "                    'mape': 'neg_mean_absolute_percentage_error',\n",
    "                    'rmse': 'neg_root_mean_squared_error'}\n",
    "        scores = cross_validate(learner, X, y, scoring=scoring, cv=5) # default scoring R2\n",
    "        print(\"R2: \" + str(scores['test_r2']))\n",
    "        print(\"mae: \" + str(scores['test_mae']))\n",
    "        print(\"mape: \" + str(scores['test_mape']))\n",
    "        print(\"rmse: \" + str(scores['test_rmse']))\n",
    "        # Store measures\n",
    "        score_data.append(['model_' + 'pn' + str(pn) + '_mn' + str(mn), np.mean(scores['test_r2']),\n",
    "                            np.mean(scores['test_mae']), np.mean(scores['test_mape']), np.mean(scores['test_rmse'])])\n",
    "\n",
    "        # Fit on all data\n",
    "        model = ensemble.RandomForestRegressor().fit(X, y)\n",
    "        if prognostics:\n",
    "            pickle.dump(model, open('prognostics/model_' + 'pn' + str(pn) + '_mn' + str(mn), 'wb'))\n",
    "        else:\n",
    "            pickle.dump(model, open('diagnostics/model_' + 'pn' + str(pn) + '_mn' + str(mn), 'wb'))\n",
    "# Transform two-dim list to dataframe\n",
    "score_df = pd.DataFrame(score_data, columns=['model', 'r2', 'mae', 'mape', 'rmse'])\n",
    "if prognostics:\n",
    "    score_df.to_excel(\"prognostics/scores.xlsx\") \n",
    "else:\n",
    "    score_df.to_excel(\"diagnostics/scores.xlsx\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process noise = 0.0, Measurement noise = 0.0\n",
      "Process noise = 0.1, Measurement noise = 0.0\n",
      "Process noise = 0.2, Measurement noise = 0.0\n",
      "Process noise = 0.30000000000000004, Measurement noise = 0.0\n",
      "Process noise = 0.4, Measurement noise = 0.0\n",
      "Process noise = 0.5, Measurement noise = 0.0\n",
      "Process noise = 0.6000000000000001, Measurement noise = 0.0\n",
      "Process noise = 0.7000000000000001, Measurement noise = 0.0\n",
      "Process noise = 0.8, Measurement noise = 0.0\n",
      "Process noise = 0.9, Measurement noise = 0.0\n",
      "Process noise = 1.0, Measurement noise = 0.0\n"
     ]
    }
   ],
   "source": [
    "### DATA-DRIVEN DIAGNOSTICS IIIa ###\n",
    "### FIT, TEST, VISUALIZE MODEL USING TRAIN AND TEST SETS ###\n",
    "\n",
    "for pn in [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]: \n",
    "    for mn in [0]:\n",
    "        print('Process noise = ' + str(0+0.1*pn) + ', Measurement noise = ' + str(0+0.1*mn))\n",
    "        if prognostics:\n",
    "            dataset = pd.read_pickle('diagnostics/data_' + 'pn' + str(pn) + '_mn' + str(mn))\n",
    "            y = dataset['RUL']\n",
    "        else:\n",
    "            dataset = pd.read_pickle('prognostics/data_' + 'pn' + str(pn) + '_mn' + str(mn))\n",
    "            y = dataset['health']\n",
    "        X = dataset[['t', 'v', 't_1', 'v_1', 't_2', 'v_2', 't_3', 'v_3']]\n",
    "        # Find index of healthy machines\n",
    "        index_df = X.index[(X['t_1'] == 0) & (X['v_1'] == 0) & (X['t_2'] == 0) & (X['v_2'] == 0) & (X['t_3'] == 0) & (X['v_3'] == 0)].tolist()\n",
    "        index_test = round(len(index_df)*0.8)\n",
    "\n",
    "        # Create train and test set without disrupting machine runs to-failure\n",
    "        X_train = X.iloc[0:(index_df[index_test])]\n",
    "        y_train = y.iloc[0:(index_df[index_test])]\n",
    "        X_test = X.iloc[index_df[index_test]:(len(X))]\n",
    "        y_test = y.iloc[index_df[index_test]:(len(y))]\n",
    "\n",
    "        ## Train\n",
    "        model = ensemble.RandomForestRegressor().fit(X_train, y_train)\n",
    "        ## Predict\n",
    "        y_pred = pd.DataFrame(model.predict(X_test), columns=['Pred'])\n",
    "        ## Analyze\n",
    "        #reset index of each DataFrame\n",
    "        X_test.reset_index(drop=True, inplace=True)\n",
    "        y_test.reset_index(drop=True, inplace=True)\n",
    "        # Concat dataframes\n",
    "        test_df = pd.concat([X_test, y_test, y_pred], axis=1)\n",
    "        # Print for visualization (e.g., in R)\n",
    "        if prognostics:\n",
    "            test_df.to_excel(\"prognostics/test_results_\" + 'pn' + str(pn) + '_mn' + str(mn) + \".xlsx\") \n",
    "        else:\n",
    "            test_df.to_excel(\"diagnostics/test_results_\" + 'pn' + str(pn) + '_mn' + str(mn) + \".xlsx\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gamma = 0.8, Learning Rate = 0.003, Activation Function = Tanh, Neurons = 8, Process noise = 0.1, Measurement noise = 0.0\n",
      "Eval num_timesteps=1000, episode_reward=-2449.60 +/- 815.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=-1798.40 +/- 765.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3000, episode_reward=-2490.80 +/- 410.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=4000, episode_reward=-2140.20 +/- 682.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=5000, episode_reward=-1809.40 +/- 422.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=6000, episode_reward=-1893.20 +/- 811.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=7000, episode_reward=-1946.20 +/- 431.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=8000, episode_reward=-1864.40 +/- 493.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=9000, episode_reward=-1891.00 +/- 245.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=10000, episode_reward=-2091.80 +/- 693.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=11000, episode_reward=-1542.00 +/- 306.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=12000, episode_reward=-2042.00 +/- 569.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=13000, episode_reward=-1727.80 +/- 610.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=14000, episode_reward=-1984.00 +/- 789.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=15000, episode_reward=-1653.00 +/- 734.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=16000, episode_reward=-1249.20 +/- 352.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=17000, episode_reward=-1924.60 +/- 122.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=18000, episode_reward=-1674.00 +/- 625.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=19000, episode_reward=-1712.60 +/- 426.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=20000, episode_reward=-1395.20 +/- 379.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=21000, episode_reward=-2152.20 +/- 722.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=22000, episode_reward=-1595.00 +/- 722.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=23000, episode_reward=-2157.60 +/- 774.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=24000, episode_reward=-2349.20 +/- 699.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=25000, episode_reward=-1558.40 +/- 368.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=26000, episode_reward=-2146.80 +/- 608.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=27000, episode_reward=-2081.00 +/- 524.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=28000, episode_reward=-2855.60 +/- 674.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=29000, episode_reward=-2136.40 +/- 500.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=30000, episode_reward=-1828.60 +/- 711.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=31000, episode_reward=-2240.20 +/- 501.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=32000, episode_reward=-1845.00 +/- 349.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=33000, episode_reward=-2246.20 +/- 601.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=34000, episode_reward=-2202.60 +/- 354.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=35000, episode_reward=-2264.40 +/- 480.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=36000, episode_reward=-1873.40 +/- 375.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=37000, episode_reward=-2026.00 +/- 491.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=38000, episode_reward=-1984.40 +/- 207.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=39000, episode_reward=-2184.40 +/- 569.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=40000, episode_reward=-2081.20 +/- 781.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=41000, episode_reward=-1839.80 +/- 528.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=42000, episode_reward=-2272.20 +/- 647.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=43000, episode_reward=-2206.20 +/- 449.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=44000, episode_reward=-1824.60 +/- 426.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=45000, episode_reward=-2094.80 +/- 667.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=46000, episode_reward=-2136.00 +/- 482.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=47000, episode_reward=-2134.60 +/- 128.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=48000, episode_reward=-2248.80 +/- 550.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=49000, episode_reward=-1945.60 +/- 608.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=50000, episode_reward=-2312.20 +/- 756.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=51000, episode_reward=186.20 +/- 401.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=52000, episode_reward=-1161.40 +/- 681.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=53000, episode_reward=-1900.20 +/- 532.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=54000, episode_reward=-3017.20 +/- 962.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=55000, episode_reward=-3014.80 +/- 496.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=56000, episode_reward=-1787.40 +/- 379.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=57000, episode_reward=-1810.00 +/- 256.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=58000, episode_reward=-817.00 +/- 457.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=59000, episode_reward=-1133.40 +/- 397.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=60000, episode_reward=-728.00 +/- 345.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=61000, episode_reward=-998.00 +/- 410.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=62000, episode_reward=-819.60 +/- 221.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=63000, episode_reward=-591.00 +/- 696.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=64000, episode_reward=-226.00 +/- 295.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=65000, episode_reward=-297.00 +/- 142.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=66000, episode_reward=-106.40 +/- 264.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=67000, episode_reward=-500.20 +/- 175.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=68000, episode_reward=-269.60 +/- 206.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=69000, episode_reward=-20.20 +/- 94.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=70000, episode_reward=160.20 +/- 136.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=71000, episode_reward=-444.80 +/- 193.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=72000, episode_reward=-291.80 +/- 221.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=73000, episode_reward=-106.60 +/- 286.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=74000, episode_reward=315.80 +/- 199.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=75000, episode_reward=76.60 +/- 248.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=76000, episode_reward=174.40 +/- 221.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=77000, episode_reward=14.40 +/- 212.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=78000, episode_reward=7.60 +/- 217.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=79000, episode_reward=288.80 +/- 156.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=80000, episode_reward=252.00 +/- 93.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=81000, episode_reward=89.80 +/- 135.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=82000, episode_reward=29.00 +/- 138.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=83000, episode_reward=-70.00 +/- 327.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=84000, episode_reward=31.00 +/- 107.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=85000, episode_reward=48.60 +/- 55.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=86000, episode_reward=14.40 +/- 172.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=87000, episode_reward=284.40 +/- 100.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=88000, episode_reward=335.60 +/- 222.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=89000, episode_reward=254.20 +/- 159.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=90000, episode_reward=171.00 +/- 120.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=91000, episode_reward=9.40 +/- 224.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=92000, episode_reward=86.60 +/- 158.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=93000, episode_reward=138.60 +/- 96.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=94000, episode_reward=31.00 +/- 282.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=95000, episode_reward=31.20 +/- 236.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=96000, episode_reward=167.40 +/- 232.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=97000, episode_reward=43.20 +/- 256.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=98000, episode_reward=173.60 +/- 244.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=99000, episode_reward=225.60 +/- 274.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=100000, episode_reward=133.60 +/- 162.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=101000, episode_reward=51.20 +/- 95.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=102000, episode_reward=136.00 +/- 218.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=103000, episode_reward=80.60 +/- 214.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=104000, episode_reward=212.00 +/- 123.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=105000, episode_reward=307.40 +/- 140.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=106000, episode_reward=211.60 +/- 175.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=107000, episode_reward=109.00 +/- 106.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=108000, episode_reward=140.60 +/- 170.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=109000, episode_reward=316.00 +/- 180.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=110000, episode_reward=416.80 +/- 185.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=111000, episode_reward=161.40 +/- 266.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=112000, episode_reward=267.20 +/- 165.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=113000, episode_reward=202.00 +/- 158.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=114000, episode_reward=370.40 +/- 101.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=115000, episode_reward=261.80 +/- 105.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=116000, episode_reward=267.40 +/- 151.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=117000, episode_reward=285.20 +/- 133.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=118000, episode_reward=202.20 +/- 83.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=119000, episode_reward=352.80 +/- 193.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=120000, episode_reward=170.80 +/- 200.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=121000, episode_reward=291.40 +/- 136.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=122000, episode_reward=211.00 +/- 125.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=123000, episode_reward=432.00 +/- 132.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=124000, episode_reward=183.40 +/- 153.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=125000, episode_reward=298.00 +/- 193.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=126000, episode_reward=205.00 +/- 151.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=127000, episode_reward=221.00 +/- 265.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=128000, episode_reward=266.80 +/- 344.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=129000, episode_reward=325.20 +/- 195.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=130000, episode_reward=176.60 +/- 296.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=131000, episode_reward=166.00 +/- 194.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=132000, episode_reward=273.80 +/- 110.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=133000, episode_reward=303.00 +/- 142.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=134000, episode_reward=157.20 +/- 285.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=135000, episode_reward=392.80 +/- 317.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=136000, episode_reward=160.00 +/- 119.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=137000, episode_reward=595.40 +/- 149.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=138000, episode_reward=643.20 +/- 268.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=139000, episode_reward=343.20 +/- 189.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=140000, episode_reward=560.00 +/- 348.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=141000, episode_reward=632.00 +/- 183.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=142000, episode_reward=676.20 +/- 363.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=143000, episode_reward=347.60 +/- 186.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=144000, episode_reward=839.80 +/- 203.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=145000, episode_reward=709.20 +/- 202.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=146000, episode_reward=764.40 +/- 444.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=147000, episode_reward=929.20 +/- 306.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=148000, episode_reward=816.00 +/- 494.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=149000, episode_reward=645.60 +/- 348.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=150000, episode_reward=504.20 +/- 196.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=151000, episode_reward=722.60 +/- 249.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=152000, episode_reward=663.00 +/- 326.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=153000, episode_reward=919.40 +/- 270.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=154000, episode_reward=812.80 +/- 169.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=155000, episode_reward=839.00 +/- 209.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=156000, episode_reward=535.60 +/- 168.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=157000, episode_reward=652.20 +/- 220.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=158000, episode_reward=650.40 +/- 119.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=159000, episode_reward=553.80 +/- 415.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=160000, episode_reward=843.80 +/- 355.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=161000, episode_reward=648.00 +/- 270.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=162000, episode_reward=428.80 +/- 329.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=163000, episode_reward=553.00 +/- 398.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=164000, episode_reward=525.80 +/- 137.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=165000, episode_reward=425.60 +/- 297.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=166000, episode_reward=450.20 +/- 242.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=167000, episode_reward=377.00 +/- 129.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=168000, episode_reward=394.80 +/- 190.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=169000, episode_reward=360.60 +/- 182.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=170000, episode_reward=352.20 +/- 166.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=171000, episode_reward=479.60 +/- 414.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=172000, episode_reward=405.20 +/- 489.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=173000, episode_reward=462.20 +/- 194.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=174000, episode_reward=380.20 +/- 230.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=175000, episode_reward=226.60 +/- 126.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=176000, episode_reward=121.80 +/- 255.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=177000, episode_reward=290.00 +/- 279.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=178000, episode_reward=63.20 +/- 208.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=179000, episode_reward=157.40 +/- 390.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=180000, episode_reward=-28.60 +/- 69.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=181000, episode_reward=346.60 +/- 209.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=182000, episode_reward=722.80 +/- 326.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=183000, episode_reward=87.80 +/- 254.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=184000, episode_reward=245.20 +/- 212.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=185000, episode_reward=283.60 +/- 59.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=186000, episode_reward=262.40 +/- 132.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=187000, episode_reward=454.80 +/- 207.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=188000, episode_reward=207.00 +/- 192.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=189000, episode_reward=370.80 +/- 177.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=190000, episode_reward=461.20 +/- 214.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=191000, episode_reward=436.20 +/- 273.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=192000, episode_reward=463.60 +/- 163.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=193000, episode_reward=389.40 +/- 214.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=194000, episode_reward=768.60 +/- 253.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=195000, episode_reward=484.00 +/- 395.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=196000, episode_reward=211.40 +/- 200.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=197000, episode_reward=619.80 +/- 193.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=198000, episode_reward=640.60 +/- 105.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=199000, episode_reward=690.00 +/- 158.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=200000, episode_reward=447.80 +/- 204.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=201000, episode_reward=670.60 +/- 166.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=202000, episode_reward=839.20 +/- 342.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=203000, episode_reward=602.40 +/- 139.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=204000, episode_reward=780.80 +/- 257.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=205000, episode_reward=934.80 +/- 337.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=206000, episode_reward=550.00 +/- 188.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=207000, episode_reward=498.60 +/- 282.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=208000, episode_reward=370.20 +/- 265.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=209000, episode_reward=760.60 +/- 454.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=210000, episode_reward=575.00 +/- 314.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=211000, episode_reward=768.80 +/- 420.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=212000, episode_reward=816.00 +/- 285.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=213000, episode_reward=686.60 +/- 414.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=214000, episode_reward=773.60 +/- 199.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=215000, episode_reward=852.80 +/- 257.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=216000, episode_reward=840.40 +/- 256.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=217000, episode_reward=511.40 +/- 264.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=218000, episode_reward=753.00 +/- 220.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=219000, episode_reward=618.00 +/- 376.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=220000, episode_reward=639.60 +/- 362.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=221000, episode_reward=673.60 +/- 227.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=222000, episode_reward=713.20 +/- 133.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=223000, episode_reward=778.20 +/- 169.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=224000, episode_reward=487.60 +/- 150.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=225000, episode_reward=779.80 +/- 311.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=226000, episode_reward=528.40 +/- 157.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=227000, episode_reward=729.40 +/- 612.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=228000, episode_reward=524.40 +/- 437.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=229000, episode_reward=914.80 +/- 397.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=230000, episode_reward=746.40 +/- 236.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=231000, episode_reward=602.40 +/- 184.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=232000, episode_reward=542.20 +/- 347.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=233000, episode_reward=615.80 +/- 314.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=234000, episode_reward=657.60 +/- 294.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=235000, episode_reward=559.20 +/- 224.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=236000, episode_reward=-619.20 +/- 2910.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=237000, episode_reward=1033.80 +/- 304.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=238000, episode_reward=813.60 +/- 137.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=239000, episode_reward=510.00 +/- 264.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=240000, episode_reward=953.80 +/- 168.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=241000, episode_reward=725.20 +/- 501.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=242000, episode_reward=696.00 +/- 342.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=243000, episode_reward=470.20 +/- 407.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=244000, episode_reward=1076.00 +/- 372.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=245000, episode_reward=999.80 +/- 172.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=246000, episode_reward=775.80 +/- 313.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=247000, episode_reward=657.60 +/- 183.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=248000, episode_reward=739.60 +/- 257.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=249000, episode_reward=927.80 +/- 320.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=250000, episode_reward=444.60 +/- 236.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=251000, episode_reward=-1951.40 +/- 5287.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=252000, episode_reward=430.60 +/- 302.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=253000, episode_reward=893.60 +/- 281.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=254000, episode_reward=609.40 +/- 319.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=255000, episode_reward=684.40 +/- 132.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=256000, episode_reward=707.80 +/- 351.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=257000, episode_reward=784.80 +/- 207.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=258000, episode_reward=758.60 +/- 293.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=259000, episode_reward=738.60 +/- 240.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=260000, episode_reward=764.80 +/- 350.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=261000, episode_reward=892.60 +/- 321.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=262000, episode_reward=798.20 +/- 218.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=263000, episode_reward=831.40 +/- 265.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=264000, episode_reward=810.20 +/- 205.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=265000, episode_reward=720.40 +/- 221.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=266000, episode_reward=980.80 +/- 252.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=267000, episode_reward=760.40 +/- 198.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=268000, episode_reward=829.60 +/- 235.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=269000, episode_reward=647.60 +/- 224.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=270000, episode_reward=841.60 +/- 212.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=271000, episode_reward=784.60 +/- 51.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=272000, episode_reward=606.20 +/- 338.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=273000, episode_reward=749.00 +/- 338.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=274000, episode_reward=716.60 +/- 357.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=275000, episode_reward=567.60 +/- 120.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=276000, episode_reward=984.60 +/- 154.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=277000, episode_reward=930.40 +/- 294.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=278000, episode_reward=603.40 +/- 360.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=279000, episode_reward=557.80 +/- 309.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=280000, episode_reward=742.80 +/- 401.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=281000, episode_reward=982.60 +/- 242.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=282000, episode_reward=706.20 +/- 300.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=283000, episode_reward=954.40 +/- 348.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=284000, episode_reward=831.00 +/- 308.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=285000, episode_reward=808.20 +/- 276.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=286000, episode_reward=970.20 +/- 306.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=287000, episode_reward=790.00 +/- 276.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=288000, episode_reward=577.20 +/- 426.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=289000, episode_reward=740.00 +/- 287.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=290000, episode_reward=749.40 +/- 296.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=291000, episode_reward=860.00 +/- 99.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=292000, episode_reward=676.60 +/- 144.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=293000, episode_reward=677.60 +/- 434.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=294000, episode_reward=808.20 +/- 310.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=295000, episode_reward=755.20 +/- 190.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=296000, episode_reward=1007.00 +/- 228.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=297000, episode_reward=627.00 +/- 314.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=298000, episode_reward=723.60 +/- 157.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=299000, episode_reward=494.00 +/- 369.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=300000, episode_reward=-873.00 +/- 3984.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=301000, episode_reward=655.40 +/- 187.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=302000, episode_reward=895.00 +/- 280.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=303000, episode_reward=985.20 +/- 232.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=304000, episode_reward=789.40 +/- 443.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=305000, episode_reward=1103.40 +/- 309.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=306000, episode_reward=929.80 +/- 266.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=307000, episode_reward=1155.80 +/- 338.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=308000, episode_reward=846.60 +/- 272.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=309000, episode_reward=789.20 +/- 586.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=310000, episode_reward=550.60 +/- 332.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=311000, episode_reward=942.60 +/- 189.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=312000, episode_reward=1040.60 +/- 147.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=313000, episode_reward=894.20 +/- 199.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=314000, episode_reward=964.80 +/- 350.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=315000, episode_reward=861.20 +/- 335.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=316000, episode_reward=871.60 +/- 218.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=317000, episode_reward=723.80 +/- 308.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=318000, episode_reward=545.00 +/- 283.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=319000, episode_reward=826.60 +/- 113.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=320000, episode_reward=998.60 +/- 207.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=321000, episode_reward=916.00 +/- 204.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=322000, episode_reward=904.00 +/- 338.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=323000, episode_reward=925.40 +/- 282.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=324000, episode_reward=1000.80 +/- 299.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=325000, episode_reward=739.00 +/- 153.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=326000, episode_reward=928.80 +/- 106.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=327000, episode_reward=838.00 +/- 308.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=328000, episode_reward=749.60 +/- 394.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=329000, episode_reward=1104.00 +/- 223.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=330000, episode_reward=760.40 +/- 244.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=331000, episode_reward=895.80 +/- 135.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=332000, episode_reward=971.20 +/- 252.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=333000, episode_reward=776.20 +/- 196.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=334000, episode_reward=1110.40 +/- 191.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=335000, episode_reward=904.20 +/- 225.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=336000, episode_reward=1210.40 +/- 167.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=337000, episode_reward=894.80 +/- 186.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=338000, episode_reward=1200.60 +/- 182.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=339000, episode_reward=891.20 +/- 231.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=340000, episode_reward=830.20 +/- 259.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=341000, episode_reward=1056.00 +/- 247.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=342000, episode_reward=726.60 +/- 222.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=343000, episode_reward=875.60 +/- 270.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=344000, episode_reward=583.20 +/- 421.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=345000, episode_reward=730.00 +/- 101.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=346000, episode_reward=791.20 +/- 250.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=347000, episode_reward=980.80 +/- 171.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=348000, episode_reward=924.60 +/- 154.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=349000, episode_reward=1019.20 +/- 308.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=350000, episode_reward=1044.60 +/- 341.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=351000, episode_reward=1094.20 +/- 193.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=352000, episode_reward=990.60 +/- 223.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=353000, episode_reward=909.40 +/- 255.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=354000, episode_reward=849.00 +/- 166.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=355000, episode_reward=779.80 +/- 266.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=356000, episode_reward=472.20 +/- 830.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=357000, episode_reward=809.00 +/- 217.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=358000, episode_reward=748.20 +/- 381.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=359000, episode_reward=977.60 +/- 150.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=360000, episode_reward=860.20 +/- 323.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=361000, episode_reward=840.60 +/- 139.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=362000, episode_reward=-2575.40 +/- 7147.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=363000, episode_reward=707.80 +/- 239.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=364000, episode_reward=760.80 +/- 157.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=365000, episode_reward=1022.60 +/- 225.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=366000, episode_reward=836.00 +/- 207.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=367000, episode_reward=679.40 +/- 202.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=368000, episode_reward=822.60 +/- 231.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=369000, episode_reward=866.40 +/- 489.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=370000, episode_reward=1065.20 +/- 227.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=371000, episode_reward=854.80 +/- 392.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=372000, episode_reward=772.80 +/- 133.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=373000, episode_reward=1087.40 +/- 189.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=374000, episode_reward=826.60 +/- 426.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=375000, episode_reward=1011.80 +/- 203.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=376000, episode_reward=793.00 +/- 254.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=377000, episode_reward=959.00 +/- 140.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=378000, episode_reward=1191.00 +/- 229.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=379000, episode_reward=979.00 +/- 244.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=380000, episode_reward=1177.80 +/- 253.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=381000, episode_reward=845.80 +/- 309.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=382000, episode_reward=1229.80 +/- 203.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=383000, episode_reward=830.20 +/- 185.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=384000, episode_reward=711.00 +/- 285.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=385000, episode_reward=904.00 +/- 215.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=386000, episode_reward=1163.60 +/- 100.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=387000, episode_reward=811.40 +/- 139.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=388000, episode_reward=1063.80 +/- 294.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=389000, episode_reward=1071.00 +/- 257.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=390000, episode_reward=790.20 +/- 214.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=391000, episode_reward=879.60 +/- 274.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=392000, episode_reward=801.80 +/- 223.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=393000, episode_reward=702.80 +/- 212.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=394000, episode_reward=985.40 +/- 337.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=395000, episode_reward=686.60 +/- 261.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=396000, episode_reward=1007.00 +/- 247.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=397000, episode_reward=-227.80 +/- 2246.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=398000, episode_reward=1324.60 +/- 322.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=399000, episode_reward=761.40 +/- 170.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=400000, episode_reward=879.40 +/- 283.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=401000, episode_reward=902.60 +/- 273.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=402000, episode_reward=858.00 +/- 331.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=403000, episode_reward=1047.60 +/- 179.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=404000, episode_reward=887.80 +/- 334.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=405000, episode_reward=817.20 +/- 269.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=406000, episode_reward=897.00 +/- 286.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=407000, episode_reward=798.40 +/- 358.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=408000, episode_reward=805.20 +/- 390.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=409000, episode_reward=864.00 +/- 345.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=410000, episode_reward=939.20 +/- 225.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=411000, episode_reward=1031.20 +/- 248.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=412000, episode_reward=1240.60 +/- 364.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=413000, episode_reward=1091.20 +/- 274.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=414000, episode_reward=870.40 +/- 311.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=415000, episode_reward=898.40 +/- 279.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=416000, episode_reward=1013.20 +/- 317.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=417000, episode_reward=1267.80 +/- 272.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=418000, episode_reward=823.40 +/- 336.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=419000, episode_reward=823.40 +/- 415.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=420000, episode_reward=798.00 +/- 191.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=421000, episode_reward=1063.00 +/- 85.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=422000, episode_reward=962.20 +/- 344.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=423000, episode_reward=949.00 +/- 158.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=424000, episode_reward=1099.60 +/- 221.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=425000, episode_reward=1036.60 +/- 152.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=426000, episode_reward=931.40 +/- 415.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=427000, episode_reward=305.60 +/- 1692.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=428000, episode_reward=1143.80 +/- 317.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=429000, episode_reward=849.00 +/- 351.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=430000, episode_reward=1028.80 +/- 183.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=431000, episode_reward=901.40 +/- 220.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=432000, episode_reward=1108.60 +/- 272.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=433000, episode_reward=1078.40 +/- 222.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=434000, episode_reward=-2079.80 +/- 6039.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=435000, episode_reward=859.20 +/- 198.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=436000, episode_reward=1000.40 +/- 219.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=437000, episode_reward=962.80 +/- 167.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=438000, episode_reward=899.60 +/- 182.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=439000, episode_reward=1035.60 +/- 139.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=440000, episode_reward=795.60 +/- 112.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=441000, episode_reward=948.20 +/- 348.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=442000, episode_reward=1200.20 +/- 336.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=443000, episode_reward=650.60 +/- 161.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=444000, episode_reward=995.20 +/- 136.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=445000, episode_reward=1246.80 +/- 272.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=446000, episode_reward=1088.20 +/- 48.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=447000, episode_reward=1087.80 +/- 230.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=448000, episode_reward=1047.80 +/- 274.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=449000, episode_reward=1088.80 +/- 352.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=450000, episode_reward=873.80 +/- 315.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=451000, episode_reward=1001.00 +/- 222.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=452000, episode_reward=852.40 +/- 188.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=453000, episode_reward=1250.80 +/- 341.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=454000, episode_reward=974.20 +/- 293.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=455000, episode_reward=916.80 +/- 431.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=456000, episode_reward=961.60 +/- 182.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=457000, episode_reward=785.00 +/- 95.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=458000, episode_reward=817.80 +/- 251.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=459000, episode_reward=782.20 +/- 252.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=460000, episode_reward=1053.80 +/- 478.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=461000, episode_reward=887.20 +/- 178.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=462000, episode_reward=1164.00 +/- 113.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=463000, episode_reward=1066.00 +/- 144.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=464000, episode_reward=1003.40 +/- 143.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=465000, episode_reward=727.80 +/- 291.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=466000, episode_reward=1043.60 +/- 343.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=467000, episode_reward=902.20 +/- 270.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=468000, episode_reward=1117.40 +/- 529.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=469000, episode_reward=842.40 +/- 406.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=470000, episode_reward=1082.80 +/- 372.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=471000, episode_reward=1112.40 +/- 176.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=472000, episode_reward=967.20 +/- 260.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=473000, episode_reward=1049.00 +/- 172.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=474000, episode_reward=871.80 +/- 145.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=475000, episode_reward=1131.20 +/- 245.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=476000, episode_reward=1095.80 +/- 191.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=477000, episode_reward=1299.60 +/- 336.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=478000, episode_reward=1243.40 +/- 187.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=479000, episode_reward=1135.40 +/- 159.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=480000, episode_reward=1181.60 +/- 315.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=481000, episode_reward=1143.00 +/- 262.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=482000, episode_reward=1296.80 +/- 269.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=483000, episode_reward=965.80 +/- 188.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=484000, episode_reward=1146.20 +/- 472.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=485000, episode_reward=979.20 +/- 143.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=486000, episode_reward=-1394.20 +/- 4977.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=487000, episode_reward=978.40 +/- 394.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=488000, episode_reward=1446.40 +/- 302.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=489000, episode_reward=1321.40 +/- 248.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=490000, episode_reward=1235.00 +/- 267.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=491000, episode_reward=1059.40 +/- 278.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=492000, episode_reward=1199.20 +/- 355.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=493000, episode_reward=1061.80 +/- 185.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=494000, episode_reward=1022.40 +/- 300.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=495000, episode_reward=840.20 +/- 326.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=496000, episode_reward=1048.60 +/- 247.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=497000, episode_reward=1247.40 +/- 318.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=498000, episode_reward=1349.20 +/- 307.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=499000, episode_reward=1171.60 +/- 260.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=500000, episode_reward=1204.80 +/- 349.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Gamma = 0.8, Learning Rate = 0.003, Activation Function = Tanh, Neurons = 32, Process noise = 0.1, Measurement noise = 0.0\n",
      "Eval num_timesteps=1000, episode_reward=-2040.60 +/- 128.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=-2080.80 +/- 123.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=3000, episode_reward=-1958.60 +/- 96.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4000, episode_reward=-1964.80 +/- 78.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=5000, episode_reward=-2016.80 +/- 144.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=6000, episode_reward=-1954.00 +/- 87.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=7000, episode_reward=-1940.40 +/- 84.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=8000, episode_reward=-1941.20 +/- 105.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=9000, episode_reward=-1929.20 +/- 74.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=10000, episode_reward=-2048.60 +/- 153.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=11000, episode_reward=-1947.20 +/- 55.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=12000, episode_reward=-2026.80 +/- 75.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=13000, episode_reward=-1982.20 +/- 148.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=14000, episode_reward=-2037.00 +/- 104.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=15000, episode_reward=-1916.60 +/- 85.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=16000, episode_reward=-1974.20 +/- 102.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=17000, episode_reward=-2120.40 +/- 51.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=18000, episode_reward=-2049.00 +/- 154.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=19000, episode_reward=-2100.40 +/- 143.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=20000, episode_reward=-2047.40 +/- 83.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=21000, episode_reward=-2026.00 +/- 193.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=22000, episode_reward=-1990.00 +/- 57.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=23000, episode_reward=-2051.00 +/- 107.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=24000, episode_reward=-2111.40 +/- 125.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=25000, episode_reward=-1999.20 +/- 174.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=26000, episode_reward=-2004.20 +/- 117.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=27000, episode_reward=-2047.40 +/- 187.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=28000, episode_reward=-2092.20 +/- 97.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=29000, episode_reward=-2002.00 +/- 94.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=30000, episode_reward=-1991.60 +/- 80.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=31000, episode_reward=-2000.00 +/- 102.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=32000, episode_reward=-1964.80 +/- 120.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=33000, episode_reward=-2009.00 +/- 79.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=34000, episode_reward=-2083.60 +/- 69.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=35000, episode_reward=-2078.20 +/- 124.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=36000, episode_reward=-2003.80 +/- 87.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=37000, episode_reward=-2123.20 +/- 121.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=38000, episode_reward=-2062.80 +/- 92.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=39000, episode_reward=-2002.40 +/- 135.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=40000, episode_reward=-1976.80 +/- 103.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=41000, episode_reward=-1995.40 +/- 133.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=42000, episode_reward=-1948.00 +/- 88.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=43000, episode_reward=-2058.80 +/- 150.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=44000, episode_reward=-1916.80 +/- 113.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=45000, episode_reward=-1973.60 +/- 46.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=46000, episode_reward=-2109.80 +/- 120.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=47000, episode_reward=-1935.40 +/- 138.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=48000, episode_reward=-1999.80 +/- 44.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=49000, episode_reward=-2053.80 +/- 76.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=50000, episode_reward=-2084.00 +/- 138.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=51000, episode_reward=-182.40 +/- 183.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=52000, episode_reward=-590.60 +/- 238.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=53000, episode_reward=139.20 +/- 254.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=54000, episode_reward=-132.40 +/- 265.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=55000, episode_reward=-596.40 +/- 203.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=56000, episode_reward=56.60 +/- 169.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=57000, episode_reward=-665.20 +/- 116.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=58000, episode_reward=-168.20 +/- 298.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=59000, episode_reward=-138.40 +/- 273.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=60000, episode_reward=-152.00 +/- 169.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=61000, episode_reward=-650.60 +/- 381.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=62000, episode_reward=62.00 +/- 352.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=63000, episode_reward=100.20 +/- 212.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=64000, episode_reward=-71.20 +/- 216.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=65000, episode_reward=-40.60 +/- 166.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=66000, episode_reward=104.60 +/- 292.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=67000, episode_reward=89.40 +/- 166.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=68000, episode_reward=207.40 +/- 204.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=69000, episode_reward=262.80 +/- 310.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=70000, episode_reward=103.60 +/- 232.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=71000, episode_reward=-146.80 +/- 86.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=72000, episode_reward=141.00 +/- 203.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=73000, episode_reward=103.20 +/- 383.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=74000, episode_reward=309.40 +/- 240.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=75000, episode_reward=93.20 +/- 176.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=76000, episode_reward=365.40 +/- 143.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=77000, episode_reward=341.40 +/- 90.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=78000, episode_reward=274.00 +/- 139.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=79000, episode_reward=295.80 +/- 203.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=80000, episode_reward=244.40 +/- 189.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=81000, episode_reward=127.20 +/- 203.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=82000, episode_reward=288.60 +/- 154.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=83000, episode_reward=335.00 +/- 172.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=84000, episode_reward=267.60 +/- 105.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=85000, episode_reward=243.60 +/- 54.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=86000, episode_reward=149.80 +/- 171.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=87000, episode_reward=124.40 +/- 275.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=88000, episode_reward=271.80 +/- 164.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=89000, episode_reward=724.00 +/- 349.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=90000, episode_reward=371.80 +/- 452.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=91000, episode_reward=740.00 +/- 265.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=92000, episode_reward=1059.80 +/- 172.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=93000, episode_reward=-2388.20 +/- 6134.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=94000, episode_reward=-2519.00 +/- 6816.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=95000, episode_reward=923.60 +/- 277.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=96000, episode_reward=790.80 +/- 268.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=97000, episode_reward=1079.40 +/- 268.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=98000, episode_reward=539.00 +/- 446.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=99000, episode_reward=711.60 +/- 296.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=100000, episode_reward=666.40 +/- 206.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=101000, episode_reward=816.20 +/- 266.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=102000, episode_reward=641.40 +/- 131.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=103000, episode_reward=1022.40 +/- 142.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=104000, episode_reward=909.20 +/- 326.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=105000, episode_reward=833.60 +/- 146.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=106000, episode_reward=437.80 +/- 248.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=107000, episode_reward=621.80 +/- 145.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=108000, episode_reward=742.00 +/- 179.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=109000, episode_reward=777.60 +/- 198.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=110000, episode_reward=558.20 +/- 305.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=111000, episode_reward=786.20 +/- 148.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=112000, episode_reward=880.40 +/- 208.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=113000, episode_reward=703.40 +/- 602.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=114000, episode_reward=530.80 +/- 146.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=115000, episode_reward=908.80 +/- 189.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=116000, episode_reward=227.40 +/- 270.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=117000, episode_reward=997.20 +/- 185.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=118000, episode_reward=620.00 +/- 245.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=119000, episode_reward=610.20 +/- 164.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=120000, episode_reward=943.20 +/- 276.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=121000, episode_reward=605.60 +/- 312.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=122000, episode_reward=752.00 +/- 135.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=123000, episode_reward=590.40 +/- 270.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=124000, episode_reward=982.00 +/- 209.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=125000, episode_reward=745.20 +/- 312.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=126000, episode_reward=1024.20 +/- 219.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=127000, episode_reward=538.20 +/- 219.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=128000, episode_reward=740.80 +/- 297.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=129000, episode_reward=1054.40 +/- 328.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=130000, episode_reward=650.60 +/- 330.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=131000, episode_reward=761.20 +/- 163.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=132000, episode_reward=-2494.80 +/- 6928.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=133000, episode_reward=1069.80 +/- 267.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=134000, episode_reward=661.80 +/- 195.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=135000, episode_reward=-595.00 +/- 3168.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=136000, episode_reward=695.80 +/- 394.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=137000, episode_reward=877.80 +/- 76.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=138000, episode_reward=1055.60 +/- 261.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=139000, episode_reward=862.00 +/- 180.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=140000, episode_reward=886.80 +/- 129.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=141000, episode_reward=706.60 +/- 185.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=142000, episode_reward=832.20 +/- 83.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=143000, episode_reward=959.20 +/- 154.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=144000, episode_reward=680.40 +/- 467.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=145000, episode_reward=753.00 +/- 301.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=146000, episode_reward=492.40 +/- 287.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=147000, episode_reward=843.00 +/- 228.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=148000, episode_reward=816.60 +/- 297.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=149000, episode_reward=890.60 +/- 263.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=150000, episode_reward=730.20 +/- 138.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=151000, episode_reward=993.20 +/- 156.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=152000, episode_reward=1007.60 +/- 218.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=153000, episode_reward=612.60 +/- 196.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=154000, episode_reward=633.60 +/- 322.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=155000, episode_reward=1093.40 +/- 337.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=156000, episode_reward=708.00 +/- 262.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=157000, episode_reward=637.20 +/- 179.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=158000, episode_reward=641.00 +/- 274.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=159000, episode_reward=844.20 +/- 248.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=160000, episode_reward=778.80 +/- 352.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=161000, episode_reward=980.20 +/- 211.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=162000, episode_reward=729.00 +/- 135.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=163000, episode_reward=613.80 +/- 402.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=164000, episode_reward=957.40 +/- 217.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=165000, episode_reward=527.00 +/- 132.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=166000, episode_reward=978.60 +/- 197.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=167000, episode_reward=807.20 +/- 246.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=168000, episode_reward=765.40 +/- 239.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=169000, episode_reward=807.60 +/- 195.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=170000, episode_reward=676.00 +/- 320.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=171000, episode_reward=822.20 +/- 154.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=172000, episode_reward=945.80 +/- 172.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=173000, episode_reward=772.40 +/- 183.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=174000, episode_reward=797.80 +/- 154.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=175000, episode_reward=689.60 +/- 193.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=176000, episode_reward=692.20 +/- 356.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=177000, episode_reward=983.00 +/- 275.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=178000, episode_reward=1103.40 +/- 204.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=179000, episode_reward=839.20 +/- 265.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=180000, episode_reward=697.00 +/- 209.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=181000, episode_reward=909.00 +/- 407.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=182000, episode_reward=724.40 +/- 321.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=183000, episode_reward=610.80 +/- 181.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=184000, episode_reward=979.20 +/- 474.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=185000, episode_reward=884.20 +/- 41.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=186000, episode_reward=994.60 +/- 283.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=187000, episode_reward=991.40 +/- 237.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=188000, episode_reward=943.80 +/- 208.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=189000, episode_reward=732.60 +/- 427.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=190000, episode_reward=775.40 +/- 232.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=191000, episode_reward=787.80 +/- 318.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=192000, episode_reward=636.00 +/- 214.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=193000, episode_reward=789.80 +/- 250.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=194000, episode_reward=1008.80 +/- 276.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=195000, episode_reward=1081.60 +/- 312.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=196000, episode_reward=973.20 +/- 406.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=197000, episode_reward=832.60 +/- 377.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=198000, episode_reward=929.80 +/- 411.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=199000, episode_reward=1001.00 +/- 335.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=200000, episode_reward=874.60 +/- 165.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=201000, episode_reward=899.80 +/- 304.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=202000, episode_reward=963.60 +/- 260.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=203000, episode_reward=1066.80 +/- 186.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=204000, episode_reward=688.80 +/- 400.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=205000, episode_reward=964.20 +/- 238.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=206000, episode_reward=750.00 +/- 218.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=207000, episode_reward=1006.40 +/- 390.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=208000, episode_reward=1064.20 +/- 177.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=209000, episode_reward=1004.20 +/- 195.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=210000, episode_reward=1020.80 +/- 101.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=211000, episode_reward=1015.00 +/- 331.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=212000, episode_reward=900.00 +/- 161.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=213000, episode_reward=912.80 +/- 168.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=214000, episode_reward=1062.60 +/- 307.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=215000, episode_reward=814.80 +/- 372.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=216000, episode_reward=1078.00 +/- 185.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=217000, episode_reward=704.80 +/- 158.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=218000, episode_reward=840.20 +/- 314.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=219000, episode_reward=1077.80 +/- 121.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=220000, episode_reward=1052.80 +/- 254.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=221000, episode_reward=808.60 +/- 452.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=222000, episode_reward=1025.00 +/- 290.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=223000, episode_reward=1135.40 +/- 340.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=224000, episode_reward=939.60 +/- 335.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=225000, episode_reward=870.40 +/- 310.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=226000, episode_reward=702.40 +/- 318.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=227000, episode_reward=914.60 +/- 391.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=228000, episode_reward=938.00 +/- 238.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=229000, episode_reward=1065.20 +/- 161.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=230000, episode_reward=869.80 +/- 149.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=231000, episode_reward=649.60 +/- 398.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=232000, episode_reward=1145.20 +/- 260.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=233000, episode_reward=1024.20 +/- 146.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=234000, episode_reward=1155.00 +/- 182.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=235000, episode_reward=1096.60 +/- 218.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=236000, episode_reward=909.60 +/- 317.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=237000, episode_reward=1018.00 +/- 413.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=238000, episode_reward=1123.80 +/- 256.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=239000, episode_reward=774.60 +/- 265.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=240000, episode_reward=826.60 +/- 386.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=241000, episode_reward=736.60 +/- 126.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=242000, episode_reward=1056.80 +/- 136.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=243000, episode_reward=975.20 +/- 236.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=244000, episode_reward=945.60 +/- 445.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=245000, episode_reward=767.40 +/- 292.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=246000, episode_reward=1075.60 +/- 225.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=247000, episode_reward=965.40 +/- 280.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=248000, episode_reward=884.80 +/- 214.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=249000, episode_reward=940.40 +/- 293.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=250000, episode_reward=905.40 +/- 198.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=251000, episode_reward=925.40 +/- 187.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=252000, episode_reward=872.60 +/- 379.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=253000, episode_reward=1088.60 +/- 302.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=254000, episode_reward=1185.40 +/- 159.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=255000, episode_reward=976.80 +/- 386.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=256000, episode_reward=687.00 +/- 458.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=257000, episode_reward=1008.80 +/- 172.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=258000, episode_reward=897.80 +/- 264.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=259000, episode_reward=709.20 +/- 426.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=260000, episode_reward=1019.80 +/- 351.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=261000, episode_reward=1506.60 +/- 195.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=262000, episode_reward=1023.20 +/- 202.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=263000, episode_reward=1080.80 +/- 267.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=264000, episode_reward=841.20 +/- 212.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=265000, episode_reward=1173.80 +/- 177.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=266000, episode_reward=1170.20 +/- 324.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=267000, episode_reward=1040.60 +/- 392.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=268000, episode_reward=1129.80 +/- 364.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=269000, episode_reward=1058.40 +/- 304.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=270000, episode_reward=1374.80 +/- 236.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=271000, episode_reward=848.20 +/- 230.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=272000, episode_reward=959.80 +/- 414.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=273000, episode_reward=1052.60 +/- 343.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=274000, episode_reward=1140.40 +/- 203.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=275000, episode_reward=1093.60 +/- 406.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=276000, episode_reward=909.40 +/- 416.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=277000, episode_reward=1175.00 +/- 362.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=278000, episode_reward=1091.40 +/- 243.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=279000, episode_reward=1112.60 +/- 229.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=280000, episode_reward=1023.00 +/- 174.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=281000, episode_reward=978.40 +/- 343.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=282000, episode_reward=976.60 +/- 340.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=283000, episode_reward=953.80 +/- 439.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=284000, episode_reward=1263.40 +/- 289.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=285000, episode_reward=1141.00 +/- 356.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=286000, episode_reward=901.40 +/- 301.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=287000, episode_reward=979.60 +/- 244.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=288000, episode_reward=927.00 +/- 269.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=289000, episode_reward=1155.20 +/- 338.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=290000, episode_reward=793.40 +/- 355.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=291000, episode_reward=1118.20 +/- 403.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=292000, episode_reward=859.20 +/- 418.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=293000, episode_reward=1113.80 +/- 128.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=294000, episode_reward=1079.20 +/- 428.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=295000, episode_reward=1475.80 +/- 254.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=296000, episode_reward=980.20 +/- 384.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=297000, episode_reward=1021.80 +/- 353.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=298000, episode_reward=1187.40 +/- 214.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=299000, episode_reward=1241.60 +/- 171.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=300000, episode_reward=928.20 +/- 301.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=301000, episode_reward=1137.40 +/- 246.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=302000, episode_reward=760.80 +/- 152.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=303000, episode_reward=1045.80 +/- 187.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=304000, episode_reward=867.00 +/- 402.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=305000, episode_reward=1163.20 +/- 535.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=306000, episode_reward=1138.20 +/- 159.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=307000, episode_reward=1186.00 +/- 292.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=308000, episode_reward=1019.60 +/- 181.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=309000, episode_reward=1141.20 +/- 251.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=310000, episode_reward=999.00 +/- 544.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=311000, episode_reward=898.60 +/- 108.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=312000, episode_reward=978.20 +/- 135.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=313000, episode_reward=1014.60 +/- 340.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=314000, episode_reward=1175.20 +/- 230.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=315000, episode_reward=832.80 +/- 308.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=316000, episode_reward=1375.40 +/- 172.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=317000, episode_reward=983.40 +/- 98.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=318000, episode_reward=907.80 +/- 336.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=319000, episode_reward=996.60 +/- 33.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=320000, episode_reward=734.60 +/- 146.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=321000, episode_reward=923.40 +/- 270.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=322000, episode_reward=1008.60 +/- 378.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=323000, episode_reward=1023.40 +/- 170.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=324000, episode_reward=952.40 +/- 224.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=325000, episode_reward=1091.40 +/- 177.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=326000, episode_reward=1240.60 +/- 278.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=327000, episode_reward=1206.00 +/- 204.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=328000, episode_reward=955.00 +/- 519.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=329000, episode_reward=900.60 +/- 316.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=330000, episode_reward=894.40 +/- 306.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=331000, episode_reward=970.80 +/- 162.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=332000, episode_reward=1094.20 +/- 299.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=333000, episode_reward=1098.00 +/- 351.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=334000, episode_reward=969.60 +/- 361.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=335000, episode_reward=946.80 +/- 532.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=336000, episode_reward=914.40 +/- 523.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=337000, episode_reward=1036.00 +/- 342.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=338000, episode_reward=1015.80 +/- 337.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=339000, episode_reward=922.20 +/- 253.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=340000, episode_reward=970.60 +/- 417.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=341000, episode_reward=1063.80 +/- 392.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=342000, episode_reward=981.40 +/- 142.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=343000, episode_reward=1141.60 +/- 292.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=344000, episode_reward=1196.00 +/- 377.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=345000, episode_reward=1214.20 +/- 329.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=346000, episode_reward=1092.80 +/- 197.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=347000, episode_reward=1006.60 +/- 312.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=348000, episode_reward=1108.60 +/- 237.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=349000, episode_reward=791.20 +/- 419.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=350000, episode_reward=1094.80 +/- 176.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=351000, episode_reward=762.80 +/- 283.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=352000, episode_reward=838.60 +/- 386.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=353000, episode_reward=1134.60 +/- 188.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=354000, episode_reward=1256.60 +/- 236.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=355000, episode_reward=1025.60 +/- 182.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=356000, episode_reward=873.40 +/- 466.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=357000, episode_reward=1167.40 +/- 281.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=358000, episode_reward=966.60 +/- 267.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=359000, episode_reward=1317.60 +/- 204.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=360000, episode_reward=639.00 +/- 234.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=361000, episode_reward=1258.20 +/- 311.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=362000, episode_reward=1080.80 +/- 190.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=363000, episode_reward=1267.20 +/- 149.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=364000, episode_reward=1285.80 +/- 194.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=365000, episode_reward=1009.80 +/- 99.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=366000, episode_reward=1139.40 +/- 337.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=367000, episode_reward=1309.20 +/- 264.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=368000, episode_reward=657.80 +/- 340.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=369000, episode_reward=1073.40 +/- 276.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=370000, episode_reward=1238.40 +/- 316.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=371000, episode_reward=849.00 +/- 265.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=372000, episode_reward=926.80 +/- 386.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=373000, episode_reward=1117.80 +/- 373.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=374000, episode_reward=1137.60 +/- 147.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=375000, episode_reward=1105.20 +/- 269.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=376000, episode_reward=1249.20 +/- 358.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=377000, episode_reward=1172.00 +/- 384.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=378000, episode_reward=1322.20 +/- 223.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=379000, episode_reward=1082.60 +/- 483.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=380000, episode_reward=887.20 +/- 258.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=381000, episode_reward=882.20 +/- 345.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=382000, episode_reward=1222.20 +/- 158.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=383000, episode_reward=1043.00 +/- 269.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=384000, episode_reward=885.40 +/- 293.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=385000, episode_reward=976.80 +/- 341.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=386000, episode_reward=967.80 +/- 248.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=387000, episode_reward=1175.60 +/- 253.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=388000, episode_reward=1263.20 +/- 276.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=389000, episode_reward=1044.80 +/- 355.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=390000, episode_reward=816.40 +/- 400.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=391000, episode_reward=720.20 +/- 320.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=392000, episode_reward=909.20 +/- 458.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=393000, episode_reward=1017.40 +/- 292.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=394000, episode_reward=896.00 +/- 248.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=395000, episode_reward=978.60 +/- 267.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=396000, episode_reward=971.80 +/- 166.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=397000, episode_reward=1158.20 +/- 380.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=398000, episode_reward=1226.20 +/- 173.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=399000, episode_reward=1240.40 +/- 189.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=400000, episode_reward=999.60 +/- 479.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=401000, episode_reward=896.20 +/- 231.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=402000, episode_reward=1149.20 +/- 203.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=403000, episode_reward=976.80 +/- 244.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=404000, episode_reward=840.60 +/- 293.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=405000, episode_reward=1069.80 +/- 518.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=406000, episode_reward=1012.20 +/- 303.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=407000, episode_reward=703.60 +/- 353.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=408000, episode_reward=729.60 +/- 291.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=409000, episode_reward=990.60 +/- 203.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=410000, episode_reward=1086.00 +/- 345.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=411000, episode_reward=1104.20 +/- 497.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=412000, episode_reward=1164.00 +/- 193.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=413000, episode_reward=1154.60 +/- 344.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=414000, episode_reward=640.80 +/- 225.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=415000, episode_reward=1177.20 +/- 189.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=416000, episode_reward=1124.20 +/- 227.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=417000, episode_reward=995.20 +/- 158.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=418000, episode_reward=1218.00 +/- 315.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=419000, episode_reward=776.60 +/- 145.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=420000, episode_reward=1212.80 +/- 292.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=421000, episode_reward=685.40 +/- 263.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=422000, episode_reward=802.20 +/- 188.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=423000, episode_reward=995.00 +/- 237.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=424000, episode_reward=1148.40 +/- 352.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=425000, episode_reward=848.60 +/- 291.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=426000, episode_reward=1193.80 +/- 287.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=427000, episode_reward=1122.60 +/- 172.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=428000, episode_reward=1183.60 +/- 223.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=429000, episode_reward=999.80 +/- 220.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=430000, episode_reward=741.40 +/- 289.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=431000, episode_reward=1433.80 +/- 168.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=432000, episode_reward=1046.60 +/- 209.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=433000, episode_reward=1106.20 +/- 425.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=434000, episode_reward=1291.80 +/- 337.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=435000, episode_reward=849.60 +/- 160.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=436000, episode_reward=796.60 +/- 178.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=437000, episode_reward=975.60 +/- 523.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=438000, episode_reward=1097.40 +/- 335.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=439000, episode_reward=988.60 +/- 235.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=440000, episode_reward=918.00 +/- 345.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=441000, episode_reward=1065.60 +/- 309.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=442000, episode_reward=680.40 +/- 232.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=443000, episode_reward=1225.40 +/- 356.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=444000, episode_reward=925.20 +/- 270.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=445000, episode_reward=952.20 +/- 208.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=446000, episode_reward=832.20 +/- 280.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=447000, episode_reward=1030.60 +/- 113.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=448000, episode_reward=940.40 +/- 269.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=449000, episode_reward=1011.60 +/- 217.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=450000, episode_reward=1063.40 +/- 106.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=451000, episode_reward=804.60 +/- 235.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=452000, episode_reward=975.20 +/- 272.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=453000, episode_reward=1177.60 +/- 156.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=454000, episode_reward=1345.20 +/- 274.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=455000, episode_reward=1247.60 +/- 336.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=456000, episode_reward=1144.60 +/- 126.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=457000, episode_reward=1181.20 +/- 237.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=458000, episode_reward=799.20 +/- 338.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=459000, episode_reward=728.40 +/- 355.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=460000, episode_reward=1322.80 +/- 290.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=461000, episode_reward=1148.80 +/- 203.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=462000, episode_reward=1299.60 +/- 371.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=463000, episode_reward=1181.20 +/- 317.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=464000, episode_reward=1086.20 +/- 150.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=465000, episode_reward=867.80 +/- 200.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=466000, episode_reward=752.60 +/- 284.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=467000, episode_reward=955.60 +/- 228.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=468000, episode_reward=960.40 +/- 349.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=469000, episode_reward=1181.20 +/- 156.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=470000, episode_reward=1154.40 +/- 230.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=471000, episode_reward=1247.00 +/- 238.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=472000, episode_reward=815.20 +/- 139.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=473000, episode_reward=894.20 +/- 191.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=474000, episode_reward=1359.20 +/- 146.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=475000, episode_reward=859.40 +/- 221.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=476000, episode_reward=878.20 +/- 358.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=477000, episode_reward=1269.40 +/- 185.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=478000, episode_reward=882.40 +/- 206.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=479000, episode_reward=903.60 +/- 357.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=480000, episode_reward=1076.20 +/- 316.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=481000, episode_reward=1156.80 +/- 156.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=482000, episode_reward=1139.80 +/- 264.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=483000, episode_reward=1200.60 +/- 334.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=484000, episode_reward=1235.20 +/- 190.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=485000, episode_reward=999.40 +/- 313.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=486000, episode_reward=1095.00 +/- 345.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=487000, episode_reward=772.60 +/- 249.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=488000, episode_reward=970.60 +/- 275.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=489000, episode_reward=1156.20 +/- 273.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=490000, episode_reward=1261.40 +/- 186.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=491000, episode_reward=721.60 +/- 363.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=492000, episode_reward=817.80 +/- 145.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=493000, episode_reward=1308.80 +/- 235.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=494000, episode_reward=950.60 +/- 277.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=495000, episode_reward=1043.00 +/- 231.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=496000, episode_reward=978.40 +/- 217.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=497000, episode_reward=890.20 +/- 107.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=498000, episode_reward=1125.00 +/- 205.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=499000, episode_reward=891.80 +/- 248.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=500000, episode_reward=1117.40 +/- 382.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Gamma = 0.8, Learning Rate = 0.003, Activation Function = Tanh, Neurons = 64, Process noise = 0.1, Measurement noise = 0.0\n",
      "Eval num_timesteps=1000, episode_reward=-1872.60 +/- 211.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=-2062.60 +/- 94.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=3000, episode_reward=-2006.60 +/- 83.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=4000, episode_reward=-2020.20 +/- 98.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=5000, episode_reward=-1969.20 +/- 127.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=6000, episode_reward=-2140.60 +/- 195.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=7000, episode_reward=-1971.60 +/- 233.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=8000, episode_reward=-2000.00 +/- 218.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=9000, episode_reward=-1980.80 +/- 133.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=10000, episode_reward=-1901.20 +/- 206.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=11000, episode_reward=-1839.40 +/- 254.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=12000, episode_reward=-1957.20 +/- 234.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=13000, episode_reward=-2054.20 +/- 114.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=14000, episode_reward=-2047.00 +/- 158.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=15000, episode_reward=-1970.80 +/- 69.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=16000, episode_reward=-1978.40 +/- 189.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=17000, episode_reward=-1990.00 +/- 164.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=18000, episode_reward=-1911.80 +/- 189.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=19000, episode_reward=-2078.60 +/- 207.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=20000, episode_reward=-2081.60 +/- 172.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=21000, episode_reward=-2088.40 +/- 119.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=22000, episode_reward=-2044.80 +/- 105.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=23000, episode_reward=-1998.00 +/- 152.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=24000, episode_reward=-2217.20 +/- 128.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=25000, episode_reward=-2059.40 +/- 164.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=26000, episode_reward=-2040.20 +/- 154.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=27000, episode_reward=-2002.60 +/- 221.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=28000, episode_reward=-2036.00 +/- 142.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=29000, episode_reward=-1942.00 +/- 267.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=30000, episode_reward=-1970.80 +/- 171.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=31000, episode_reward=-1780.80 +/- 112.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=32000, episode_reward=-2017.20 +/- 202.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=33000, episode_reward=-2123.80 +/- 57.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=34000, episode_reward=-2142.20 +/- 91.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=35000, episode_reward=-1993.00 +/- 298.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=36000, episode_reward=-2011.80 +/- 90.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=37000, episode_reward=-2006.60 +/- 168.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=38000, episode_reward=-2037.60 +/- 99.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=39000, episode_reward=-2226.00 +/- 55.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=40000, episode_reward=-1987.20 +/- 143.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=41000, episode_reward=-1972.20 +/- 201.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=42000, episode_reward=-2015.40 +/- 179.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=43000, episode_reward=-2128.00 +/- 185.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=44000, episode_reward=-1889.40 +/- 240.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=45000, episode_reward=-1941.00 +/- 207.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=46000, episode_reward=-2064.40 +/- 184.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=47000, episode_reward=-1911.40 +/- 204.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=48000, episode_reward=-2153.20 +/- 174.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=49000, episode_reward=-1950.80 +/- 86.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=50000, episode_reward=-1993.20 +/- 164.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=51000, episode_reward=-7300.60 +/- 2816.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=52000, episode_reward=385.20 +/- 104.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=53000, episode_reward=-225.80 +/- 297.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=54000, episode_reward=418.60 +/- 186.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=55000, episode_reward=391.00 +/- 105.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=56000, episode_reward=309.20 +/- 157.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=57000, episode_reward=532.40 +/- 190.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=58000, episode_reward=-20.60 +/- 281.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=59000, episode_reward=366.60 +/- 133.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=60000, episode_reward=120.40 +/- 182.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=61000, episode_reward=-11.20 +/- 179.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=62000, episode_reward=-455.40 +/- 329.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=63000, episode_reward=-298.20 +/- 440.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=64000, episode_reward=90.80 +/- 248.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=65000, episode_reward=188.20 +/- 120.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=66000, episode_reward=296.20 +/- 297.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=67000, episode_reward=156.60 +/- 121.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=68000, episode_reward=417.40 +/- 154.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=69000, episode_reward=221.20 +/- 215.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=70000, episode_reward=250.80 +/- 158.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=71000, episode_reward=392.20 +/- 423.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=72000, episode_reward=696.20 +/- 140.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=73000, episode_reward=623.80 +/- 293.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=74000, episode_reward=308.20 +/- 227.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=75000, episode_reward=846.80 +/- 446.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=76000, episode_reward=579.20 +/- 229.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=77000, episode_reward=679.60 +/- 234.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=78000, episode_reward=411.60 +/- 213.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=79000, episode_reward=712.80 +/- 181.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=80000, episode_reward=394.40 +/- 362.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=81000, episode_reward=428.20 +/- 133.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=82000, episode_reward=328.20 +/- 144.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=83000, episode_reward=369.60 +/- 226.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=84000, episode_reward=715.80 +/- 257.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=85000, episode_reward=841.40 +/- 157.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=86000, episode_reward=971.60 +/- 349.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=87000, episode_reward=552.20 +/- 342.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=88000, episode_reward=644.20 +/- 264.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=89000, episode_reward=619.20 +/- 170.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=90000, episode_reward=373.80 +/- 249.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=91000, episode_reward=465.00 +/- 70.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=92000, episode_reward=433.00 +/- 225.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=93000, episode_reward=472.60 +/- 141.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=94000, episode_reward=396.80 +/- 155.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=95000, episode_reward=447.80 +/- 110.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=96000, episode_reward=515.00 +/- 171.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=97000, episode_reward=917.80 +/- 236.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=98000, episode_reward=438.60 +/- 112.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=99000, episode_reward=445.80 +/- 245.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=100000, episode_reward=382.20 +/- 100.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=101000, episode_reward=673.00 +/- 194.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=102000, episode_reward=540.40 +/- 444.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=103000, episode_reward=403.60 +/- 148.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=104000, episode_reward=572.80 +/- 189.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=105000, episode_reward=371.40 +/- 160.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=106000, episode_reward=489.20 +/- 180.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=107000, episode_reward=804.20 +/- 236.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=108000, episode_reward=308.40 +/- 428.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=109000, episode_reward=403.80 +/- 166.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=110000, episode_reward=372.40 +/- 79.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=111000, episode_reward=428.80 +/- 56.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=112000, episode_reward=608.00 +/- 97.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=113000, episode_reward=691.80 +/- 253.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=114000, episode_reward=824.60 +/- 259.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=115000, episode_reward=778.60 +/- 221.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=116000, episode_reward=857.00 +/- 354.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=117000, episode_reward=720.00 +/- 287.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=118000, episode_reward=844.20 +/- 356.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=119000, episode_reward=-4352.20 +/- 6847.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=120000, episode_reward=869.60 +/- 459.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=121000, episode_reward=962.40 +/- 418.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=122000, episode_reward=764.00 +/- 217.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=123000, episode_reward=-129.00 +/- 2268.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=124000, episode_reward=744.80 +/- 257.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=125000, episode_reward=838.40 +/- 307.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=126000, episode_reward=728.00 +/- 291.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=127000, episode_reward=970.40 +/- 191.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=128000, episode_reward=896.40 +/- 166.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=129000, episode_reward=-1193.80 +/- 3918.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=130000, episode_reward=611.00 +/- 170.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=131000, episode_reward=730.80 +/- 274.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=132000, episode_reward=627.00 +/- 383.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=133000, episode_reward=851.20 +/- 309.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=134000, episode_reward=832.20 +/- 190.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=135000, episode_reward=991.60 +/- 230.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=136000, episode_reward=635.40 +/- 399.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=137000, episode_reward=691.40 +/- 223.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=138000, episode_reward=636.60 +/- 164.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=139000, episode_reward=688.80 +/- 146.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=140000, episode_reward=1124.20 +/- 266.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=141000, episode_reward=567.00 +/- 323.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=142000, episode_reward=746.80 +/- 318.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=143000, episode_reward=910.40 +/- 240.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=144000, episode_reward=1101.20 +/- 247.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=145000, episode_reward=800.00 +/- 211.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=146000, episode_reward=860.60 +/- 228.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=147000, episode_reward=1002.20 +/- 164.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=148000, episode_reward=681.00 +/- 414.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=149000, episode_reward=908.40 +/- 359.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=150000, episode_reward=781.00 +/- 311.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=151000, episode_reward=1037.40 +/- 213.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=152000, episode_reward=828.20 +/- 247.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=153000, episode_reward=952.20 +/- 465.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=154000, episode_reward=683.60 +/- 179.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=155000, episode_reward=981.20 +/- 167.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=156000, episode_reward=893.20 +/- 236.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=157000, episode_reward=1112.40 +/- 206.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=158000, episode_reward=816.80 +/- 207.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=159000, episode_reward=896.40 +/- 239.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=160000, episode_reward=662.00 +/- 267.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=161000, episode_reward=852.40 +/- 413.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=162000, episode_reward=918.60 +/- 380.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=163000, episode_reward=750.00 +/- 256.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=164000, episode_reward=1113.40 +/- 342.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=165000, episode_reward=826.20 +/- 256.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=166000, episode_reward=772.20 +/- 384.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=167000, episode_reward=970.80 +/- 125.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=168000, episode_reward=759.80 +/- 296.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=169000, episode_reward=940.00 +/- 275.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=170000, episode_reward=807.20 +/- 180.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=171000, episode_reward=1098.80 +/- 146.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=172000, episode_reward=1121.40 +/- 192.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=173000, episode_reward=683.20 +/- 110.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=174000, episode_reward=893.80 +/- 343.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=175000, episode_reward=542.00 +/- 272.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=176000, episode_reward=875.60 +/- 233.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=177000, episode_reward=1202.80 +/- 170.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=178000, episode_reward=464.40 +/- 191.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=179000, episode_reward=923.20 +/- 159.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=180000, episode_reward=855.60 +/- 167.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=181000, episode_reward=1101.80 +/- 253.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=182000, episode_reward=524.00 +/- 182.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=183000, episode_reward=1280.40 +/- 264.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=184000, episode_reward=630.60 +/- 414.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=185000, episode_reward=845.40 +/- 206.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=186000, episode_reward=504.60 +/- 277.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=187000, episode_reward=776.80 +/- 128.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=188000, episode_reward=642.80 +/- 230.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=189000, episode_reward=814.40 +/- 122.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=190000, episode_reward=970.80 +/- 301.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=191000, episode_reward=657.00 +/- 228.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=192000, episode_reward=1155.80 +/- 274.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=193000, episode_reward=717.20 +/- 211.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=194000, episode_reward=471.80 +/- 121.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=195000, episode_reward=945.20 +/- 430.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=196000, episode_reward=1022.80 +/- 442.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=197000, episode_reward=910.20 +/- 268.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=198000, episode_reward=907.00 +/- 245.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=199000, episode_reward=999.60 +/- 183.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=200000, episode_reward=870.20 +/- 252.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=201000, episode_reward=1045.40 +/- 428.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=202000, episode_reward=597.00 +/- 203.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=203000, episode_reward=1051.80 +/- 196.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=204000, episode_reward=1030.80 +/- 110.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=205000, episode_reward=851.20 +/- 136.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=206000, episode_reward=954.20 +/- 168.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=207000, episode_reward=1104.60 +/- 543.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=208000, episode_reward=1035.80 +/- 266.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=209000, episode_reward=1047.40 +/- 350.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=210000, episode_reward=841.60 +/- 288.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=211000, episode_reward=672.60 +/- 198.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=212000, episode_reward=915.00 +/- 219.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=213000, episode_reward=903.20 +/- 282.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=214000, episode_reward=794.60 +/- 117.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=215000, episode_reward=1067.40 +/- 317.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=216000, episode_reward=498.40 +/- 173.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=217000, episode_reward=1013.80 +/- 234.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=218000, episode_reward=944.80 +/- 339.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=219000, episode_reward=940.40 +/- 359.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=220000, episode_reward=1020.00 +/- 184.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=221000, episode_reward=1019.00 +/- 204.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=222000, episode_reward=672.80 +/- 190.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=223000, episode_reward=1033.80 +/- 416.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=224000, episode_reward=951.60 +/- 304.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=225000, episode_reward=1021.80 +/- 161.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=226000, episode_reward=848.20 +/- 284.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=227000, episode_reward=716.20 +/- 313.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=228000, episode_reward=959.80 +/- 364.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=229000, episode_reward=1152.60 +/- 250.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=230000, episode_reward=991.00 +/- 212.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=231000, episode_reward=917.40 +/- 283.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=232000, episode_reward=742.20 +/- 361.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=233000, episode_reward=918.00 +/- 462.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=234000, episode_reward=479.80 +/- 227.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=235000, episode_reward=1138.40 +/- 177.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=236000, episode_reward=882.20 +/- 370.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=237000, episode_reward=893.20 +/- 253.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=238000, episode_reward=886.60 +/- 315.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=239000, episode_reward=891.00 +/- 288.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=240000, episode_reward=1245.60 +/- 370.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=241000, episode_reward=924.20 +/- 292.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=242000, episode_reward=804.80 +/- 201.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=243000, episode_reward=982.60 +/- 387.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=244000, episode_reward=951.20 +/- 156.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=245000, episode_reward=978.80 +/- 343.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=246000, episode_reward=1075.40 +/- 241.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=247000, episode_reward=831.60 +/- 118.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=248000, episode_reward=1063.40 +/- 388.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=249000, episode_reward=862.20 +/- 258.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=250000, episode_reward=570.40 +/- 144.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=251000, episode_reward=924.00 +/- 156.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=252000, episode_reward=1167.00 +/- 290.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=253000, episode_reward=883.40 +/- 143.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=254000, episode_reward=636.00 +/- 184.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=255000, episode_reward=685.20 +/- 242.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=256000, episode_reward=699.00 +/- 175.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=257000, episode_reward=909.60 +/- 359.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=258000, episode_reward=778.60 +/- 191.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=259000, episode_reward=1136.40 +/- 510.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=260000, episode_reward=832.40 +/- 166.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=261000, episode_reward=1070.20 +/- 146.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=262000, episode_reward=916.20 +/- 307.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=263000, episode_reward=1171.80 +/- 209.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=264000, episode_reward=1046.80 +/- 391.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=265000, episode_reward=704.20 +/- 239.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=266000, episode_reward=800.20 +/- 131.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=267000, episode_reward=611.00 +/- 187.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=268000, episode_reward=894.60 +/- 467.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=269000, episode_reward=911.40 +/- 445.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=270000, episode_reward=1040.00 +/- 193.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=271000, episode_reward=1103.20 +/- 170.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=272000, episode_reward=951.40 +/- 328.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=273000, episode_reward=902.00 +/- 207.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=274000, episode_reward=677.80 +/- 255.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=275000, episode_reward=607.20 +/- 272.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=276000, episode_reward=1073.80 +/- 363.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=277000, episode_reward=959.00 +/- 204.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=278000, episode_reward=1030.20 +/- 307.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=279000, episode_reward=1338.60 +/- 295.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=280000, episode_reward=941.20 +/- 327.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=281000, episode_reward=911.20 +/- 344.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=282000, episode_reward=841.40 +/- 299.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=283000, episode_reward=728.20 +/- 231.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=284000, episode_reward=729.80 +/- 492.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=285000, episode_reward=765.00 +/- 311.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=286000, episode_reward=1033.20 +/- 180.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=287000, episode_reward=888.80 +/- 430.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=288000, episode_reward=1060.40 +/- 272.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=289000, episode_reward=649.60 +/- 398.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=290000, episode_reward=967.80 +/- 523.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=291000, episode_reward=673.20 +/- 169.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=292000, episode_reward=923.60 +/- 442.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=293000, episode_reward=853.20 +/- 315.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=294000, episode_reward=793.60 +/- 145.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=295000, episode_reward=815.20 +/- 273.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=296000, episode_reward=883.20 +/- 432.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=297000, episode_reward=1114.00 +/- 222.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=298000, episode_reward=783.80 +/- 196.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=299000, episode_reward=901.20 +/- 363.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=300000, episode_reward=1089.20 +/- 326.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=301000, episode_reward=722.20 +/- 246.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=302000, episode_reward=1125.00 +/- 297.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=303000, episode_reward=1121.00 +/- 111.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=304000, episode_reward=925.60 +/- 308.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=305000, episode_reward=922.60 +/- 227.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=306000, episode_reward=1130.00 +/- 274.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=307000, episode_reward=641.20 +/- 192.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=308000, episode_reward=940.80 +/- 323.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=309000, episode_reward=669.80 +/- 282.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=310000, episode_reward=1029.20 +/- 199.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=311000, episode_reward=975.80 +/- 213.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=312000, episode_reward=1156.20 +/- 135.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=313000, episode_reward=1004.20 +/- 324.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=314000, episode_reward=938.40 +/- 370.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=315000, episode_reward=993.60 +/- 288.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=316000, episode_reward=886.40 +/- 336.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=317000, episode_reward=1186.00 +/- 425.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=318000, episode_reward=965.60 +/- 210.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=319000, episode_reward=1016.20 +/- 307.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=320000, episode_reward=1174.40 +/- 447.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=321000, episode_reward=953.60 +/- 217.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=322000, episode_reward=1166.20 +/- 264.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=323000, episode_reward=967.40 +/- 328.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=324000, episode_reward=900.60 +/- 367.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=325000, episode_reward=845.40 +/- 188.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=326000, episode_reward=1046.40 +/- 331.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=327000, episode_reward=1026.40 +/- 383.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=328000, episode_reward=794.40 +/- 509.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=329000, episode_reward=796.20 +/- 258.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=330000, episode_reward=1135.40 +/- 297.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=331000, episode_reward=864.60 +/- 330.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=332000, episode_reward=1195.00 +/- 303.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=333000, episode_reward=664.60 +/- 317.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=334000, episode_reward=1226.60 +/- 326.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=335000, episode_reward=1158.80 +/- 253.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=336000, episode_reward=1089.00 +/- 446.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=337000, episode_reward=1131.60 +/- 225.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=338000, episode_reward=861.20 +/- 124.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=339000, episode_reward=970.20 +/- 341.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=340000, episode_reward=995.00 +/- 317.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=341000, episode_reward=1030.60 +/- 414.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=342000, episode_reward=1169.60 +/- 343.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=343000, episode_reward=688.80 +/- 355.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=344000, episode_reward=1181.60 +/- 152.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=345000, episode_reward=569.00 +/- 206.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=346000, episode_reward=1115.40 +/- 181.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=347000, episode_reward=975.00 +/- 409.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=348000, episode_reward=863.60 +/- 425.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=349000, episode_reward=1166.40 +/- 205.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=350000, episode_reward=942.60 +/- 403.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=351000, episode_reward=1022.80 +/- 448.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=352000, episode_reward=1185.20 +/- 208.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=353000, episode_reward=1021.40 +/- 478.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=354000, episode_reward=894.40 +/- 392.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=355000, episode_reward=1061.60 +/- 360.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=356000, episode_reward=1164.40 +/- 215.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=357000, episode_reward=1082.80 +/- 38.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=358000, episode_reward=843.40 +/- 221.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=359000, episode_reward=1114.20 +/- 139.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=360000, episode_reward=949.00 +/- 299.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=361000, episode_reward=953.40 +/- 180.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=362000, episode_reward=697.80 +/- 269.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=363000, episode_reward=990.00 +/- 382.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=364000, episode_reward=1097.80 +/- 249.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=365000, episode_reward=1188.20 +/- 386.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=366000, episode_reward=1010.00 +/- 342.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=367000, episode_reward=948.20 +/- 257.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=368000, episode_reward=910.20 +/- 318.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=369000, episode_reward=842.80 +/- 164.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=370000, episode_reward=1559.80 +/- 166.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=371000, episode_reward=887.60 +/- 123.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=372000, episode_reward=1070.00 +/- 217.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=373000, episode_reward=962.80 +/- 429.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=374000, episode_reward=1133.20 +/- 375.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=375000, episode_reward=775.00 +/- 245.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=376000, episode_reward=729.60 +/- 395.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=377000, episode_reward=861.00 +/- 361.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=378000, episode_reward=773.60 +/- 403.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=379000, episode_reward=957.60 +/- 445.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=380000, episode_reward=948.60 +/- 244.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=381000, episode_reward=578.80 +/- 300.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=382000, episode_reward=993.00 +/- 315.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=383000, episode_reward=1026.20 +/- 239.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=384000, episode_reward=990.00 +/- 255.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=385000, episode_reward=1184.20 +/- 291.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=386000, episode_reward=1124.00 +/- 202.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=387000, episode_reward=1140.60 +/- 312.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=388000, episode_reward=998.60 +/- 192.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=389000, episode_reward=937.00 +/- 334.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=390000, episode_reward=1016.00 +/- 212.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=391000, episode_reward=952.80 +/- 274.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=392000, episode_reward=1336.60 +/- 333.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=393000, episode_reward=890.80 +/- 244.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=394000, episode_reward=1016.60 +/- 338.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=395000, episode_reward=956.40 +/- 350.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=396000, episode_reward=1132.80 +/- 172.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=397000, episode_reward=1038.60 +/- 134.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=398000, episode_reward=1031.20 +/- 232.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=399000, episode_reward=797.40 +/- 153.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=400000, episode_reward=921.60 +/- 234.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=401000, episode_reward=1369.80 +/- 235.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=402000, episode_reward=933.40 +/- 240.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=403000, episode_reward=478.60 +/- 232.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=404000, episode_reward=1034.40 +/- 474.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=405000, episode_reward=899.00 +/- 154.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=406000, episode_reward=828.00 +/- 392.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=407000, episode_reward=670.00 +/- 472.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=408000, episode_reward=1041.20 +/- 382.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=409000, episode_reward=893.80 +/- 96.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=410000, episode_reward=1027.80 +/- 243.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=411000, episode_reward=969.60 +/- 323.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=412000, episode_reward=1009.00 +/- 125.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=413000, episode_reward=1335.00 +/- 201.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=414000, episode_reward=948.60 +/- 271.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=415000, episode_reward=1183.40 +/- 252.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=416000, episode_reward=871.80 +/- 178.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=417000, episode_reward=1126.20 +/- 249.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=418000, episode_reward=849.40 +/- 289.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=419000, episode_reward=807.20 +/- 264.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=420000, episode_reward=1073.80 +/- 227.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=421000, episode_reward=939.00 +/- 338.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=422000, episode_reward=1110.40 +/- 250.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=423000, episode_reward=1113.20 +/- 396.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=424000, episode_reward=726.40 +/- 284.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=425000, episode_reward=842.00 +/- 218.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=426000, episode_reward=900.40 +/- 247.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=427000, episode_reward=1056.20 +/- 141.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=428000, episode_reward=984.20 +/- 352.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=429000, episode_reward=814.60 +/- 198.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=430000, episode_reward=925.80 +/- 194.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=431000, episode_reward=865.60 +/- 212.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=432000, episode_reward=1018.40 +/- 484.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=433000, episode_reward=1090.60 +/- 400.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=434000, episode_reward=899.00 +/- 246.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=435000, episode_reward=807.60 +/- 330.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=436000, episode_reward=1020.00 +/- 182.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=437000, episode_reward=1176.00 +/- 323.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=438000, episode_reward=1141.60 +/- 157.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=439000, episode_reward=986.60 +/- 115.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=440000, episode_reward=856.60 +/- 182.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=441000, episode_reward=1207.00 +/- 353.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=442000, episode_reward=731.60 +/- 279.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=443000, episode_reward=916.80 +/- 440.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=444000, episode_reward=814.80 +/- 280.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=445000, episode_reward=1117.00 +/- 276.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=446000, episode_reward=1399.80 +/- 390.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=447000, episode_reward=870.40 +/- 335.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=448000, episode_reward=969.00 +/- 287.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=449000, episode_reward=1002.40 +/- 593.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=450000, episode_reward=686.40 +/- 232.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=451000, episode_reward=819.60 +/- 276.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=452000, episode_reward=1161.20 +/- 312.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=453000, episode_reward=897.60 +/- 436.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=454000, episode_reward=846.00 +/- 251.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=455000, episode_reward=826.80 +/- 161.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=456000, episode_reward=1212.80 +/- 205.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=457000, episode_reward=933.00 +/- 288.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=458000, episode_reward=1124.20 +/- 330.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=459000, episode_reward=767.20 +/- 383.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=460000, episode_reward=897.00 +/- 368.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=461000, episode_reward=753.40 +/- 314.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=462000, episode_reward=868.40 +/- 186.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=463000, episode_reward=915.60 +/- 406.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=464000, episode_reward=864.60 +/- 453.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=465000, episode_reward=1170.40 +/- 245.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=466000, episode_reward=863.40 +/- 160.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=467000, episode_reward=929.00 +/- 249.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=468000, episode_reward=934.00 +/- 489.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=469000, episode_reward=864.80 +/- 228.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=470000, episode_reward=863.60 +/- 370.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=471000, episode_reward=1059.80 +/- 194.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=472000, episode_reward=801.60 +/- 174.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=473000, episode_reward=939.40 +/- 387.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=474000, episode_reward=1107.40 +/- 304.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=475000, episode_reward=1204.60 +/- 266.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=476000, episode_reward=809.40 +/- 370.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=477000, episode_reward=998.00 +/- 236.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=478000, episode_reward=859.40 +/- 275.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=479000, episode_reward=862.60 +/- 197.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=480000, episode_reward=918.60 +/- 363.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=481000, episode_reward=951.00 +/- 226.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=482000, episode_reward=907.20 +/- 301.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=483000, episode_reward=975.40 +/- 288.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=484000, episode_reward=1109.40 +/- 288.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=485000, episode_reward=644.20 +/- 257.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=486000, episode_reward=992.20 +/- 269.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=487000, episode_reward=762.80 +/- 366.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=488000, episode_reward=776.60 +/- 178.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=489000, episode_reward=840.00 +/- 199.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=490000, episode_reward=969.40 +/- 200.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=491000, episode_reward=781.60 +/- 326.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=492000, episode_reward=563.40 +/- 186.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=493000, episode_reward=995.80 +/- 177.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=494000, episode_reward=868.60 +/- 220.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=495000, episode_reward=1066.00 +/- 310.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=496000, episode_reward=573.20 +/- 313.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=497000, episode_reward=931.20 +/- 236.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=498000, episode_reward=1009.40 +/- 257.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=499000, episode_reward=1166.80 +/- 178.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=500000, episode_reward=772.00 +/- 397.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Gamma = 0.8, Learning Rate = 0.003, Activation Function = ReLU, Neurons = 8, Process noise = 0.1, Measurement noise = 0.0\n",
      "Eval num_timesteps=1000, episode_reward=-1481.00 +/- 112.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=-1478.00 +/- 29.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3000, episode_reward=-1395.00 +/- 66.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4000, episode_reward=-1538.00 +/- 105.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=5000, episode_reward=-1555.00 +/- 85.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=6000, episode_reward=-1547.00 +/- 83.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=7000, episode_reward=-1499.00 +/- 57.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=8000, episode_reward=-1542.00 +/- 44.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=9000, episode_reward=-1510.00 +/- 62.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=10000, episode_reward=-1467.00 +/- 95.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=11000, episode_reward=-1524.00 +/- 70.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=12000, episode_reward=-1530.00 +/- 75.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=13000, episode_reward=-1528.00 +/- 54.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=14000, episode_reward=-1492.00 +/- 109.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=15000, episode_reward=-1426.00 +/- 89.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=16000, episode_reward=-1523.00 +/- 58.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=17000, episode_reward=-1540.00 +/- 96.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=18000, episode_reward=-1495.00 +/- 70.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=19000, episode_reward=-1540.00 +/- 77.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=20000, episode_reward=-1453.00 +/- 49.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=21000, episode_reward=-1545.00 +/- 98.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=22000, episode_reward=-1540.00 +/- 82.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=23000, episode_reward=-1534.00 +/- 60.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=24000, episode_reward=-1518.00 +/- 67.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=25000, episode_reward=-1472.00 +/- 66.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=26000, episode_reward=-1438.00 +/- 77.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=27000, episode_reward=-1517.00 +/- 82.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=28000, episode_reward=-1496.00 +/- 97.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=29000, episode_reward=-1547.00 +/- 117.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=30000, episode_reward=-1463.00 +/- 73.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=31000, episode_reward=-1506.00 +/- 61.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=32000, episode_reward=-1470.00 +/- 87.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=33000, episode_reward=-1540.00 +/- 50.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=34000, episode_reward=-1483.00 +/- 91.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=35000, episode_reward=-1476.00 +/- 86.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=36000, episode_reward=-1513.00 +/- 86.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=37000, episode_reward=-1565.00 +/- 60.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=38000, episode_reward=-1494.00 +/- 73.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=39000, episode_reward=-1479.00 +/- 35.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=40000, episode_reward=-1589.00 +/- 34.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=41000, episode_reward=-1543.00 +/- 104.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=42000, episode_reward=-1537.00 +/- 81.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=43000, episode_reward=-1486.00 +/- 79.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=44000, episode_reward=-1468.00 +/- 99.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=45000, episode_reward=-1473.00 +/- 89.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=46000, episode_reward=-1461.00 +/- 50.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=47000, episode_reward=-1468.00 +/- 103.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=48000, episode_reward=-1485.00 +/- 60.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=49000, episode_reward=-1533.00 +/- 77.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=50000, episode_reward=-1523.00 +/- 74.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=51000, episode_reward=-187.00 +/- 52.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=52000, episode_reward=487.20 +/- 163.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=53000, episode_reward=231.80 +/- 170.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=54000, episode_reward=318.00 +/- 254.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=55000, episode_reward=428.00 +/- 203.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=56000, episode_reward=444.80 +/- 175.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=57000, episode_reward=334.00 +/- 241.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=58000, episode_reward=439.20 +/- 226.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=59000, episode_reward=289.00 +/- 122.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=60000, episode_reward=345.40 +/- 69.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=61000, episode_reward=299.80 +/- 75.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=62000, episode_reward=90.40 +/- 138.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=63000, episode_reward=316.00 +/- 215.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=64000, episode_reward=450.20 +/- 103.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=65000, episode_reward=297.40 +/- 258.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=66000, episode_reward=372.20 +/- 118.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=67000, episode_reward=357.80 +/- 130.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=68000, episode_reward=214.20 +/- 139.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=69000, episode_reward=340.80 +/- 148.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=70000, episode_reward=328.80 +/- 224.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=71000, episode_reward=245.60 +/- 117.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=72000, episode_reward=343.40 +/- 120.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=73000, episode_reward=168.00 +/- 226.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=74000, episode_reward=418.20 +/- 228.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=75000, episode_reward=431.00 +/- 75.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=76000, episode_reward=362.00 +/- 172.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=77000, episode_reward=394.40 +/- 152.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=78000, episode_reward=411.40 +/- 170.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=79000, episode_reward=376.00 +/- 76.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=80000, episode_reward=332.40 +/- 179.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=81000, episode_reward=178.00 +/- 204.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=82000, episode_reward=428.20 +/- 88.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=83000, episode_reward=338.00 +/- 320.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=84000, episode_reward=291.40 +/- 71.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=85000, episode_reward=423.40 +/- 190.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=86000, episode_reward=285.40 +/- 195.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=87000, episode_reward=224.60 +/- 214.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=88000, episode_reward=300.40 +/- 60.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=89000, episode_reward=413.00 +/- 169.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=90000, episode_reward=201.80 +/- 148.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=91000, episode_reward=304.40 +/- 159.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=92000, episode_reward=353.60 +/- 310.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=93000, episode_reward=235.00 +/- 56.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=94000, episode_reward=367.20 +/- 95.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=95000, episode_reward=335.00 +/- 174.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=96000, episode_reward=371.40 +/- 274.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=97000, episode_reward=393.40 +/- 96.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=98000, episode_reward=410.60 +/- 106.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=99000, episode_reward=224.40 +/- 126.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=100000, episode_reward=417.20 +/- 238.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=101000, episode_reward=155.40 +/- 144.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=102000, episode_reward=507.60 +/- 131.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=103000, episode_reward=348.20 +/- 172.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=104000, episode_reward=417.20 +/- 229.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=105000, episode_reward=183.80 +/- 194.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=106000, episode_reward=395.80 +/- 92.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=107000, episode_reward=437.60 +/- 178.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=108000, episode_reward=387.80 +/- 255.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=109000, episode_reward=406.40 +/- 229.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=110000, episode_reward=349.80 +/- 178.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=111000, episode_reward=522.40 +/- 114.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=112000, episode_reward=318.60 +/- 124.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=113000, episode_reward=366.20 +/- 150.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=114000, episode_reward=424.40 +/- 183.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=115000, episode_reward=331.00 +/- 124.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=116000, episode_reward=248.00 +/- 83.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=117000, episode_reward=327.60 +/- 281.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=118000, episode_reward=238.80 +/- 250.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=119000, episode_reward=240.60 +/- 216.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=120000, episode_reward=484.80 +/- 253.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=121000, episode_reward=402.20 +/- 123.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=122000, episode_reward=345.60 +/- 134.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=123000, episode_reward=372.20 +/- 100.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=124000, episode_reward=618.00 +/- 231.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=125000, episode_reward=313.00 +/- 312.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=126000, episode_reward=283.20 +/- 109.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=127000, episode_reward=487.80 +/- 174.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=128000, episode_reward=265.40 +/- 175.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=129000, episode_reward=389.00 +/- 102.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=130000, episode_reward=450.20 +/- 165.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=131000, episode_reward=279.00 +/- 135.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=132000, episode_reward=596.80 +/- 221.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=133000, episode_reward=267.60 +/- 307.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=134000, episode_reward=164.60 +/- 189.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=135000, episode_reward=443.00 +/- 197.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=136000, episode_reward=386.80 +/- 250.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=137000, episode_reward=382.00 +/- 202.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=138000, episode_reward=426.60 +/- 99.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=139000, episode_reward=259.00 +/- 203.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=140000, episode_reward=415.20 +/- 180.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=141000, episode_reward=600.80 +/- 125.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=142000, episode_reward=393.40 +/- 146.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=143000, episode_reward=294.20 +/- 246.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=144000, episode_reward=324.60 +/- 253.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=145000, episode_reward=18.80 +/- 288.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=146000, episode_reward=224.20 +/- 89.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=147000, episode_reward=464.40 +/- 161.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=148000, episode_reward=345.40 +/- 27.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=149000, episode_reward=484.20 +/- 70.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=150000, episode_reward=284.60 +/- 227.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=151000, episode_reward=239.00 +/- 275.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=152000, episode_reward=383.40 +/- 148.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=153000, episode_reward=235.00 +/- 117.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=154000, episode_reward=542.20 +/- 235.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=155000, episode_reward=435.40 +/- 270.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=156000, episode_reward=516.80 +/- 127.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=157000, episode_reward=482.20 +/- 186.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=158000, episode_reward=428.60 +/- 196.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=159000, episode_reward=395.40 +/- 197.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=160000, episode_reward=354.60 +/- 191.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=161000, episode_reward=456.20 +/- 184.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=162000, episode_reward=307.20 +/- 191.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=163000, episode_reward=379.00 +/- 200.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=164000, episode_reward=319.00 +/- 163.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=165000, episode_reward=488.00 +/- 172.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=166000, episode_reward=336.40 +/- 170.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=167000, episode_reward=297.20 +/- 365.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=168000, episode_reward=245.00 +/- 213.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=169000, episode_reward=261.20 +/- 199.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=170000, episode_reward=502.40 +/- 180.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=171000, episode_reward=118.60 +/- 102.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=172000, episode_reward=382.80 +/- 292.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=173000, episode_reward=379.40 +/- 112.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=174000, episode_reward=434.40 +/- 245.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=175000, episode_reward=433.20 +/- 172.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=176000, episode_reward=359.60 +/- 145.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=177000, episode_reward=215.20 +/- 324.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=178000, episode_reward=262.20 +/- 315.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=179000, episode_reward=419.60 +/- 199.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=180000, episode_reward=335.80 +/- 106.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=181000, episode_reward=410.20 +/- 171.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=182000, episode_reward=330.60 +/- 169.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=183000, episode_reward=464.20 +/- 112.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=184000, episode_reward=374.00 +/- 163.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=185000, episode_reward=474.00 +/- 280.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=186000, episode_reward=373.40 +/- 205.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=187000, episode_reward=178.60 +/- 99.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=188000, episode_reward=455.40 +/- 153.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=189000, episode_reward=262.00 +/- 127.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=190000, episode_reward=197.40 +/- 127.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=191000, episode_reward=418.00 +/- 267.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=192000, episode_reward=407.00 +/- 188.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=193000, episode_reward=408.40 +/- 135.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=194000, episode_reward=470.40 +/- 192.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=195000, episode_reward=423.00 +/- 182.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=196000, episode_reward=232.60 +/- 283.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=197000, episode_reward=346.00 +/- 287.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=198000, episode_reward=416.00 +/- 259.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=199000, episode_reward=582.40 +/- 113.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=200000, episode_reward=348.60 +/- 282.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=201000, episode_reward=480.20 +/- 109.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=202000, episode_reward=405.00 +/- 132.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=203000, episode_reward=404.40 +/- 136.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=204000, episode_reward=189.60 +/- 199.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=205000, episode_reward=262.80 +/- 104.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=206000, episode_reward=594.20 +/- 113.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=207000, episode_reward=468.00 +/- 208.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=208000, episode_reward=358.80 +/- 126.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=209000, episode_reward=457.60 +/- 121.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=210000, episode_reward=186.60 +/- 197.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=211000, episode_reward=275.60 +/- 135.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=212000, episode_reward=433.20 +/- 56.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=213000, episode_reward=444.00 +/- 123.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=214000, episode_reward=513.00 +/- 128.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=215000, episode_reward=338.00 +/- 150.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=216000, episode_reward=430.40 +/- 299.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=217000, episode_reward=440.60 +/- 133.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=218000, episode_reward=545.20 +/- 136.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=219000, episode_reward=463.40 +/- 110.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=220000, episode_reward=305.00 +/- 172.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=221000, episode_reward=244.60 +/- 163.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=222000, episode_reward=419.60 +/- 68.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=223000, episode_reward=582.00 +/- 126.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=224000, episode_reward=365.20 +/- 217.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=225000, episode_reward=388.00 +/- 95.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=226000, episode_reward=276.80 +/- 123.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=227000, episode_reward=321.60 +/- 272.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=228000, episode_reward=471.80 +/- 97.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=229000, episode_reward=312.60 +/- 210.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=230000, episode_reward=420.00 +/- 216.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=231000, episode_reward=408.40 +/- 177.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=232000, episode_reward=478.40 +/- 180.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=233000, episode_reward=291.40 +/- 119.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=234000, episode_reward=357.40 +/- 133.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=235000, episode_reward=450.00 +/- 58.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=236000, episode_reward=431.20 +/- 147.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=237000, episode_reward=299.00 +/- 253.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=238000, episode_reward=303.80 +/- 235.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=239000, episode_reward=360.20 +/- 57.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=240000, episode_reward=466.20 +/- 121.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=241000, episode_reward=305.40 +/- 243.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=242000, episode_reward=223.20 +/- 144.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=243000, episode_reward=322.80 +/- 200.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=244000, episode_reward=408.00 +/- 69.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=245000, episode_reward=313.60 +/- 115.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=246000, episode_reward=379.80 +/- 163.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=247000, episode_reward=342.00 +/- 181.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=248000, episode_reward=397.60 +/- 343.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=249000, episode_reward=456.80 +/- 327.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=250000, episode_reward=301.00 +/- 71.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=251000, episode_reward=286.60 +/- 164.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=252000, episode_reward=456.60 +/- 169.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=253000, episode_reward=334.80 +/- 114.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=254000, episode_reward=346.20 +/- 125.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=255000, episode_reward=309.00 +/- 262.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=256000, episode_reward=347.40 +/- 106.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=257000, episode_reward=252.40 +/- 161.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=258000, episode_reward=283.20 +/- 194.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=259000, episode_reward=402.80 +/- 66.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=260000, episode_reward=385.60 +/- 308.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=261000, episode_reward=479.60 +/- 251.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=262000, episode_reward=298.80 +/- 252.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=263000, episode_reward=587.80 +/- 200.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=264000, episode_reward=204.40 +/- 203.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=265000, episode_reward=335.80 +/- 220.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=266000, episode_reward=371.00 +/- 241.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=267000, episode_reward=348.00 +/- 108.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=268000, episode_reward=429.40 +/- 211.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=269000, episode_reward=238.60 +/- 97.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=270000, episode_reward=421.20 +/- 155.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=271000, episode_reward=308.40 +/- 148.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=272000, episode_reward=208.00 +/- 132.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=273000, episode_reward=321.20 +/- 189.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=274000, episode_reward=431.40 +/- 134.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=275000, episode_reward=135.60 +/- 140.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=276000, episode_reward=403.40 +/- 173.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=277000, episode_reward=325.20 +/- 165.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=278000, episode_reward=255.40 +/- 212.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=279000, episode_reward=385.80 +/- 135.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=280000, episode_reward=367.80 +/- 171.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=281000, episode_reward=222.00 +/- 147.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=282000, episode_reward=312.00 +/- 170.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=283000, episode_reward=336.00 +/- 182.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=284000, episode_reward=344.00 +/- 150.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=285000, episode_reward=335.40 +/- 153.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=286000, episode_reward=430.00 +/- 87.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=287000, episode_reward=557.80 +/- 269.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=288000, episode_reward=297.40 +/- 95.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=289000, episode_reward=361.60 +/- 151.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=290000, episode_reward=484.60 +/- 115.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=291000, episode_reward=436.00 +/- 106.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=292000, episode_reward=338.00 +/- 161.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=293000, episode_reward=333.60 +/- 73.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=294000, episode_reward=301.20 +/- 305.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=295000, episode_reward=501.60 +/- 205.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=296000, episode_reward=328.60 +/- 193.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=297000, episode_reward=403.80 +/- 104.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=298000, episode_reward=260.20 +/- 156.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=299000, episode_reward=387.00 +/- 193.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=300000, episode_reward=326.40 +/- 321.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=301000, episode_reward=376.40 +/- 223.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=302000, episode_reward=258.80 +/- 121.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=303000, episode_reward=294.00 +/- 140.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=304000, episode_reward=425.40 +/- 59.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=305000, episode_reward=401.60 +/- 256.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=306000, episode_reward=457.80 +/- 267.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=307000, episode_reward=400.00 +/- 167.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=308000, episode_reward=352.20 +/- 151.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=309000, episode_reward=466.00 +/- 136.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=310000, episode_reward=464.40 +/- 225.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=311000, episode_reward=277.60 +/- 137.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=312000, episode_reward=330.80 +/- 266.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=313000, episode_reward=453.20 +/- 164.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=314000, episode_reward=283.60 +/- 272.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=315000, episode_reward=390.20 +/- 77.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=316000, episode_reward=376.40 +/- 245.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=317000, episode_reward=327.20 +/- 361.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=318000, episode_reward=460.60 +/- 203.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=319000, episode_reward=301.20 +/- 209.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=320000, episode_reward=415.80 +/- 96.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=321000, episode_reward=458.20 +/- 230.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=322000, episode_reward=322.40 +/- 216.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=323000, episode_reward=390.20 +/- 151.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=324000, episode_reward=319.00 +/- 161.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=325000, episode_reward=401.80 +/- 67.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=326000, episode_reward=372.40 +/- 214.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=327000, episode_reward=349.00 +/- 290.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=328000, episode_reward=487.40 +/- 129.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=329000, episode_reward=293.20 +/- 268.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=330000, episode_reward=378.60 +/- 220.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=331000, episode_reward=482.20 +/- 160.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=332000, episode_reward=480.20 +/- 113.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=333000, episode_reward=417.40 +/- 155.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=334000, episode_reward=515.60 +/- 327.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=335000, episode_reward=353.20 +/- 197.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=336000, episode_reward=355.60 +/- 155.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=337000, episode_reward=273.20 +/- 275.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=338000, episode_reward=357.60 +/- 113.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=339000, episode_reward=359.40 +/- 124.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=340000, episode_reward=348.80 +/- 184.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=341000, episode_reward=348.80 +/- 103.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=342000, episode_reward=222.60 +/- 121.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=343000, episode_reward=267.60 +/- 124.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=344000, episode_reward=410.20 +/- 160.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=345000, episode_reward=229.20 +/- 136.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=346000, episode_reward=276.00 +/- 142.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=347000, episode_reward=464.00 +/- 114.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=348000, episode_reward=601.40 +/- 191.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=349000, episode_reward=571.00 +/- 172.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=350000, episode_reward=310.60 +/- 100.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=351000, episode_reward=454.20 +/- 186.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=352000, episode_reward=485.80 +/- 239.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=353000, episode_reward=374.80 +/- 320.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=354000, episode_reward=261.80 +/- 147.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=355000, episode_reward=347.00 +/- 114.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=356000, episode_reward=438.00 +/- 161.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=357000, episode_reward=303.20 +/- 294.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=358000, episode_reward=464.60 +/- 110.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=359000, episode_reward=439.20 +/- 250.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=360000, episode_reward=345.00 +/- 173.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=361000, episode_reward=316.60 +/- 164.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=362000, episode_reward=285.20 +/- 98.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=363000, episode_reward=439.20 +/- 123.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=364000, episode_reward=276.00 +/- 144.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=365000, episode_reward=450.80 +/- 113.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=366000, episode_reward=356.80 +/- 180.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=367000, episode_reward=597.20 +/- 220.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=368000, episode_reward=442.00 +/- 125.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=369000, episode_reward=384.60 +/- 160.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=370000, episode_reward=323.60 +/- 227.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=371000, episode_reward=335.40 +/- 189.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=372000, episode_reward=286.00 +/- 173.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=373000, episode_reward=484.60 +/- 134.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=374000, episode_reward=437.40 +/- 229.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=375000, episode_reward=329.40 +/- 137.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=376000, episode_reward=368.20 +/- 284.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=377000, episode_reward=463.20 +/- 172.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=378000, episode_reward=270.40 +/- 142.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=379000, episode_reward=398.60 +/- 162.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=380000, episode_reward=443.20 +/- 81.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=381000, episode_reward=177.60 +/- 70.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=382000, episode_reward=286.00 +/- 148.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=383000, episode_reward=337.40 +/- 222.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=384000, episode_reward=477.40 +/- 83.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=385000, episode_reward=446.20 +/- 162.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=386000, episode_reward=277.40 +/- 104.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=387000, episode_reward=133.60 +/- 103.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=388000, episode_reward=340.40 +/- 121.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=389000, episode_reward=383.00 +/- 107.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=390000, episode_reward=411.20 +/- 103.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=391000, episode_reward=314.60 +/- 206.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=392000, episode_reward=510.80 +/- 178.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=393000, episode_reward=382.80 +/- 101.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=394000, episode_reward=501.40 +/- 190.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=395000, episode_reward=238.40 +/- 251.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=396000, episode_reward=392.80 +/- 130.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=397000, episode_reward=387.80 +/- 186.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=398000, episode_reward=358.20 +/- 173.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=399000, episode_reward=473.00 +/- 134.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=400000, episode_reward=359.40 +/- 149.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=401000, episode_reward=432.80 +/- 92.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=402000, episode_reward=323.80 +/- 197.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=403000, episode_reward=353.80 +/- 246.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=404000, episode_reward=327.40 +/- 126.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=405000, episode_reward=295.40 +/- 328.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=406000, episode_reward=387.80 +/- 156.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=407000, episode_reward=560.80 +/- 204.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=408000, episode_reward=275.60 +/- 231.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=409000, episode_reward=455.40 +/- 145.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=410000, episode_reward=355.00 +/- 83.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=411000, episode_reward=425.40 +/- 164.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=412000, episode_reward=466.60 +/- 158.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=413000, episode_reward=524.80 +/- 161.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=414000, episode_reward=388.00 +/- 279.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=415000, episode_reward=257.80 +/- 153.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=416000, episode_reward=491.00 +/- 127.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=417000, episode_reward=313.00 +/- 148.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=418000, episode_reward=483.00 +/- 152.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=419000, episode_reward=320.00 +/- 185.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=420000, episode_reward=668.60 +/- 24.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=421000, episode_reward=341.40 +/- 197.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=422000, episode_reward=293.00 +/- 87.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=423000, episode_reward=359.40 +/- 127.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=424000, episode_reward=418.80 +/- 217.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=425000, episode_reward=336.40 +/- 165.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=426000, episode_reward=421.00 +/- 218.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=427000, episode_reward=216.20 +/- 71.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=428000, episode_reward=289.40 +/- 145.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=429000, episode_reward=281.40 +/- 158.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=430000, episode_reward=481.00 +/- 224.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=431000, episode_reward=391.20 +/- 233.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=432000, episode_reward=455.80 +/- 171.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=433000, episode_reward=449.80 +/- 258.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=434000, episode_reward=452.00 +/- 167.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=435000, episode_reward=266.20 +/- 271.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=436000, episode_reward=407.40 +/- 188.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=437000, episode_reward=310.60 +/- 101.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=438000, episode_reward=303.00 +/- 150.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=439000, episode_reward=442.80 +/- 177.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=440000, episode_reward=324.40 +/- 174.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=441000, episode_reward=461.40 +/- 70.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=442000, episode_reward=353.60 +/- 187.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=443000, episode_reward=314.80 +/- 159.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=444000, episode_reward=342.80 +/- 238.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=445000, episode_reward=270.00 +/- 237.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=446000, episode_reward=303.40 +/- 140.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=447000, episode_reward=412.00 +/- 172.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=448000, episode_reward=483.00 +/- 240.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=449000, episode_reward=416.60 +/- 184.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=450000, episode_reward=412.00 +/- 102.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=451000, episode_reward=324.60 +/- 134.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=452000, episode_reward=343.40 +/- 138.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=453000, episode_reward=560.80 +/- 46.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=454000, episode_reward=393.60 +/- 22.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=455000, episode_reward=239.40 +/- 170.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=456000, episode_reward=505.40 +/- 162.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=457000, episode_reward=384.00 +/- 339.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=458000, episode_reward=484.20 +/- 139.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=459000, episode_reward=239.80 +/- 152.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=460000, episode_reward=416.80 +/- 152.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=461000, episode_reward=448.80 +/- 113.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=462000, episode_reward=302.80 +/- 177.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=463000, episode_reward=317.40 +/- 212.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=464000, episode_reward=489.80 +/- 142.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=465000, episode_reward=224.80 +/- 257.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=466000, episode_reward=403.80 +/- 267.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=467000, episode_reward=576.80 +/- 129.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=468000, episode_reward=336.80 +/- 250.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=469000, episode_reward=492.40 +/- 271.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=470000, episode_reward=214.80 +/- 242.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=471000, episode_reward=282.00 +/- 181.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=472000, episode_reward=419.00 +/- 208.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=473000, episode_reward=314.40 +/- 120.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=474000, episode_reward=344.60 +/- 160.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=475000, episode_reward=321.80 +/- 58.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=476000, episode_reward=595.60 +/- 161.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=477000, episode_reward=367.80 +/- 218.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=478000, episode_reward=347.40 +/- 141.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=479000, episode_reward=417.80 +/- 184.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=480000, episode_reward=336.00 +/- 245.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=481000, episode_reward=312.60 +/- 124.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=482000, episode_reward=250.00 +/- 227.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=483000, episode_reward=562.20 +/- 159.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=484000, episode_reward=331.60 +/- 167.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=485000, episode_reward=466.20 +/- 152.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=486000, episode_reward=287.60 +/- 209.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=487000, episode_reward=331.60 +/- 154.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=488000, episode_reward=494.20 +/- 179.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=489000, episode_reward=389.20 +/- 239.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=490000, episode_reward=483.80 +/- 139.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=491000, episode_reward=265.00 +/- 137.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=492000, episode_reward=463.60 +/- 99.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=493000, episode_reward=334.00 +/- 259.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=494000, episode_reward=391.60 +/- 120.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=495000, episode_reward=398.80 +/- 130.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=496000, episode_reward=379.40 +/- 163.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=497000, episode_reward=379.40 +/- 124.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=498000, episode_reward=467.80 +/- 119.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=499000, episode_reward=412.80 +/- 180.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=500000, episode_reward=362.40 +/- 218.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Gamma = 0.8, Learning Rate = 0.003, Activation Function = ReLU, Neurons = 32, Process noise = 0.1, Measurement noise = 0.0\n",
      "Eval num_timesteps=1000, episode_reward=-1518.20 +/- 550.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=-1525.40 +/- 548.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=3000, episode_reward=-1581.40 +/- 366.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=4000, episode_reward=-1448.60 +/- 343.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=5000, episode_reward=-1380.20 +/- 183.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=6000, episode_reward=-1832.40 +/- 931.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=7000, episode_reward=-1412.20 +/- 299.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=8000, episode_reward=-1621.40 +/- 444.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=9000, episode_reward=-1664.40 +/- 320.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=10000, episode_reward=-1704.40 +/- 414.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=11000, episode_reward=-1574.80 +/- 197.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=12000, episode_reward=-2004.60 +/- 993.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=13000, episode_reward=-1550.60 +/- 266.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=14000, episode_reward=-1854.80 +/- 591.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=15000, episode_reward=-1269.40 +/- 234.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=16000, episode_reward=-1523.20 +/- 474.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=17000, episode_reward=-1311.20 +/- 427.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=18000, episode_reward=-1531.60 +/- 414.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=19000, episode_reward=-1241.60 +/- 214.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=20000, episode_reward=-1358.60 +/- 153.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=21000, episode_reward=-1880.40 +/- 1032.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=22000, episode_reward=-1769.40 +/- 451.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=23000, episode_reward=-2356.20 +/- 1214.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=24000, episode_reward=-1495.80 +/- 547.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=25000, episode_reward=-1343.00 +/- 218.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=26000, episode_reward=-1801.20 +/- 336.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=27000, episode_reward=-1455.60 +/- 289.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=28000, episode_reward=-1437.40 +/- 296.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=29000, episode_reward=-1856.60 +/- 560.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=30000, episode_reward=-1394.80 +/- 128.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=31000, episode_reward=-1450.40 +/- 342.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=32000, episode_reward=-1252.00 +/- 94.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=33000, episode_reward=-1192.80 +/- 309.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=34000, episode_reward=-2018.60 +/- 717.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=35000, episode_reward=-1758.20 +/- 517.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=36000, episode_reward=-1520.00 +/- 373.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=37000, episode_reward=-1452.40 +/- 186.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=38000, episode_reward=-1141.80 +/- 101.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=39000, episode_reward=-1429.20 +/- 450.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=40000, episode_reward=-1537.60 +/- 360.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=41000, episode_reward=-1271.60 +/- 179.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=42000, episode_reward=-2184.20 +/- 831.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=43000, episode_reward=-1460.00 +/- 597.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=44000, episode_reward=-1299.80 +/- 297.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=45000, episode_reward=-1539.00 +/- 251.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=46000, episode_reward=-1619.00 +/- 607.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=47000, episode_reward=-1780.80 +/- 786.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=48000, episode_reward=-1460.60 +/- 258.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=49000, episode_reward=-1666.20 +/- 386.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=50000, episode_reward=-1510.60 +/- 417.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=51000, episode_reward=498.60 +/- 170.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=52000, episode_reward=-316.20 +/- 929.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=53000, episode_reward=281.80 +/- 57.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=54000, episode_reward=354.20 +/- 200.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=55000, episode_reward=104.80 +/- 181.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=56000, episode_reward=365.20 +/- 216.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=57000, episode_reward=355.60 +/- 299.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=58000, episode_reward=260.60 +/- 190.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=59000, episode_reward=198.00 +/- 201.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=60000, episode_reward=401.20 +/- 129.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=61000, episode_reward=255.60 +/- 162.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=62000, episode_reward=433.60 +/- 182.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=63000, episode_reward=441.60 +/- 121.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=64000, episode_reward=382.60 +/- 182.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=65000, episode_reward=457.60 +/- 249.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=66000, episode_reward=319.00 +/- 305.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=67000, episode_reward=386.00 +/- 202.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=68000, episode_reward=361.00 +/- 124.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=69000, episode_reward=355.80 +/- 117.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=70000, episode_reward=399.40 +/- 274.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=71000, episode_reward=344.80 +/- 296.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=72000, episode_reward=372.60 +/- 115.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=73000, episode_reward=277.40 +/- 154.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=74000, episode_reward=367.40 +/- 145.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=75000, episode_reward=196.80 +/- 141.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=76000, episode_reward=229.60 +/- 91.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=77000, episode_reward=309.00 +/- 112.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=78000, episode_reward=256.80 +/- 201.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=79000, episode_reward=406.20 +/- 214.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=80000, episode_reward=440.20 +/- 235.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=81000, episode_reward=361.60 +/- 67.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=82000, episode_reward=337.80 +/- 180.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=83000, episode_reward=415.00 +/- 217.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=84000, episode_reward=327.60 +/- 178.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=85000, episode_reward=184.40 +/- 183.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=86000, episode_reward=540.20 +/- 258.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=87000, episode_reward=522.00 +/- 81.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=88000, episode_reward=410.60 +/- 133.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=89000, episode_reward=433.20 +/- 222.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=90000, episode_reward=344.00 +/- 193.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=91000, episode_reward=404.40 +/- 119.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=92000, episode_reward=348.20 +/- 225.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=93000, episode_reward=268.80 +/- 155.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=94000, episode_reward=393.20 +/- 214.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=95000, episode_reward=416.20 +/- 206.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=96000, episode_reward=410.20 +/- 292.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=97000, episode_reward=421.40 +/- 173.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=98000, episode_reward=110.20 +/- 238.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=99000, episode_reward=300.80 +/- 112.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=100000, episode_reward=431.60 +/- 218.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=101000, episode_reward=383.40 +/- 96.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=102000, episode_reward=437.60 +/- 183.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=103000, episode_reward=293.40 +/- 130.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=104000, episode_reward=503.60 +/- 126.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=105000, episode_reward=275.20 +/- 185.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=106000, episode_reward=476.40 +/- 208.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=107000, episode_reward=481.40 +/- 103.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=108000, episode_reward=348.20 +/- 219.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=109000, episode_reward=292.20 +/- 338.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=110000, episode_reward=338.80 +/- 125.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=111000, episode_reward=283.80 +/- 159.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=112000, episode_reward=457.80 +/- 202.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=113000, episode_reward=361.20 +/- 203.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=114000, episode_reward=397.60 +/- 98.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=115000, episode_reward=147.20 +/- 194.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=116000, episode_reward=364.60 +/- 183.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=117000, episode_reward=361.20 +/- 227.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=118000, episode_reward=305.00 +/- 166.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=119000, episode_reward=403.60 +/- 147.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=120000, episode_reward=311.00 +/- 26.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=121000, episode_reward=404.40 +/- 150.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=122000, episode_reward=352.40 +/- 156.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=123000, episode_reward=397.80 +/- 177.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=124000, episode_reward=392.00 +/- 167.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=125000, episode_reward=302.20 +/- 327.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=126000, episode_reward=451.40 +/- 153.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=127000, episode_reward=462.00 +/- 147.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=128000, episode_reward=384.60 +/- 130.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=129000, episode_reward=412.40 +/- 228.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=130000, episode_reward=289.00 +/- 180.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=131000, episode_reward=325.20 +/- 213.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=132000, episode_reward=477.40 +/- 154.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=133000, episode_reward=520.20 +/- 252.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=134000, episode_reward=608.80 +/- 187.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=135000, episode_reward=190.20 +/- 250.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=136000, episode_reward=304.60 +/- 200.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=137000, episode_reward=242.20 +/- 194.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=138000, episode_reward=485.60 +/- 279.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=139000, episode_reward=298.20 +/- 189.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=140000, episode_reward=364.00 +/- 200.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=141000, episode_reward=210.20 +/- 53.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=142000, episode_reward=482.20 +/- 126.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=143000, episode_reward=229.00 +/- 155.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=144000, episode_reward=524.80 +/- 260.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=145000, episode_reward=227.40 +/- 122.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=146000, episode_reward=567.40 +/- 127.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=147000, episode_reward=502.20 +/- 234.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=148000, episode_reward=570.60 +/- 239.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=149000, episode_reward=486.60 +/- 170.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=150000, episode_reward=326.80 +/- 211.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=151000, episode_reward=404.40 +/- 145.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=152000, episode_reward=636.60 +/- 245.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=153000, episode_reward=274.00 +/- 231.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=154000, episode_reward=389.40 +/- 85.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=155000, episode_reward=650.80 +/- 206.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=156000, episode_reward=435.80 +/- 217.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=157000, episode_reward=1076.80 +/- 165.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=158000, episode_reward=958.60 +/- 152.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=159000, episode_reward=976.00 +/- 447.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=160000, episode_reward=1258.60 +/- 232.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=161000, episode_reward=1139.60 +/- 232.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=162000, episode_reward=1171.20 +/- 339.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=163000, episode_reward=1132.00 +/- 263.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=164000, episode_reward=1163.20 +/- 173.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=165000, episode_reward=1187.40 +/- 232.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=166000, episode_reward=1082.00 +/- 258.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=167000, episode_reward=1183.20 +/- 417.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=168000, episode_reward=738.00 +/- 180.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=169000, episode_reward=1027.00 +/- 198.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=170000, episode_reward=972.00 +/- 173.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=171000, episode_reward=998.60 +/- 336.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=172000, episode_reward=649.40 +/- 315.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=173000, episode_reward=1007.20 +/- 277.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=174000, episode_reward=1232.40 +/- 226.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=175000, episode_reward=825.00 +/- 314.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=176000, episode_reward=798.20 +/- 120.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=177000, episode_reward=923.00 +/- 200.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=178000, episode_reward=1075.80 +/- 298.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=179000, episode_reward=902.80 +/- 245.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=180000, episode_reward=730.00 +/- 227.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=181000, episode_reward=1208.00 +/- 373.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=182000, episode_reward=1082.80 +/- 371.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=183000, episode_reward=970.60 +/- 527.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=184000, episode_reward=1136.80 +/- 168.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=185000, episode_reward=1327.40 +/- 156.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=186000, episode_reward=932.40 +/- 162.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=187000, episode_reward=967.00 +/- 299.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=188000, episode_reward=1271.80 +/- 420.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=189000, episode_reward=946.00 +/- 275.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=190000, episode_reward=1089.00 +/- 348.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=191000, episode_reward=952.00 +/- 324.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=192000, episode_reward=1349.20 +/- 216.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=193000, episode_reward=665.00 +/- 165.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=194000, episode_reward=1118.00 +/- 175.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=195000, episode_reward=1142.00 +/- 291.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=196000, episode_reward=988.20 +/- 335.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=197000, episode_reward=1379.20 +/- 181.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=198000, episode_reward=1129.40 +/- 206.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=199000, episode_reward=1018.40 +/- 238.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=200000, episode_reward=1203.00 +/- 170.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=201000, episode_reward=1115.20 +/- 132.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=202000, episode_reward=908.00 +/- 482.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=203000, episode_reward=929.60 +/- 158.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=204000, episode_reward=1330.20 +/- 316.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=205000, episode_reward=929.60 +/- 260.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=206000, episode_reward=1037.00 +/- 353.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=207000, episode_reward=834.60 +/- 206.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=208000, episode_reward=861.40 +/- 407.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=209000, episode_reward=929.60 +/- 326.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=210000, episode_reward=681.40 +/- 294.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=211000, episode_reward=1149.20 +/- 219.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=212000, episode_reward=979.60 +/- 330.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=213000, episode_reward=1006.60 +/- 305.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=214000, episode_reward=1350.80 +/- 337.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=215000, episode_reward=1043.20 +/- 120.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=216000, episode_reward=958.00 +/- 200.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=217000, episode_reward=812.60 +/- 252.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=218000, episode_reward=1133.80 +/- 177.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=219000, episode_reward=807.80 +/- 325.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=220000, episode_reward=1100.80 +/- 312.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=221000, episode_reward=844.00 +/- 342.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=222000, episode_reward=1034.00 +/- 306.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=223000, episode_reward=846.60 +/- 483.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=224000, episode_reward=732.00 +/- 331.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=225000, episode_reward=1137.40 +/- 100.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=226000, episode_reward=975.80 +/- 249.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=227000, episode_reward=1100.80 +/- 126.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=228000, episode_reward=1167.80 +/- 435.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=229000, episode_reward=1485.60 +/- 322.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=230000, episode_reward=706.20 +/- 186.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=231000, episode_reward=1503.00 +/- 230.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=232000, episode_reward=1042.20 +/- 203.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=233000, episode_reward=1022.40 +/- 372.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=234000, episode_reward=1226.40 +/- 257.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=235000, episode_reward=938.20 +/- 233.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=236000, episode_reward=1004.40 +/- 172.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=237000, episode_reward=909.00 +/- 246.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=238000, episode_reward=879.60 +/- 328.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=239000, episode_reward=886.60 +/- 392.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=240000, episode_reward=1011.40 +/- 317.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=241000, episode_reward=1130.20 +/- 348.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=242000, episode_reward=1217.60 +/- 91.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=243000, episode_reward=1162.80 +/- 176.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=244000, episode_reward=841.80 +/- 187.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=245000, episode_reward=1035.60 +/- 331.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=246000, episode_reward=1206.40 +/- 347.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=247000, episode_reward=1208.60 +/- 247.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=248000, episode_reward=953.00 +/- 212.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=249000, episode_reward=545.20 +/- 339.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=250000, episode_reward=1060.80 +/- 377.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=251000, episode_reward=854.60 +/- 461.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=252000, episode_reward=1071.40 +/- 155.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=253000, episode_reward=1247.00 +/- 285.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=254000, episode_reward=807.60 +/- 244.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=255000, episode_reward=1121.60 +/- 242.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=256000, episode_reward=1069.80 +/- 286.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=257000, episode_reward=1204.40 +/- 275.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=258000, episode_reward=1097.00 +/- 133.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=259000, episode_reward=1002.80 +/- 140.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=260000, episode_reward=1048.40 +/- 184.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=261000, episode_reward=1085.40 +/- 173.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=262000, episode_reward=1026.80 +/- 400.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=263000, episode_reward=892.80 +/- 413.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=264000, episode_reward=975.40 +/- 413.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=265000, episode_reward=1070.40 +/- 216.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=266000, episode_reward=1347.80 +/- 219.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=267000, episode_reward=927.20 +/- 434.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=268000, episode_reward=1342.60 +/- 364.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=269000, episode_reward=1017.20 +/- 221.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=270000, episode_reward=1003.80 +/- 289.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=271000, episode_reward=1053.20 +/- 354.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=272000, episode_reward=1091.00 +/- 395.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=273000, episode_reward=1057.60 +/- 213.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=274000, episode_reward=815.80 +/- 301.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=275000, episode_reward=1063.00 +/- 414.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=276000, episode_reward=1086.40 +/- 157.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=277000, episode_reward=1091.00 +/- 314.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=278000, episode_reward=1245.40 +/- 245.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=279000, episode_reward=1446.80 +/- 165.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=280000, episode_reward=1078.40 +/- 354.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=281000, episode_reward=1028.40 +/- 210.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=282000, episode_reward=796.00 +/- 333.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=283000, episode_reward=1044.80 +/- 344.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=284000, episode_reward=775.20 +/- 284.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=285000, episode_reward=901.20 +/- 336.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=286000, episode_reward=1108.40 +/- 465.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=287000, episode_reward=1160.20 +/- 177.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=288000, episode_reward=1451.20 +/- 188.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=289000, episode_reward=911.00 +/- 251.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=290000, episode_reward=1196.00 +/- 458.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=291000, episode_reward=1077.20 +/- 273.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=292000, episode_reward=720.00 +/- 272.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=293000, episode_reward=631.40 +/- 411.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=294000, episode_reward=847.80 +/- 203.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=295000, episode_reward=804.00 +/- 199.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=296000, episode_reward=723.20 +/- 266.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=297000, episode_reward=920.80 +/- 274.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=298000, episode_reward=674.60 +/- 258.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=299000, episode_reward=1061.60 +/- 170.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=300000, episode_reward=499.60 +/- 260.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=301000, episode_reward=769.80 +/- 246.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=302000, episode_reward=1002.60 +/- 210.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=303000, episode_reward=1136.00 +/- 143.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=304000, episode_reward=737.80 +/- 366.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=305000, episode_reward=1089.20 +/- 374.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=306000, episode_reward=1048.60 +/- 180.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=307000, episode_reward=1082.60 +/- 506.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=308000, episode_reward=1111.60 +/- 255.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=309000, episode_reward=917.80 +/- 176.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=310000, episode_reward=1024.00 +/- 464.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=311000, episode_reward=1075.40 +/- 446.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=312000, episode_reward=1051.20 +/- 121.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=313000, episode_reward=861.80 +/- 369.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=314000, episode_reward=1011.20 +/- 290.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=315000, episode_reward=1014.40 +/- 323.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=316000, episode_reward=905.60 +/- 348.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=317000, episode_reward=1035.20 +/- 315.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=318000, episode_reward=859.00 +/- 112.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=319000, episode_reward=854.80 +/- 409.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=320000, episode_reward=948.40 +/- 314.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=321000, episode_reward=603.40 +/- 243.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=322000, episode_reward=1258.00 +/- 224.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=323000, episode_reward=1215.80 +/- 413.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=324000, episode_reward=955.60 +/- 316.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=325000, episode_reward=1065.80 +/- 329.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=326000, episode_reward=849.00 +/- 213.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=327000, episode_reward=1063.20 +/- 417.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=328000, episode_reward=617.20 +/- 310.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=329000, episode_reward=773.00 +/- 262.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=330000, episode_reward=1004.80 +/- 416.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=331000, episode_reward=1089.80 +/- 499.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=332000, episode_reward=920.60 +/- 133.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=333000, episode_reward=814.80 +/- 234.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=334000, episode_reward=1052.20 +/- 241.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=335000, episode_reward=1182.40 +/- 285.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=336000, episode_reward=845.00 +/- 247.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=337000, episode_reward=703.80 +/- 463.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=338000, episode_reward=1256.00 +/- 465.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=339000, episode_reward=940.00 +/- 174.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=340000, episode_reward=1098.60 +/- 125.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=341000, episode_reward=868.00 +/- 251.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=342000, episode_reward=1056.00 +/- 227.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=343000, episode_reward=1542.00 +/- 390.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=344000, episode_reward=1072.40 +/- 236.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=345000, episode_reward=1345.40 +/- 282.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=346000, episode_reward=1213.40 +/- 322.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=347000, episode_reward=708.40 +/- 254.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=348000, episode_reward=941.60 +/- 479.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=349000, episode_reward=1242.40 +/- 324.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=350000, episode_reward=674.00 +/- 351.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=351000, episode_reward=1062.20 +/- 222.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=352000, episode_reward=992.80 +/- 398.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=353000, episode_reward=861.80 +/- 352.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=354000, episode_reward=1254.40 +/- 317.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=355000, episode_reward=931.40 +/- 188.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=356000, episode_reward=778.00 +/- 285.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=357000, episode_reward=1264.40 +/- 229.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=358000, episode_reward=939.40 +/- 161.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=359000, episode_reward=965.40 +/- 248.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=360000, episode_reward=1112.20 +/- 177.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=361000, episode_reward=1074.40 +/- 228.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=362000, episode_reward=1118.60 +/- 167.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=363000, episode_reward=1008.40 +/- 354.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=364000, episode_reward=1140.60 +/- 336.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=365000, episode_reward=1293.20 +/- 154.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=366000, episode_reward=964.60 +/- 143.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=367000, episode_reward=1068.80 +/- 247.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=368000, episode_reward=1181.80 +/- 267.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=369000, episode_reward=1044.80 +/- 285.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=370000, episode_reward=1154.40 +/- 162.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=371000, episode_reward=1070.80 +/- 300.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=372000, episode_reward=1523.40 +/- 286.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=373000, episode_reward=1289.20 +/- 356.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=374000, episode_reward=1225.40 +/- 312.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=375000, episode_reward=1281.00 +/- 566.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=376000, episode_reward=1080.40 +/- 269.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=377000, episode_reward=854.60 +/- 284.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=378000, episode_reward=1237.80 +/- 296.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=379000, episode_reward=1076.80 +/- 411.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=380000, episode_reward=1093.00 +/- 178.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=381000, episode_reward=1334.60 +/- 176.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=382000, episode_reward=973.60 +/- 193.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=383000, episode_reward=960.20 +/- 117.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=384000, episode_reward=1088.40 +/- 461.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=385000, episode_reward=1081.00 +/- 408.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=386000, episode_reward=1256.40 +/- 302.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=387000, episode_reward=854.00 +/- 314.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=388000, episode_reward=1004.80 +/- 373.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=389000, episode_reward=1216.20 +/- 355.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=390000, episode_reward=1514.20 +/- 301.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=391000, episode_reward=1349.00 +/- 423.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=392000, episode_reward=1015.80 +/- 213.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=393000, episode_reward=970.60 +/- 412.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=394000, episode_reward=990.20 +/- 311.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=395000, episode_reward=835.80 +/- 307.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=396000, episode_reward=810.20 +/- 365.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=397000, episode_reward=1098.80 +/- 344.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=398000, episode_reward=953.60 +/- 318.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=399000, episode_reward=954.00 +/- 180.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=400000, episode_reward=1032.40 +/- 238.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=401000, episode_reward=1216.40 +/- 330.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=402000, episode_reward=1294.40 +/- 272.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=403000, episode_reward=1084.80 +/- 180.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=404000, episode_reward=856.80 +/- 278.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=405000, episode_reward=1088.60 +/- 301.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=406000, episode_reward=1177.40 +/- 158.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=407000, episode_reward=909.20 +/- 135.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=408000, episode_reward=944.20 +/- 410.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=409000, episode_reward=1163.00 +/- 241.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=410000, episode_reward=875.20 +/- 229.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=411000, episode_reward=959.20 +/- 380.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=412000, episode_reward=1108.20 +/- 201.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=413000, episode_reward=1241.60 +/- 335.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=414000, episode_reward=965.80 +/- 59.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=415000, episode_reward=924.00 +/- 214.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=416000, episode_reward=861.80 +/- 297.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=417000, episode_reward=918.80 +/- 378.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=418000, episode_reward=801.60 +/- 325.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=419000, episode_reward=1347.00 +/- 236.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=420000, episode_reward=1076.20 +/- 270.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=421000, episode_reward=1487.40 +/- 330.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=422000, episode_reward=1252.20 +/- 324.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=423000, episode_reward=1058.00 +/- 164.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=424000, episode_reward=1266.60 +/- 149.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=425000, episode_reward=912.40 +/- 565.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=426000, episode_reward=1149.00 +/- 203.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=427000, episode_reward=1273.20 +/- 192.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=428000, episode_reward=1075.20 +/- 305.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=429000, episode_reward=1082.00 +/- 224.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=430000, episode_reward=1189.00 +/- 379.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=431000, episode_reward=1416.20 +/- 175.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=432000, episode_reward=1347.60 +/- 201.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=433000, episode_reward=1019.80 +/- 263.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=434000, episode_reward=1211.60 +/- 274.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=435000, episode_reward=813.80 +/- 200.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=436000, episode_reward=1147.80 +/- 345.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=437000, episode_reward=1023.00 +/- 349.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=438000, episode_reward=898.20 +/- 153.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=439000, episode_reward=1076.80 +/- 258.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=440000, episode_reward=1494.60 +/- 287.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=441000, episode_reward=1247.80 +/- 501.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=442000, episode_reward=601.40 +/- 200.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=443000, episode_reward=895.40 +/- 313.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=444000, episode_reward=971.00 +/- 313.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=445000, episode_reward=894.40 +/- 203.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=446000, episode_reward=811.40 +/- 330.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=447000, episode_reward=868.80 +/- 452.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=448000, episode_reward=990.20 +/- 420.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=449000, episode_reward=765.60 +/- 420.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=450000, episode_reward=1019.00 +/- 362.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=451000, episode_reward=906.20 +/- 328.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=452000, episode_reward=947.60 +/- 253.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=453000, episode_reward=871.80 +/- 218.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=454000, episode_reward=876.00 +/- 359.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=455000, episode_reward=932.20 +/- 341.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=456000, episode_reward=1088.60 +/- 352.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=457000, episode_reward=858.60 +/- 151.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=458000, episode_reward=883.00 +/- 308.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=459000, episode_reward=1048.80 +/- 395.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=460000, episode_reward=1314.60 +/- 227.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=461000, episode_reward=1306.80 +/- 482.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=462000, episode_reward=670.40 +/- 179.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=463000, episode_reward=760.00 +/- 273.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=464000, episode_reward=909.60 +/- 140.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=465000, episode_reward=1350.20 +/- 335.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=466000, episode_reward=973.80 +/- 123.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=467000, episode_reward=547.60 +/- 355.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=468000, episode_reward=961.20 +/- 334.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=469000, episode_reward=712.60 +/- 255.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=470000, episode_reward=878.40 +/- 367.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=471000, episode_reward=637.60 +/- 151.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=472000, episode_reward=694.00 +/- 248.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=473000, episode_reward=1106.40 +/- 194.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=474000, episode_reward=817.20 +/- 170.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=475000, episode_reward=1188.80 +/- 257.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=476000, episode_reward=1483.20 +/- 290.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=477000, episode_reward=1165.00 +/- 143.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=478000, episode_reward=986.00 +/- 122.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=479000, episode_reward=783.40 +/- 206.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=480000, episode_reward=826.60 +/- 256.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=481000, episode_reward=845.60 +/- 268.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=482000, episode_reward=1114.00 +/- 303.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=483000, episode_reward=1165.00 +/- 278.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=484000, episode_reward=1209.80 +/- 469.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=485000, episode_reward=913.40 +/- 292.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=486000, episode_reward=1093.80 +/- 283.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=487000, episode_reward=887.60 +/- 471.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=488000, episode_reward=664.80 +/- 244.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=489000, episode_reward=985.20 +/- 458.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=490000, episode_reward=995.80 +/- 125.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=491000, episode_reward=796.20 +/- 401.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=492000, episode_reward=778.20 +/- 242.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=493000, episode_reward=1076.60 +/- 125.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=494000, episode_reward=998.60 +/- 309.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=495000, episode_reward=744.60 +/- 200.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=496000, episode_reward=824.40 +/- 273.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=497000, episode_reward=827.00 +/- 240.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=498000, episode_reward=917.40 +/- 295.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=499000, episode_reward=913.40 +/- 453.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=500000, episode_reward=983.60 +/- 327.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Gamma = 0.8, Learning Rate = 0.003, Activation Function = ReLU, Neurons = 64, Process noise = 0.1, Measurement noise = 0.0\n",
      "Eval num_timesteps=1000, episode_reward=-1446.40 +/- 214.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=-1139.20 +/- 342.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3000, episode_reward=-1474.40 +/- 285.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=4000, episode_reward=-1218.80 +/- 382.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=5000, episode_reward=-1362.60 +/- 411.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=6000, episode_reward=-910.40 +/- 389.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=7000, episode_reward=-1450.00 +/- 247.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=8000, episode_reward=-1498.20 +/- 156.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=9000, episode_reward=-1209.60 +/- 260.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=10000, episode_reward=-1517.60 +/- 148.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=11000, episode_reward=-1500.00 +/- 231.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=12000, episode_reward=-1215.40 +/- 508.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=13000, episode_reward=-1294.20 +/- 292.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=14000, episode_reward=-1195.40 +/- 400.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=15000, episode_reward=-1589.20 +/- 218.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=16000, episode_reward=-1577.60 +/- 460.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=17000, episode_reward=-1392.00 +/- 384.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=18000, episode_reward=-1019.60 +/- 228.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=19000, episode_reward=-1355.20 +/- 405.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=20000, episode_reward=-1596.80 +/- 200.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=21000, episode_reward=-1060.80 +/- 634.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=22000, episode_reward=-1161.60 +/- 333.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=23000, episode_reward=-1297.20 +/- 652.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=24000, episode_reward=-1219.00 +/- 583.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=25000, episode_reward=-1611.00 +/- 378.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=26000, episode_reward=-1375.60 +/- 231.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=27000, episode_reward=-1197.60 +/- 527.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=28000, episode_reward=-1154.40 +/- 439.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=29000, episode_reward=-1008.60 +/- 370.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=30000, episode_reward=-868.60 +/- 555.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=31000, episode_reward=-1427.80 +/- 620.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=32000, episode_reward=-1272.80 +/- 393.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=33000, episode_reward=-1240.60 +/- 485.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=34000, episode_reward=-1231.80 +/- 379.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=35000, episode_reward=-1275.20 +/- 308.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=36000, episode_reward=-1210.20 +/- 336.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=37000, episode_reward=-1389.40 +/- 342.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=38000, episode_reward=-1307.80 +/- 434.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=39000, episode_reward=-1158.40 +/- 456.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=40000, episode_reward=-1351.80 +/- 363.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=41000, episode_reward=-1500.00 +/- 469.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=42000, episode_reward=-1509.00 +/- 336.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=43000, episode_reward=-1132.00 +/- 301.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=44000, episode_reward=-1245.60 +/- 220.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=45000, episode_reward=-1485.80 +/- 383.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=46000, episode_reward=-1028.40 +/- 328.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=47000, episode_reward=-1375.00 +/- 426.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=48000, episode_reward=-1161.40 +/- 395.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=49000, episode_reward=-1624.00 +/- 323.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=50000, episode_reward=-1474.60 +/- 440.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=51000, episode_reward=436.20 +/- 95.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=52000, episode_reward=137.60 +/- 225.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=53000, episode_reward=312.80 +/- 144.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=54000, episode_reward=268.80 +/- 137.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=55000, episode_reward=170.00 +/- 183.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=56000, episode_reward=136.60 +/- 227.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=57000, episode_reward=419.80 +/- 188.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=58000, episode_reward=164.60 +/- 205.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=59000, episode_reward=-519.40 +/- 854.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=60000, episode_reward=202.40 +/- 280.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=61000, episode_reward=141.00 +/- 110.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=62000, episode_reward=421.80 +/- 127.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=63000, episode_reward=127.80 +/- 130.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=64000, episode_reward=27.40 +/- 251.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=65000, episode_reward=302.00 +/- 145.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=66000, episode_reward=324.60 +/- 259.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=67000, episode_reward=480.40 +/- 223.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=68000, episode_reward=186.80 +/- 211.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=69000, episode_reward=340.80 +/- 164.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=70000, episode_reward=478.40 +/- 137.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=71000, episode_reward=277.40 +/- 168.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=72000, episode_reward=302.40 +/- 219.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=73000, episode_reward=380.80 +/- 183.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=74000, episode_reward=422.00 +/- 134.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=75000, episode_reward=323.00 +/- 220.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=76000, episode_reward=403.00 +/- 162.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=77000, episode_reward=290.80 +/- 145.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=78000, episode_reward=348.20 +/- 151.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=79000, episode_reward=304.60 +/- 270.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=80000, episode_reward=249.80 +/- 104.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=81000, episode_reward=326.80 +/- 111.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=82000, episode_reward=273.80 +/- 169.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=83000, episode_reward=168.40 +/- 140.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=84000, episode_reward=340.60 +/- 49.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=85000, episode_reward=301.60 +/- 243.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=86000, episode_reward=340.40 +/- 309.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=87000, episode_reward=458.40 +/- 129.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=88000, episode_reward=181.60 +/- 202.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=89000, episode_reward=254.40 +/- 263.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=90000, episode_reward=243.80 +/- 127.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=91000, episode_reward=296.40 +/- 115.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=92000, episode_reward=413.80 +/- 185.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=93000, episode_reward=-201.60 +/- 287.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=94000, episode_reward=172.00 +/- 268.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=95000, episode_reward=473.20 +/- 159.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=96000, episode_reward=249.60 +/- 136.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=97000, episode_reward=347.60 +/- 296.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=98000, episode_reward=318.80 +/- 168.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=99000, episode_reward=288.00 +/- 138.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=100000, episode_reward=384.40 +/- 201.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=101000, episode_reward=332.20 +/- 153.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=102000, episode_reward=339.20 +/- 244.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=103000, episode_reward=326.40 +/- 200.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=104000, episode_reward=391.00 +/- 163.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=105000, episode_reward=175.80 +/- 182.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=106000, episode_reward=331.40 +/- 146.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=107000, episode_reward=374.00 +/- 202.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=108000, episode_reward=570.60 +/- 89.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=109000, episode_reward=425.20 +/- 70.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=110000, episode_reward=285.80 +/- 165.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=111000, episode_reward=330.40 +/- 272.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=112000, episode_reward=238.60 +/- 102.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=113000, episode_reward=227.80 +/- 61.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=114000, episode_reward=319.80 +/- 91.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=115000, episode_reward=366.60 +/- 39.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=116000, episode_reward=484.00 +/- 161.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=117000, episode_reward=468.40 +/- 175.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=118000, episode_reward=437.20 +/- 316.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=119000, episode_reward=517.60 +/- 258.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=120000, episode_reward=258.40 +/- 101.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=121000, episode_reward=129.20 +/- 100.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=122000, episode_reward=252.20 +/- 29.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=123000, episode_reward=165.00 +/- 183.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=124000, episode_reward=332.20 +/- 240.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=125000, episode_reward=306.60 +/- 155.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=126000, episode_reward=283.20 +/- 210.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=127000, episode_reward=181.60 +/- 125.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=128000, episode_reward=273.20 +/- 219.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=129000, episode_reward=303.60 +/- 219.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=130000, episode_reward=375.80 +/- 82.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=131000, episode_reward=472.80 +/- 159.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=132000, episode_reward=792.40 +/- 151.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=133000, episode_reward=447.60 +/- 274.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=134000, episode_reward=247.40 +/- 118.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=135000, episode_reward=653.80 +/- 166.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=136000, episode_reward=676.80 +/- 315.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=137000, episode_reward=700.20 +/- 153.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=138000, episode_reward=492.80 +/- 161.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=139000, episode_reward=586.20 +/- 156.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=140000, episode_reward=168.20 +/- 147.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=141000, episode_reward=535.00 +/- 239.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=142000, episode_reward=644.00 +/- 323.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=143000, episode_reward=793.40 +/- 237.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=144000, episode_reward=732.20 +/- 327.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=145000, episode_reward=661.00 +/- 140.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=146000, episode_reward=708.00 +/- 233.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=147000, episode_reward=979.00 +/- 297.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=148000, episode_reward=803.20 +/- 120.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=149000, episode_reward=792.80 +/- 375.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=150000, episode_reward=931.80 +/- 306.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=151000, episode_reward=672.40 +/- 420.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=152000, episode_reward=1011.20 +/- 413.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=153000, episode_reward=789.00 +/- 372.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=154000, episode_reward=896.80 +/- 430.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=155000, episode_reward=730.80 +/- 394.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=156000, episode_reward=878.40 +/- 193.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=157000, episode_reward=736.80 +/- 417.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=158000, episode_reward=555.00 +/- 402.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=159000, episode_reward=909.60 +/- 307.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=160000, episode_reward=825.60 +/- 241.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=161000, episode_reward=898.00 +/- 269.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=162000, episode_reward=787.20 +/- 409.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=163000, episode_reward=885.00 +/- 381.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=164000, episode_reward=1091.20 +/- 195.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=165000, episode_reward=783.20 +/- 334.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=166000, episode_reward=1034.80 +/- 234.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=167000, episode_reward=863.60 +/- 204.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=168000, episode_reward=1166.40 +/- 306.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=169000, episode_reward=1148.00 +/- 122.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=170000, episode_reward=1385.00 +/- 120.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=171000, episode_reward=907.40 +/- 321.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=172000, episode_reward=968.40 +/- 230.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=173000, episode_reward=944.20 +/- 198.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=174000, episode_reward=930.80 +/- 238.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=175000, episode_reward=1044.20 +/- 384.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=176000, episode_reward=1005.00 +/- 306.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=177000, episode_reward=1019.60 +/- 147.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=178000, episode_reward=1431.40 +/- 267.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=179000, episode_reward=924.20 +/- 295.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=180000, episode_reward=1272.20 +/- 348.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=181000, episode_reward=789.60 +/- 382.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=182000, episode_reward=1000.60 +/- 347.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=183000, episode_reward=1116.80 +/- 284.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=184000, episode_reward=1088.80 +/- 252.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=185000, episode_reward=1101.20 +/- 221.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=186000, episode_reward=877.60 +/- 289.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=187000, episode_reward=1123.20 +/- 314.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=188000, episode_reward=1253.40 +/- 143.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=189000, episode_reward=1259.40 +/- 473.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=190000, episode_reward=917.40 +/- 337.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=191000, episode_reward=793.00 +/- 282.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=192000, episode_reward=722.60 +/- 162.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=193000, episode_reward=820.60 +/- 268.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=194000, episode_reward=1290.60 +/- 204.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=195000, episode_reward=1017.20 +/- 217.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=196000, episode_reward=1182.20 +/- 246.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=197000, episode_reward=742.80 +/- 345.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=198000, episode_reward=1397.40 +/- 333.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=199000, episode_reward=866.40 +/- 407.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=200000, episode_reward=710.00 +/- 286.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=201000, episode_reward=698.40 +/- 224.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=202000, episode_reward=1060.00 +/- 154.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=203000, episode_reward=1103.60 +/- 249.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=204000, episode_reward=1109.80 +/- 177.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=205000, episode_reward=643.80 +/- 165.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=206000, episode_reward=1048.60 +/- 321.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=207000, episode_reward=791.00 +/- 472.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=208000, episode_reward=990.00 +/- 105.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=209000, episode_reward=1130.00 +/- 260.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=210000, episode_reward=1205.80 +/- 307.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=211000, episode_reward=1057.60 +/- 293.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=212000, episode_reward=625.00 +/- 197.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=213000, episode_reward=735.80 +/- 198.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=214000, episode_reward=1059.80 +/- 393.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=215000, episode_reward=1152.80 +/- 221.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=216000, episode_reward=853.40 +/- 281.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=217000, episode_reward=652.80 +/- 106.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=218000, episode_reward=783.00 +/- 404.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=219000, episode_reward=1137.20 +/- 269.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=220000, episode_reward=651.40 +/- 194.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=221000, episode_reward=805.00 +/- 347.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=222000, episode_reward=819.40 +/- 98.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=223000, episode_reward=762.40 +/- 167.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=224000, episode_reward=858.60 +/- 115.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=225000, episode_reward=699.60 +/- 432.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=226000, episode_reward=946.20 +/- 354.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=227000, episode_reward=992.40 +/- 222.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=228000, episode_reward=1191.20 +/- 212.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=229000, episode_reward=953.80 +/- 356.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=230000, episode_reward=919.80 +/- 237.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=231000, episode_reward=1009.20 +/- 559.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=232000, episode_reward=1205.80 +/- 321.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=233000, episode_reward=829.40 +/- 333.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=234000, episode_reward=1280.80 +/- 204.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=235000, episode_reward=1155.60 +/- 308.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=236000, episode_reward=943.60 +/- 155.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=237000, episode_reward=1028.80 +/- 449.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=238000, episode_reward=1279.40 +/- 360.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=239000, episode_reward=718.40 +/- 269.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=240000, episode_reward=923.60 +/- 175.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=241000, episode_reward=1376.00 +/- 437.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=242000, episode_reward=962.60 +/- 296.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=243000, episode_reward=1391.20 +/- 216.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=244000, episode_reward=916.80 +/- 74.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=245000, episode_reward=937.00 +/- 165.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=246000, episode_reward=761.00 +/- 422.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=247000, episode_reward=671.00 +/- 461.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=248000, episode_reward=858.20 +/- 84.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=249000, episode_reward=1066.00 +/- 485.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=250000, episode_reward=835.60 +/- 223.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=251000, episode_reward=1340.40 +/- 345.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=252000, episode_reward=1114.00 +/- 430.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=253000, episode_reward=1101.40 +/- 374.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=254000, episode_reward=836.40 +/- 238.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=255000, episode_reward=905.20 +/- 394.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=256000, episode_reward=906.60 +/- 224.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=257000, episode_reward=1008.60 +/- 253.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=258000, episode_reward=827.20 +/- 251.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=259000, episode_reward=1106.20 +/- 479.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=260000, episode_reward=1202.00 +/- 202.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=261000, episode_reward=1033.20 +/- 233.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=262000, episode_reward=702.40 +/- 357.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=263000, episode_reward=626.80 +/- 181.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=264000, episode_reward=637.40 +/- 389.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=265000, episode_reward=690.60 +/- 323.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=266000, episode_reward=900.80 +/- 365.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=267000, episode_reward=1411.00 +/- 343.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=268000, episode_reward=932.60 +/- 212.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=269000, episode_reward=1273.80 +/- 242.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=270000, episode_reward=1307.00 +/- 207.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=271000, episode_reward=906.80 +/- 258.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=272000, episode_reward=969.00 +/- 374.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=273000, episode_reward=1195.40 +/- 141.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=274000, episode_reward=1312.80 +/- 153.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=275000, episode_reward=935.40 +/- 197.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=276000, episode_reward=1069.80 +/- 260.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=277000, episode_reward=1140.00 +/- 212.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=278000, episode_reward=1128.80 +/- 198.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=279000, episode_reward=1236.20 +/- 413.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=280000, episode_reward=1221.00 +/- 184.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=281000, episode_reward=1404.20 +/- 197.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=282000, episode_reward=1040.80 +/- 103.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=283000, episode_reward=1447.00 +/- 185.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=284000, episode_reward=1193.00 +/- 215.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=285000, episode_reward=931.20 +/- 373.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=286000, episode_reward=793.60 +/- 228.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=287000, episode_reward=923.60 +/- 273.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=288000, episode_reward=1289.60 +/- 334.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=289000, episode_reward=971.40 +/- 194.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=290000, episode_reward=963.00 +/- 141.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=291000, episode_reward=969.00 +/- 277.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=292000, episode_reward=938.20 +/- 543.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=293000, episode_reward=1007.20 +/- 275.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=294000, episode_reward=759.20 +/- 454.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=295000, episode_reward=1201.60 +/- 284.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=296000, episode_reward=936.40 +/- 322.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=297000, episode_reward=1035.20 +/- 251.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=298000, episode_reward=1155.00 +/- 404.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=299000, episode_reward=931.60 +/- 337.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=300000, episode_reward=1173.80 +/- 187.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=301000, episode_reward=1304.00 +/- 251.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=302000, episode_reward=978.20 +/- 632.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=303000, episode_reward=883.80 +/- 383.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=304000, episode_reward=1035.20 +/- 244.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=305000, episode_reward=1032.80 +/- 275.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=306000, episode_reward=1358.60 +/- 257.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=307000, episode_reward=1232.60 +/- 313.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=308000, episode_reward=1267.80 +/- 252.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=309000, episode_reward=1146.00 +/- 391.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=310000, episode_reward=1028.60 +/- 344.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=311000, episode_reward=800.00 +/- 360.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=312000, episode_reward=1389.40 +/- 211.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=313000, episode_reward=1125.40 +/- 420.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=314000, episode_reward=1182.40 +/- 215.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=315000, episode_reward=1057.80 +/- 223.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=316000, episode_reward=1353.20 +/- 120.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=317000, episode_reward=1135.60 +/- 282.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=318000, episode_reward=1060.60 +/- 226.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=319000, episode_reward=792.00 +/- 419.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=320000, episode_reward=809.80 +/- 219.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=321000, episode_reward=959.80 +/- 440.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=322000, episode_reward=902.60 +/- 137.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=323000, episode_reward=667.00 +/- 286.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=324000, episode_reward=1134.40 +/- 300.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=325000, episode_reward=932.60 +/- 191.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=326000, episode_reward=954.00 +/- 236.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=327000, episode_reward=715.40 +/- 227.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=328000, episode_reward=820.00 +/- 94.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=329000, episode_reward=727.20 +/- 336.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=330000, episode_reward=1138.60 +/- 208.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=331000, episode_reward=1002.80 +/- 210.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=332000, episode_reward=1087.00 +/- 140.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=333000, episode_reward=1142.00 +/- 355.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=334000, episode_reward=968.60 +/- 298.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=335000, episode_reward=1000.60 +/- 243.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=336000, episode_reward=934.60 +/- 181.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=337000, episode_reward=702.00 +/- 204.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=338000, episode_reward=1192.20 +/- 119.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=339000, episode_reward=1143.60 +/- 305.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=340000, episode_reward=1015.20 +/- 173.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=341000, episode_reward=1153.00 +/- 209.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=342000, episode_reward=802.80 +/- 147.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=343000, episode_reward=1447.60 +/- 188.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=344000, episode_reward=801.80 +/- 226.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=345000, episode_reward=1073.80 +/- 469.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=346000, episode_reward=1190.80 +/- 418.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=347000, episode_reward=1039.00 +/- 245.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=348000, episode_reward=913.20 +/- 393.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=349000, episode_reward=1213.80 +/- 464.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=350000, episode_reward=1082.80 +/- 331.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=351000, episode_reward=1156.40 +/- 318.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=352000, episode_reward=1153.80 +/- 338.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=353000, episode_reward=916.60 +/- 199.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=354000, episode_reward=1236.40 +/- 458.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=355000, episode_reward=762.40 +/- 363.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=356000, episode_reward=1016.20 +/- 100.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=357000, episode_reward=827.80 +/- 490.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=358000, episode_reward=816.60 +/- 161.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=359000, episode_reward=1264.80 +/- 251.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=360000, episode_reward=996.00 +/- 214.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=361000, episode_reward=1251.00 +/- 164.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=362000, episode_reward=590.80 +/- 172.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=363000, episode_reward=827.40 +/- 337.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=364000, episode_reward=726.40 +/- 342.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=365000, episode_reward=851.00 +/- 415.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=366000, episode_reward=787.80 +/- 264.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=367000, episode_reward=763.20 +/- 304.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=368000, episode_reward=1056.40 +/- 245.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=369000, episode_reward=1054.60 +/- 286.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=370000, episode_reward=1073.60 +/- 336.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=371000, episode_reward=844.60 +/- 256.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=372000, episode_reward=603.80 +/- 468.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=373000, episode_reward=852.80 +/- 353.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=374000, episode_reward=907.80 +/- 254.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=375000, episode_reward=1015.00 +/- 226.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=376000, episode_reward=524.40 +/- 158.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=377000, episode_reward=976.60 +/- 362.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=378000, episode_reward=663.60 +/- 236.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=379000, episode_reward=671.60 +/- 184.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=380000, episode_reward=1225.60 +/- 258.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=381000, episode_reward=879.00 +/- 421.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=382000, episode_reward=707.20 +/- 451.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=383000, episode_reward=951.00 +/- 299.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=384000, episode_reward=804.80 +/- 129.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=385000, episode_reward=941.00 +/- 343.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=386000, episode_reward=1086.20 +/- 337.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=387000, episode_reward=775.40 +/- 136.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=388000, episode_reward=836.40 +/- 500.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=389000, episode_reward=993.40 +/- 323.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=390000, episode_reward=877.40 +/- 349.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=391000, episode_reward=1104.20 +/- 277.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=392000, episode_reward=800.60 +/- 298.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=393000, episode_reward=1210.60 +/- 293.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=394000, episode_reward=1016.80 +/- 239.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=395000, episode_reward=864.60 +/- 166.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=396000, episode_reward=972.80 +/- 134.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=397000, episode_reward=1201.40 +/- 211.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=398000, episode_reward=715.20 +/- 218.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=399000, episode_reward=1027.40 +/- 304.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=400000, episode_reward=1029.60 +/- 177.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=401000, episode_reward=1433.00 +/- 358.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=402000, episode_reward=1401.40 +/- 373.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=403000, episode_reward=1354.80 +/- 388.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=404000, episode_reward=1268.80 +/- 179.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=405000, episode_reward=1023.40 +/- 314.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=406000, episode_reward=1197.00 +/- 187.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=407000, episode_reward=1071.20 +/- 133.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=408000, episode_reward=1473.60 +/- 315.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=409000, episode_reward=1138.00 +/- 274.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=410000, episode_reward=1099.40 +/- 287.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=411000, episode_reward=853.40 +/- 78.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=412000, episode_reward=809.20 +/- 211.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=413000, episode_reward=878.80 +/- 435.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=414000, episode_reward=767.80 +/- 454.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=415000, episode_reward=917.20 +/- 362.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=416000, episode_reward=935.60 +/- 361.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=417000, episode_reward=989.80 +/- 204.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=418000, episode_reward=927.80 +/- 391.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=419000, episode_reward=1136.80 +/- 228.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=420000, episode_reward=688.80 +/- 164.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=421000, episode_reward=1294.80 +/- 243.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=422000, episode_reward=1297.20 +/- 273.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=423000, episode_reward=720.80 +/- 221.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=424000, episode_reward=1098.80 +/- 408.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=425000, episode_reward=824.60 +/- 332.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=426000, episode_reward=1037.20 +/- 180.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=427000, episode_reward=981.80 +/- 307.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=428000, episode_reward=730.20 +/- 395.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=429000, episode_reward=1054.80 +/- 271.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=430000, episode_reward=766.80 +/- 322.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=431000, episode_reward=988.80 +/- 307.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=432000, episode_reward=1179.80 +/- 435.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=433000, episode_reward=1071.00 +/- 250.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=434000, episode_reward=1079.80 +/- 192.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=435000, episode_reward=1316.40 +/- 192.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=436000, episode_reward=1096.40 +/- 189.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=437000, episode_reward=875.60 +/- 247.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=438000, episode_reward=981.80 +/- 166.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=439000, episode_reward=1178.20 +/- 308.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=440000, episode_reward=1300.60 +/- 340.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=441000, episode_reward=1157.60 +/- 358.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=442000, episode_reward=1013.20 +/- 307.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=443000, episode_reward=1194.20 +/- 238.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=444000, episode_reward=1026.00 +/- 277.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=445000, episode_reward=648.20 +/- 243.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=446000, episode_reward=1263.60 +/- 288.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=447000, episode_reward=1060.40 +/- 597.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=448000, episode_reward=1207.60 +/- 512.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=449000, episode_reward=652.80 +/- 344.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=450000, episode_reward=948.20 +/- 415.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=451000, episode_reward=985.00 +/- 322.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=452000, episode_reward=953.00 +/- 467.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=453000, episode_reward=1211.80 +/- 256.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=454000, episode_reward=753.60 +/- 290.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=455000, episode_reward=742.20 +/- 460.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=456000, episode_reward=638.00 +/- 201.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=457000, episode_reward=1061.80 +/- 229.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=458000, episode_reward=1141.00 +/- 257.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=459000, episode_reward=849.60 +/- 343.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=460000, episode_reward=964.80 +/- 498.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=461000, episode_reward=854.40 +/- 303.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=462000, episode_reward=928.80 +/- 303.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=463000, episode_reward=1082.60 +/- 225.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=464000, episode_reward=823.40 +/- 203.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=465000, episode_reward=886.60 +/- 208.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=466000, episode_reward=851.20 +/- 195.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=467000, episode_reward=1096.80 +/- 281.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=468000, episode_reward=840.60 +/- 114.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=469000, episode_reward=1131.20 +/- 249.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=470000, episode_reward=1010.80 +/- 434.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=471000, episode_reward=880.80 +/- 163.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=472000, episode_reward=826.80 +/- 354.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=473000, episode_reward=1041.40 +/- 53.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=474000, episode_reward=1025.80 +/- 268.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=475000, episode_reward=904.40 +/- 448.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=476000, episode_reward=671.20 +/- 200.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=477000, episode_reward=878.20 +/- 453.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=478000, episode_reward=1128.40 +/- 270.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=479000, episode_reward=1062.00 +/- 377.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=480000, episode_reward=702.00 +/- 271.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=481000, episode_reward=776.20 +/- 293.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=482000, episode_reward=925.20 +/- 481.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=483000, episode_reward=1067.80 +/- 216.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=484000, episode_reward=1069.80 +/- 270.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=485000, episode_reward=1106.40 +/- 231.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=486000, episode_reward=924.40 +/- 247.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=487000, episode_reward=493.20 +/- 146.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=488000, episode_reward=866.00 +/- 390.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=489000, episode_reward=881.60 +/- 227.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=490000, episode_reward=918.00 +/- 278.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=491000, episode_reward=1001.80 +/- 349.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=492000, episode_reward=947.40 +/- 289.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=493000, episode_reward=705.40 +/- 103.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=494000, episode_reward=1028.80 +/- 179.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=495000, episode_reward=1011.60 +/- 174.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=496000, episode_reward=695.80 +/- 323.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=497000, episode_reward=929.00 +/- 346.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=498000, episode_reward=875.00 +/- 287.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=499000, episode_reward=929.60 +/- 358.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=500000, episode_reward=750.80 +/- 185.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Gamma = 0.8, Learning Rate = 0.0003, Activation Function = Tanh, Neurons = 8, Process noise = 0.1, Measurement noise = 0.0\n",
      "Eval num_timesteps=1000, episode_reward=-5024.80 +/- 809.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=-5189.20 +/- 499.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=3000, episode_reward=-4836.40 +/- 543.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4000, episode_reward=-4427.00 +/- 309.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=5000, episode_reward=-4529.80 +/- 541.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=6000, episode_reward=-5070.80 +/- 532.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=7000, episode_reward=-5046.40 +/- 661.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=8000, episode_reward=-4781.80 +/- 402.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=9000, episode_reward=-4832.00 +/- 509.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=10000, episode_reward=-4764.60 +/- 640.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=11000, episode_reward=-4867.00 +/- 438.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=12000, episode_reward=-5027.60 +/- 458.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=13000, episode_reward=-4480.80 +/- 650.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=14000, episode_reward=-4970.60 +/- 480.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=15000, episode_reward=-5141.00 +/- 207.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=16000, episode_reward=-5508.60 +/- 735.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=17000, episode_reward=-4413.60 +/- 182.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=18000, episode_reward=-5080.40 +/- 539.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=19000, episode_reward=-5040.60 +/- 700.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=20000, episode_reward=-4508.60 +/- 498.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=21000, episode_reward=-4589.00 +/- 368.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=22000, episode_reward=-4850.00 +/- 447.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=23000, episode_reward=-5213.40 +/- 683.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=24000, episode_reward=-4857.80 +/- 222.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=25000, episode_reward=-4685.00 +/- 409.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=26000, episode_reward=-4766.20 +/- 680.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=27000, episode_reward=-5363.00 +/- 582.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=28000, episode_reward=-5326.20 +/- 485.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=29000, episode_reward=-4622.40 +/- 445.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=30000, episode_reward=-4436.40 +/- 537.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=31000, episode_reward=-5155.60 +/- 413.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=32000, episode_reward=-5211.20 +/- 909.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=33000, episode_reward=-5170.20 +/- 497.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=34000, episode_reward=-5120.60 +/- 313.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=35000, episode_reward=-4987.20 +/- 306.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=36000, episode_reward=-5111.60 +/- 664.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=37000, episode_reward=-5109.80 +/- 587.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=38000, episode_reward=-5269.20 +/- 747.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=39000, episode_reward=-4758.40 +/- 598.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=40000, episode_reward=-4895.40 +/- 524.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=41000, episode_reward=-4390.00 +/- 417.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=42000, episode_reward=-4601.40 +/- 357.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=43000, episode_reward=-4793.00 +/- 666.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=44000, episode_reward=-5197.00 +/- 422.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=45000, episode_reward=-5225.00 +/- 371.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=46000, episode_reward=-5017.60 +/- 438.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=47000, episode_reward=-5030.80 +/- 617.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=48000, episode_reward=-4584.00 +/- 400.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=49000, episode_reward=-5035.20 +/- 384.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=50000, episode_reward=-4626.20 +/- 277.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=51000, episode_reward=-1598.40 +/- 547.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=52000, episode_reward=-709.40 +/- 279.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=53000, episode_reward=-237.80 +/- 879.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=54000, episode_reward=-147.60 +/- 551.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=55000, episode_reward=-124.80 +/- 825.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=56000, episode_reward=57.00 +/- 179.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=57000, episode_reward=23.40 +/- 379.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=58000, episode_reward=65.80 +/- 393.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=59000, episode_reward=27.20 +/- 158.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=60000, episode_reward=22.60 +/- 314.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=61000, episode_reward=-17.80 +/- 342.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=62000, episode_reward=286.80 +/- 155.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=63000, episode_reward=271.20 +/- 252.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=64000, episode_reward=27.60 +/- 160.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=65000, episode_reward=226.60 +/- 300.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=66000, episode_reward=328.40 +/- 308.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=67000, episode_reward=119.00 +/- 224.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=68000, episode_reward=80.20 +/- 254.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=69000, episode_reward=-218.60 +/- 324.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=70000, episode_reward=153.80 +/- 381.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=71000, episode_reward=-123.20 +/- 206.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=72000, episode_reward=104.20 +/- 333.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=73000, episode_reward=178.20 +/- 294.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=74000, episode_reward=-4.60 +/- 205.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=75000, episode_reward=126.60 +/- 184.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=76000, episode_reward=-35.80 +/- 127.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=77000, episode_reward=-135.80 +/- 458.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=78000, episode_reward=72.40 +/- 435.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=79000, episode_reward=196.00 +/- 200.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=80000, episode_reward=-103.20 +/- 459.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=81000, episode_reward=193.80 +/- 50.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=82000, episode_reward=220.00 +/- 387.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=83000, episode_reward=108.60 +/- 90.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=84000, episode_reward=118.40 +/- 135.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=85000, episode_reward=-100.40 +/- 620.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=86000, episode_reward=190.60 +/- 105.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=87000, episode_reward=51.00 +/- 562.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=88000, episode_reward=-445.60 +/- 610.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=89000, episode_reward=99.60 +/- 317.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=90000, episode_reward=115.00 +/- 224.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=91000, episode_reward=278.40 +/- 195.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=92000, episode_reward=129.20 +/- 332.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=93000, episode_reward=175.40 +/- 128.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=94000, episode_reward=170.40 +/- 374.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=95000, episode_reward=230.20 +/- 165.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=96000, episode_reward=179.20 +/- 291.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=97000, episode_reward=149.20 +/- 179.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=98000, episode_reward=57.80 +/- 470.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=99000, episode_reward=321.00 +/- 250.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=100000, episode_reward=70.20 +/- 150.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=101000, episode_reward=269.80 +/- 331.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=102000, episode_reward=115.80 +/- 270.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=103000, episode_reward=183.20 +/- 221.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=104000, episode_reward=-83.80 +/- 329.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=105000, episode_reward=102.20 +/- 153.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=106000, episode_reward=349.80 +/- 183.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=107000, episode_reward=126.20 +/- 183.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=108000, episode_reward=299.40 +/- 216.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=109000, episode_reward=-225.00 +/- 252.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=110000, episode_reward=187.00 +/- 308.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=111000, episode_reward=143.80 +/- 418.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=112000, episode_reward=98.60 +/- 248.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=113000, episode_reward=283.80 +/- 278.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=114000, episode_reward=115.60 +/- 194.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=115000, episode_reward=131.60 +/- 228.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=116000, episode_reward=151.40 +/- 231.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=117000, episode_reward=97.40 +/- 245.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=118000, episode_reward=-153.80 +/- 188.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=119000, episode_reward=84.20 +/- 245.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=120000, episode_reward=137.00 +/- 145.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=121000, episode_reward=131.60 +/- 254.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=122000, episode_reward=231.80 +/- 150.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=123000, episode_reward=123.40 +/- 163.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=124000, episode_reward=122.20 +/- 140.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=125000, episode_reward=47.20 +/- 163.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=126000, episode_reward=99.00 +/- 264.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=127000, episode_reward=265.00 +/- 123.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=128000, episode_reward=297.60 +/- 168.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=129000, episode_reward=93.60 +/- 385.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=130000, episode_reward=299.60 +/- 161.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=131000, episode_reward=28.80 +/- 242.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=132000, episode_reward=71.00 +/- 138.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=133000, episode_reward=318.40 +/- 162.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=134000, episode_reward=439.00 +/- 425.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=135000, episode_reward=189.00 +/- 265.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=136000, episode_reward=219.60 +/- 262.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=137000, episode_reward=-87.00 +/- 293.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=138000, episode_reward=196.40 +/- 355.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=139000, episode_reward=224.60 +/- 211.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=140000, episode_reward=208.60 +/- 166.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=141000, episode_reward=76.00 +/- 182.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=142000, episode_reward=108.00 +/- 174.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=143000, episode_reward=97.00 +/- 184.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=144000, episode_reward=-81.40 +/- 219.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=145000, episode_reward=-24.20 +/- 694.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=146000, episode_reward=133.20 +/- 338.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=147000, episode_reward=352.80 +/- 260.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=148000, episode_reward=181.80 +/- 158.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=149000, episode_reward=101.00 +/- 305.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=150000, episode_reward=340.20 +/- 115.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=151000, episode_reward=178.80 +/- 264.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=152000, episode_reward=199.00 +/- 113.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=153000, episode_reward=212.00 +/- 398.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=154000, episode_reward=115.80 +/- 115.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=155000, episode_reward=552.20 +/- 91.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=156000, episode_reward=361.00 +/- 279.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=157000, episode_reward=455.20 +/- 189.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=158000, episode_reward=-29.20 +/- 281.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=159000, episode_reward=169.40 +/- 146.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=160000, episode_reward=93.80 +/- 336.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=161000, episode_reward=293.00 +/- 330.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=162000, episode_reward=356.20 +/- 279.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=163000, episode_reward=409.60 +/- 343.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=164000, episode_reward=442.00 +/- 463.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=165000, episode_reward=455.20 +/- 373.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=166000, episode_reward=488.80 +/- 318.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=167000, episode_reward=333.40 +/- 267.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=168000, episode_reward=539.80 +/- 374.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=169000, episode_reward=18.00 +/- 604.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=170000, episode_reward=611.20 +/- 302.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=171000, episode_reward=505.20 +/- 398.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=172000, episode_reward=505.00 +/- 270.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=173000, episode_reward=4.60 +/- 402.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=174000, episode_reward=988.80 +/- 293.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=175000, episode_reward=540.60 +/- 389.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=176000, episode_reward=206.40 +/- 371.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=177000, episode_reward=700.60 +/- 487.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=178000, episode_reward=832.80 +/- 246.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=179000, episode_reward=731.80 +/- 269.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=180000, episode_reward=401.20 +/- 304.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=181000, episode_reward=92.80 +/- 678.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=182000, episode_reward=82.60 +/- 679.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=183000, episode_reward=562.00 +/- 174.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=184000, episode_reward=752.60 +/- 306.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=185000, episode_reward=725.80 +/- 142.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=186000, episode_reward=406.60 +/- 296.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=187000, episode_reward=659.00 +/- 303.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=188000, episode_reward=271.40 +/- 438.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=189000, episode_reward=577.80 +/- 369.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=190000, episode_reward=645.80 +/- 224.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=191000, episode_reward=23.60 +/- 1548.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=192000, episode_reward=391.60 +/- 290.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=193000, episode_reward=657.80 +/- 379.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=194000, episode_reward=604.60 +/- 226.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=195000, episode_reward=696.20 +/- 399.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=196000, episode_reward=-163.60 +/- 1170.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=197000, episode_reward=639.60 +/- 303.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=198000, episode_reward=741.80 +/- 245.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=199000, episode_reward=776.40 +/- 206.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=200000, episode_reward=180.60 +/- 798.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=201000, episode_reward=540.60 +/- 656.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=202000, episode_reward=-252.20 +/- 1215.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=203000, episode_reward=916.00 +/- 270.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=204000, episode_reward=481.20 +/- 646.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=205000, episode_reward=674.60 +/- 273.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=206000, episode_reward=900.00 +/- 332.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=207000, episode_reward=624.20 +/- 346.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=208000, episode_reward=868.40 +/- 762.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=209000, episode_reward=397.00 +/- 306.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=210000, episode_reward=642.60 +/- 604.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=211000, episode_reward=803.40 +/- 472.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=212000, episode_reward=923.40 +/- 167.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=213000, episode_reward=914.40 +/- 289.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=214000, episode_reward=879.80 +/- 254.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=215000, episode_reward=547.80 +/- 392.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=216000, episode_reward=825.60 +/- 315.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=217000, episode_reward=883.40 +/- 253.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=218000, episode_reward=627.40 +/- 229.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=219000, episode_reward=480.20 +/- 337.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=220000, episode_reward=478.40 +/- 111.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=221000, episode_reward=642.60 +/- 183.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=222000, episode_reward=768.60 +/- 178.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=223000, episode_reward=558.80 +/- 251.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=224000, episode_reward=879.00 +/- 568.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=225000, episode_reward=884.20 +/- 211.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=226000, episode_reward=688.40 +/- 268.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=227000, episode_reward=768.00 +/- 435.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=228000, episode_reward=829.00 +/- 230.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=229000, episode_reward=758.00 +/- 251.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=230000, episode_reward=968.80 +/- 217.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=231000, episode_reward=802.60 +/- 439.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=232000, episode_reward=790.40 +/- 215.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=233000, episode_reward=677.00 +/- 213.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=234000, episode_reward=691.80 +/- 197.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=235000, episode_reward=888.40 +/- 324.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=236000, episode_reward=703.20 +/- 123.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=237000, episode_reward=492.00 +/- 383.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=238000, episode_reward=687.40 +/- 158.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=239000, episode_reward=716.00 +/- 186.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=240000, episode_reward=645.20 +/- 303.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=241000, episode_reward=890.00 +/- 395.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=242000, episode_reward=728.40 +/- 156.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=243000, episode_reward=720.40 +/- 310.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=244000, episode_reward=972.40 +/- 235.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=245000, episode_reward=846.80 +/- 340.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=246000, episode_reward=490.40 +/- 325.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=247000, episode_reward=789.40 +/- 187.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=248000, episode_reward=740.00 +/- 294.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=249000, episode_reward=572.00 +/- 313.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=250000, episode_reward=1062.60 +/- 97.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=251000, episode_reward=1034.80 +/- 315.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=252000, episode_reward=986.20 +/- 277.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=253000, episode_reward=520.00 +/- 138.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=254000, episode_reward=799.80 +/- 392.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=255000, episode_reward=781.40 +/- 402.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=256000, episode_reward=722.40 +/- 356.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=257000, episode_reward=857.80 +/- 208.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=258000, episode_reward=896.40 +/- 268.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=259000, episode_reward=786.80 +/- 321.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=260000, episode_reward=616.40 +/- 296.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=261000, episode_reward=912.20 +/- 286.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=262000, episode_reward=905.20 +/- 358.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=263000, episode_reward=893.00 +/- 284.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=264000, episode_reward=918.20 +/- 302.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=265000, episode_reward=702.80 +/- 256.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=266000, episode_reward=543.60 +/- 241.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=267000, episode_reward=937.40 +/- 239.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=268000, episode_reward=589.20 +/- 454.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=269000, episode_reward=510.60 +/- 205.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=270000, episode_reward=688.80 +/- 64.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=271000, episode_reward=764.20 +/- 333.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=272000, episode_reward=-788.20 +/- 3011.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=273000, episode_reward=598.20 +/- 300.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=274000, episode_reward=915.40 +/- 215.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=275000, episode_reward=754.60 +/- 220.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=276000, episode_reward=801.00 +/- 176.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=277000, episode_reward=337.80 +/- 244.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=278000, episode_reward=530.40 +/- 233.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=279000, episode_reward=825.40 +/- 179.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=280000, episode_reward=788.00 +/- 354.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=281000, episode_reward=758.60 +/- 292.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=282000, episode_reward=498.20 +/- 393.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=283000, episode_reward=-1908.00 +/- 4780.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=284000, episode_reward=676.80 +/- 290.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=285000, episode_reward=716.60 +/- 211.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=286000, episode_reward=702.60 +/- 203.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=287000, episode_reward=751.80 +/- 331.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=288000, episode_reward=487.40 +/- 177.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=289000, episode_reward=734.20 +/- 318.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=290000, episode_reward=732.60 +/- 269.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=291000, episode_reward=482.20 +/- 295.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=292000, episode_reward=800.40 +/- 498.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=293000, episode_reward=643.20 +/- 286.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=294000, episode_reward=389.40 +/- 250.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=295000, episode_reward=601.60 +/- 382.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=296000, episode_reward=699.00 +/- 206.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=297000, episode_reward=706.20 +/- 195.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=298000, episode_reward=431.60 +/- 350.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=299000, episode_reward=695.20 +/- 309.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=300000, episode_reward=-2116.40 +/- 5941.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=301000, episode_reward=635.60 +/- 299.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=302000, episode_reward=964.40 +/- 178.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=303000, episode_reward=896.40 +/- 170.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=304000, episode_reward=701.60 +/- 412.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=305000, episode_reward=544.20 +/- 541.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=306000, episode_reward=614.20 +/- 377.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=307000, episode_reward=548.60 +/- 179.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=308000, episode_reward=722.00 +/- 79.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=309000, episode_reward=293.60 +/- 119.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=310000, episode_reward=474.60 +/- 488.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=311000, episode_reward=614.60 +/- 191.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=312000, episode_reward=694.20 +/- 258.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=313000, episode_reward=568.60 +/- 331.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=314000, episode_reward=365.60 +/- 533.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=315000, episode_reward=672.60 +/- 350.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=316000, episode_reward=745.20 +/- 264.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=317000, episode_reward=557.60 +/- 330.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=318000, episode_reward=563.80 +/- 370.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=319000, episode_reward=655.40 +/- 388.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=320000, episode_reward=598.40 +/- 332.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=321000, episode_reward=1052.20 +/- 240.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=322000, episode_reward=681.20 +/- 328.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=323000, episode_reward=326.80 +/- 371.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=324000, episode_reward=540.00 +/- 297.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=325000, episode_reward=342.80 +/- 193.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=326000, episode_reward=478.80 +/- 158.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=327000, episode_reward=550.80 +/- 255.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=328000, episode_reward=460.40 +/- 326.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=329000, episode_reward=651.20 +/- 112.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=330000, episode_reward=654.00 +/- 352.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=331000, episode_reward=536.80 +/- 198.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=332000, episode_reward=557.20 +/- 326.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=333000, episode_reward=562.80 +/- 266.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=334000, episode_reward=694.40 +/- 221.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=335000, episode_reward=278.20 +/- 413.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=336000, episode_reward=729.80 +/- 122.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=337000, episode_reward=714.80 +/- 266.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=338000, episode_reward=734.20 +/- 116.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=339000, episode_reward=576.60 +/- 254.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=340000, episode_reward=570.60 +/- 435.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=341000, episode_reward=521.20 +/- 256.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=342000, episode_reward=350.40 +/- 355.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=343000, episode_reward=517.40 +/- 339.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=344000, episode_reward=719.00 +/- 232.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=345000, episode_reward=648.00 +/- 370.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=346000, episode_reward=540.40 +/- 255.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=347000, episode_reward=360.60 +/- 121.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=348000, episode_reward=690.80 +/- 264.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=349000, episode_reward=486.80 +/- 414.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=350000, episode_reward=433.00 +/- 402.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=351000, episode_reward=720.00 +/- 65.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=352000, episode_reward=363.60 +/- 493.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=353000, episode_reward=498.00 +/- 329.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=354000, episode_reward=460.00 +/- 393.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=355000, episode_reward=687.00 +/- 428.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=356000, episode_reward=539.60 +/- 98.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=357000, episode_reward=684.40 +/- 363.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=358000, episode_reward=586.40 +/- 300.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=359000, episode_reward=428.60 +/- 85.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=360000, episode_reward=428.20 +/- 331.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=361000, episode_reward=410.00 +/- 235.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=362000, episode_reward=47.80 +/- 184.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=363000, episode_reward=590.00 +/- 203.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=364000, episode_reward=152.40 +/- 174.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=365000, episode_reward=440.00 +/- 325.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=366000, episode_reward=394.60 +/- 94.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=367000, episode_reward=331.20 +/- 108.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=368000, episode_reward=438.20 +/- 261.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=369000, episode_reward=351.80 +/- 283.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=370000, episode_reward=649.00 +/- 348.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=371000, episode_reward=619.20 +/- 138.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=372000, episode_reward=554.20 +/- 195.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=373000, episode_reward=663.80 +/- 357.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=374000, episode_reward=593.00 +/- 195.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=375000, episode_reward=257.80 +/- 213.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=376000, episode_reward=476.00 +/- 336.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=377000, episode_reward=637.80 +/- 387.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=378000, episode_reward=233.00 +/- 234.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=379000, episode_reward=304.60 +/- 209.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=380000, episode_reward=502.60 +/- 318.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=381000, episode_reward=258.60 +/- 453.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=382000, episode_reward=445.80 +/- 336.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=383000, episode_reward=623.80 +/- 336.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=384000, episode_reward=202.80 +/- 230.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=385000, episode_reward=532.80 +/- 147.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=386000, episode_reward=168.00 +/- 258.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=387000, episode_reward=320.40 +/- 391.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=388000, episode_reward=367.60 +/- 173.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=389000, episode_reward=372.80 +/- 252.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=390000, episode_reward=255.00 +/- 203.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=391000, episode_reward=259.20 +/- 492.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=392000, episode_reward=402.20 +/- 348.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=393000, episode_reward=260.40 +/- 415.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=394000, episode_reward=554.60 +/- 275.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=395000, episode_reward=257.60 +/- 282.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=396000, episode_reward=170.60 +/- 274.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=397000, episode_reward=362.00 +/- 325.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=398000, episode_reward=75.60 +/- 340.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=399000, episode_reward=243.40 +/- 421.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=400000, episode_reward=429.60 +/- 301.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=401000, episode_reward=284.40 +/- 407.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=402000, episode_reward=796.60 +/- 397.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=403000, episode_reward=343.80 +/- 148.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=404000, episode_reward=590.80 +/- 234.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=405000, episode_reward=541.40 +/- 230.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=406000, episode_reward=378.00 +/- 364.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=407000, episode_reward=340.00 +/- 275.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=408000, episode_reward=562.80 +/- 253.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=409000, episode_reward=174.00 +/- 424.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=410000, episode_reward=615.60 +/- 270.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=411000, episode_reward=309.00 +/- 324.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=412000, episode_reward=567.40 +/- 126.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=413000, episode_reward=320.00 +/- 336.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=414000, episode_reward=698.60 +/- 407.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=415000, episode_reward=151.00 +/- 371.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=416000, episode_reward=289.80 +/- 199.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=417000, episode_reward=98.00 +/- 237.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=418000, episode_reward=432.60 +/- 434.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=419000, episode_reward=209.20 +/- 359.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=420000, episode_reward=151.80 +/- 130.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=421000, episode_reward=410.80 +/- 503.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=422000, episode_reward=191.60 +/- 418.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=423000, episode_reward=500.60 +/- 319.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=424000, episode_reward=491.00 +/- 313.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=425000, episode_reward=436.80 +/- 481.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=426000, episode_reward=410.00 +/- 220.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=427000, episode_reward=270.80 +/- 318.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=428000, episode_reward=299.40 +/- 218.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=429000, episode_reward=394.00 +/- 320.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=430000, episode_reward=281.20 +/- 432.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=431000, episode_reward=394.20 +/- 365.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=432000, episode_reward=276.20 +/- 341.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=433000, episode_reward=381.80 +/- 198.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=434000, episode_reward=613.00 +/- 250.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=435000, episode_reward=52.80 +/- 215.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=436000, episode_reward=79.80 +/- 237.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=437000, episode_reward=508.80 +/- 352.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=438000, episode_reward=227.40 +/- 328.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=439000, episode_reward=155.80 +/- 163.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=440000, episode_reward=303.40 +/- 325.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=441000, episode_reward=530.80 +/- 146.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=442000, episode_reward=48.00 +/- 211.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=443000, episode_reward=216.80 +/- 265.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=444000, episode_reward=454.20 +/- 226.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=445000, episode_reward=487.80 +/- 292.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=446000, episode_reward=170.80 +/- 435.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=447000, episode_reward=414.60 +/- 271.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=448000, episode_reward=418.40 +/- 364.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=449000, episode_reward=516.00 +/- 206.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=450000, episode_reward=352.20 +/- 360.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=451000, episode_reward=189.00 +/- 258.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=452000, episode_reward=308.20 +/- 275.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=453000, episode_reward=369.40 +/- 318.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=454000, episode_reward=189.80 +/- 432.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=455000, episode_reward=403.40 +/- 173.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=456000, episode_reward=340.20 +/- 158.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=457000, episode_reward=474.00 +/- 215.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=458000, episode_reward=523.00 +/- 178.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=459000, episode_reward=370.80 +/- 293.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=460000, episode_reward=306.00 +/- 112.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=461000, episode_reward=395.00 +/- 370.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=462000, episode_reward=495.00 +/- 389.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=463000, episode_reward=454.80 +/- 403.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=464000, episode_reward=413.80 +/- 481.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=465000, episode_reward=427.20 +/- 115.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=466000, episode_reward=417.00 +/- 299.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=467000, episode_reward=416.40 +/- 230.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=468000, episode_reward=255.80 +/- 276.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=469000, episode_reward=136.40 +/- 197.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=470000, episode_reward=192.00 +/- 256.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=471000, episode_reward=424.80 +/- 132.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=472000, episode_reward=546.60 +/- 255.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=473000, episode_reward=371.20 +/- 437.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=474000, episode_reward=261.20 +/- 429.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=475000, episode_reward=548.20 +/- 265.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=476000, episode_reward=352.00 +/- 150.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=477000, episode_reward=248.00 +/- 262.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=478000, episode_reward=297.40 +/- 278.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=479000, episode_reward=223.20 +/- 471.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=480000, episode_reward=155.00 +/- 359.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=481000, episode_reward=546.20 +/- 267.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=482000, episode_reward=235.60 +/- 159.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=483000, episode_reward=446.80 +/- 314.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=484000, episode_reward=414.40 +/- 261.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=485000, episode_reward=319.20 +/- 218.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=486000, episode_reward=401.00 +/- 166.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=487000, episode_reward=374.60 +/- 192.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=488000, episode_reward=304.20 +/- 296.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=489000, episode_reward=427.40 +/- 164.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=490000, episode_reward=562.40 +/- 265.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=491000, episode_reward=276.40 +/- 373.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=492000, episode_reward=386.40 +/- 244.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=493000, episode_reward=215.80 +/- 369.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=494000, episode_reward=228.20 +/- 251.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=495000, episode_reward=81.40 +/- 400.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=496000, episode_reward=366.20 +/- 128.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=497000, episode_reward=331.60 +/- 284.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=498000, episode_reward=320.60 +/- 270.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=499000, episode_reward=245.40 +/- 360.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=500000, episode_reward=529.40 +/- 140.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Gamma = 0.8, Learning Rate = 0.0003, Activation Function = Tanh, Neurons = 32, Process noise = 0.1, Measurement noise = 0.0\n",
      "Eval num_timesteps=1000, episode_reward=-938.60 +/- 126.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=-914.20 +/- 56.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3000, episode_reward=-1076.00 +/- 104.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=4000, episode_reward=-1009.60 +/- 182.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=5000, episode_reward=-939.60 +/- 69.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=6000, episode_reward=-1036.80 +/- 154.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=7000, episode_reward=-861.20 +/- 114.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=8000, episode_reward=-1025.60 +/- 194.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=9000, episode_reward=-971.20 +/- 180.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=10000, episode_reward=-915.40 +/- 162.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=11000, episode_reward=-951.20 +/- 107.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=12000, episode_reward=-1098.60 +/- 169.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=13000, episode_reward=-1101.40 +/- 148.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=14000, episode_reward=-967.60 +/- 117.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=15000, episode_reward=-1085.60 +/- 149.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=16000, episode_reward=-915.40 +/- 172.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=17000, episode_reward=-925.20 +/- 207.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=18000, episode_reward=-1143.80 +/- 94.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=19000, episode_reward=-985.60 +/- 152.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=20000, episode_reward=-865.40 +/- 225.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=21000, episode_reward=-1036.40 +/- 116.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=22000, episode_reward=-924.80 +/- 155.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=23000, episode_reward=-909.00 +/- 136.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=24000, episode_reward=-1211.60 +/- 176.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=25000, episode_reward=-891.40 +/- 196.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=26000, episode_reward=-837.40 +/- 204.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=27000, episode_reward=-831.20 +/- 133.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=28000, episode_reward=-895.60 +/- 78.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=29000, episode_reward=-870.20 +/- 175.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=30000, episode_reward=-923.80 +/- 319.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=31000, episode_reward=-890.60 +/- 138.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=32000, episode_reward=-1053.60 +/- 183.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=33000, episode_reward=-1020.00 +/- 87.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=34000, episode_reward=-934.80 +/- 103.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=35000, episode_reward=-1099.20 +/- 199.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=36000, episode_reward=-931.20 +/- 116.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=37000, episode_reward=-800.40 +/- 175.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=38000, episode_reward=-1064.80 +/- 216.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=39000, episode_reward=-935.60 +/- 65.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=40000, episode_reward=-1009.20 +/- 106.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=41000, episode_reward=-992.20 +/- 147.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=42000, episode_reward=-866.40 +/- 209.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=43000, episode_reward=-772.20 +/- 220.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=44000, episode_reward=-936.60 +/- 116.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=45000, episode_reward=-893.80 +/- 133.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=46000, episode_reward=-957.80 +/- 90.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=47000, episode_reward=-985.40 +/- 215.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=48000, episode_reward=-901.60 +/- 73.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=49000, episode_reward=-801.40 +/- 252.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=50000, episode_reward=-827.20 +/- 222.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=51000, episode_reward=-5964.80 +/- 1503.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=52000, episode_reward=-2689.40 +/- 618.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=53000, episode_reward=-387.80 +/- 308.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=54000, episode_reward=-416.00 +/- 263.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=55000, episode_reward=-124.40 +/- 392.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=56000, episode_reward=-175.60 +/- 328.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=57000, episode_reward=-393.00 +/- 267.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=58000, episode_reward=-159.60 +/- 165.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=59000, episode_reward=-428.80 +/- 295.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=60000, episode_reward=-224.40 +/- 256.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=61000, episode_reward=-819.80 +/- 272.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=62000, episode_reward=-1178.20 +/- 185.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=63000, episode_reward=-1027.00 +/- 171.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=64000, episode_reward=-978.60 +/- 286.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=65000, episode_reward=-1144.80 +/- 228.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=66000, episode_reward=-530.60 +/- 284.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=67000, episode_reward=-955.40 +/- 217.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=68000, episode_reward=-990.80 +/- 223.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=69000, episode_reward=-598.00 +/- 137.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=70000, episode_reward=-393.00 +/- 172.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=71000, episode_reward=-560.80 +/- 303.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=72000, episode_reward=-591.40 +/- 409.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=73000, episode_reward=-570.40 +/- 215.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=74000, episode_reward=-746.80 +/- 186.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=75000, episode_reward=-305.80 +/- 270.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=76000, episode_reward=-400.80 +/- 109.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=77000, episode_reward=-235.20 +/- 129.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=78000, episode_reward=-151.60 +/- 327.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=79000, episode_reward=-400.00 +/- 333.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=80000, episode_reward=-24.40 +/- 197.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=81000, episode_reward=-238.20 +/- 175.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=82000, episode_reward=-347.20 +/- 158.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=83000, episode_reward=-285.00 +/- 197.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=84000, episode_reward=-318.00 +/- 324.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=85000, episode_reward=-185.60 +/- 259.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=86000, episode_reward=-58.20 +/- 501.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=87000, episode_reward=-115.40 +/- 205.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=88000, episode_reward=27.20 +/- 66.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=89000, episode_reward=123.60 +/- 163.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=90000, episode_reward=0.20 +/- 155.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=91000, episode_reward=31.20 +/- 286.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=92000, episode_reward=-80.60 +/- 228.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=93000, episode_reward=-154.20 +/- 209.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=94000, episode_reward=-60.80 +/- 155.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=95000, episode_reward=152.40 +/- 129.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=96000, episode_reward=-46.20 +/- 127.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=97000, episode_reward=142.60 +/- 172.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=98000, episode_reward=-75.60 +/- 183.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=99000, episode_reward=152.80 +/- 192.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=100000, episode_reward=77.20 +/- 185.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=101000, episode_reward=249.40 +/- 170.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=102000, episode_reward=51.60 +/- 101.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=103000, episode_reward=-63.80 +/- 195.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=104000, episode_reward=86.40 +/- 281.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=105000, episode_reward=81.60 +/- 172.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=106000, episode_reward=26.60 +/- 59.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=107000, episode_reward=91.00 +/- 167.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=108000, episode_reward=35.20 +/- 174.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=109000, episode_reward=73.00 +/- 172.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=110000, episode_reward=212.40 +/- 230.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=111000, episode_reward=140.00 +/- 98.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=112000, episode_reward=-66.60 +/- 115.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=113000, episode_reward=7.80 +/- 126.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=114000, episode_reward=249.20 +/- 223.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=115000, episode_reward=247.60 +/- 274.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=116000, episode_reward=313.80 +/- 199.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=117000, episode_reward=36.80 +/- 62.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=118000, episode_reward=187.40 +/- 200.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=119000, episode_reward=106.40 +/- 207.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=120000, episode_reward=111.20 +/- 167.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=121000, episode_reward=122.40 +/- 67.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=122000, episode_reward=194.60 +/- 125.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=123000, episode_reward=72.40 +/- 120.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=124000, episode_reward=22.60 +/- 167.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=125000, episode_reward=79.60 +/- 136.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=126000, episode_reward=0.40 +/- 116.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=127000, episode_reward=197.20 +/- 142.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=128000, episode_reward=80.20 +/- 139.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=129000, episode_reward=189.60 +/- 217.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=130000, episode_reward=161.80 +/- 187.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=131000, episode_reward=79.80 +/- 197.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=132000, episode_reward=182.40 +/- 205.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=133000, episode_reward=111.60 +/- 187.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=134000, episode_reward=34.20 +/- 193.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=135000, episode_reward=240.40 +/- 91.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=136000, episode_reward=-45.80 +/- 221.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=137000, episode_reward=315.00 +/- 168.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=138000, episode_reward=281.80 +/- 97.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=139000, episode_reward=146.40 +/- 217.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=140000, episode_reward=209.40 +/- 234.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=141000, episode_reward=162.20 +/- 100.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=142000, episode_reward=204.20 +/- 125.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=143000, episode_reward=-91.00 +/- 301.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=144000, episode_reward=102.60 +/- 161.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=145000, episode_reward=220.80 +/- 127.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=146000, episode_reward=195.00 +/- 220.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=147000, episode_reward=208.00 +/- 298.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=148000, episode_reward=254.40 +/- 112.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=149000, episode_reward=185.60 +/- 44.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=150000, episode_reward=300.20 +/- 233.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=151000, episode_reward=252.80 +/- 307.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=152000, episode_reward=217.20 +/- 352.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=153000, episode_reward=83.40 +/- 112.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=154000, episode_reward=215.00 +/- 307.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=155000, episode_reward=340.40 +/- 387.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=156000, episode_reward=296.20 +/- 168.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=157000, episode_reward=445.60 +/- 321.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=158000, episode_reward=340.20 +/- 282.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=159000, episode_reward=677.40 +/- 278.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=160000, episode_reward=489.00 +/- 186.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=161000, episode_reward=398.00 +/- 322.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=162000, episode_reward=589.60 +/- 245.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=163000, episode_reward=692.40 +/- 357.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=164000, episode_reward=1034.80 +/- 141.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=165000, episode_reward=735.40 +/- 325.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=166000, episode_reward=898.00 +/- 330.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=167000, episode_reward=700.80 +/- 298.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=168000, episode_reward=629.00 +/- 309.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=169000, episode_reward=-1605.00 +/- 4728.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=170000, episode_reward=509.20 +/- 305.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=171000, episode_reward=840.80 +/- 244.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=172000, episode_reward=801.40 +/- 510.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=173000, episode_reward=864.00 +/- 176.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=174000, episode_reward=-1824.20 +/- 5313.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=175000, episode_reward=-1590.00 +/- 5021.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=176000, episode_reward=142.00 +/- 1107.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=177000, episode_reward=-1434.00 +/- 4454.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=178000, episode_reward=665.00 +/- 314.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=179000, episode_reward=817.40 +/- 180.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=180000, episode_reward=690.60 +/- 125.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=181000, episode_reward=876.00 +/- 319.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=182000, episode_reward=863.20 +/- 201.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=183000, episode_reward=1056.20 +/- 159.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=184000, episode_reward=1001.20 +/- 373.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=185000, episode_reward=1026.20 +/- 158.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=186000, episode_reward=-1151.40 +/- 4266.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=187000, episode_reward=1034.20 +/- 347.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=188000, episode_reward=800.20 +/- 189.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=189000, episode_reward=614.20 +/- 290.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=190000, episode_reward=-786.00 +/- 3250.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=191000, episode_reward=980.40 +/- 79.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=192000, episode_reward=1107.40 +/- 264.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=193000, episode_reward=-2557.20 +/- 6827.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=194000, episode_reward=923.40 +/- 437.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=195000, episode_reward=580.00 +/- 217.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=196000, episode_reward=964.60 +/- 176.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=197000, episode_reward=730.40 +/- 324.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=198000, episode_reward=671.40 +/- 192.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=199000, episode_reward=979.20 +/- 188.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=200000, episode_reward=769.20 +/- 305.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=201000, episode_reward=1116.00 +/- 126.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=202000, episode_reward=-1119.20 +/- 3685.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=203000, episode_reward=928.20 +/- 532.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=204000, episode_reward=-2773.80 +/- 6274.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=205000, episode_reward=968.20 +/- 226.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=206000, episode_reward=490.60 +/- 441.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=207000, episode_reward=800.40 +/- 244.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=208000, episode_reward=-356.40 +/- 2402.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=209000, episode_reward=774.00 +/- 296.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=210000, episode_reward=977.00 +/- 343.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=211000, episode_reward=695.20 +/- 298.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=212000, episode_reward=-7035.60 +/- 6914.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=213000, episode_reward=348.00 +/- 237.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=214000, episode_reward=784.60 +/- 304.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=215000, episode_reward=736.40 +/- 271.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=216000, episode_reward=593.00 +/- 264.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=217000, episode_reward=767.00 +/- 228.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=218000, episode_reward=721.00 +/- 342.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=219000, episode_reward=579.40 +/- 223.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=220000, episode_reward=704.80 +/- 279.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=221000, episode_reward=812.20 +/- 263.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=222000, episode_reward=716.40 +/- 165.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=223000, episode_reward=516.80 +/- 435.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=224000, episode_reward=768.40 +/- 298.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=225000, episode_reward=996.60 +/- 270.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=226000, episode_reward=632.40 +/- 170.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=227000, episode_reward=644.80 +/- 326.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=228000, episode_reward=710.60 +/- 64.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=229000, episode_reward=711.00 +/- 286.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=230000, episode_reward=736.40 +/- 316.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=231000, episode_reward=714.20 +/- 196.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=232000, episode_reward=920.40 +/- 172.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=233000, episode_reward=828.60 +/- 308.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=234000, episode_reward=453.40 +/- 265.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=235000, episode_reward=876.80 +/- 185.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=236000, episode_reward=605.40 +/- 265.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=237000, episode_reward=661.20 +/- 263.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=238000, episode_reward=1093.00 +/- 287.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=239000, episode_reward=619.40 +/- 390.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=240000, episode_reward=896.60 +/- 268.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=241000, episode_reward=814.60 +/- 227.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=242000, episode_reward=416.00 +/- 175.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=243000, episode_reward=735.60 +/- 317.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=244000, episode_reward=905.20 +/- 578.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=245000, episode_reward=949.80 +/- 265.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=246000, episode_reward=840.60 +/- 128.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=247000, episode_reward=757.00 +/- 247.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=248000, episode_reward=1083.20 +/- 247.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=249000, episode_reward=923.80 +/- 374.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=250000, episode_reward=899.60 +/- 331.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=251000, episode_reward=1160.80 +/- 352.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=252000, episode_reward=682.20 +/- 385.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=253000, episode_reward=682.00 +/- 279.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=254000, episode_reward=949.20 +/- 316.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=255000, episode_reward=734.00 +/- 383.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=256000, episode_reward=927.60 +/- 283.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=257000, episode_reward=874.80 +/- 355.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=258000, episode_reward=920.40 +/- 165.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=259000, episode_reward=956.40 +/- 285.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=260000, episode_reward=919.20 +/- 182.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=261000, episode_reward=1111.40 +/- 339.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=262000, episode_reward=829.20 +/- 429.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=263000, episode_reward=833.20 +/- 251.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=264000, episode_reward=1146.40 +/- 205.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=265000, episode_reward=1153.20 +/- 448.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=266000, episode_reward=1049.80 +/- 378.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=267000, episode_reward=929.20 +/- 341.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=268000, episode_reward=994.00 +/- 323.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=269000, episode_reward=846.60 +/- 365.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=270000, episode_reward=824.20 +/- 288.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=271000, episode_reward=1103.60 +/- 101.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=272000, episode_reward=590.40 +/- 370.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=273000, episode_reward=759.60 +/- 332.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=274000, episode_reward=875.00 +/- 373.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=275000, episode_reward=854.00 +/- 503.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=276000, episode_reward=908.00 +/- 187.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=277000, episode_reward=749.20 +/- 350.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=278000, episode_reward=1067.80 +/- 221.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=279000, episode_reward=812.40 +/- 382.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=280000, episode_reward=955.00 +/- 243.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=281000, episode_reward=1089.60 +/- 236.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=282000, episode_reward=1160.00 +/- 376.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=283000, episode_reward=911.20 +/- 502.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=284000, episode_reward=1135.60 +/- 512.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=285000, episode_reward=846.00 +/- 396.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=286000, episode_reward=1192.80 +/- 255.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=287000, episode_reward=987.00 +/- 357.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=288000, episode_reward=883.40 +/- 331.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=289000, episode_reward=894.00 +/- 444.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=290000, episode_reward=910.40 +/- 245.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=291000, episode_reward=951.00 +/- 259.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=292000, episode_reward=1018.40 +/- 300.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=293000, episode_reward=729.00 +/- 230.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=294000, episode_reward=868.60 +/- 288.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=295000, episode_reward=837.20 +/- 301.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=296000, episode_reward=1159.20 +/- 479.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=297000, episode_reward=782.60 +/- 263.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=298000, episode_reward=976.60 +/- 55.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=299000, episode_reward=828.60 +/- 356.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=300000, episode_reward=1090.00 +/- 129.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=301000, episode_reward=664.20 +/- 141.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=302000, episode_reward=1009.20 +/- 237.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=303000, episode_reward=786.60 +/- 259.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=304000, episode_reward=726.00 +/- 238.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=305000, episode_reward=944.20 +/- 298.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=306000, episode_reward=820.00 +/- 358.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=307000, episode_reward=1002.80 +/- 124.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=308000, episode_reward=497.20 +/- 170.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=309000, episode_reward=474.20 +/- 273.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=310000, episode_reward=643.40 +/- 293.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=311000, episode_reward=673.20 +/- 388.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=312000, episode_reward=1051.20 +/- 436.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=313000, episode_reward=981.20 +/- 280.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=314000, episode_reward=686.60 +/- 299.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=315000, episode_reward=893.40 +/- 181.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=316000, episode_reward=695.40 +/- 386.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=317000, episode_reward=940.80 +/- 170.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=318000, episode_reward=740.20 +/- 254.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=319000, episode_reward=1021.20 +/- 168.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=320000, episode_reward=622.00 +/- 301.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=321000, episode_reward=760.80 +/- 269.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=322000, episode_reward=850.20 +/- 322.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=323000, episode_reward=759.20 +/- 387.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=324000, episode_reward=676.80 +/- 307.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=325000, episode_reward=561.40 +/- 231.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=326000, episode_reward=954.00 +/- 148.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=327000, episode_reward=867.60 +/- 53.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=328000, episode_reward=765.80 +/- 175.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=329000, episode_reward=1106.00 +/- 316.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=330000, episode_reward=970.00 +/- 378.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=331000, episode_reward=893.20 +/- 189.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=332000, episode_reward=891.20 +/- 489.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=333000, episode_reward=1000.80 +/- 171.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=334000, episode_reward=651.60 +/- 231.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=335000, episode_reward=996.00 +/- 228.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=336000, episode_reward=615.40 +/- 187.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=337000, episode_reward=651.60 +/- 311.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=338000, episode_reward=731.20 +/- 282.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=339000, episode_reward=860.60 +/- 438.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=340000, episode_reward=988.00 +/- 404.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=341000, episode_reward=843.20 +/- 228.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=342000, episode_reward=772.00 +/- 331.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=343000, episode_reward=1038.80 +/- 361.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=344000, episode_reward=853.40 +/- 288.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=345000, episode_reward=654.00 +/- 32.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=346000, episode_reward=656.00 +/- 421.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=347000, episode_reward=817.80 +/- 507.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=348000, episode_reward=864.40 +/- 191.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=349000, episode_reward=952.00 +/- 387.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=350000, episode_reward=678.40 +/- 431.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=351000, episode_reward=930.00 +/- 406.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=352000, episode_reward=855.00 +/- 480.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=353000, episode_reward=812.00 +/- 355.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=354000, episode_reward=982.20 +/- 128.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=355000, episode_reward=703.40 +/- 296.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=356000, episode_reward=823.40 +/- 332.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=357000, episode_reward=1145.00 +/- 179.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=358000, episode_reward=783.00 +/- 256.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=359000, episode_reward=1098.40 +/- 309.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=360000, episode_reward=833.20 +/- 235.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=361000, episode_reward=945.80 +/- 377.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=362000, episode_reward=917.20 +/- 274.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=363000, episode_reward=1008.60 +/- 346.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=364000, episode_reward=749.00 +/- 208.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=365000, episode_reward=780.20 +/- 242.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=366000, episode_reward=816.40 +/- 332.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=367000, episode_reward=595.80 +/- 356.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=368000, episode_reward=695.80 +/- 72.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=369000, episode_reward=863.60 +/- 310.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=370000, episode_reward=1092.60 +/- 358.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=371000, episode_reward=764.20 +/- 248.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=372000, episode_reward=933.40 +/- 178.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=373000, episode_reward=832.40 +/- 213.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=374000, episode_reward=880.40 +/- 226.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=375000, episode_reward=894.00 +/- 597.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=376000, episode_reward=948.00 +/- 204.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=377000, episode_reward=939.20 +/- 382.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=378000, episode_reward=918.20 +/- 212.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=379000, episode_reward=1070.20 +/- 341.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=380000, episode_reward=853.20 +/- 292.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=381000, episode_reward=699.00 +/- 400.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=382000, episode_reward=799.20 +/- 274.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=383000, episode_reward=1003.20 +/- 356.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=384000, episode_reward=865.60 +/- 362.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=385000, episode_reward=1095.60 +/- 237.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=386000, episode_reward=971.80 +/- 196.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=387000, episode_reward=1061.40 +/- 219.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=388000, episode_reward=1021.00 +/- 130.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=389000, episode_reward=577.00 +/- 323.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=390000, episode_reward=1164.20 +/- 421.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=391000, episode_reward=704.80 +/- 315.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=392000, episode_reward=955.60 +/- 215.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=393000, episode_reward=763.20 +/- 340.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=394000, episode_reward=882.80 +/- 202.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=395000, episode_reward=984.60 +/- 287.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=396000, episode_reward=846.60 +/- 314.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=397000, episode_reward=758.00 +/- 227.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=398000, episode_reward=807.60 +/- 351.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=399000, episode_reward=959.20 +/- 218.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=400000, episode_reward=1009.60 +/- 206.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=401000, episode_reward=713.80 +/- 290.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=402000, episode_reward=694.60 +/- 283.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=403000, episode_reward=1049.00 +/- 166.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=404000, episode_reward=905.40 +/- 246.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=405000, episode_reward=927.40 +/- 344.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=406000, episode_reward=1066.20 +/- 87.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=407000, episode_reward=844.00 +/- 236.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=408000, episode_reward=864.80 +/- 378.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=409000, episode_reward=732.00 +/- 175.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=410000, episode_reward=754.20 +/- 362.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=411000, episode_reward=712.20 +/- 477.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=412000, episode_reward=558.60 +/- 244.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=413000, episode_reward=1014.00 +/- 260.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=414000, episode_reward=845.40 +/- 196.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=415000, episode_reward=945.20 +/- 293.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=416000, episode_reward=1271.00 +/- 204.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=417000, episode_reward=928.80 +/- 226.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=418000, episode_reward=909.40 +/- 360.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=419000, episode_reward=750.00 +/- 257.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=420000, episode_reward=1159.20 +/- 337.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=421000, episode_reward=1053.40 +/- 172.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=422000, episode_reward=1046.60 +/- 341.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=423000, episode_reward=938.80 +/- 132.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=424000, episode_reward=876.60 +/- 302.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=425000, episode_reward=1039.00 +/- 296.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=426000, episode_reward=962.20 +/- 312.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=427000, episode_reward=720.20 +/- 435.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=428000, episode_reward=934.40 +/- 372.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=429000, episode_reward=809.80 +/- 234.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=430000, episode_reward=945.60 +/- 345.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=431000, episode_reward=903.80 +/- 234.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=432000, episode_reward=963.00 +/- 418.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=433000, episode_reward=898.80 +/- 411.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=434000, episode_reward=774.40 +/- 146.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=435000, episode_reward=1032.20 +/- 320.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=436000, episode_reward=887.20 +/- 360.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=437000, episode_reward=812.20 +/- 358.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=438000, episode_reward=703.00 +/- 145.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=439000, episode_reward=1053.40 +/- 253.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=440000, episode_reward=826.40 +/- 348.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=441000, episode_reward=795.60 +/- 189.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=442000, episode_reward=1024.00 +/- 191.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=443000, episode_reward=644.60 +/- 223.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=444000, episode_reward=1047.00 +/- 321.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=445000, episode_reward=880.60 +/- 178.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=446000, episode_reward=1021.20 +/- 146.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=447000, episode_reward=925.40 +/- 247.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=448000, episode_reward=717.60 +/- 268.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=449000, episode_reward=916.20 +/- 372.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=450000, episode_reward=735.00 +/- 244.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=451000, episode_reward=968.40 +/- 450.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=452000, episode_reward=1036.80 +/- 163.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=453000, episode_reward=855.40 +/- 135.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=454000, episode_reward=888.40 +/- 497.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=455000, episode_reward=989.60 +/- 388.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=456000, episode_reward=497.00 +/- 324.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=457000, episode_reward=901.40 +/- 205.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=458000, episode_reward=872.00 +/- 343.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=459000, episode_reward=986.20 +/- 456.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=460000, episode_reward=1145.00 +/- 231.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=461000, episode_reward=931.40 +/- 347.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=462000, episode_reward=583.60 +/- 201.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=463000, episode_reward=1043.00 +/- 388.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=464000, episode_reward=1128.20 +/- 292.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=465000, episode_reward=1045.00 +/- 319.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=466000, episode_reward=1192.80 +/- 167.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=467000, episode_reward=840.40 +/- 267.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=468000, episode_reward=1046.40 +/- 311.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=469000, episode_reward=859.40 +/- 311.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=470000, episode_reward=780.00 +/- 259.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=471000, episode_reward=799.60 +/- 394.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=472000, episode_reward=713.00 +/- 411.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=473000, episode_reward=1064.60 +/- 318.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=474000, episode_reward=920.80 +/- 173.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=475000, episode_reward=898.00 +/- 508.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=476000, episode_reward=940.60 +/- 298.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=477000, episode_reward=808.20 +/- 186.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=478000, episode_reward=984.40 +/- 56.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=479000, episode_reward=803.80 +/- 225.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=480000, episode_reward=995.80 +/- 266.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=481000, episode_reward=765.80 +/- 292.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=482000, episode_reward=940.00 +/- 192.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=483000, episode_reward=1128.60 +/- 442.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=484000, episode_reward=1030.80 +/- 410.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=485000, episode_reward=906.00 +/- 250.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=486000, episode_reward=789.40 +/- 451.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=487000, episode_reward=846.80 +/- 196.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=488000, episode_reward=973.80 +/- 260.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=489000, episode_reward=1010.40 +/- 336.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=490000, episode_reward=898.60 +/- 183.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=491000, episode_reward=836.20 +/- 236.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=492000, episode_reward=1048.20 +/- 668.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=493000, episode_reward=874.40 +/- 152.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=494000, episode_reward=847.60 +/- 326.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=495000, episode_reward=863.00 +/- 506.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=496000, episode_reward=1075.60 +/- 367.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=497000, episode_reward=834.20 +/- 305.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=498000, episode_reward=1043.40 +/- 356.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=499000, episode_reward=1146.20 +/- 89.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=500000, episode_reward=1143.60 +/- 321.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Gamma = 0.8, Learning Rate = 0.0003, Activation Function = Tanh, Neurons = 64, Process noise = 0.1, Measurement noise = 0.0\n",
      "Eval num_timesteps=1000, episode_reward=-2155.80 +/- 383.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=-2329.40 +/- 130.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=3000, episode_reward=-2252.40 +/- 124.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=4000, episode_reward=-2166.00 +/- 156.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=5000, episode_reward=-2333.00 +/- 166.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=6000, episode_reward=-2171.40 +/- 352.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=7000, episode_reward=-2259.00 +/- 202.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=8000, episode_reward=-2286.00 +/- 266.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=9000, episode_reward=-2338.40 +/- 62.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=10000, episode_reward=-2137.00 +/- 196.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=11000, episode_reward=-2078.20 +/- 361.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=12000, episode_reward=-2166.60 +/- 167.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=13000, episode_reward=-2253.80 +/- 245.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=14000, episode_reward=-2291.40 +/- 141.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=15000, episode_reward=-2101.40 +/- 201.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=16000, episode_reward=-2304.20 +/- 156.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=17000, episode_reward=-2253.20 +/- 141.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=18000, episode_reward=-2273.40 +/- 140.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=19000, episode_reward=-2013.60 +/- 197.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=20000, episode_reward=-2392.80 +/- 91.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=21000, episode_reward=-1804.40 +/- 486.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=22000, episode_reward=-2344.20 +/- 133.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=23000, episode_reward=-2123.40 +/- 176.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=24000, episode_reward=-2199.40 +/- 166.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=25000, episode_reward=-2187.60 +/- 312.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=26000, episode_reward=-1996.80 +/- 357.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=27000, episode_reward=-2294.20 +/- 85.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=28000, episode_reward=-2129.40 +/- 405.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=29000, episode_reward=-2312.00 +/- 220.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=30000, episode_reward=-2342.60 +/- 155.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=31000, episode_reward=-2164.40 +/- 89.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=32000, episode_reward=-2194.20 +/- 255.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=33000, episode_reward=-2038.60 +/- 256.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=34000, episode_reward=-2081.80 +/- 369.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=35000, episode_reward=-2285.60 +/- 126.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=36000, episode_reward=-2319.80 +/- 87.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=37000, episode_reward=-2360.00 +/- 142.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=38000, episode_reward=-2168.80 +/- 156.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=39000, episode_reward=-2125.20 +/- 119.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=40000, episode_reward=-2206.60 +/- 159.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=41000, episode_reward=-2214.00 +/- 200.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=42000, episode_reward=-2153.00 +/- 266.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=43000, episode_reward=-2383.20 +/- 176.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=44000, episode_reward=-2238.60 +/- 65.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=45000, episode_reward=-2281.00 +/- 88.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=46000, episode_reward=-2244.00 +/- 293.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=47000, episode_reward=-2249.60 +/- 347.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=48000, episode_reward=-2209.40 +/- 207.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=49000, episode_reward=-2042.60 +/- 262.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=50000, episode_reward=-2276.80 +/- 174.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=51000, episode_reward=-364.80 +/- 441.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=52000, episode_reward=-191.40 +/- 232.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=53000, episode_reward=-65.00 +/- 538.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=54000, episode_reward=-158.60 +/- 297.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=55000, episode_reward=-764.40 +/- 257.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=56000, episode_reward=-1694.80 +/- 530.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=57000, episode_reward=-517.80 +/- 102.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=58000, episode_reward=-560.00 +/- 79.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=59000, episode_reward=-295.20 +/- 248.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=60000, episode_reward=-205.60 +/- 277.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=61000, episode_reward=-673.80 +/- 292.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=62000, episode_reward=-354.20 +/- 250.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=63000, episode_reward=-18.00 +/- 147.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=64000, episode_reward=-652.20 +/- 229.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=65000, episode_reward=-406.80 +/- 230.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=66000, episode_reward=-167.40 +/- 84.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=67000, episode_reward=48.00 +/- 174.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=68000, episode_reward=-204.20 +/- 175.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=69000, episode_reward=-242.20 +/- 209.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=70000, episode_reward=-277.80 +/- 160.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=71000, episode_reward=-164.00 +/- 229.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=72000, episode_reward=-284.40 +/- 361.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=73000, episode_reward=-25.20 +/- 125.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=74000, episode_reward=-166.00 +/- 195.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=75000, episode_reward=52.20 +/- 189.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=76000, episode_reward=-118.00 +/- 182.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=77000, episode_reward=79.40 +/- 206.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=78000, episode_reward=26.80 +/- 204.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=79000, episode_reward=-142.60 +/- 190.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=80000, episode_reward=123.60 +/- 270.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=81000, episode_reward=39.40 +/- 275.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=82000, episode_reward=129.40 +/- 256.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=83000, episode_reward=-20.60 +/- 201.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=84000, episode_reward=34.40 +/- 189.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=85000, episode_reward=-258.40 +/- 149.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=86000, episode_reward=190.40 +/- 94.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=87000, episode_reward=286.40 +/- 113.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=88000, episode_reward=207.80 +/- 194.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=89000, episode_reward=96.60 +/- 105.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=90000, episode_reward=111.80 +/- 56.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=91000, episode_reward=50.00 +/- 198.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=92000, episode_reward=-44.80 +/- 214.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=93000, episode_reward=94.20 +/- 191.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=94000, episode_reward=2.60 +/- 124.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=95000, episode_reward=255.20 +/- 135.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=96000, episode_reward=202.80 +/- 211.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=97000, episode_reward=208.60 +/- 301.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=98000, episode_reward=-1.00 +/- 159.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=99000, episode_reward=8.00 +/- 177.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=100000, episode_reward=167.00 +/- 80.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=101000, episode_reward=270.60 +/- 120.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=102000, episode_reward=157.40 +/- 247.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=103000, episode_reward=248.40 +/- 206.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=104000, episode_reward=-57.60 +/- 147.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=105000, episode_reward=274.40 +/- 136.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=106000, episode_reward=349.00 +/- 141.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=107000, episode_reward=363.20 +/- 137.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=108000, episode_reward=148.00 +/- 117.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=109000, episode_reward=269.20 +/- 58.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=110000, episode_reward=228.00 +/- 255.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=111000, episode_reward=349.00 +/- 127.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=112000, episode_reward=175.20 +/- 116.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=113000, episode_reward=226.00 +/- 84.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=114000, episode_reward=216.20 +/- 218.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=115000, episode_reward=250.20 +/- 215.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=116000, episode_reward=96.60 +/- 143.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=117000, episode_reward=-16.80 +/- 184.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=118000, episode_reward=130.80 +/- 235.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=119000, episode_reward=138.60 +/- 188.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=120000, episode_reward=316.00 +/- 142.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=121000, episode_reward=159.40 +/- 213.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=122000, episode_reward=312.20 +/- 301.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=123000, episode_reward=251.20 +/- 63.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=124000, episode_reward=174.20 +/- 262.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=125000, episode_reward=222.40 +/- 198.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=126000, episode_reward=317.00 +/- 245.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=127000, episode_reward=368.40 +/- 326.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=128000, episode_reward=336.80 +/- 181.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=129000, episode_reward=434.40 +/- 101.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=130000, episode_reward=277.80 +/- 383.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=131000, episode_reward=35.20 +/- 198.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=132000, episode_reward=289.40 +/- 220.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=133000, episode_reward=293.80 +/- 140.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=134000, episode_reward=505.60 +/- 294.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=135000, episode_reward=513.80 +/- 256.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=136000, episode_reward=619.40 +/- 216.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=137000, episode_reward=789.20 +/- 377.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=138000, episode_reward=777.80 +/- 386.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=139000, episode_reward=506.80 +/- 573.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=140000, episode_reward=813.00 +/- 254.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=141000, episode_reward=482.60 +/- 271.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=142000, episode_reward=827.00 +/- 341.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=143000, episode_reward=909.20 +/- 249.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=144000, episode_reward=412.80 +/- 423.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=145000, episode_reward=713.00 +/- 222.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=146000, episode_reward=778.80 +/- 515.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=147000, episode_reward=573.40 +/- 232.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=148000, episode_reward=864.20 +/- 254.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=149000, episode_reward=588.80 +/- 399.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=150000, episode_reward=734.80 +/- 241.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=151000, episode_reward=928.60 +/- 202.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=152000, episode_reward=853.80 +/- 202.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=153000, episode_reward=493.00 +/- 105.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=154000, episode_reward=575.40 +/- 224.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=155000, episode_reward=689.60 +/- 206.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=156000, episode_reward=919.20 +/- 244.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=157000, episode_reward=625.00 +/- 296.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=158000, episode_reward=398.60 +/- 254.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=159000, episode_reward=712.80 +/- 428.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=160000, episode_reward=582.80 +/- 180.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=161000, episode_reward=579.80 +/- 285.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=162000, episode_reward=655.40 +/- 260.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=163000, episode_reward=561.80 +/- 364.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=164000, episode_reward=584.40 +/- 339.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=165000, episode_reward=958.00 +/- 295.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=166000, episode_reward=770.00 +/- 128.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=167000, episode_reward=671.20 +/- 224.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=168000, episode_reward=541.20 +/- 247.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=169000, episode_reward=531.00 +/- 330.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=170000, episode_reward=665.20 +/- 264.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=171000, episode_reward=729.40 +/- 269.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=172000, episode_reward=989.40 +/- 402.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=173000, episode_reward=771.40 +/- 408.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=174000, episode_reward=741.00 +/- 338.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=175000, episode_reward=691.00 +/- 227.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=176000, episode_reward=794.80 +/- 406.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=177000, episode_reward=502.60 +/- 223.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=178000, episode_reward=863.80 +/- 183.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=179000, episode_reward=839.80 +/- 225.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=180000, episode_reward=659.20 +/- 88.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=181000, episode_reward=699.80 +/- 182.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=182000, episode_reward=669.40 +/- 88.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=183000, episode_reward=796.80 +/- 27.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=184000, episode_reward=814.60 +/- 337.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=185000, episode_reward=620.60 +/- 185.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=186000, episode_reward=453.00 +/- 204.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=187000, episode_reward=751.40 +/- 250.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=188000, episode_reward=548.00 +/- 274.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=189000, episode_reward=509.20 +/- 219.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=190000, episode_reward=678.80 +/- 399.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=191000, episode_reward=1068.00 +/- 369.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=192000, episode_reward=931.00 +/- 243.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=193000, episode_reward=810.60 +/- 587.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=194000, episode_reward=564.80 +/- 339.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=195000, episode_reward=624.20 +/- 304.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=196000, episode_reward=595.80 +/- 267.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=197000, episode_reward=69.80 +/- 1246.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=198000, episode_reward=977.00 +/- 243.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=199000, episode_reward=876.20 +/- 230.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=200000, episode_reward=728.60 +/- 212.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=201000, episode_reward=900.80 +/- 382.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=202000, episode_reward=1038.20 +/- 460.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=203000, episode_reward=1004.00 +/- 229.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=204000, episode_reward=1055.80 +/- 207.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=205000, episode_reward=660.80 +/- 386.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=206000, episode_reward=1036.80 +/- 217.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=207000, episode_reward=862.00 +/- 48.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=208000, episode_reward=703.60 +/- 310.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=209000, episode_reward=844.40 +/- 239.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=210000, episode_reward=941.20 +/- 304.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=211000, episode_reward=999.20 +/- 408.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=212000, episode_reward=1025.20 +/- 315.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=213000, episode_reward=548.40 +/- 423.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=214000, episode_reward=816.00 +/- 321.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=215000, episode_reward=729.80 +/- 85.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=216000, episode_reward=633.20 +/- 143.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=217000, episode_reward=754.40 +/- 275.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=218000, episode_reward=572.40 +/- 525.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=219000, episode_reward=830.60 +/- 214.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=220000, episode_reward=739.00 +/- 519.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=221000, episode_reward=1130.20 +/- 219.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=222000, episode_reward=729.00 +/- 462.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=223000, episode_reward=856.60 +/- 103.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=224000, episode_reward=935.00 +/- 287.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=225000, episode_reward=1084.80 +/- 322.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=226000, episode_reward=974.00 +/- 201.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=227000, episode_reward=881.00 +/- 197.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=228000, episode_reward=791.40 +/- 184.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=229000, episode_reward=1052.40 +/- 124.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=230000, episode_reward=743.60 +/- 332.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=231000, episode_reward=966.20 +/- 305.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=232000, episode_reward=1131.00 +/- 188.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=233000, episode_reward=896.60 +/- 148.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=234000, episode_reward=1234.00 +/- 277.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=235000, episode_reward=1094.20 +/- 338.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=236000, episode_reward=1140.00 +/- 330.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=237000, episode_reward=800.20 +/- 227.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=238000, episode_reward=729.40 +/- 243.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=239000, episode_reward=810.40 +/- 373.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=240000, episode_reward=1073.00 +/- 334.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=241000, episode_reward=962.00 +/- 313.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=242000, episode_reward=772.40 +/- 170.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=243000, episode_reward=1191.80 +/- 291.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=244000, episode_reward=1024.80 +/- 317.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=245000, episode_reward=740.20 +/- 276.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=246000, episode_reward=938.60 +/- 482.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=247000, episode_reward=1022.60 +/- 320.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=248000, episode_reward=838.20 +/- 306.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=249000, episode_reward=1116.00 +/- 278.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=250000, episode_reward=789.00 +/- 243.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=251000, episode_reward=1070.20 +/- 228.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=252000, episode_reward=931.40 +/- 382.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=253000, episode_reward=1071.60 +/- 207.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=254000, episode_reward=895.60 +/- 367.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=255000, episode_reward=982.00 +/- 237.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=256000, episode_reward=956.40 +/- 312.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=257000, episode_reward=635.00 +/- 214.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=258000, episode_reward=1006.20 +/- 503.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=259000, episode_reward=993.80 +/- 196.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=260000, episode_reward=849.40 +/- 319.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=261000, episode_reward=995.00 +/- 263.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=262000, episode_reward=1024.80 +/- 290.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=263000, episode_reward=1058.20 +/- 294.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=264000, episode_reward=1072.80 +/- 477.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=265000, episode_reward=722.80 +/- 287.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=266000, episode_reward=895.00 +/- 158.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=267000, episode_reward=789.20 +/- 261.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=268000, episode_reward=836.60 +/- 256.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=269000, episode_reward=873.20 +/- 194.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=270000, episode_reward=938.60 +/- 356.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=271000, episode_reward=674.20 +/- 441.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=272000, episode_reward=1106.00 +/- 296.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=273000, episode_reward=875.20 +/- 327.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=274000, episode_reward=1124.40 +/- 180.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=275000, episode_reward=739.60 +/- 330.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=276000, episode_reward=1096.00 +/- 188.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=277000, episode_reward=1003.20 +/- 133.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=278000, episode_reward=950.80 +/- 459.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=279000, episode_reward=1281.60 +/- 245.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=280000, episode_reward=859.40 +/- 397.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=281000, episode_reward=890.40 +/- 211.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=282000, episode_reward=954.40 +/- 252.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=283000, episode_reward=796.20 +/- 198.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=284000, episode_reward=765.80 +/- 376.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=285000, episode_reward=962.20 +/- 464.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=286000, episode_reward=991.80 +/- 205.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=287000, episode_reward=799.20 +/- 186.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=288000, episode_reward=968.00 +/- 498.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=289000, episode_reward=1022.00 +/- 338.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=290000, episode_reward=1019.20 +/- 296.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=291000, episode_reward=1010.60 +/- 179.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=292000, episode_reward=1196.40 +/- 162.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=293000, episode_reward=1024.40 +/- 264.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=294000, episode_reward=1075.60 +/- 399.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=295000, episode_reward=1014.40 +/- 296.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=296000, episode_reward=918.80 +/- 382.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=297000, episode_reward=-2615.80 +/- 6872.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=298000, episode_reward=912.40 +/- 344.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=299000, episode_reward=1039.60 +/- 452.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=300000, episode_reward=845.20 +/- 491.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=301000, episode_reward=785.20 +/- 249.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=302000, episode_reward=811.40 +/- 160.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=303000, episode_reward=857.80 +/- 164.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=304000, episode_reward=844.80 +/- 212.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=305000, episode_reward=915.80 +/- 242.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=306000, episode_reward=1074.20 +/- 235.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=307000, episode_reward=856.40 +/- 301.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=308000, episode_reward=717.60 +/- 347.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=309000, episode_reward=1185.80 +/- 292.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=310000, episode_reward=1089.20 +/- 120.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=311000, episode_reward=1072.80 +/- 262.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=312000, episode_reward=979.20 +/- 112.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=313000, episode_reward=872.40 +/- 265.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=314000, episode_reward=579.00 +/- 413.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=315000, episode_reward=853.40 +/- 246.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=316000, episode_reward=735.40 +/- 321.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=317000, episode_reward=961.40 +/- 345.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=318000, episode_reward=743.60 +/- 140.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=319000, episode_reward=602.60 +/- 301.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=320000, episode_reward=1040.40 +/- 328.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=321000, episode_reward=796.00 +/- 193.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=322000, episode_reward=1065.20 +/- 304.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=323000, episode_reward=950.40 +/- 348.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=324000, episode_reward=1296.40 +/- 326.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=325000, episode_reward=1220.60 +/- 400.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=326000, episode_reward=1036.80 +/- 228.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=327000, episode_reward=1192.20 +/- 248.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=328000, episode_reward=981.80 +/- 284.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=329000, episode_reward=910.40 +/- 409.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=330000, episode_reward=864.00 +/- 217.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=331000, episode_reward=1084.80 +/- 141.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=332000, episode_reward=1187.00 +/- 189.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=333000, episode_reward=1110.00 +/- 215.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=334000, episode_reward=1079.40 +/- 230.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=335000, episode_reward=1161.60 +/- 315.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=336000, episode_reward=886.80 +/- 244.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=337000, episode_reward=1114.40 +/- 290.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=338000, episode_reward=770.20 +/- 311.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=339000, episode_reward=1064.40 +/- 322.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=340000, episode_reward=1285.00 +/- 308.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=341000, episode_reward=726.00 +/- 289.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=342000, episode_reward=1231.00 +/- 136.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=343000, episode_reward=973.60 +/- 331.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=344000, episode_reward=978.80 +/- 442.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=345000, episode_reward=826.80 +/- 353.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=346000, episode_reward=1219.80 +/- 57.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=347000, episode_reward=1065.40 +/- 131.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=348000, episode_reward=840.60 +/- 106.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=349000, episode_reward=941.80 +/- 218.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=350000, episode_reward=952.20 +/- 256.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=351000, episode_reward=1329.00 +/- 254.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=352000, episode_reward=927.40 +/- 248.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=353000, episode_reward=1212.40 +/- 214.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=354000, episode_reward=1156.00 +/- 243.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=355000, episode_reward=1086.40 +/- 320.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=356000, episode_reward=1163.20 +/- 225.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=357000, episode_reward=1036.80 +/- 411.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=358000, episode_reward=1256.80 +/- 370.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=359000, episode_reward=1071.60 +/- 201.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=360000, episode_reward=1174.80 +/- 228.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=361000, episode_reward=952.00 +/- 244.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=362000, episode_reward=929.60 +/- 231.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=363000, episode_reward=1223.80 +/- 250.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=364000, episode_reward=1149.60 +/- 157.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=365000, episode_reward=1139.00 +/- 162.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=366000, episode_reward=955.60 +/- 330.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=367000, episode_reward=1092.60 +/- 303.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=368000, episode_reward=1221.40 +/- 336.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=369000, episode_reward=966.80 +/- 253.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=370000, episode_reward=1171.80 +/- 313.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=371000, episode_reward=1098.80 +/- 359.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=372000, episode_reward=1121.80 +/- 181.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=373000, episode_reward=1038.60 +/- 260.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=374000, episode_reward=1040.60 +/- 516.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=375000, episode_reward=870.40 +/- 260.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=376000, episode_reward=1231.20 +/- 208.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=377000, episode_reward=899.00 +/- 195.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=378000, episode_reward=1284.40 +/- 431.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=379000, episode_reward=1258.40 +/- 286.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=380000, episode_reward=947.20 +/- 247.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=381000, episode_reward=821.20 +/- 63.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=382000, episode_reward=1323.20 +/- 238.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=383000, episode_reward=1222.00 +/- 609.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=384000, episode_reward=967.00 +/- 105.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=385000, episode_reward=1089.80 +/- 294.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=386000, episode_reward=796.20 +/- 326.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=387000, episode_reward=1094.20 +/- 471.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=388000, episode_reward=1477.20 +/- 221.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=389000, episode_reward=1496.00 +/- 243.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=390000, episode_reward=1057.00 +/- 71.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=391000, episode_reward=1164.60 +/- 239.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=392000, episode_reward=1044.00 +/- 389.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=393000, episode_reward=1075.20 +/- 138.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=394000, episode_reward=1249.80 +/- 236.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=395000, episode_reward=844.00 +/- 208.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=396000, episode_reward=1196.00 +/- 302.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=397000, episode_reward=926.40 +/- 203.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=398000, episode_reward=1045.40 +/- 172.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=399000, episode_reward=934.80 +/- 189.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=400000, episode_reward=949.20 +/- 228.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=401000, episode_reward=968.80 +/- 200.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=402000, episode_reward=684.40 +/- 143.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=403000, episode_reward=930.20 +/- 193.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=404000, episode_reward=1076.20 +/- 103.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=405000, episode_reward=1167.80 +/- 227.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=406000, episode_reward=1152.80 +/- 263.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=407000, episode_reward=839.40 +/- 378.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=408000, episode_reward=783.80 +/- 329.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=409000, episode_reward=814.80 +/- 234.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=410000, episode_reward=1267.00 +/- 166.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=411000, episode_reward=1068.20 +/- 267.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=412000, episode_reward=1172.00 +/- 259.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=413000, episode_reward=1175.60 +/- 231.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=414000, episode_reward=1029.60 +/- 250.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=415000, episode_reward=896.60 +/- 221.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=416000, episode_reward=963.60 +/- 301.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=417000, episode_reward=1129.80 +/- 342.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=418000, episode_reward=1055.80 +/- 310.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=419000, episode_reward=1049.60 +/- 454.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=420000, episode_reward=1119.40 +/- 174.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=421000, episode_reward=900.80 +/- 505.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=422000, episode_reward=933.80 +/- 366.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=423000, episode_reward=851.80 +/- 293.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=424000, episode_reward=866.60 +/- 276.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=425000, episode_reward=1169.00 +/- 183.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=426000, episode_reward=1136.20 +/- 264.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=427000, episode_reward=1002.40 +/- 468.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=428000, episode_reward=1117.60 +/- 373.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=429000, episode_reward=1070.00 +/- 359.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=430000, episode_reward=871.20 +/- 267.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=431000, episode_reward=1083.00 +/- 306.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=432000, episode_reward=944.20 +/- 231.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=433000, episode_reward=977.40 +/- 262.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=434000, episode_reward=951.20 +/- 282.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=435000, episode_reward=1119.00 +/- 152.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=436000, episode_reward=990.20 +/- 220.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=437000, episode_reward=680.20 +/- 502.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=438000, episode_reward=752.80 +/- 183.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=439000, episode_reward=949.40 +/- 244.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=440000, episode_reward=1134.80 +/- 336.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=441000, episode_reward=838.60 +/- 242.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=442000, episode_reward=969.60 +/- 246.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=443000, episode_reward=1017.60 +/- 211.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=444000, episode_reward=1116.40 +/- 204.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=445000, episode_reward=1293.60 +/- 261.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=446000, episode_reward=1105.00 +/- 441.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=447000, episode_reward=1186.40 +/- 262.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=448000, episode_reward=950.80 +/- 213.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=449000, episode_reward=693.00 +/- 359.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=450000, episode_reward=1036.00 +/- 431.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=451000, episode_reward=1094.00 +/- 241.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=452000, episode_reward=1370.60 +/- 207.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=453000, episode_reward=1038.00 +/- 382.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=454000, episode_reward=848.00 +/- 445.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=455000, episode_reward=1091.00 +/- 348.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=456000, episode_reward=1111.20 +/- 224.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=457000, episode_reward=1191.40 +/- 302.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=458000, episode_reward=1081.20 +/- 169.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=459000, episode_reward=1226.00 +/- 444.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=460000, episode_reward=993.20 +/- 260.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=461000, episode_reward=1073.80 +/- 261.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=462000, episode_reward=765.80 +/- 210.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=463000, episode_reward=1025.20 +/- 294.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=464000, episode_reward=1039.00 +/- 254.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=465000, episode_reward=920.20 +/- 388.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=466000, episode_reward=941.60 +/- 221.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=467000, episode_reward=814.20 +/- 340.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=468000, episode_reward=899.00 +/- 495.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=469000, episode_reward=1031.20 +/- 368.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=470000, episode_reward=1089.00 +/- 191.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=471000, episode_reward=1024.20 +/- 318.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=472000, episode_reward=692.20 +/- 210.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=473000, episode_reward=1153.00 +/- 320.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=474000, episode_reward=1001.20 +/- 202.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=475000, episode_reward=1004.20 +/- 142.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=476000, episode_reward=1100.40 +/- 370.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=477000, episode_reward=704.20 +/- 191.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=478000, episode_reward=1278.60 +/- 316.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=479000, episode_reward=969.00 +/- 338.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=480000, episode_reward=1033.80 +/- 386.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=481000, episode_reward=1177.20 +/- 130.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=482000, episode_reward=1042.60 +/- 377.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=483000, episode_reward=1025.80 +/- 224.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=484000, episode_reward=733.20 +/- 470.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=485000, episode_reward=1059.40 +/- 313.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=486000, episode_reward=917.40 +/- 399.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=487000, episode_reward=989.00 +/- 318.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=488000, episode_reward=978.60 +/- 353.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=489000, episode_reward=987.60 +/- 392.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=490000, episode_reward=910.20 +/- 282.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=491000, episode_reward=1178.80 +/- 338.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=492000, episode_reward=1044.40 +/- 255.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=493000, episode_reward=1036.40 +/- 194.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=494000, episode_reward=915.80 +/- 167.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=495000, episode_reward=1162.00 +/- 227.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=496000, episode_reward=1114.20 +/- 346.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=497000, episode_reward=923.80 +/- 404.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=498000, episode_reward=1334.80 +/- 316.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=499000, episode_reward=1051.40 +/- 416.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=500000, episode_reward=986.40 +/- 182.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Gamma = 0.8, Learning Rate = 0.0003, Activation Function = ReLU, Neurons = 8, Process noise = 0.1, Measurement noise = 0.0\n",
      "Eval num_timesteps=1000, episode_reward=40.40 +/- 148.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=19.60 +/- 47.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=3000, episode_reward=66.40 +/- 70.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4000, episode_reward=161.80 +/- 123.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=5000, episode_reward=-61.20 +/- 184.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=6000, episode_reward=-31.60 +/- 208.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=7000, episode_reward=106.40 +/- 151.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=8000, episode_reward=123.60 +/- 163.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=9000, episode_reward=73.60 +/- 193.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=10000, episode_reward=120.60 +/- 114.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=11000, episode_reward=114.60 +/- 41.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=12000, episode_reward=80.40 +/- 133.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=13000, episode_reward=19.00 +/- 153.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=14000, episode_reward=125.60 +/- 136.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=15000, episode_reward=69.40 +/- 111.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=16000, episode_reward=39.80 +/- 146.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=17000, episode_reward=11.40 +/- 110.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=18000, episode_reward=31.80 +/- 163.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=19000, episode_reward=101.00 +/- 100.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=20000, episode_reward=55.60 +/- 167.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=21000, episode_reward=-81.80 +/- 226.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=22000, episode_reward=62.00 +/- 141.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=23000, episode_reward=50.20 +/- 159.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=24000, episode_reward=-31.00 +/- 171.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=25000, episode_reward=88.00 +/- 163.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=26000, episode_reward=44.60 +/- 162.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=27000, episode_reward=94.60 +/- 166.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=28000, episode_reward=175.00 +/- 65.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=29000, episode_reward=39.80 +/- 201.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=30000, episode_reward=81.60 +/- 175.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=31000, episode_reward=65.40 +/- 210.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=32000, episode_reward=110.40 +/- 120.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=33000, episode_reward=97.60 +/- 171.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=34000, episode_reward=40.20 +/- 131.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=35000, episode_reward=77.20 +/- 142.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=36000, episode_reward=73.00 +/- 213.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=37000, episode_reward=111.40 +/- 164.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=38000, episode_reward=12.80 +/- 155.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=39000, episode_reward=-65.40 +/- 160.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=40000, episode_reward=-1.80 +/- 143.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=41000, episode_reward=-6.80 +/- 136.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=42000, episode_reward=-27.60 +/- 193.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=43000, episode_reward=138.00 +/- 146.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=44000, episode_reward=54.20 +/- 209.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=45000, episode_reward=79.60 +/- 121.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=46000, episode_reward=122.60 +/- 169.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=47000, episode_reward=158.00 +/- 148.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=48000, episode_reward=-123.80 +/- 200.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=49000, episode_reward=-71.60 +/- 151.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=50000, episode_reward=3.80 +/- 193.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=51000, episode_reward=-7.20 +/- 118.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=52000, episode_reward=99.40 +/- 204.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=53000, episode_reward=70.20 +/- 183.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=54000, episode_reward=67.00 +/- 163.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=55000, episode_reward=-29.60 +/- 154.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=56000, episode_reward=163.80 +/- 136.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=57000, episode_reward=-95.60 +/- 312.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=58000, episode_reward=-1093.20 +/- 689.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=59000, episode_reward=-2995.60 +/- 278.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=60000, episode_reward=-1313.60 +/- 235.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=61000, episode_reward=-118.00 +/- 152.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=62000, episode_reward=38.80 +/- 119.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=63000, episode_reward=203.40 +/- 182.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=64000, episode_reward=161.80 +/- 123.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=65000, episode_reward=182.60 +/- 80.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=66000, episode_reward=196.00 +/- 168.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=67000, episode_reward=10.00 +/- 274.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=68000, episode_reward=254.00 +/- 207.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=69000, episode_reward=246.80 +/- 99.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=70000, episode_reward=300.40 +/- 212.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=71000, episode_reward=9.40 +/- 231.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=72000, episode_reward=178.80 +/- 50.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=73000, episode_reward=68.40 +/- 71.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=74000, episode_reward=64.80 +/- 151.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=75000, episode_reward=220.80 +/- 193.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=76000, episode_reward=32.40 +/- 153.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=77000, episode_reward=172.40 +/- 306.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=78000, episode_reward=57.60 +/- 288.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=79000, episode_reward=226.20 +/- 78.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=80000, episode_reward=42.80 +/- 98.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=81000, episode_reward=241.40 +/- 142.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=82000, episode_reward=-35.00 +/- 82.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=83000, episode_reward=71.20 +/- 185.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=84000, episode_reward=14.00 +/- 194.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=85000, episode_reward=64.20 +/- 250.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=86000, episode_reward=96.60 +/- 229.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=87000, episode_reward=-50.60 +/- 138.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=88000, episode_reward=39.00 +/- 138.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=89000, episode_reward=155.00 +/- 329.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=90000, episode_reward=244.40 +/- 175.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=91000, episode_reward=193.40 +/- 89.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=92000, episode_reward=28.20 +/- 350.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=93000, episode_reward=310.00 +/- 250.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=94000, episode_reward=232.00 +/- 167.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=95000, episode_reward=17.80 +/- 90.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=96000, episode_reward=80.40 +/- 213.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=97000, episode_reward=278.60 +/- 98.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=98000, episode_reward=364.20 +/- 46.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=99000, episode_reward=57.40 +/- 221.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=100000, episode_reward=282.80 +/- 83.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=101000, episode_reward=166.20 +/- 191.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=102000, episode_reward=82.20 +/- 114.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=103000, episode_reward=49.40 +/- 70.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=104000, episode_reward=202.00 +/- 97.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=105000, episode_reward=224.40 +/- 190.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=106000, episode_reward=94.80 +/- 168.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=107000, episode_reward=341.60 +/- 90.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=108000, episode_reward=240.20 +/- 247.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=109000, episode_reward=138.60 +/- 212.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=110000, episode_reward=-7.20 +/- 281.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=111000, episode_reward=170.00 +/- 224.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=112000, episode_reward=196.60 +/- 151.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=113000, episode_reward=207.20 +/- 133.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=114000, episode_reward=82.20 +/- 320.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=115000, episode_reward=183.00 +/- 154.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=116000, episode_reward=150.00 +/- 341.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=117000, episode_reward=118.20 +/- 131.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=118000, episode_reward=127.20 +/- 204.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=119000, episode_reward=286.20 +/- 225.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=120000, episode_reward=308.80 +/- 64.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=121000, episode_reward=161.00 +/- 169.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=122000, episode_reward=148.00 +/- 159.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=123000, episode_reward=271.00 +/- 133.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=124000, episode_reward=202.00 +/- 112.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=125000, episode_reward=312.00 +/- 260.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=126000, episode_reward=115.60 +/- 255.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=127000, episode_reward=247.60 +/- 256.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=128000, episode_reward=114.40 +/- 380.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=129000, episode_reward=278.20 +/- 156.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=130000, episode_reward=153.20 +/- 190.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=131000, episode_reward=169.40 +/- 96.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=132000, episode_reward=71.40 +/- 247.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=133000, episode_reward=288.60 +/- 203.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=134000, episode_reward=244.00 +/- 213.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=135000, episode_reward=260.60 +/- 150.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=136000, episode_reward=167.60 +/- 159.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=137000, episode_reward=212.00 +/- 223.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=138000, episode_reward=136.60 +/- 184.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=139000, episode_reward=90.20 +/- 301.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=140000, episode_reward=101.40 +/- 211.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=141000, episode_reward=299.60 +/- 193.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=142000, episode_reward=204.60 +/- 155.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=143000, episode_reward=325.00 +/- 85.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=144000, episode_reward=47.00 +/- 156.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=145000, episode_reward=163.20 +/- 137.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=146000, episode_reward=193.00 +/- 247.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=147000, episode_reward=381.60 +/- 249.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=148000, episode_reward=263.40 +/- 243.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=149000, episode_reward=276.60 +/- 137.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=150000, episode_reward=384.20 +/- 203.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=151000, episode_reward=244.00 +/- 190.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=152000, episode_reward=42.60 +/- 172.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=153000, episode_reward=217.00 +/- 86.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=154000, episode_reward=250.60 +/- 153.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=155000, episode_reward=372.20 +/- 230.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=156000, episode_reward=76.80 +/- 184.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=157000, episode_reward=255.60 +/- 294.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=158000, episode_reward=187.40 +/- 160.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=159000, episode_reward=402.00 +/- 135.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=160000, episode_reward=236.80 +/- 157.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=161000, episode_reward=263.60 +/- 233.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=162000, episode_reward=259.20 +/- 176.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=163000, episode_reward=291.40 +/- 112.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=164000, episode_reward=186.00 +/- 256.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=165000, episode_reward=333.80 +/- 188.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=166000, episode_reward=303.60 +/- 160.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=167000, episode_reward=289.00 +/- 255.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=168000, episode_reward=340.40 +/- 155.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=169000, episode_reward=474.40 +/- 162.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=170000, episode_reward=255.40 +/- 215.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=171000, episode_reward=436.80 +/- 165.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=172000, episode_reward=195.80 +/- 226.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=173000, episode_reward=216.20 +/- 245.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=174000, episode_reward=253.60 +/- 104.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=175000, episode_reward=326.20 +/- 123.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=176000, episode_reward=278.80 +/- 229.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=177000, episode_reward=222.80 +/- 309.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=178000, episode_reward=311.00 +/- 228.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=179000, episode_reward=319.00 +/- 177.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=180000, episode_reward=353.00 +/- 151.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=181000, episode_reward=321.40 +/- 248.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=182000, episode_reward=121.20 +/- 119.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=183000, episode_reward=360.60 +/- 90.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=184000, episode_reward=265.00 +/- 217.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=185000, episode_reward=260.40 +/- 191.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=186000, episode_reward=40.00 +/- 168.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=187000, episode_reward=144.00 +/- 171.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=188000, episode_reward=364.40 +/- 218.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=189000, episode_reward=371.80 +/- 175.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=190000, episode_reward=230.60 +/- 245.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=191000, episode_reward=190.80 +/- 142.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=192000, episode_reward=129.60 +/- 156.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=193000, episode_reward=285.00 +/- 303.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=194000, episode_reward=368.60 +/- 124.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=195000, episode_reward=320.60 +/- 144.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=196000, episode_reward=219.00 +/- 203.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=197000, episode_reward=228.40 +/- 107.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=198000, episode_reward=150.60 +/- 274.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=199000, episode_reward=84.80 +/- 184.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=200000, episode_reward=227.20 +/- 200.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=201000, episode_reward=244.60 +/- 183.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=202000, episode_reward=200.40 +/- 165.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=203000, episode_reward=329.00 +/- 274.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=204000, episode_reward=126.60 +/- 182.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=205000, episode_reward=125.40 +/- 196.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=206000, episode_reward=400.20 +/- 243.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=207000, episode_reward=159.60 +/- 144.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=208000, episode_reward=173.20 +/- 324.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=209000, episode_reward=208.00 +/- 209.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=210000, episode_reward=189.80 +/- 211.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=211000, episode_reward=181.40 +/- 225.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=212000, episode_reward=286.00 +/- 139.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=213000, episode_reward=380.20 +/- 156.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=214000, episode_reward=325.00 +/- 161.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=215000, episode_reward=237.00 +/- 139.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=216000, episode_reward=233.20 +/- 81.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=217000, episode_reward=295.20 +/- 205.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=218000, episode_reward=419.60 +/- 116.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=219000, episode_reward=269.60 +/- 129.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=220000, episode_reward=304.40 +/- 288.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=221000, episode_reward=214.40 +/- 220.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=222000, episode_reward=104.20 +/- 121.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=223000, episode_reward=376.80 +/- 123.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=224000, episode_reward=504.00 +/- 159.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=225000, episode_reward=142.80 +/- 104.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=226000, episode_reward=432.00 +/- 177.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=227000, episode_reward=314.40 +/- 122.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=228000, episode_reward=71.80 +/- 96.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=229000, episode_reward=258.80 +/- 161.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=230000, episode_reward=320.20 +/- 214.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=231000, episode_reward=297.80 +/- 66.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=232000, episode_reward=405.80 +/- 287.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=233000, episode_reward=386.00 +/- 210.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=234000, episode_reward=242.40 +/- 182.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=235000, episode_reward=238.00 +/- 192.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=236000, episode_reward=244.60 +/- 124.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=237000, episode_reward=120.40 +/- 161.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=238000, episode_reward=499.00 +/- 158.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=239000, episode_reward=314.00 +/- 91.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=240000, episode_reward=221.00 +/- 193.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=241000, episode_reward=431.40 +/- 238.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=242000, episode_reward=321.80 +/- 203.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=243000, episode_reward=244.40 +/- 118.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=244000, episode_reward=386.80 +/- 210.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=245000, episode_reward=308.20 +/- 161.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=246000, episode_reward=25.80 +/- 117.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=247000, episode_reward=460.20 +/- 330.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=248000, episode_reward=288.00 +/- 218.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=249000, episode_reward=305.00 +/- 216.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=250000, episode_reward=151.60 +/- 176.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=251000, episode_reward=256.60 +/- 249.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=252000, episode_reward=206.80 +/- 147.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=253000, episode_reward=322.80 +/- 90.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=254000, episode_reward=155.60 +/- 139.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=255000, episode_reward=305.40 +/- 169.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=256000, episode_reward=185.20 +/- 269.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=257000, episode_reward=218.60 +/- 108.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=258000, episode_reward=131.00 +/- 275.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=259000, episode_reward=230.60 +/- 177.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=260000, episode_reward=376.00 +/- 68.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=261000, episode_reward=270.00 +/- 125.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=262000, episode_reward=239.20 +/- 300.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=263000, episode_reward=299.20 +/- 166.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=264000, episode_reward=293.00 +/- 92.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=265000, episode_reward=256.00 +/- 89.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=266000, episode_reward=261.80 +/- 216.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=267000, episode_reward=358.40 +/- 257.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=268000, episode_reward=275.40 +/- 225.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=269000, episode_reward=264.80 +/- 174.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=270000, episode_reward=353.60 +/- 132.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=271000, episode_reward=294.60 +/- 148.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=272000, episode_reward=210.60 +/- 209.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=273000, episode_reward=297.60 +/- 104.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=274000, episode_reward=254.60 +/- 197.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=275000, episode_reward=236.60 +/- 188.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=276000, episode_reward=214.00 +/- 385.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=277000, episode_reward=213.20 +/- 69.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=278000, episode_reward=338.20 +/- 93.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=279000, episode_reward=158.60 +/- 306.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=280000, episode_reward=232.40 +/- 180.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=281000, episode_reward=119.20 +/- 269.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=282000, episode_reward=243.80 +/- 180.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=283000, episode_reward=313.80 +/- 241.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=284000, episode_reward=372.80 +/- 59.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=285000, episode_reward=311.20 +/- 168.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=286000, episode_reward=400.20 +/- 195.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=287000, episode_reward=205.20 +/- 77.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=288000, episode_reward=438.40 +/- 134.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=289000, episode_reward=289.40 +/- 61.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=290000, episode_reward=103.00 +/- 210.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=291000, episode_reward=367.40 +/- 161.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=292000, episode_reward=312.40 +/- 120.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=293000, episode_reward=321.60 +/- 107.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=294000, episode_reward=155.00 +/- 206.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=295000, episode_reward=304.60 +/- 198.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=296000, episode_reward=225.80 +/- 193.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=297000, episode_reward=248.60 +/- 164.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=298000, episode_reward=245.00 +/- 124.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=299000, episode_reward=265.60 +/- 221.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=300000, episode_reward=176.80 +/- 233.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=301000, episode_reward=277.60 +/- 269.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=302000, episode_reward=270.20 +/- 135.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=303000, episode_reward=422.00 +/- 144.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=304000, episode_reward=162.20 +/- 108.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=305000, episode_reward=167.20 +/- 176.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=306000, episode_reward=246.60 +/- 224.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=307000, episode_reward=293.60 +/- 127.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=308000, episode_reward=320.20 +/- 169.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=309000, episode_reward=282.60 +/- 246.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=310000, episode_reward=277.40 +/- 298.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=311000, episode_reward=332.80 +/- 163.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=312000, episode_reward=343.00 +/- 164.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=313000, episode_reward=285.00 +/- 245.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=314000, episode_reward=467.00 +/- 183.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=315000, episode_reward=219.80 +/- 243.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=316000, episode_reward=468.80 +/- 180.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=317000, episode_reward=249.80 +/- 105.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=318000, episode_reward=223.20 +/- 130.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=319000, episode_reward=288.40 +/- 141.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=320000, episode_reward=226.80 +/- 274.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=321000, episode_reward=341.20 +/- 249.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=322000, episode_reward=266.20 +/- 194.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=323000, episode_reward=223.20 +/- 110.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=324000, episode_reward=287.20 +/- 91.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=325000, episode_reward=335.00 +/- 140.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=326000, episode_reward=296.00 +/- 228.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=327000, episode_reward=229.40 +/- 210.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=328000, episode_reward=83.60 +/- 158.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=329000, episode_reward=452.20 +/- 97.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=330000, episode_reward=325.60 +/- 198.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=331000, episode_reward=567.40 +/- 125.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=332000, episode_reward=166.80 +/- 264.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=333000, episode_reward=171.60 +/- 111.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=334000, episode_reward=341.00 +/- 161.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=335000, episode_reward=360.80 +/- 153.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=336000, episode_reward=361.00 +/- 185.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=337000, episode_reward=178.80 +/- 172.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=338000, episode_reward=224.20 +/- 182.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=339000, episode_reward=408.80 +/- 100.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=340000, episode_reward=312.60 +/- 189.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=341000, episode_reward=252.60 +/- 113.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=342000, episode_reward=300.00 +/- 250.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=343000, episode_reward=481.80 +/- 131.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=344000, episode_reward=333.20 +/- 250.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=345000, episode_reward=174.80 +/- 187.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=346000, episode_reward=329.60 +/- 186.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=347000, episode_reward=441.20 +/- 145.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=348000, episode_reward=267.60 +/- 212.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=349000, episode_reward=210.60 +/- 13.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=350000, episode_reward=99.00 +/- 243.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=351000, episode_reward=341.40 +/- 166.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=352000, episode_reward=161.20 +/- 165.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=353000, episode_reward=330.00 +/- 187.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=354000, episode_reward=381.60 +/- 205.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=355000, episode_reward=324.40 +/- 249.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=356000, episode_reward=284.60 +/- 107.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=357000, episode_reward=181.80 +/- 207.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=358000, episode_reward=353.20 +/- 139.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=359000, episode_reward=221.80 +/- 111.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=360000, episode_reward=283.60 +/- 155.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=361000, episode_reward=96.60 +/- 151.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=362000, episode_reward=252.20 +/- 224.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=363000, episode_reward=291.60 +/- 189.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=364000, episode_reward=352.60 +/- 117.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=365000, episode_reward=441.00 +/- 222.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=366000, episode_reward=314.40 +/- 207.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=367000, episode_reward=353.80 +/- 249.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=368000, episode_reward=330.80 +/- 112.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=369000, episode_reward=403.20 +/- 150.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=370000, episode_reward=204.60 +/- 192.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=371000, episode_reward=284.20 +/- 134.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=372000, episode_reward=118.40 +/- 260.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=373000, episode_reward=387.00 +/- 69.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=374000, episode_reward=325.00 +/- 132.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=375000, episode_reward=273.40 +/- 234.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=376000, episode_reward=192.40 +/- 192.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=377000, episode_reward=384.00 +/- 200.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=378000, episode_reward=248.40 +/- 131.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=379000, episode_reward=205.40 +/- 94.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=380000, episode_reward=300.20 +/- 111.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=381000, episode_reward=345.60 +/- 177.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=382000, episode_reward=450.80 +/- 78.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=383000, episode_reward=239.40 +/- 179.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=384000, episode_reward=312.00 +/- 134.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=385000, episode_reward=272.60 +/- 261.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=386000, episode_reward=276.80 +/- 320.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=387000, episode_reward=278.80 +/- 244.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=388000, episode_reward=273.60 +/- 240.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=389000, episode_reward=361.20 +/- 98.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=390000, episode_reward=237.00 +/- 175.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=391000, episode_reward=118.60 +/- 129.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=392000, episode_reward=249.60 +/- 104.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=393000, episode_reward=249.00 +/- 125.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=394000, episode_reward=157.60 +/- 264.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=395000, episode_reward=230.40 +/- 114.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=396000, episode_reward=283.80 +/- 173.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=397000, episode_reward=416.00 +/- 104.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=398000, episode_reward=379.20 +/- 108.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=399000, episode_reward=227.00 +/- 266.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=400000, episode_reward=138.60 +/- 221.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=401000, episode_reward=365.40 +/- 210.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=402000, episode_reward=431.60 +/- 308.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=403000, episode_reward=278.40 +/- 184.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=404000, episode_reward=308.00 +/- 207.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=405000, episode_reward=425.20 +/- 66.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=406000, episode_reward=235.00 +/- 165.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=407000, episode_reward=240.20 +/- 313.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=408000, episode_reward=394.40 +/- 172.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=409000, episode_reward=257.60 +/- 290.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=410000, episode_reward=204.40 +/- 105.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=411000, episode_reward=362.20 +/- 110.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=412000, episode_reward=381.00 +/- 148.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=413000, episode_reward=309.80 +/- 155.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=414000, episode_reward=210.80 +/- 181.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=415000, episode_reward=354.20 +/- 114.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=416000, episode_reward=293.20 +/- 236.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=417000, episode_reward=242.40 +/- 353.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=418000, episode_reward=355.20 +/- 174.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=419000, episode_reward=300.00 +/- 117.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=420000, episode_reward=303.60 +/- 201.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=421000, episode_reward=327.80 +/- 232.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=422000, episode_reward=379.60 +/- 294.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=423000, episode_reward=411.80 +/- 249.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=424000, episode_reward=355.20 +/- 134.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=425000, episode_reward=402.60 +/- 84.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=426000, episode_reward=172.40 +/- 155.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=427000, episode_reward=167.00 +/- 165.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=428000, episode_reward=351.60 +/- 126.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=429000, episode_reward=304.40 +/- 109.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=430000, episode_reward=330.20 +/- 195.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=431000, episode_reward=233.40 +/- 384.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=432000, episode_reward=344.00 +/- 139.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=433000, episode_reward=206.60 +/- 225.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=434000, episode_reward=284.60 +/- 143.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=435000, episode_reward=283.00 +/- 143.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=436000, episode_reward=301.80 +/- 144.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=437000, episode_reward=202.80 +/- 163.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=438000, episode_reward=289.80 +/- 199.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=439000, episode_reward=118.20 +/- 203.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=440000, episode_reward=402.00 +/- 206.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=441000, episode_reward=224.60 +/- 174.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=442000, episode_reward=226.00 +/- 100.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=443000, episode_reward=141.60 +/- 158.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=444000, episode_reward=246.80 +/- 147.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=445000, episode_reward=234.60 +/- 144.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=446000, episode_reward=304.00 +/- 154.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=447000, episode_reward=222.80 +/- 217.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=448000, episode_reward=183.60 +/- 167.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=449000, episode_reward=218.40 +/- 164.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=450000, episode_reward=375.40 +/- 225.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=451000, episode_reward=417.40 +/- 77.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=452000, episode_reward=332.40 +/- 242.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=453000, episode_reward=438.60 +/- 143.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=454000, episode_reward=364.40 +/- 103.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=455000, episode_reward=220.20 +/- 229.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=456000, episode_reward=363.60 +/- 191.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=457000, episode_reward=320.60 +/- 252.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=458000, episode_reward=102.40 +/- 200.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=459000, episode_reward=149.20 +/- 133.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=460000, episode_reward=163.20 +/- 176.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=461000, episode_reward=417.60 +/- 96.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=462000, episode_reward=302.40 +/- 207.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=463000, episode_reward=272.00 +/- 99.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=464000, episode_reward=255.40 +/- 134.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=465000, episode_reward=332.40 +/- 290.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=466000, episode_reward=293.60 +/- 164.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=467000, episode_reward=359.60 +/- 382.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=468000, episode_reward=267.80 +/- 276.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=469000, episode_reward=329.60 +/- 106.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=470000, episode_reward=312.00 +/- 167.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=471000, episode_reward=292.80 +/- 304.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=472000, episode_reward=421.20 +/- 261.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=473000, episode_reward=272.00 +/- 165.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=474000, episode_reward=127.40 +/- 95.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=475000, episode_reward=473.80 +/- 135.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=476000, episode_reward=445.60 +/- 135.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=477000, episode_reward=528.60 +/- 164.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=478000, episode_reward=358.20 +/- 183.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=479000, episode_reward=153.60 +/- 182.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=480000, episode_reward=403.00 +/- 151.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=481000, episode_reward=291.60 +/- 165.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=482000, episode_reward=304.40 +/- 145.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=483000, episode_reward=288.40 +/- 255.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=484000, episode_reward=193.00 +/- 212.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=485000, episode_reward=216.40 +/- 187.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=486000, episode_reward=276.40 +/- 158.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=487000, episode_reward=255.40 +/- 171.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=488000, episode_reward=236.00 +/- 191.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=489000, episode_reward=343.40 +/- 120.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=490000, episode_reward=397.80 +/- 125.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=491000, episode_reward=262.80 +/- 140.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=492000, episode_reward=363.80 +/- 101.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=493000, episode_reward=211.80 +/- 171.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=494000, episode_reward=138.00 +/- 250.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=495000, episode_reward=243.60 +/- 216.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=496000, episode_reward=334.60 +/- 231.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=497000, episode_reward=245.80 +/- 84.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=498000, episode_reward=200.20 +/- 184.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=499000, episode_reward=426.80 +/- 113.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=500000, episode_reward=281.20 +/- 184.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Gamma = 0.8, Learning Rate = 0.0003, Activation Function = ReLU, Neurons = 32, Process noise = 0.1, Measurement noise = 0.0\n",
      "Eval num_timesteps=1000, episode_reward=-1443.00 +/- 63.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=-1439.00 +/- 98.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3000, episode_reward=-1530.00 +/- 77.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=4000, episode_reward=-1448.00 +/- 57.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=5000, episode_reward=-1526.00 +/- 74.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=6000, episode_reward=-1457.00 +/- 74.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=7000, episode_reward=-1439.00 +/- 62.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=8000, episode_reward=-1426.00 +/- 91.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=9000, episode_reward=-1478.00 +/- 42.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=10000, episode_reward=-1504.00 +/- 43.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=11000, episode_reward=-1446.00 +/- 58.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=12000, episode_reward=-1517.00 +/- 94.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=13000, episode_reward=-1551.00 +/- 35.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=14000, episode_reward=-1440.00 +/- 65.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=15000, episode_reward=-1514.00 +/- 19.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=16000, episode_reward=-1457.00 +/- 97.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=17000, episode_reward=-1535.00 +/- 72.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=18000, episode_reward=-1525.00 +/- 57.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=19000, episode_reward=-1589.00 +/- 97.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=20000, episode_reward=-1511.00 +/- 70.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=21000, episode_reward=-1492.00 +/- 96.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=22000, episode_reward=-1552.00 +/- 44.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=23000, episode_reward=-1493.00 +/- 29.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=24000, episode_reward=-1451.00 +/- 89.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=25000, episode_reward=-1466.00 +/- 64.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=26000, episode_reward=-1457.00 +/- 42.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=27000, episode_reward=-1509.00 +/- 40.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=28000, episode_reward=-1521.00 +/- 78.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=29000, episode_reward=-1482.00 +/- 66.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=30000, episode_reward=-1517.00 +/- 50.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=31000, episode_reward=-1549.00 +/- 82.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=32000, episode_reward=-1522.00 +/- 40.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=33000, episode_reward=-1446.00 +/- 119.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=34000, episode_reward=-1497.00 +/- 86.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=35000, episode_reward=-1487.00 +/- 106.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=36000, episode_reward=-1483.00 +/- 56.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=37000, episode_reward=-1549.00 +/- 135.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=38000, episode_reward=-1508.00 +/- 97.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=39000, episode_reward=-1512.00 +/- 80.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=40000, episode_reward=-1465.00 +/- 85.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=41000, episode_reward=-1484.00 +/- 111.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=42000, episode_reward=-1477.00 +/- 72.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=43000, episode_reward=-1436.00 +/- 112.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=44000, episode_reward=-1436.00 +/- 86.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=45000, episode_reward=-1451.00 +/- 79.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=46000, episode_reward=-1553.00 +/- 77.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=47000, episode_reward=-1499.00 +/- 58.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=48000, episode_reward=-1474.00 +/- 95.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=49000, episode_reward=-1510.00 +/- 115.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=50000, episode_reward=-1558.00 +/- 69.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=51000, episode_reward=-200.60 +/- 220.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=52000, episode_reward=-202.60 +/- 480.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=53000, episode_reward=-666.20 +/- 349.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=54000, episode_reward=-283.00 +/- 348.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=55000, episode_reward=-92.00 +/- 367.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=56000, episode_reward=53.60 +/- 115.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=57000, episode_reward=229.20 +/- 104.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=58000, episode_reward=394.20 +/- 122.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=59000, episode_reward=182.20 +/- 153.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=60000, episode_reward=118.20 +/- 247.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=61000, episode_reward=267.80 +/- 239.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=62000, episode_reward=191.40 +/- 106.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=63000, episode_reward=276.60 +/- 206.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=64000, episode_reward=119.80 +/- 206.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=65000, episode_reward=384.80 +/- 186.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=66000, episode_reward=335.20 +/- 193.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=67000, episode_reward=329.40 +/- 267.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=68000, episode_reward=415.20 +/- 210.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=69000, episode_reward=421.00 +/- 128.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=70000, episode_reward=327.20 +/- 145.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=71000, episode_reward=131.40 +/- 144.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=72000, episode_reward=342.20 +/- 149.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=73000, episode_reward=384.60 +/- 84.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=74000, episode_reward=353.60 +/- 159.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=75000, episode_reward=236.00 +/- 195.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=76000, episode_reward=361.00 +/- 100.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=77000, episode_reward=261.80 +/- 84.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=78000, episode_reward=224.80 +/- 230.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=79000, episode_reward=327.00 +/- 234.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=80000, episode_reward=308.40 +/- 82.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=81000, episode_reward=157.00 +/- 159.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=82000, episode_reward=450.00 +/- 196.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=83000, episode_reward=204.20 +/- 371.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=84000, episode_reward=364.00 +/- 142.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=85000, episode_reward=335.20 +/- 245.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=86000, episode_reward=339.00 +/- 222.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=87000, episode_reward=330.20 +/- 71.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=88000, episode_reward=130.40 +/- 255.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=89000, episode_reward=330.00 +/- 138.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=90000, episode_reward=159.40 +/- 233.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=91000, episode_reward=186.20 +/- 174.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=92000, episode_reward=378.40 +/- 120.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=93000, episode_reward=241.20 +/- 302.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=94000, episode_reward=399.60 +/- 198.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=95000, episode_reward=351.80 +/- 191.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=96000, episode_reward=277.00 +/- 154.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=97000, episode_reward=405.40 +/- 121.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=98000, episode_reward=408.20 +/- 114.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=99000, episode_reward=370.00 +/- 126.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=100000, episode_reward=311.20 +/- 216.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=101000, episode_reward=161.20 +/- 182.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=102000, episode_reward=350.00 +/- 226.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=103000, episode_reward=426.80 +/- 293.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=104000, episode_reward=468.20 +/- 105.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=105000, episode_reward=452.60 +/- 123.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=106000, episode_reward=362.00 +/- 222.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=107000, episode_reward=321.00 +/- 149.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=108000, episode_reward=327.80 +/- 316.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=109000, episode_reward=403.80 +/- 242.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=110000, episode_reward=243.80 +/- 198.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=111000, episode_reward=312.40 +/- 161.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=112000, episode_reward=246.40 +/- 214.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=113000, episode_reward=348.80 +/- 170.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=114000, episode_reward=300.80 +/- 171.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=115000, episode_reward=190.80 +/- 107.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=116000, episode_reward=99.20 +/- 241.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=117000, episode_reward=437.40 +/- 164.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=118000, episode_reward=277.40 +/- 246.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=119000, episode_reward=221.40 +/- 165.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=120000, episode_reward=294.00 +/- 233.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=121000, episode_reward=403.40 +/- 178.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=122000, episode_reward=408.00 +/- 60.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=123000, episode_reward=271.00 +/- 147.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=124000, episode_reward=320.20 +/- 109.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=125000, episode_reward=584.40 +/- 177.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=126000, episode_reward=467.60 +/- 78.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=127000, episode_reward=395.60 +/- 248.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=128000, episode_reward=460.20 +/- 202.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=129000, episode_reward=416.00 +/- 148.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=130000, episode_reward=314.40 +/- 120.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=131000, episode_reward=246.00 +/- 296.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=132000, episode_reward=151.60 +/- 230.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=133000, episode_reward=366.80 +/- 234.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=134000, episode_reward=250.40 +/- 281.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=135000, episode_reward=421.20 +/- 116.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=136000, episode_reward=495.60 +/- 183.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=137000, episode_reward=362.80 +/- 198.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=138000, episode_reward=335.00 +/- 191.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=139000, episode_reward=388.20 +/- 191.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=140000, episode_reward=452.40 +/- 163.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=141000, episode_reward=294.80 +/- 73.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=142000, episode_reward=239.40 +/- 172.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=143000, episode_reward=283.60 +/- 245.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=144000, episode_reward=286.60 +/- 136.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=145000, episode_reward=361.80 +/- 197.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=146000, episode_reward=539.60 +/- 74.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=147000, episode_reward=268.40 +/- 102.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=148000, episode_reward=254.00 +/- 114.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=149000, episode_reward=364.20 +/- 164.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=150000, episode_reward=386.20 +/- 187.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=151000, episode_reward=200.60 +/- 222.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=152000, episode_reward=278.60 +/- 201.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=153000, episode_reward=240.80 +/- 254.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=154000, episode_reward=508.40 +/- 177.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=155000, episode_reward=458.60 +/- 37.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=156000, episode_reward=439.60 +/- 84.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=157000, episode_reward=320.80 +/- 252.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=158000, episode_reward=421.80 +/- 213.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=159000, episode_reward=348.00 +/- 180.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=160000, episode_reward=460.60 +/- 244.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=161000, episode_reward=334.20 +/- 149.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=162000, episode_reward=185.80 +/- 43.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=163000, episode_reward=230.60 +/- 160.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=164000, episode_reward=248.80 +/- 178.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=165000, episode_reward=494.80 +/- 190.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=166000, episode_reward=405.60 +/- 138.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=167000, episode_reward=339.80 +/- 115.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=168000, episode_reward=495.60 +/- 113.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=169000, episode_reward=361.80 +/- 97.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=170000, episode_reward=149.40 +/- 102.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=171000, episode_reward=153.80 +/- 166.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=172000, episode_reward=227.00 +/- 214.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=173000, episode_reward=238.00 +/- 170.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=174000, episode_reward=313.00 +/- 80.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=175000, episode_reward=259.40 +/- 190.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=176000, episode_reward=222.00 +/- 221.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=177000, episode_reward=456.80 +/- 139.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=178000, episode_reward=345.60 +/- 193.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=179000, episode_reward=303.20 +/- 185.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=180000, episode_reward=326.00 +/- 213.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=181000, episode_reward=357.40 +/- 237.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=182000, episode_reward=363.40 +/- 186.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=183000, episode_reward=291.20 +/- 90.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=184000, episode_reward=218.60 +/- 226.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=185000, episode_reward=435.20 +/- 161.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=186000, episode_reward=281.20 +/- 234.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=187000, episode_reward=338.20 +/- 190.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=188000, episode_reward=308.40 +/- 194.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=189000, episode_reward=349.80 +/- 219.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=190000, episode_reward=439.80 +/- 219.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=191000, episode_reward=416.60 +/- 273.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=192000, episode_reward=392.00 +/- 118.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=193000, episode_reward=262.80 +/- 111.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=194000, episode_reward=470.20 +/- 216.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=195000, episode_reward=306.00 +/- 196.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=196000, episode_reward=230.80 +/- 186.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=197000, episode_reward=319.60 +/- 150.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=198000, episode_reward=281.40 +/- 295.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=199000, episode_reward=347.60 +/- 83.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=200000, episode_reward=245.40 +/- 127.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=201000, episode_reward=482.60 +/- 47.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=202000, episode_reward=410.40 +/- 244.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=203000, episode_reward=363.00 +/- 200.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=204000, episode_reward=371.80 +/- 112.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=205000, episode_reward=336.40 +/- 159.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=206000, episode_reward=405.00 +/- 210.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=207000, episode_reward=303.00 +/- 105.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=208000, episode_reward=437.80 +/- 137.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=209000, episode_reward=288.20 +/- 143.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=210000, episode_reward=425.20 +/- 173.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=211000, episode_reward=456.80 +/- 482.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=212000, episode_reward=367.60 +/- 133.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=213000, episode_reward=309.40 +/- 127.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=214000, episode_reward=301.40 +/- 207.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=215000, episode_reward=366.80 +/- 217.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=216000, episode_reward=307.40 +/- 134.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=217000, episode_reward=366.20 +/- 191.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=218000, episode_reward=423.00 +/- 222.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=219000, episode_reward=203.20 +/- 192.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=220000, episode_reward=382.60 +/- 142.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=221000, episode_reward=454.80 +/- 81.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=222000, episode_reward=300.00 +/- 142.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=223000, episode_reward=354.00 +/- 290.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=224000, episode_reward=210.80 +/- 287.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=225000, episode_reward=465.00 +/- 248.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=226000, episode_reward=306.80 +/- 67.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=227000, episode_reward=566.60 +/- 190.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=228000, episode_reward=429.40 +/- 190.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=229000, episode_reward=400.00 +/- 189.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=230000, episode_reward=509.80 +/- 161.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=231000, episode_reward=462.80 +/- 148.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=232000, episode_reward=435.00 +/- 128.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=233000, episode_reward=387.00 +/- 177.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=234000, episode_reward=192.40 +/- 117.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=235000, episode_reward=292.80 +/- 242.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=236000, episode_reward=294.60 +/- 61.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=237000, episode_reward=252.80 +/- 236.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=238000, episode_reward=136.20 +/- 81.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=239000, episode_reward=331.60 +/- 133.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=240000, episode_reward=365.40 +/- 302.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=241000, episode_reward=454.20 +/- 145.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=242000, episode_reward=388.60 +/- 133.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=243000, episode_reward=308.60 +/- 106.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=244000, episode_reward=410.00 +/- 119.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=245000, episode_reward=410.40 +/- 178.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=246000, episode_reward=291.20 +/- 150.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=247000, episode_reward=336.60 +/- 240.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=248000, episode_reward=309.60 +/- 197.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=249000, episode_reward=316.40 +/- 148.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=250000, episode_reward=396.20 +/- 187.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=251000, episode_reward=366.60 +/- 209.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=252000, episode_reward=441.80 +/- 184.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=253000, episode_reward=337.60 +/- 192.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=254000, episode_reward=363.40 +/- 146.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=255000, episode_reward=301.60 +/- 157.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=256000, episode_reward=148.80 +/- 149.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=257000, episode_reward=400.20 +/- 123.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=258000, episode_reward=291.60 +/- 172.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=259000, episode_reward=417.80 +/- 129.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=260000, episode_reward=414.00 +/- 157.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=261000, episode_reward=277.20 +/- 186.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=262000, episode_reward=582.00 +/- 156.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=263000, episode_reward=336.20 +/- 135.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=264000, episode_reward=488.60 +/- 164.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=265000, episode_reward=451.60 +/- 182.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=266000, episode_reward=394.20 +/- 224.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=267000, episode_reward=283.00 +/- 395.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=268000, episode_reward=272.00 +/- 153.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=269000, episode_reward=350.80 +/- 185.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=270000, episode_reward=541.60 +/- 148.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=271000, episode_reward=431.60 +/- 123.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=272000, episode_reward=279.20 +/- 202.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=273000, episode_reward=218.60 +/- 162.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=274000, episode_reward=356.60 +/- 309.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=275000, episode_reward=358.20 +/- 223.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=276000, episode_reward=452.20 +/- 221.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=277000, episode_reward=254.20 +/- 205.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=278000, episode_reward=315.80 +/- 155.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=279000, episode_reward=252.80 +/- 135.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=280000, episode_reward=152.60 +/- 119.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=281000, episode_reward=530.60 +/- 41.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=282000, episode_reward=273.20 +/- 187.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=283000, episode_reward=204.00 +/- 134.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=284000, episode_reward=411.20 +/- 170.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=285000, episode_reward=403.80 +/- 199.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=286000, episode_reward=357.40 +/- 210.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=287000, episode_reward=398.00 +/- 169.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=288000, episode_reward=351.80 +/- 115.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=289000, episode_reward=349.20 +/- 302.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=290000, episode_reward=400.80 +/- 321.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=291000, episode_reward=335.20 +/- 79.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=292000, episode_reward=287.00 +/- 228.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=293000, episode_reward=322.80 +/- 216.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=294000, episode_reward=364.60 +/- 203.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=295000, episode_reward=380.40 +/- 198.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=296000, episode_reward=263.40 +/- 193.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=297000, episode_reward=346.60 +/- 277.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=298000, episode_reward=302.00 +/- 216.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=299000, episode_reward=415.00 +/- 45.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=300000, episode_reward=262.60 +/- 120.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=301000, episode_reward=409.60 +/- 171.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=302000, episode_reward=310.40 +/- 261.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=303000, episode_reward=373.00 +/- 281.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=304000, episode_reward=516.60 +/- 85.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=305000, episode_reward=466.00 +/- 130.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=306000, episode_reward=332.60 +/- 267.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=307000, episode_reward=284.60 +/- 215.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=308000, episode_reward=260.00 +/- 69.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=309000, episode_reward=409.40 +/- 193.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=310000, episode_reward=396.80 +/- 129.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=311000, episode_reward=457.00 +/- 181.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=312000, episode_reward=165.00 +/- 97.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=313000, episode_reward=458.00 +/- 127.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=314000, episode_reward=324.40 +/- 144.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=315000, episode_reward=433.40 +/- 247.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=316000, episode_reward=308.80 +/- 183.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=317000, episode_reward=397.00 +/- 129.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=318000, episode_reward=312.80 +/- 108.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=319000, episode_reward=356.20 +/- 181.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=320000, episode_reward=424.20 +/- 260.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=321000, episode_reward=506.00 +/- 154.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=322000, episode_reward=190.80 +/- 110.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=323000, episode_reward=382.00 +/- 195.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=324000, episode_reward=425.60 +/- 170.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=325000, episode_reward=506.60 +/- 185.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=326000, episode_reward=363.80 +/- 78.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=327000, episode_reward=271.80 +/- 268.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=328000, episode_reward=400.40 +/- 214.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=329000, episode_reward=484.40 +/- 68.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=330000, episode_reward=405.40 +/- 131.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=331000, episode_reward=323.20 +/- 186.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=332000, episode_reward=269.80 +/- 256.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=333000, episode_reward=427.80 +/- 227.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=334000, episode_reward=327.20 +/- 119.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=335000, episode_reward=114.60 +/- 255.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=336000, episode_reward=619.40 +/- 157.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=337000, episode_reward=467.00 +/- 75.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=338000, episode_reward=406.20 +/- 239.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=339000, episode_reward=479.20 +/- 112.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=340000, episode_reward=365.20 +/- 211.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=341000, episode_reward=218.00 +/- 132.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=342000, episode_reward=269.20 +/- 173.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=343000, episode_reward=266.20 +/- 131.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=344000, episode_reward=424.20 +/- 272.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=345000, episode_reward=439.20 +/- 234.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=346000, episode_reward=324.80 +/- 218.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=347000, episode_reward=259.20 +/- 215.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=348000, episode_reward=263.80 +/- 184.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=349000, episode_reward=173.60 +/- 135.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=350000, episode_reward=334.20 +/- 132.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=351000, episode_reward=461.60 +/- 97.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=352000, episode_reward=214.80 +/- 268.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=353000, episode_reward=273.40 +/- 319.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=354000, episode_reward=506.80 +/- 246.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=355000, episode_reward=460.60 +/- 138.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=356000, episode_reward=368.20 +/- 154.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=357000, episode_reward=390.00 +/- 214.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=358000, episode_reward=353.20 +/- 45.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=359000, episode_reward=262.80 +/- 109.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=360000, episode_reward=394.40 +/- 197.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=361000, episode_reward=409.80 +/- 53.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=362000, episode_reward=321.80 +/- 114.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=363000, episode_reward=275.00 +/- 242.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=364000, episode_reward=510.20 +/- 76.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=365000, episode_reward=354.40 +/- 127.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=366000, episode_reward=542.80 +/- 163.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=367000, episode_reward=343.60 +/- 131.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=368000, episode_reward=380.40 +/- 149.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=369000, episode_reward=348.20 +/- 134.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=370000, episode_reward=353.40 +/- 226.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=371000, episode_reward=477.00 +/- 130.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=372000, episode_reward=253.80 +/- 174.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=373000, episode_reward=282.00 +/- 330.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=374000, episode_reward=301.40 +/- 224.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=375000, episode_reward=504.00 +/- 112.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=376000, episode_reward=469.20 +/- 149.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=377000, episode_reward=358.20 +/- 98.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=378000, episode_reward=314.80 +/- 258.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=379000, episode_reward=361.00 +/- 186.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=380000, episode_reward=475.60 +/- 237.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=381000, episode_reward=433.00 +/- 119.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=382000, episode_reward=380.80 +/- 215.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=383000, episode_reward=366.60 +/- 166.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=384000, episode_reward=406.40 +/- 147.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=385000, episode_reward=374.00 +/- 93.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=386000, episode_reward=424.60 +/- 172.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=387000, episode_reward=334.60 +/- 178.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=388000, episode_reward=443.60 +/- 206.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=389000, episode_reward=554.20 +/- 88.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=390000, episode_reward=413.80 +/- 119.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=391000, episode_reward=507.40 +/- 193.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=392000, episode_reward=409.00 +/- 189.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=393000, episode_reward=390.00 +/- 70.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=394000, episode_reward=443.20 +/- 258.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=395000, episode_reward=657.20 +/- 124.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=396000, episode_reward=421.00 +/- 289.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=397000, episode_reward=311.00 +/- 187.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=398000, episode_reward=368.40 +/- 109.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=399000, episode_reward=299.20 +/- 266.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=400000, episode_reward=516.40 +/- 119.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=401000, episode_reward=271.40 +/- 110.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=402000, episode_reward=419.40 +/- 68.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=403000, episode_reward=409.00 +/- 171.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=404000, episode_reward=353.40 +/- 195.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=405000, episode_reward=180.20 +/- 129.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=406000, episode_reward=187.20 +/- 166.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=407000, episode_reward=508.20 +/- 194.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=408000, episode_reward=365.60 +/- 160.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=409000, episode_reward=471.20 +/- 101.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=410000, episode_reward=294.20 +/- 168.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=411000, episode_reward=412.80 +/- 85.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=412000, episode_reward=496.60 +/- 174.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=413000, episode_reward=481.20 +/- 394.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=414000, episode_reward=415.40 +/- 89.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=415000, episode_reward=234.60 +/- 263.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=416000, episode_reward=411.60 +/- 273.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=417000, episode_reward=229.60 +/- 325.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=418000, episode_reward=493.80 +/- 71.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=419000, episode_reward=362.00 +/- 283.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=420000, episode_reward=475.60 +/- 128.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=421000, episode_reward=468.40 +/- 82.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=422000, episode_reward=333.60 +/- 182.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=423000, episode_reward=504.80 +/- 181.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=424000, episode_reward=340.20 +/- 243.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=425000, episode_reward=366.60 +/- 152.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=426000, episode_reward=246.40 +/- 227.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=427000, episode_reward=495.20 +/- 222.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=428000, episode_reward=460.80 +/- 44.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=429000, episode_reward=299.20 +/- 183.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=430000, episode_reward=476.20 +/- 194.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=431000, episode_reward=295.60 +/- 194.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=432000, episode_reward=496.20 +/- 48.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=433000, episode_reward=493.00 +/- 195.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=434000, episode_reward=592.60 +/- 132.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=435000, episode_reward=451.60 +/- 74.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=436000, episode_reward=478.20 +/- 106.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=437000, episode_reward=333.40 +/- 271.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=438000, episode_reward=460.40 +/- 227.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=439000, episode_reward=181.60 +/- 104.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=440000, episode_reward=285.40 +/- 127.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=441000, episode_reward=391.40 +/- 323.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=442000, episode_reward=537.40 +/- 76.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=443000, episode_reward=547.20 +/- 32.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=444000, episode_reward=461.40 +/- 189.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=445000, episode_reward=449.60 +/- 119.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=446000, episode_reward=407.60 +/- 159.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=447000, episode_reward=375.60 +/- 153.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=448000, episode_reward=479.00 +/- 194.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=449000, episode_reward=221.80 +/- 205.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=450000, episode_reward=513.60 +/- 206.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=451000, episode_reward=342.40 +/- 161.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=452000, episode_reward=395.80 +/- 262.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=453000, episode_reward=177.40 +/- 185.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=454000, episode_reward=291.20 +/- 229.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=455000, episode_reward=290.20 +/- 233.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=456000, episode_reward=451.40 +/- 128.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=457000, episode_reward=287.20 +/- 67.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=458000, episode_reward=265.80 +/- 121.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=459000, episode_reward=366.60 +/- 238.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=460000, episode_reward=361.80 +/- 120.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=461000, episode_reward=435.60 +/- 179.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=462000, episode_reward=392.00 +/- 327.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=463000, episode_reward=395.40 +/- 122.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=464000, episode_reward=445.60 +/- 199.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=465000, episode_reward=493.80 +/- 276.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=466000, episode_reward=542.80 +/- 125.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=467000, episode_reward=544.60 +/- 264.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=468000, episode_reward=466.00 +/- 205.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=469000, episode_reward=410.60 +/- 252.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=470000, episode_reward=463.20 +/- 135.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=471000, episode_reward=330.60 +/- 247.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=472000, episode_reward=313.20 +/- 191.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=473000, episode_reward=543.00 +/- 87.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=474000, episode_reward=517.60 +/- 125.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=475000, episode_reward=288.80 +/- 232.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=476000, episode_reward=212.00 +/- 274.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=477000, episode_reward=489.20 +/- 149.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=478000, episode_reward=490.20 +/- 124.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=479000, episode_reward=447.20 +/- 127.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=480000, episode_reward=456.20 +/- 253.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=481000, episode_reward=337.00 +/- 87.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=482000, episode_reward=401.20 +/- 182.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=483000, episode_reward=511.60 +/- 196.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=484000, episode_reward=401.80 +/- 193.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=485000, episode_reward=305.40 +/- 187.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=486000, episode_reward=348.40 +/- 163.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=487000, episode_reward=303.20 +/- 167.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=488000, episode_reward=366.20 +/- 183.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=489000, episode_reward=425.80 +/- 166.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=490000, episode_reward=390.80 +/- 194.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=491000, episode_reward=453.00 +/- 175.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=492000, episode_reward=392.60 +/- 146.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=493000, episode_reward=142.40 +/- 112.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=494000, episode_reward=299.40 +/- 357.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=495000, episode_reward=410.00 +/- 193.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=496000, episode_reward=478.40 +/- 188.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=497000, episode_reward=435.80 +/- 211.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=498000, episode_reward=532.60 +/- 177.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=499000, episode_reward=486.40 +/- 198.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=500000, episode_reward=479.80 +/- 259.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Gamma = 0.8, Learning Rate = 0.0003, Activation Function = ReLU, Neurons = 64, Process noise = 0.1, Measurement noise = 0.0\n",
      "Eval num_timesteps=1000, episode_reward=-1457.00 +/- 59.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=-1446.00 +/- 62.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3000, episode_reward=-1553.00 +/- 81.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=4000, episode_reward=-1467.00 +/- 70.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=5000, episode_reward=-1484.00 +/- 88.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=6000, episode_reward=-1539.00 +/- 67.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=7000, episode_reward=-1444.00 +/- 69.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=8000, episode_reward=-1451.00 +/- 134.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=9000, episode_reward=-1575.00 +/- 123.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=10000, episode_reward=-1514.00 +/- 130.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=11000, episode_reward=-1530.00 +/- 98.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=12000, episode_reward=-1442.00 +/- 138.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=13000, episode_reward=-1503.00 +/- 65.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=14000, episode_reward=-1436.00 +/- 83.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=15000, episode_reward=-1465.00 +/- 35.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=16000, episode_reward=-1516.00 +/- 77.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=17000, episode_reward=-1439.00 +/- 68.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=18000, episode_reward=-1573.00 +/- 106.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=19000, episode_reward=-1542.00 +/- 62.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=20000, episode_reward=-1522.00 +/- 15.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=21000, episode_reward=-1529.00 +/- 106.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=22000, episode_reward=-1452.00 +/- 79.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=23000, episode_reward=-1492.00 +/- 92.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=24000, episode_reward=-1517.00 +/- 15.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=25000, episode_reward=-1495.00 +/- 76.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=26000, episode_reward=-1501.00 +/- 92.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=27000, episode_reward=-1546.00 +/- 104.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=28000, episode_reward=-1556.00 +/- 50.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=29000, episode_reward=-1461.00 +/- 29.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=30000, episode_reward=-1458.00 +/- 98.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=31000, episode_reward=-1501.00 +/- 54.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=32000, episode_reward=-1522.00 +/- 86.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=33000, episode_reward=-1543.00 +/- 36.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=34000, episode_reward=-1535.00 +/- 52.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=35000, episode_reward=-1483.00 +/- 125.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=36000, episode_reward=-1517.00 +/- 77.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=37000, episode_reward=-1511.00 +/- 40.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=38000, episode_reward=-1490.00 +/- 85.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=39000, episode_reward=-1555.00 +/- 27.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=40000, episode_reward=-1513.00 +/- 48.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=41000, episode_reward=-1526.00 +/- 66.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=42000, episode_reward=-1466.00 +/- 62.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=43000, episode_reward=-1543.00 +/- 96.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=44000, episode_reward=-1556.00 +/- 35.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=45000, episode_reward=-1463.00 +/- 35.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=46000, episode_reward=-1459.00 +/- 81.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=47000, episode_reward=-1465.00 +/- 93.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=48000, episode_reward=-1499.00 +/- 82.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=49000, episode_reward=-1543.00 +/- 59.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=50000, episode_reward=-1512.00 +/- 57.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=51000, episode_reward=-5933.80 +/- 1586.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=52000, episode_reward=-803.40 +/- 1069.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=53000, episode_reward=112.00 +/- 203.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=54000, episode_reward=252.20 +/- 116.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=55000, episode_reward=464.00 +/- 197.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=56000, episode_reward=389.60 +/- 121.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=57000, episode_reward=446.20 +/- 159.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=58000, episode_reward=388.80 +/- 230.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=59000, episode_reward=430.60 +/- 103.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=60000, episode_reward=358.80 +/- 84.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=61000, episode_reward=335.20 +/- 120.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=62000, episode_reward=291.60 +/- 424.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=63000, episode_reward=227.80 +/- 222.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=64000, episode_reward=304.20 +/- 161.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=65000, episode_reward=333.40 +/- 181.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=66000, episode_reward=340.40 +/- 231.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=67000, episode_reward=372.80 +/- 227.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=68000, episode_reward=37.00 +/- 172.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=69000, episode_reward=260.80 +/- 143.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=70000, episode_reward=322.00 +/- 199.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=71000, episode_reward=339.20 +/- 244.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=72000, episode_reward=258.20 +/- 146.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=73000, episode_reward=257.20 +/- 218.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=74000, episode_reward=299.60 +/- 137.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=75000, episode_reward=177.00 +/- 77.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=76000, episode_reward=43.20 +/- 76.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=77000, episode_reward=262.00 +/- 75.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=78000, episode_reward=352.40 +/- 232.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=79000, episode_reward=326.60 +/- 127.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=80000, episode_reward=431.40 +/- 122.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=81000, episode_reward=288.20 +/- 153.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=82000, episode_reward=327.60 +/- 83.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=83000, episode_reward=143.20 +/- 88.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=84000, episode_reward=268.60 +/- 196.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=85000, episode_reward=375.20 +/- 207.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=86000, episode_reward=298.80 +/- 322.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=87000, episode_reward=276.40 +/- 124.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=88000, episode_reward=226.40 +/- 256.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=89000, episode_reward=94.20 +/- 266.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=90000, episode_reward=153.80 +/- 137.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=91000, episode_reward=325.60 +/- 167.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=92000, episode_reward=324.60 +/- 146.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=93000, episode_reward=337.60 +/- 333.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=94000, episode_reward=333.40 +/- 231.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=95000, episode_reward=359.60 +/- 139.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=96000, episode_reward=449.80 +/- 160.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=97000, episode_reward=227.40 +/- 148.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=98000, episode_reward=345.40 +/- 179.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=99000, episode_reward=269.60 +/- 187.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=100000, episode_reward=366.60 +/- 181.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=101000, episode_reward=240.60 +/- 250.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=102000, episode_reward=358.80 +/- 165.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=103000, episode_reward=202.40 +/- 288.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=104000, episode_reward=454.00 +/- 150.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=105000, episode_reward=475.40 +/- 253.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=106000, episode_reward=436.80 +/- 213.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=107000, episode_reward=433.20 +/- 329.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=108000, episode_reward=254.60 +/- 133.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=109000, episode_reward=479.20 +/- 113.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=110000, episode_reward=175.20 +/- 229.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=111000, episode_reward=416.60 +/- 183.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=112000, episode_reward=402.20 +/- 247.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=113000, episode_reward=439.80 +/- 104.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=114000, episode_reward=417.00 +/- 184.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=115000, episode_reward=370.80 +/- 240.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=116000, episode_reward=320.40 +/- 88.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=117000, episode_reward=459.80 +/- 140.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=118000, episode_reward=353.60 +/- 265.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=119000, episode_reward=256.20 +/- 114.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=120000, episode_reward=406.60 +/- 98.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=121000, episode_reward=458.80 +/- 129.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=122000, episode_reward=401.80 +/- 198.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=123000, episode_reward=114.00 +/- 233.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=124000, episode_reward=162.80 +/- 228.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=125000, episode_reward=261.00 +/- 324.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=126000, episode_reward=491.00 +/- 137.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=127000, episode_reward=469.60 +/- 152.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=128000, episode_reward=253.00 +/- 172.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=129000, episode_reward=540.60 +/- 169.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=130000, episode_reward=191.20 +/- 168.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=131000, episode_reward=374.40 +/- 232.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=132000, episode_reward=464.60 +/- 186.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=133000, episode_reward=373.20 +/- 118.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=134000, episode_reward=373.40 +/- 137.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=135000, episode_reward=358.00 +/- 86.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=136000, episode_reward=286.40 +/- 253.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=137000, episode_reward=465.40 +/- 177.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=138000, episode_reward=363.20 +/- 276.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=139000, episode_reward=496.80 +/- 150.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=140000, episode_reward=353.60 +/- 144.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=141000, episode_reward=221.40 +/- 150.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=142000, episode_reward=370.20 +/- 126.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=143000, episode_reward=342.60 +/- 159.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=144000, episode_reward=326.60 +/- 226.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=145000, episode_reward=424.20 +/- 193.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=146000, episode_reward=183.00 +/- 151.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=147000, episode_reward=339.60 +/- 154.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=148000, episode_reward=471.40 +/- 188.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=149000, episode_reward=405.00 +/- 213.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=150000, episode_reward=297.40 +/- 178.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=151000, episode_reward=447.40 +/- 167.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=152000, episode_reward=374.60 +/- 181.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=153000, episode_reward=471.20 +/- 87.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=154000, episode_reward=353.80 +/- 289.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=155000, episode_reward=296.00 +/- 149.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=156000, episode_reward=372.20 +/- 212.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=157000, episode_reward=604.40 +/- 60.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=158000, episode_reward=464.40 +/- 154.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=159000, episode_reward=340.40 +/- 241.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=160000, episode_reward=398.60 +/- 135.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=161000, episode_reward=508.20 +/- 151.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=162000, episode_reward=279.80 +/- 138.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=163000, episode_reward=407.80 +/- 189.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=164000, episode_reward=396.40 +/- 244.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=165000, episode_reward=384.60 +/- 136.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=166000, episode_reward=276.20 +/- 273.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=167000, episode_reward=226.40 +/- 172.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=168000, episode_reward=377.20 +/- 138.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=169000, episode_reward=389.00 +/- 129.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=170000, episode_reward=498.00 +/- 211.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=171000, episode_reward=332.20 +/- 115.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=172000, episode_reward=255.40 +/- 153.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=173000, episode_reward=511.80 +/- 177.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=174000, episode_reward=345.20 +/- 109.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=175000, episode_reward=367.40 +/- 126.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=176000, episode_reward=482.80 +/- 157.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=177000, episode_reward=358.20 +/- 211.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=178000, episode_reward=342.60 +/- 141.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=179000, episode_reward=384.20 +/- 258.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=180000, episode_reward=395.20 +/- 295.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=181000, episode_reward=332.40 +/- 188.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=182000, episode_reward=425.40 +/- 175.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=183000, episode_reward=431.60 +/- 63.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=184000, episode_reward=315.20 +/- 98.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=185000, episode_reward=279.40 +/- 102.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=186000, episode_reward=483.20 +/- 101.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=187000, episode_reward=451.60 +/- 239.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=188000, episode_reward=445.00 +/- 198.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=189000, episode_reward=450.20 +/- 198.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=190000, episode_reward=424.00 +/- 105.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=191000, episode_reward=303.60 +/- 83.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=192000, episode_reward=298.60 +/- 84.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=193000, episode_reward=393.20 +/- 113.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=194000, episode_reward=420.00 +/- 122.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=195000, episode_reward=429.40 +/- 205.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=196000, episode_reward=487.80 +/- 57.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=197000, episode_reward=344.40 +/- 175.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=198000, episode_reward=508.20 +/- 180.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=199000, episode_reward=281.40 +/- 181.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=200000, episode_reward=320.60 +/- 222.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=201000, episode_reward=333.80 +/- 156.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=202000, episode_reward=361.80 +/- 175.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=203000, episode_reward=326.00 +/- 242.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=204000, episode_reward=400.60 +/- 135.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=205000, episode_reward=441.60 +/- 46.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=206000, episode_reward=277.40 +/- 76.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=207000, episode_reward=482.20 +/- 82.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=208000, episode_reward=403.20 +/- 208.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=209000, episode_reward=435.60 +/- 172.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=210000, episode_reward=373.00 +/- 221.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=211000, episode_reward=437.00 +/- 179.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=212000, episode_reward=445.40 +/- 186.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=213000, episode_reward=325.60 +/- 123.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=214000, episode_reward=479.20 +/- 90.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=215000, episode_reward=386.40 +/- 147.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=216000, episode_reward=353.80 +/- 139.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=217000, episode_reward=273.80 +/- 191.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=218000, episode_reward=324.80 +/- 211.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=219000, episode_reward=535.00 +/- 243.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=220000, episode_reward=472.60 +/- 202.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=221000, episode_reward=219.40 +/- 175.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=222000, episode_reward=594.80 +/- 72.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=223000, episode_reward=550.00 +/- 188.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=224000, episode_reward=342.60 +/- 161.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=225000, episode_reward=393.60 +/- 191.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=226000, episode_reward=366.80 +/- 196.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=227000, episode_reward=301.00 +/- 141.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=228000, episode_reward=374.00 +/- 164.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=229000, episode_reward=218.80 +/- 147.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=230000, episode_reward=408.20 +/- 240.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=231000, episode_reward=488.60 +/- 215.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=232000, episode_reward=410.40 +/- 235.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=233000, episode_reward=353.80 +/- 184.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=234000, episode_reward=599.80 +/- 94.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=235000, episode_reward=348.20 +/- 304.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=236000, episode_reward=527.80 +/- 119.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=237000, episode_reward=375.60 +/- 233.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=238000, episode_reward=291.60 +/- 147.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=239000, episode_reward=498.20 +/- 279.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=240000, episode_reward=483.40 +/- 181.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=241000, episode_reward=322.60 +/- 90.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=242000, episode_reward=281.60 +/- 185.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=243000, episode_reward=348.00 +/- 218.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=244000, episode_reward=509.00 +/- 145.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=245000, episode_reward=343.80 +/- 268.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=246000, episode_reward=247.20 +/- 182.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=247000, episode_reward=431.20 +/- 65.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=248000, episode_reward=431.00 +/- 239.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=249000, episode_reward=376.00 +/- 197.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=250000, episode_reward=167.00 +/- 61.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=251000, episode_reward=415.60 +/- 125.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=252000, episode_reward=454.80 +/- 308.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=253000, episode_reward=371.60 +/- 168.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=254000, episode_reward=400.80 +/- 305.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=255000, episode_reward=349.40 +/- 150.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=256000, episode_reward=534.60 +/- 123.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=257000, episode_reward=463.20 +/- 79.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=258000, episode_reward=412.60 +/- 140.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=259000, episode_reward=454.40 +/- 170.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=260000, episode_reward=617.40 +/- 200.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=261000, episode_reward=399.00 +/- 112.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=262000, episode_reward=289.20 +/- 289.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=263000, episode_reward=717.00 +/- 141.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=264000, episode_reward=322.60 +/- 148.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=265000, episode_reward=532.40 +/- 101.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=266000, episode_reward=327.80 +/- 170.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=267000, episode_reward=455.80 +/- 208.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=268000, episode_reward=407.40 +/- 99.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=269000, episode_reward=442.40 +/- 190.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=270000, episode_reward=410.00 +/- 385.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=271000, episode_reward=405.80 +/- 92.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=272000, episode_reward=375.00 +/- 156.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=273000, episode_reward=481.60 +/- 118.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=274000, episode_reward=332.60 +/- 114.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=275000, episode_reward=362.40 +/- 145.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=276000, episode_reward=447.00 +/- 289.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=277000, episode_reward=396.40 +/- 216.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=278000, episode_reward=424.40 +/- 29.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=279000, episode_reward=407.80 +/- 275.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=280000, episode_reward=532.00 +/- 156.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=281000, episode_reward=466.40 +/- 84.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=282000, episode_reward=491.60 +/- 68.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=283000, episode_reward=452.60 +/- 161.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=284000, episode_reward=467.40 +/- 125.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=285000, episode_reward=504.00 +/- 125.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=286000, episode_reward=339.00 +/- 286.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=287000, episode_reward=385.80 +/- 133.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=288000, episode_reward=239.00 +/- 237.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=289000, episode_reward=387.60 +/- 132.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=290000, episode_reward=438.80 +/- 153.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=291000, episode_reward=467.00 +/- 135.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=292000, episode_reward=310.40 +/- 166.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=293000, episode_reward=441.00 +/- 161.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=294000, episode_reward=454.20 +/- 174.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=295000, episode_reward=376.40 +/- 176.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=296000, episode_reward=235.00 +/- 180.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=297000, episode_reward=397.40 +/- 232.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=298000, episode_reward=491.00 +/- 141.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=299000, episode_reward=487.60 +/- 43.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=300000, episode_reward=550.60 +/- 169.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=301000, episode_reward=381.00 +/- 36.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=302000, episode_reward=338.60 +/- 212.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=303000, episode_reward=490.60 +/- 84.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=304000, episode_reward=417.60 +/- 103.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=305000, episode_reward=333.40 +/- 182.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=306000, episode_reward=253.20 +/- 131.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=307000, episode_reward=561.00 +/- 228.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=308000, episode_reward=390.80 +/- 182.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=309000, episode_reward=335.80 +/- 191.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=310000, episode_reward=437.60 +/- 287.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=311000, episode_reward=416.80 +/- 98.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=312000, episode_reward=347.40 +/- 104.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=313000, episode_reward=556.20 +/- 205.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=314000, episode_reward=507.00 +/- 172.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=315000, episode_reward=345.20 +/- 234.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=316000, episode_reward=438.20 +/- 164.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=317000, episode_reward=401.00 +/- 321.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=318000, episode_reward=421.20 +/- 315.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=319000, episode_reward=348.20 +/- 204.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=320000, episode_reward=401.60 +/- 135.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=321000, episode_reward=644.40 +/- 307.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=322000, episode_reward=568.20 +/- 258.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=323000, episode_reward=696.00 +/- 289.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=324000, episode_reward=440.80 +/- 159.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=325000, episode_reward=443.20 +/- 219.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=326000, episode_reward=458.60 +/- 212.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=327000, episode_reward=437.00 +/- 77.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=328000, episode_reward=548.20 +/- 141.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=329000, episode_reward=270.40 +/- 129.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=330000, episode_reward=223.80 +/- 170.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=331000, episode_reward=763.40 +/- 174.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=332000, episode_reward=541.60 +/- 283.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=333000, episode_reward=740.00 +/- 247.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=334000, episode_reward=427.00 +/- 176.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=335000, episode_reward=419.20 +/- 329.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=336000, episode_reward=427.00 +/- 85.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=337000, episode_reward=840.40 +/- 164.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=338000, episode_reward=530.00 +/- 315.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=339000, episode_reward=590.20 +/- 296.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=340000, episode_reward=729.60 +/- 154.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=341000, episode_reward=498.00 +/- 227.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=342000, episode_reward=895.00 +/- 179.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=343000, episode_reward=496.40 +/- 116.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=344000, episode_reward=551.80 +/- 208.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=345000, episode_reward=534.80 +/- 305.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=346000, episode_reward=1015.00 +/- 288.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=347000, episode_reward=522.20 +/- 157.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=348000, episode_reward=669.00 +/- 322.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=349000, episode_reward=473.40 +/- 200.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=350000, episode_reward=550.40 +/- 309.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=351000, episode_reward=580.00 +/- 271.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=352000, episode_reward=850.80 +/- 125.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=353000, episode_reward=772.20 +/- 127.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=354000, episode_reward=355.40 +/- 174.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=355000, episode_reward=492.80 +/- 198.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=356000, episode_reward=888.20 +/- 366.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=357000, episode_reward=732.40 +/- 109.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=358000, episode_reward=850.60 +/- 273.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=359000, episode_reward=468.40 +/- 255.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=360000, episode_reward=577.40 +/- 494.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=361000, episode_reward=629.00 +/- 278.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=362000, episode_reward=849.40 +/- 190.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=363000, episode_reward=649.60 +/- 438.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=364000, episode_reward=737.00 +/- 275.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=365000, episode_reward=686.40 +/- 381.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=366000, episode_reward=882.60 +/- 207.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=367000, episode_reward=827.40 +/- 315.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=368000, episode_reward=620.20 +/- 393.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=369000, episode_reward=804.40 +/- 200.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=370000, episode_reward=900.60 +/- 287.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=371000, episode_reward=771.20 +/- 248.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=372000, episode_reward=775.80 +/- 182.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=373000, episode_reward=941.00 +/- 253.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=374000, episode_reward=454.80 +/- 239.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=375000, episode_reward=673.20 +/- 252.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=376000, episode_reward=531.20 +/- 336.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=377000, episode_reward=644.40 +/- 238.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=378000, episode_reward=705.20 +/- 234.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=379000, episode_reward=575.40 +/- 239.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=380000, episode_reward=408.20 +/- 326.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=381000, episode_reward=636.40 +/- 277.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=382000, episode_reward=528.60 +/- 281.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=383000, episode_reward=596.80 +/- 261.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=384000, episode_reward=610.20 +/- 217.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=385000, episode_reward=663.00 +/- 273.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=386000, episode_reward=413.60 +/- 142.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=387000, episode_reward=600.00 +/- 249.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=388000, episode_reward=626.60 +/- 107.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=389000, episode_reward=642.00 +/- 261.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=390000, episode_reward=778.60 +/- 170.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=391000, episode_reward=766.20 +/- 208.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=392000, episode_reward=670.60 +/- 273.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=393000, episode_reward=354.80 +/- 310.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=394000, episode_reward=669.00 +/- 249.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=395000, episode_reward=838.20 +/- 503.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=396000, episode_reward=634.40 +/- 286.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=397000, episode_reward=660.40 +/- 400.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=398000, episode_reward=620.60 +/- 460.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=399000, episode_reward=639.40 +/- 252.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=400000, episode_reward=551.00 +/- 213.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=401000, episode_reward=648.20 +/- 303.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=402000, episode_reward=691.60 +/- 284.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=403000, episode_reward=784.40 +/- 424.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=404000, episode_reward=697.60 +/- 144.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=405000, episode_reward=770.20 +/- 184.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=406000, episode_reward=616.80 +/- 454.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=407000, episode_reward=759.40 +/- 152.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=408000, episode_reward=883.20 +/- 284.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=409000, episode_reward=769.20 +/- 361.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=410000, episode_reward=605.80 +/- 202.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=411000, episode_reward=540.80 +/- 200.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=412000, episode_reward=898.20 +/- 264.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=413000, episode_reward=689.80 +/- 99.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=414000, episode_reward=793.20 +/- 145.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=415000, episode_reward=612.20 +/- 210.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=416000, episode_reward=756.80 +/- 180.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=417000, episode_reward=566.80 +/- 298.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=418000, episode_reward=831.20 +/- 233.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=419000, episode_reward=718.80 +/- 176.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=420000, episode_reward=835.40 +/- 193.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=421000, episode_reward=532.60 +/- 84.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=422000, episode_reward=513.00 +/- 309.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=423000, episode_reward=845.60 +/- 232.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=424000, episode_reward=675.20 +/- 278.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=425000, episode_reward=530.40 +/- 470.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=426000, episode_reward=501.80 +/- 271.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=427000, episode_reward=624.20 +/- 214.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=428000, episode_reward=754.60 +/- 165.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=429000, episode_reward=937.20 +/- 152.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=430000, episode_reward=660.40 +/- 252.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=431000, episode_reward=757.60 +/- 236.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=432000, episode_reward=647.80 +/- 123.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=433000, episode_reward=724.80 +/- 250.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=434000, episode_reward=865.40 +/- 272.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=435000, episode_reward=686.00 +/- 266.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=436000, episode_reward=699.40 +/- 252.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=437000, episode_reward=658.80 +/- 254.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=438000, episode_reward=450.00 +/- 266.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=439000, episode_reward=660.20 +/- 65.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=440000, episode_reward=517.20 +/- 198.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=441000, episode_reward=922.60 +/- 334.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=442000, episode_reward=759.20 +/- 219.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=443000, episode_reward=769.60 +/- 220.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=444000, episode_reward=524.20 +/- 170.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=445000, episode_reward=555.60 +/- 176.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=446000, episode_reward=575.40 +/- 357.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=447000, episode_reward=527.60 +/- 182.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=448000, episode_reward=725.40 +/- 325.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=449000, episode_reward=593.40 +/- 239.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=450000, episode_reward=589.00 +/- 293.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=451000, episode_reward=611.60 +/- 261.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=452000, episode_reward=501.00 +/- 136.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=453000, episode_reward=412.20 +/- 187.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=454000, episode_reward=607.80 +/- 174.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=455000, episode_reward=562.80 +/- 212.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=456000, episode_reward=500.80 +/- 188.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=457000, episode_reward=406.00 +/- 203.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=458000, episode_reward=782.60 +/- 239.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=459000, episode_reward=727.60 +/- 155.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=460000, episode_reward=622.40 +/- 360.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=461000, episode_reward=719.80 +/- 137.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=462000, episode_reward=727.80 +/- 263.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=463000, episode_reward=469.40 +/- 173.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=464000, episode_reward=700.40 +/- 214.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=465000, episode_reward=375.20 +/- 169.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=466000, episode_reward=482.80 +/- 182.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=467000, episode_reward=624.60 +/- 286.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=468000, episode_reward=815.20 +/- 285.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=469000, episode_reward=654.00 +/- 176.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=470000, episode_reward=505.40 +/- 265.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=471000, episode_reward=839.00 +/- 213.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=472000, episode_reward=524.60 +/- 154.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=473000, episode_reward=845.00 +/- 164.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=474000, episode_reward=488.80 +/- 189.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=475000, episode_reward=582.40 +/- 165.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=476000, episode_reward=598.20 +/- 131.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=477000, episode_reward=904.40 +/- 279.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=478000, episode_reward=670.20 +/- 234.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=479000, episode_reward=774.40 +/- 261.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=480000, episode_reward=567.00 +/- 300.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=481000, episode_reward=638.80 +/- 65.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=482000, episode_reward=612.40 +/- 107.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=483000, episode_reward=785.80 +/- 133.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=484000, episode_reward=762.40 +/- 166.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=485000, episode_reward=575.40 +/- 307.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=486000, episode_reward=472.80 +/- 328.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=487000, episode_reward=543.40 +/- 300.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=488000, episode_reward=641.80 +/- 350.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=489000, episode_reward=561.60 +/- 128.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=490000, episode_reward=726.60 +/- 56.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=491000, episode_reward=605.80 +/- 152.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=492000, episode_reward=567.20 +/- 304.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=493000, episode_reward=693.60 +/- 275.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=494000, episode_reward=670.20 +/- 184.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=495000, episode_reward=541.00 +/- 287.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=496000, episode_reward=704.80 +/- 309.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=497000, episode_reward=632.60 +/- 168.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=498000, episode_reward=630.40 +/- 200.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=499000, episode_reward=459.00 +/- 176.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=500000, episode_reward=723.40 +/- 222.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Gamma = 0.8, Learning Rate = 3e-05, Activation Function = Tanh, Neurons = 8, Process noise = 0.1, Measurement noise = 0.0\n",
      "Eval num_timesteps=1000, episode_reward=-5985.20 +/- 1803.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=-8132.20 +/- 760.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=3000, episode_reward=-7738.80 +/- 1821.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=4000, episode_reward=-8469.00 +/- 1464.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=5000, episode_reward=-7878.20 +/- 1208.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=6000, episode_reward=-8804.40 +/- 1557.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=7000, episode_reward=-7012.60 +/- 2069.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=8000, episode_reward=-7212.00 +/- 1621.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=9000, episode_reward=-7957.40 +/- 582.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=10000, episode_reward=-7507.40 +/- 620.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=11000, episode_reward=-8890.00 +/- 2093.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=12000, episode_reward=-9474.00 +/- 1495.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=13000, episode_reward=-8387.60 +/- 1791.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=14000, episode_reward=-7825.40 +/- 1783.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=15000, episode_reward=-7277.60 +/- 1048.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=16000, episode_reward=-6489.60 +/- 1205.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=17000, episode_reward=-7454.20 +/- 2694.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=18000, episode_reward=-7943.00 +/- 1131.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=19000, episode_reward=-8107.00 +/- 1853.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=20000, episode_reward=-6769.20 +/- 1750.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=21000, episode_reward=-8541.20 +/- 2721.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=22000, episode_reward=-8315.60 +/- 1605.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=23000, episode_reward=-6219.00 +/- 2322.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=24000, episode_reward=-7389.40 +/- 1392.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=25000, episode_reward=-7056.00 +/- 1957.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=26000, episode_reward=-5589.60 +/- 1755.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=27000, episode_reward=-7864.60 +/- 931.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=28000, episode_reward=-7671.00 +/- 1228.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=29000, episode_reward=-9792.00 +/- 2135.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=30000, episode_reward=-8735.20 +/- 2557.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=31000, episode_reward=-7727.20 +/- 1349.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=32000, episode_reward=-7096.40 +/- 2456.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=33000, episode_reward=-7831.60 +/- 1035.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=34000, episode_reward=-7383.80 +/- 2336.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=35000, episode_reward=-7851.00 +/- 2742.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=36000, episode_reward=-6952.80 +/- 1455.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=37000, episode_reward=-8209.60 +/- 1159.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=38000, episode_reward=-7791.40 +/- 1805.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=39000, episode_reward=-7837.80 +/- 2309.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=40000, episode_reward=-7514.20 +/- 1405.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=41000, episode_reward=-6709.60 +/- 1855.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=42000, episode_reward=-7090.00 +/- 1496.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=43000, episode_reward=-7306.80 +/- 3341.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=44000, episode_reward=-8401.40 +/- 2432.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=45000, episode_reward=-7278.80 +/- 1727.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=46000, episode_reward=-7169.20 +/- 1469.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=47000, episode_reward=-8635.00 +/- 1354.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=48000, episode_reward=-7960.80 +/- 2118.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=49000, episode_reward=-6468.80 +/- 1193.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=50000, episode_reward=-6513.40 +/- 2176.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=51000, episode_reward=-7373.00 +/- 2532.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=52000, episode_reward=-8387.80 +/- 1826.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=53000, episode_reward=-8670.20 +/- 574.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=54000, episode_reward=-1409.00 +/- 113.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=55000, episode_reward=-1302.60 +/- 399.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=56000, episode_reward=-892.80 +/- 131.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=57000, episode_reward=-867.20 +/- 277.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=58000, episode_reward=-1047.20 +/- 227.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=59000, episode_reward=-1024.80 +/- 255.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=60000, episode_reward=-909.00 +/- 253.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=61000, episode_reward=-733.60 +/- 89.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=62000, episode_reward=-955.00 +/- 177.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=63000, episode_reward=-627.80 +/- 208.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=64000, episode_reward=-956.80 +/- 414.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=65000, episode_reward=-940.00 +/- 210.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=66000, episode_reward=-692.60 +/- 70.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=67000, episode_reward=-812.60 +/- 229.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=68000, episode_reward=-822.80 +/- 196.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=69000, episode_reward=-646.20 +/- 137.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=70000, episode_reward=-723.60 +/- 272.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=71000, episode_reward=-747.00 +/- 118.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=72000, episode_reward=-646.60 +/- 150.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=73000, episode_reward=-850.80 +/- 246.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=74000, episode_reward=-742.00 +/- 295.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=75000, episode_reward=-762.60 +/- 187.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=76000, episode_reward=-662.20 +/- 196.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=77000, episode_reward=-767.60 +/- 297.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=78000, episode_reward=-589.40 +/- 246.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=79000, episode_reward=-701.80 +/- 161.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=80000, episode_reward=-823.40 +/- 217.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=81000, episode_reward=-833.80 +/- 278.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=82000, episode_reward=-779.80 +/- 165.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=83000, episode_reward=-609.60 +/- 173.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=84000, episode_reward=-683.40 +/- 165.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=85000, episode_reward=-805.20 +/- 247.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=86000, episode_reward=-909.80 +/- 289.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=87000, episode_reward=-601.60 +/- 452.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=88000, episode_reward=-857.40 +/- 213.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=89000, episode_reward=-721.20 +/- 285.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=90000, episode_reward=-851.20 +/- 138.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=91000, episode_reward=-1036.00 +/- 281.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=92000, episode_reward=-815.00 +/- 98.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=93000, episode_reward=-945.00 +/- 355.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=94000, episode_reward=-527.80 +/- 107.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=95000, episode_reward=-850.20 +/- 371.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=96000, episode_reward=-693.80 +/- 392.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=97000, episode_reward=-822.00 +/- 198.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=98000, episode_reward=-763.00 +/- 106.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=99000, episode_reward=-793.00 +/- 160.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=100000, episode_reward=-688.20 +/- 286.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=101000, episode_reward=-932.20 +/- 319.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=102000, episode_reward=-841.80 +/- 302.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=103000, episode_reward=-948.00 +/- 312.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=104000, episode_reward=-778.60 +/- 345.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=105000, episode_reward=-680.00 +/- 170.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=106000, episode_reward=-746.40 +/- 185.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=107000, episode_reward=-703.80 +/- 157.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=108000, episode_reward=-987.20 +/- 447.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=109000, episode_reward=-909.00 +/- 221.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=110000, episode_reward=-871.40 +/- 251.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=111000, episode_reward=-602.60 +/- 207.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=112000, episode_reward=-629.60 +/- 312.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=113000, episode_reward=-520.80 +/- 130.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=114000, episode_reward=-685.20 +/- 433.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=115000, episode_reward=-396.00 +/- 254.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=116000, episode_reward=-400.40 +/- 191.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=117000, episode_reward=288.60 +/- 256.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=118000, episode_reward=-2.60 +/- 162.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=119000, episode_reward=-97.40 +/- 311.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=120000, episode_reward=246.40 +/- 183.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=121000, episode_reward=-24.80 +/- 194.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=122000, episode_reward=222.20 +/- 174.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=123000, episode_reward=110.40 +/- 242.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=124000, episode_reward=50.60 +/- 212.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=125000, episode_reward=-17.20 +/- 75.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=126000, episode_reward=8.60 +/- 302.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=127000, episode_reward=-161.00 +/- 229.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=128000, episode_reward=314.20 +/- 121.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=129000, episode_reward=171.60 +/- 315.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=130000, episode_reward=200.20 +/- 96.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=131000, episode_reward=205.80 +/- 184.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=132000, episode_reward=91.80 +/- 186.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=133000, episode_reward=83.80 +/- 227.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=134000, episode_reward=-103.60 +/- 207.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=135000, episode_reward=-55.20 +/- 325.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=136000, episode_reward=122.40 +/- 133.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=137000, episode_reward=120.20 +/- 194.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=138000, episode_reward=-159.00 +/- 204.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=139000, episode_reward=-2.40 +/- 247.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=140000, episode_reward=-65.40 +/- 92.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=141000, episode_reward=253.60 +/- 294.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=142000, episode_reward=78.80 +/- 105.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=143000, episode_reward=104.60 +/- 159.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=144000, episode_reward=224.80 +/- 135.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=145000, episode_reward=153.60 +/- 110.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=146000, episode_reward=168.20 +/- 115.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=147000, episode_reward=-13.00 +/- 128.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=148000, episode_reward=111.80 +/- 254.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=149000, episode_reward=-44.20 +/- 407.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=150000, episode_reward=21.40 +/- 194.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=151000, episode_reward=39.20 +/- 234.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=152000, episode_reward=60.00 +/- 140.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=153000, episode_reward=104.00 +/- 236.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=154000, episode_reward=-37.40 +/- 167.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=155000, episode_reward=74.60 +/- 218.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=156000, episode_reward=-169.80 +/- 116.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=157000, episode_reward=-28.80 +/- 298.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=158000, episode_reward=25.00 +/- 344.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=159000, episode_reward=113.20 +/- 194.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=160000, episode_reward=219.20 +/- 265.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=161000, episode_reward=217.00 +/- 351.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=162000, episode_reward=83.20 +/- 271.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=163000, episode_reward=291.80 +/- 242.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=164000, episode_reward=80.80 +/- 121.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=165000, episode_reward=167.60 +/- 326.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=166000, episode_reward=229.20 +/- 188.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=167000, episode_reward=219.40 +/- 243.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=168000, episode_reward=-34.40 +/- 201.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=169000, episode_reward=155.20 +/- 166.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=170000, episode_reward=-31.60 +/- 470.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=171000, episode_reward=321.80 +/- 385.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=172000, episode_reward=17.00 +/- 210.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=173000, episode_reward=52.20 +/- 176.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=174000, episode_reward=341.20 +/- 306.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=175000, episode_reward=73.60 +/- 195.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=176000, episode_reward=36.80 +/- 346.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=177000, episode_reward=83.60 +/- 139.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=178000, episode_reward=221.40 +/- 241.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=179000, episode_reward=153.60 +/- 157.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=180000, episode_reward=236.60 +/- 201.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=181000, episode_reward=273.80 +/- 287.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=182000, episode_reward=408.00 +/- 135.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=183000, episode_reward=168.60 +/- 223.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=184000, episode_reward=151.80 +/- 175.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=185000, episode_reward=192.20 +/- 378.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=186000, episode_reward=312.20 +/- 210.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=187000, episode_reward=218.20 +/- 209.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=188000, episode_reward=329.40 +/- 268.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=189000, episode_reward=223.20 +/- 126.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=190000, episode_reward=73.00 +/- 285.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=191000, episode_reward=129.00 +/- 421.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=192000, episode_reward=149.40 +/- 244.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=193000, episode_reward=225.00 +/- 176.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=194000, episode_reward=264.60 +/- 219.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=195000, episode_reward=88.80 +/- 247.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=196000, episode_reward=136.60 +/- 144.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=197000, episode_reward=-13.20 +/- 322.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=198000, episode_reward=30.00 +/- 193.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=199000, episode_reward=166.60 +/- 204.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=200000, episode_reward=137.00 +/- 333.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=201000, episode_reward=206.00 +/- 241.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=202000, episode_reward=239.40 +/- 322.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=203000, episode_reward=30.00 +/- 289.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=204000, episode_reward=151.20 +/- 143.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=205000, episode_reward=200.00 +/- 180.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=206000, episode_reward=258.40 +/- 219.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=207000, episode_reward=233.00 +/- 84.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=208000, episode_reward=-172.20 +/- 642.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=209000, episode_reward=-162.40 +/- 416.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=210000, episode_reward=-3.40 +/- 347.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=211000, episode_reward=198.40 +/- 229.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=212000, episode_reward=275.20 +/- 148.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=213000, episode_reward=222.80 +/- 221.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=214000, episode_reward=23.20 +/- 431.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=215000, episode_reward=100.60 +/- 84.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=216000, episode_reward=187.20 +/- 383.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=217000, episode_reward=202.80 +/- 264.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=218000, episode_reward=132.00 +/- 188.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=219000, episode_reward=124.40 +/- 139.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=220000, episode_reward=109.80 +/- 346.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=221000, episode_reward=12.00 +/- 100.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=222000, episode_reward=150.00 +/- 177.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=223000, episode_reward=140.00 +/- 210.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=224000, episode_reward=230.80 +/- 330.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=225000, episode_reward=221.20 +/- 189.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=226000, episode_reward=-180.00 +/- 436.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=227000, episode_reward=-62.60 +/- 134.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=228000, episode_reward=68.20 +/- 371.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=229000, episode_reward=150.60 +/- 180.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=230000, episode_reward=78.00 +/- 274.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=231000, episode_reward=155.60 +/- 58.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=232000, episode_reward=159.80 +/- 246.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=233000, episode_reward=83.80 +/- 288.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=234000, episode_reward=1.60 +/- 146.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=235000, episode_reward=183.20 +/- 135.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=236000, episode_reward=200.40 +/- 126.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=237000, episode_reward=202.00 +/- 170.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=238000, episode_reward=-78.20 +/- 445.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=239000, episode_reward=49.40 +/- 188.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=240000, episode_reward=30.00 +/- 271.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=241000, episode_reward=-51.80 +/- 412.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=242000, episode_reward=-119.80 +/- 346.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=243000, episode_reward=41.00 +/- 183.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=244000, episode_reward=324.20 +/- 334.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=245000, episode_reward=28.40 +/- 162.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=246000, episode_reward=284.20 +/- 179.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=247000, episode_reward=265.60 +/- 196.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=248000, episode_reward=161.00 +/- 186.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=249000, episode_reward=-25.60 +/- 316.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=250000, episode_reward=293.80 +/- 314.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=251000, episode_reward=229.80 +/- 139.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=252000, episode_reward=68.40 +/- 175.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=253000, episode_reward=-20.60 +/- 352.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=254000, episode_reward=55.40 +/- 179.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=255000, episode_reward=235.00 +/- 206.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=256000, episode_reward=231.20 +/- 241.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=257000, episode_reward=172.20 +/- 341.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=258000, episode_reward=-216.60 +/- 539.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=259000, episode_reward=57.20 +/- 265.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=260000, episode_reward=-200.80 +/- 267.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=261000, episode_reward=177.40 +/- 251.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=262000, episode_reward=381.20 +/- 280.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=263000, episode_reward=186.00 +/- 223.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=264000, episode_reward=28.60 +/- 345.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=265000, episode_reward=155.80 +/- 290.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=266000, episode_reward=402.60 +/- 214.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=267000, episode_reward=60.40 +/- 480.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=268000, episode_reward=-244.80 +/- 624.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=269000, episode_reward=149.60 +/- 239.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=270000, episode_reward=16.40 +/- 233.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=271000, episode_reward=-260.60 +/- 771.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=272000, episode_reward=244.80 +/- 246.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=273000, episode_reward=203.00 +/- 226.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=274000, episode_reward=153.20 +/- 253.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=275000, episode_reward=129.40 +/- 196.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=276000, episode_reward=-68.40 +/- 190.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=277000, episode_reward=-307.20 +/- 402.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=278000, episode_reward=-161.40 +/- 287.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=279000, episode_reward=104.20 +/- 182.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=280000, episode_reward=417.20 +/- 252.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=281000, episode_reward=321.80 +/- 296.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=282000, episode_reward=329.20 +/- 150.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=283000, episode_reward=369.40 +/- 180.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=284000, episode_reward=103.80 +/- 415.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=285000, episode_reward=203.20 +/- 196.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=286000, episode_reward=99.60 +/- 249.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=287000, episode_reward=154.60 +/- 401.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=288000, episode_reward=127.40 +/- 526.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=289000, episode_reward=232.00 +/- 263.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=290000, episode_reward=11.00 +/- 366.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=291000, episode_reward=159.60 +/- 257.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=292000, episode_reward=186.20 +/- 315.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=293000, episode_reward=25.40 +/- 255.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=294000, episode_reward=162.20 +/- 381.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=295000, episode_reward=110.20 +/- 171.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=296000, episode_reward=-35.60 +/- 192.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=297000, episode_reward=96.00 +/- 143.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=298000, episode_reward=187.80 +/- 267.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=299000, episode_reward=103.20 +/- 159.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=300000, episode_reward=-63.20 +/- 475.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=301000, episode_reward=141.80 +/- 196.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=302000, episode_reward=36.40 +/- 432.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=303000, episode_reward=274.20 +/- 271.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=304000, episode_reward=-88.60 +/- 292.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=305000, episode_reward=89.80 +/- 114.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=306000, episode_reward=140.60 +/- 172.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=307000, episode_reward=246.60 +/- 161.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=308000, episode_reward=81.00 +/- 318.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=309000, episode_reward=359.20 +/- 231.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=310000, episode_reward=307.00 +/- 63.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=311000, episode_reward=-7.80 +/- 404.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=312000, episode_reward=344.20 +/- 162.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=313000, episode_reward=133.60 +/- 146.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=314000, episode_reward=-107.20 +/- 368.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=315000, episode_reward=273.80 +/- 156.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=316000, episode_reward=300.80 +/- 175.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=317000, episode_reward=157.20 +/- 284.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=318000, episode_reward=453.60 +/- 234.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=319000, episode_reward=-27.20 +/- 369.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=320000, episode_reward=468.20 +/- 199.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=321000, episode_reward=216.80 +/- 301.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=322000, episode_reward=385.20 +/- 170.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=323000, episode_reward=167.60 +/- 317.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=324000, episode_reward=8.40 +/- 292.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=325000, episode_reward=138.60 +/- 281.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=326000, episode_reward=245.60 +/- 110.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=327000, episode_reward=171.60 +/- 429.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=328000, episode_reward=164.20 +/- 177.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=329000, episode_reward=113.60 +/- 273.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=330000, episode_reward=-61.80 +/- 428.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=331000, episode_reward=40.40 +/- 240.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=332000, episode_reward=144.80 +/- 121.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=333000, episode_reward=47.20 +/- 200.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=334000, episode_reward=121.60 +/- 124.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=335000, episode_reward=227.80 +/- 82.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=336000, episode_reward=-26.60 +/- 166.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=337000, episode_reward=344.20 +/- 143.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=338000, episode_reward=209.60 +/- 123.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=339000, episode_reward=-110.60 +/- 296.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=340000, episode_reward=317.40 +/- 203.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=341000, episode_reward=52.40 +/- 326.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=342000, episode_reward=326.40 +/- 162.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=343000, episode_reward=103.80 +/- 171.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=344000, episode_reward=27.40 +/- 139.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=345000, episode_reward=132.80 +/- 270.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=346000, episode_reward=312.20 +/- 269.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=347000, episode_reward=151.00 +/- 139.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=348000, episode_reward=158.40 +/- 77.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=349000, episode_reward=183.20 +/- 278.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=350000, episode_reward=240.60 +/- 128.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=351000, episode_reward=182.20 +/- 160.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=352000, episode_reward=302.80 +/- 122.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=353000, episode_reward=93.40 +/- 119.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=354000, episode_reward=-27.00 +/- 280.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=355000, episode_reward=99.20 +/- 333.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=356000, episode_reward=234.40 +/- 316.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=357000, episode_reward=276.00 +/- 243.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=358000, episode_reward=172.20 +/- 211.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=359000, episode_reward=249.00 +/- 209.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=360000, episode_reward=217.20 +/- 95.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=361000, episode_reward=159.40 +/- 250.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=362000, episode_reward=209.20 +/- 177.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=363000, episode_reward=22.20 +/- 339.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=364000, episode_reward=32.80 +/- 254.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=365000, episode_reward=359.20 +/- 199.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=366000, episode_reward=459.00 +/- 159.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=367000, episode_reward=74.00 +/- 317.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=368000, episode_reward=54.80 +/- 298.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=369000, episode_reward=275.80 +/- 92.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=370000, episode_reward=264.20 +/- 395.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=371000, episode_reward=-184.20 +/- 433.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=372000, episode_reward=71.60 +/- 81.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=373000, episode_reward=390.40 +/- 166.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=374000, episode_reward=90.40 +/- 302.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=375000, episode_reward=279.40 +/- 230.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=376000, episode_reward=192.60 +/- 113.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=377000, episode_reward=-1.60 +/- 318.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=378000, episode_reward=99.80 +/- 525.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=379000, episode_reward=175.60 +/- 276.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=380000, episode_reward=252.80 +/- 258.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=381000, episode_reward=23.20 +/- 93.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=382000, episode_reward=183.80 +/- 175.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=383000, episode_reward=27.80 +/- 141.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=384000, episode_reward=-35.00 +/- 256.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=385000, episode_reward=137.60 +/- 183.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=386000, episode_reward=64.20 +/- 374.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=387000, episode_reward=73.80 +/- 315.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=388000, episode_reward=176.80 +/- 284.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=389000, episode_reward=37.00 +/- 268.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=390000, episode_reward=195.60 +/- 51.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=391000, episode_reward=40.40 +/- 222.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=392000, episode_reward=-121.60 +/- 498.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=393000, episode_reward=8.80 +/- 172.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=394000, episode_reward=146.40 +/- 101.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=395000, episode_reward=91.40 +/- 186.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=396000, episode_reward=9.40 +/- 392.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=397000, episode_reward=209.60 +/- 182.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=398000, episode_reward=261.00 +/- 205.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=399000, episode_reward=72.80 +/- 347.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=400000, episode_reward=193.80 +/- 275.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=401000, episode_reward=151.40 +/- 137.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=402000, episode_reward=205.60 +/- 204.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=403000, episode_reward=267.60 +/- 126.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=404000, episode_reward=265.40 +/- 242.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=405000, episode_reward=330.20 +/- 165.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=406000, episode_reward=-112.40 +/- 249.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=407000, episode_reward=87.20 +/- 222.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=408000, episode_reward=146.80 +/- 168.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=409000, episode_reward=18.60 +/- 204.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=410000, episode_reward=165.60 +/- 230.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=411000, episode_reward=119.20 +/- 169.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=412000, episode_reward=280.80 +/- 92.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=413000, episode_reward=42.20 +/- 315.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=414000, episode_reward=25.80 +/- 143.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=415000, episode_reward=118.60 +/- 229.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=416000, episode_reward=-128.20 +/- 261.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=417000, episode_reward=119.40 +/- 259.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=418000, episode_reward=-38.80 +/- 330.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=419000, episode_reward=63.20 +/- 264.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=420000, episode_reward=49.40 +/- 351.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=421000, episode_reward=210.60 +/- 219.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=422000, episode_reward=179.40 +/- 245.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=423000, episode_reward=173.00 +/- 160.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=424000, episode_reward=238.00 +/- 141.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=425000, episode_reward=5.60 +/- 429.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=426000, episode_reward=128.60 +/- 336.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=427000, episode_reward=58.40 +/- 377.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=428000, episode_reward=71.20 +/- 374.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=429000, episode_reward=319.80 +/- 218.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=430000, episode_reward=136.00 +/- 187.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=431000, episode_reward=53.60 +/- 174.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=432000, episode_reward=197.00 +/- 197.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=433000, episode_reward=70.80 +/- 254.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=434000, episode_reward=159.60 +/- 220.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=435000, episode_reward=227.00 +/- 150.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=436000, episode_reward=-93.80 +/- 362.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=437000, episode_reward=-38.00 +/- 608.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=438000, episode_reward=205.20 +/- 177.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=439000, episode_reward=124.00 +/- 278.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=440000, episode_reward=52.40 +/- 211.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=441000, episode_reward=343.40 +/- 106.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=442000, episode_reward=-82.40 +/- 168.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=443000, episode_reward=133.60 +/- 268.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=444000, episode_reward=158.80 +/- 130.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=445000, episode_reward=171.80 +/- 185.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=446000, episode_reward=199.00 +/- 203.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=447000, episode_reward=-219.60 +/- 318.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=448000, episode_reward=273.20 +/- 157.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=449000, episode_reward=167.80 +/- 274.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=450000, episode_reward=130.60 +/- 326.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=451000, episode_reward=398.80 +/- 190.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=452000, episode_reward=216.20 +/- 300.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=453000, episode_reward=237.80 +/- 75.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=454000, episode_reward=78.20 +/- 143.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=455000, episode_reward=136.60 +/- 185.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=456000, episode_reward=-107.00 +/- 353.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=457000, episode_reward=197.20 +/- 278.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=458000, episode_reward=146.60 +/- 152.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=459000, episode_reward=-38.00 +/- 344.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=460000, episode_reward=135.40 +/- 138.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=461000, episode_reward=263.80 +/- 99.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=462000, episode_reward=382.40 +/- 207.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=463000, episode_reward=366.80 +/- 237.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=464000, episode_reward=148.00 +/- 120.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=465000, episode_reward=-129.60 +/- 212.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=466000, episode_reward=-31.60 +/- 537.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=467000, episode_reward=138.80 +/- 400.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=468000, episode_reward=84.80 +/- 113.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=469000, episode_reward=281.40 +/- 210.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=470000, episode_reward=182.40 +/- 273.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=471000, episode_reward=249.60 +/- 185.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=472000, episode_reward=83.80 +/- 248.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=473000, episode_reward=121.80 +/- 262.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=474000, episode_reward=163.40 +/- 241.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=475000, episode_reward=108.00 +/- 122.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=476000, episode_reward=1.00 +/- 298.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=477000, episode_reward=70.60 +/- 99.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=478000, episode_reward=-12.20 +/- 512.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=479000, episode_reward=186.60 +/- 112.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=480000, episode_reward=34.60 +/- 422.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=481000, episode_reward=264.40 +/- 224.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=482000, episode_reward=120.80 +/- 160.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=483000, episode_reward=257.60 +/- 238.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=484000, episode_reward=-18.00 +/- 310.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=485000, episode_reward=208.60 +/- 357.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=486000, episode_reward=263.40 +/- 164.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=487000, episode_reward=-76.60 +/- 131.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=488000, episode_reward=242.40 +/- 190.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=489000, episode_reward=140.40 +/- 360.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=490000, episode_reward=97.00 +/- 234.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=491000, episode_reward=266.00 +/- 194.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=492000, episode_reward=156.00 +/- 188.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=493000, episode_reward=76.00 +/- 294.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=494000, episode_reward=2.80 +/- 131.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=495000, episode_reward=256.00 +/- 161.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=496000, episode_reward=129.40 +/- 68.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=497000, episode_reward=116.60 +/- 224.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=498000, episode_reward=110.60 +/- 191.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=499000, episode_reward=-62.00 +/- 273.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=500000, episode_reward=211.00 +/- 178.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Gamma = 0.8, Learning Rate = 3e-05, Activation Function = Tanh, Neurons = 32, Process noise = 0.1, Measurement noise = 0.0\n",
      "Eval num_timesteps=1000, episode_reward=-2415.60 +/- 33.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=-2375.00 +/- 92.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3000, episode_reward=-2297.20 +/- 70.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4000, episode_reward=-2364.20 +/- 39.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=5000, episode_reward=-2398.80 +/- 101.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=6000, episode_reward=-2361.20 +/- 92.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=7000, episode_reward=-2454.80 +/- 163.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=8000, episode_reward=-2343.20 +/- 116.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=9000, episode_reward=-2316.60 +/- 93.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=10000, episode_reward=-2342.80 +/- 66.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=11000, episode_reward=-2332.40 +/- 150.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=12000, episode_reward=-2388.60 +/- 77.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=13000, episode_reward=-2428.20 +/- 69.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=14000, episode_reward=-2359.20 +/- 66.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=15000, episode_reward=-2392.00 +/- 49.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=16000, episode_reward=-2361.60 +/- 80.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=17000, episode_reward=-2409.00 +/- 78.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=18000, episode_reward=-2382.60 +/- 78.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=19000, episode_reward=-2405.40 +/- 49.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=20000, episode_reward=-2317.20 +/- 49.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=21000, episode_reward=-2427.80 +/- 74.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=22000, episode_reward=-2345.00 +/- 80.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=23000, episode_reward=-2431.80 +/- 79.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=24000, episode_reward=-2381.60 +/- 69.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=25000, episode_reward=-2365.40 +/- 69.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=26000, episode_reward=-2338.20 +/- 106.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=27000, episode_reward=-2338.60 +/- 74.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=28000, episode_reward=-2359.80 +/- 74.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=29000, episode_reward=-2359.20 +/- 72.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=30000, episode_reward=-2434.00 +/- 98.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=31000, episode_reward=-2297.20 +/- 90.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=32000, episode_reward=-2341.00 +/- 41.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=33000, episode_reward=-2385.20 +/- 108.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=34000, episode_reward=-2448.00 +/- 133.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=35000, episode_reward=-2385.80 +/- 74.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=36000, episode_reward=-2359.60 +/- 74.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=37000, episode_reward=-2367.80 +/- 105.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=38000, episode_reward=-2393.00 +/- 75.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=39000, episode_reward=-2413.40 +/- 63.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=40000, episode_reward=-2327.20 +/- 72.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=41000, episode_reward=-2295.80 +/- 175.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=42000, episode_reward=-2337.20 +/- 118.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=43000, episode_reward=-2469.40 +/- 78.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=44000, episode_reward=-2458.40 +/- 74.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=45000, episode_reward=-2380.60 +/- 80.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=46000, episode_reward=-2259.80 +/- 140.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=47000, episode_reward=-2347.80 +/- 56.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=48000, episode_reward=-2301.80 +/- 99.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=49000, episode_reward=-2360.80 +/- 65.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=50000, episode_reward=-2408.20 +/- 124.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=51000, episode_reward=-442.80 +/- 47.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=52000, episode_reward=421.00 +/- 100.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=53000, episode_reward=288.20 +/- 45.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=54000, episode_reward=343.60 +/- 180.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=55000, episode_reward=418.60 +/- 229.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=56000, episode_reward=491.00 +/- 137.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=57000, episode_reward=288.40 +/- 65.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=58000, episode_reward=183.60 +/- 160.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=59000, episode_reward=433.20 +/- 83.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=60000, episode_reward=419.60 +/- 243.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=61000, episode_reward=444.00 +/- 202.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=62000, episode_reward=250.40 +/- 271.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=63000, episode_reward=184.00 +/- 96.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=64000, episode_reward=412.40 +/- 138.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=65000, episode_reward=269.60 +/- 117.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=66000, episode_reward=276.40 +/- 190.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=67000, episode_reward=234.60 +/- 172.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=68000, episode_reward=325.00 +/- 173.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=69000, episode_reward=54.40 +/- 184.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=70000, episode_reward=147.20 +/- 251.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=71000, episode_reward=191.00 +/- 205.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=72000, episode_reward=-53.00 +/- 255.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=73000, episode_reward=115.00 +/- 157.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=74000, episode_reward=51.40 +/- 277.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=75000, episode_reward=-897.20 +/- 1122.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=76000, episode_reward=161.40 +/- 193.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=77000, episode_reward=-124.20 +/- 466.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=78000, episode_reward=-228.80 +/- 674.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=79000, episode_reward=-12.80 +/- 215.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=80000, episode_reward=-132.00 +/- 836.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=81000, episode_reward=-72.60 +/- 354.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=82000, episode_reward=83.60 +/- 324.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=83000, episode_reward=166.00 +/- 114.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=84000, episode_reward=-782.20 +/- 1188.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=85000, episode_reward=-131.20 +/- 236.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=86000, episode_reward=-114.40 +/- 749.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=87000, episode_reward=74.80 +/- 342.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=88000, episode_reward=-205.20 +/- 436.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=89000, episode_reward=-167.00 +/- 417.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=90000, episode_reward=-633.00 +/- 1056.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=91000, episode_reward=-121.40 +/- 301.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=92000, episode_reward=230.20 +/- 197.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=93000, episode_reward=144.20 +/- 351.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=94000, episode_reward=-231.20 +/- 538.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=95000, episode_reward=-443.40 +/- 745.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=96000, episode_reward=82.40 +/- 330.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=97000, episode_reward=82.60 +/- 157.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=98000, episode_reward=-568.80 +/- 829.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=99000, episode_reward=144.40 +/- 413.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=100000, episode_reward=-342.20 +/- 205.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=101000, episode_reward=-12.60 +/- 252.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=102000, episode_reward=-83.20 +/- 734.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=103000, episode_reward=255.20 +/- 339.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=104000, episode_reward=-57.60 +/- 108.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=105000, episode_reward=-357.80 +/- 807.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=106000, episode_reward=134.80 +/- 299.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=107000, episode_reward=-51.80 +/- 573.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=108000, episode_reward=-69.40 +/- 213.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=109000, episode_reward=30.40 +/- 211.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=110000, episode_reward=119.00 +/- 278.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=111000, episode_reward=-152.60 +/- 489.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=112000, episode_reward=34.80 +/- 428.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=113000, episode_reward=-218.40 +/- 625.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=114000, episode_reward=-406.00 +/- 534.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=115000, episode_reward=31.40 +/- 184.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=116000, episode_reward=-4.00 +/- 269.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=117000, episode_reward=-379.00 +/- 195.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=118000, episode_reward=-39.40 +/- 153.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=119000, episode_reward=264.00 +/- 367.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=120000, episode_reward=-218.00 +/- 393.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=121000, episode_reward=-596.80 +/- 282.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=122000, episode_reward=-4.80 +/- 309.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=123000, episode_reward=-117.60 +/- 326.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=124000, episode_reward=-364.80 +/- 419.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=125000, episode_reward=71.40 +/- 432.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=126000, episode_reward=80.20 +/- 116.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=127000, episode_reward=-48.80 +/- 238.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=128000, episode_reward=1.40 +/- 264.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=129000, episode_reward=85.80 +/- 416.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=130000, episode_reward=149.00 +/- 255.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=131000, episode_reward=-85.60 +/- 209.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=132000, episode_reward=2.20 +/- 301.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=133000, episode_reward=-201.60 +/- 332.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=134000, episode_reward=135.60 +/- 279.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=135000, episode_reward=-285.20 +/- 260.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=136000, episode_reward=-9.80 +/- 393.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=137000, episode_reward=135.60 +/- 274.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=138000, episode_reward=124.80 +/- 295.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=139000, episode_reward=-62.00 +/- 300.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=140000, episode_reward=-83.60 +/- 352.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=141000, episode_reward=33.60 +/- 281.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=142000, episode_reward=102.20 +/- 142.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=143000, episode_reward=19.00 +/- 317.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=144000, episode_reward=118.80 +/- 201.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=145000, episode_reward=-10.40 +/- 238.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=146000, episode_reward=208.20 +/- 267.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=147000, episode_reward=188.40 +/- 392.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=148000, episode_reward=-61.00 +/- 357.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=149000, episode_reward=78.80 +/- 255.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=150000, episode_reward=199.60 +/- 224.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=151000, episode_reward=70.80 +/- 213.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=152000, episode_reward=-53.80 +/- 145.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=153000, episode_reward=191.20 +/- 147.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=154000, episode_reward=104.80 +/- 399.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=155000, episode_reward=103.80 +/- 185.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=156000, episode_reward=235.00 +/- 273.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=157000, episode_reward=-84.80 +/- 446.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=158000, episode_reward=171.40 +/- 207.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=159000, episode_reward=163.80 +/- 282.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=160000, episode_reward=-37.40 +/- 298.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=161000, episode_reward=133.80 +/- 71.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=162000, episode_reward=78.00 +/- 413.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=163000, episode_reward=-74.80 +/- 482.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=164000, episode_reward=24.80 +/- 146.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=165000, episode_reward=-323.40 +/- 397.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=166000, episode_reward=-83.60 +/- 170.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=167000, episode_reward=227.20 +/- 175.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=168000, episode_reward=199.40 +/- 210.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=169000, episode_reward=60.00 +/- 287.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=170000, episode_reward=-23.00 +/- 214.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=171000, episode_reward=168.60 +/- 261.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=172000, episode_reward=-99.80 +/- 128.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=173000, episode_reward=105.60 +/- 284.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=174000, episode_reward=98.00 +/- 57.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=175000, episode_reward=47.80 +/- 96.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=176000, episode_reward=-47.00 +/- 100.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=177000, episode_reward=-146.80 +/- 197.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=178000, episode_reward=283.00 +/- 221.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=179000, episode_reward=-96.40 +/- 492.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=180000, episode_reward=78.60 +/- 186.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=181000, episode_reward=77.60 +/- 338.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=182000, episode_reward=53.40 +/- 387.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=183000, episode_reward=-206.40 +/- 310.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=184000, episode_reward=-88.20 +/- 321.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=185000, episode_reward=96.00 +/- 309.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=186000, episode_reward=313.60 +/- 232.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=187000, episode_reward=278.40 +/- 156.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=188000, episode_reward=80.20 +/- 245.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=189000, episode_reward=-111.40 +/- 316.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=190000, episode_reward=57.80 +/- 172.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=191000, episode_reward=-129.80 +/- 184.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=192000, episode_reward=3.80 +/- 325.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=193000, episode_reward=64.60 +/- 378.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=194000, episode_reward=53.60 +/- 204.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=195000, episode_reward=37.40 +/- 197.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=196000, episode_reward=-132.40 +/- 308.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=197000, episode_reward=7.60 +/- 178.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=198000, episode_reward=-38.60 +/- 225.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=199000, episode_reward=142.40 +/- 144.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=200000, episode_reward=196.60 +/- 223.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=201000, episode_reward=33.40 +/- 113.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=202000, episode_reward=215.20 +/- 198.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=203000, episode_reward=164.00 +/- 420.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=204000, episode_reward=310.40 +/- 203.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=205000, episode_reward=-211.80 +/- 269.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=206000, episode_reward=182.40 +/- 126.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=207000, episode_reward=-54.60 +/- 411.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=208000, episode_reward=294.20 +/- 156.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=209000, episode_reward=79.80 +/- 273.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=210000, episode_reward=-37.40 +/- 333.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=211000, episode_reward=32.80 +/- 454.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=212000, episode_reward=73.80 +/- 265.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=213000, episode_reward=171.20 +/- 213.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=214000, episode_reward=220.40 +/- 255.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=215000, episode_reward=103.60 +/- 184.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=216000, episode_reward=165.00 +/- 165.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=217000, episode_reward=-136.60 +/- 172.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=218000, episode_reward=-80.20 +/- 623.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=219000, episode_reward=190.40 +/- 110.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=220000, episode_reward=241.20 +/- 82.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=221000, episode_reward=83.20 +/- 262.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=222000, episode_reward=177.60 +/- 204.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=223000, episode_reward=-1.60 +/- 239.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=224000, episode_reward=-97.40 +/- 216.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=225000, episode_reward=30.00 +/- 165.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=226000, episode_reward=160.00 +/- 194.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=227000, episode_reward=129.80 +/- 328.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=228000, episode_reward=185.60 +/- 326.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=229000, episode_reward=94.40 +/- 265.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=230000, episode_reward=255.40 +/- 197.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=231000, episode_reward=82.20 +/- 236.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=232000, episode_reward=91.20 +/- 232.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=233000, episode_reward=288.00 +/- 157.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=234000, episode_reward=289.20 +/- 141.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=235000, episode_reward=155.40 +/- 179.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=236000, episode_reward=243.40 +/- 103.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=237000, episode_reward=-228.40 +/- 116.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=238000, episode_reward=219.80 +/- 364.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=239000, episode_reward=118.60 +/- 142.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=240000, episode_reward=202.00 +/- 257.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=241000, episode_reward=14.40 +/- 412.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=242000, episode_reward=117.60 +/- 120.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=243000, episode_reward=16.20 +/- 294.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=244000, episode_reward=194.80 +/- 183.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=245000, episode_reward=44.00 +/- 286.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=246000, episode_reward=254.00 +/- 222.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=247000, episode_reward=115.00 +/- 171.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=248000, episode_reward=-25.40 +/- 435.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=249000, episode_reward=134.80 +/- 169.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=250000, episode_reward=269.20 +/- 106.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=251000, episode_reward=-94.40 +/- 397.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=252000, episode_reward=51.00 +/- 251.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=253000, episode_reward=55.60 +/- 263.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=254000, episode_reward=72.00 +/- 308.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=255000, episode_reward=-44.60 +/- 97.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=256000, episode_reward=57.80 +/- 187.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=257000, episode_reward=81.20 +/- 397.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=258000, episode_reward=180.00 +/- 303.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=259000, episode_reward=243.00 +/- 328.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=260000, episode_reward=21.80 +/- 154.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=261000, episode_reward=-37.80 +/- 345.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=262000, episode_reward=102.20 +/- 182.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=263000, episode_reward=-102.60 +/- 248.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=264000, episode_reward=188.80 +/- 246.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=265000, episode_reward=332.20 +/- 148.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=266000, episode_reward=93.20 +/- 179.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=267000, episode_reward=15.80 +/- 334.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=268000, episode_reward=97.40 +/- 266.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=269000, episode_reward=-32.80 +/- 251.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=270000, episode_reward=123.40 +/- 298.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=271000, episode_reward=177.80 +/- 330.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=272000, episode_reward=99.40 +/- 154.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=273000, episode_reward=-39.80 +/- 187.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=274000, episode_reward=366.40 +/- 215.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=275000, episode_reward=392.60 +/- 188.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=276000, episode_reward=209.60 +/- 173.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=277000, episode_reward=60.80 +/- 235.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=278000, episode_reward=-36.40 +/- 297.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=279000, episode_reward=111.20 +/- 336.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=280000, episode_reward=202.20 +/- 277.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=281000, episode_reward=109.40 +/- 379.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=282000, episode_reward=369.80 +/- 131.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=283000, episode_reward=198.80 +/- 158.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=284000, episode_reward=-34.00 +/- 176.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=285000, episode_reward=282.40 +/- 299.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=286000, episode_reward=203.20 +/- 145.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=287000, episode_reward=56.20 +/- 317.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=288000, episode_reward=73.00 +/- 215.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=289000, episode_reward=154.80 +/- 93.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=290000, episode_reward=-124.00 +/- 163.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=291000, episode_reward=0.60 +/- 412.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=292000, episode_reward=289.60 +/- 216.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=293000, episode_reward=57.40 +/- 95.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=294000, episode_reward=344.80 +/- 262.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=295000, episode_reward=202.40 +/- 274.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=296000, episode_reward=159.00 +/- 277.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=297000, episode_reward=262.40 +/- 209.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=298000, episode_reward=219.60 +/- 333.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=299000, episode_reward=107.00 +/- 249.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=300000, episode_reward=147.80 +/- 219.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=301000, episode_reward=223.80 +/- 319.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=302000, episode_reward=103.80 +/- 111.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=303000, episode_reward=137.20 +/- 205.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=304000, episode_reward=-89.20 +/- 232.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=305000, episode_reward=245.00 +/- 246.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=306000, episode_reward=149.00 +/- 332.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=307000, episode_reward=192.40 +/- 253.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=308000, episode_reward=82.00 +/- 79.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=309000, episode_reward=87.20 +/- 274.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=310000, episode_reward=240.60 +/- 176.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=311000, episode_reward=176.00 +/- 184.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=312000, episode_reward=194.20 +/- 108.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=313000, episode_reward=375.40 +/- 363.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=314000, episode_reward=273.00 +/- 336.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=315000, episode_reward=179.80 +/- 225.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=316000, episode_reward=211.20 +/- 234.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=317000, episode_reward=175.80 +/- 274.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=318000, episode_reward=264.60 +/- 246.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=319000, episode_reward=242.00 +/- 90.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=320000, episode_reward=314.40 +/- 199.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=321000, episode_reward=229.20 +/- 144.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=322000, episode_reward=97.60 +/- 158.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=323000, episode_reward=258.80 +/- 184.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=324000, episode_reward=161.40 +/- 293.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=325000, episode_reward=61.20 +/- 195.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=326000, episode_reward=205.80 +/- 167.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=327000, episode_reward=318.40 +/- 164.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=328000, episode_reward=290.00 +/- 208.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=329000, episode_reward=73.40 +/- 167.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=330000, episode_reward=137.00 +/- 234.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=331000, episode_reward=167.40 +/- 192.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=332000, episode_reward=325.60 +/- 249.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=333000, episode_reward=113.40 +/- 184.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=334000, episode_reward=397.60 +/- 178.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=335000, episode_reward=82.40 +/- 210.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=336000, episode_reward=155.20 +/- 146.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=337000, episode_reward=316.40 +/- 230.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=338000, episode_reward=270.80 +/- 145.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=339000, episode_reward=76.20 +/- 370.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=340000, episode_reward=305.00 +/- 162.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=341000, episode_reward=74.20 +/- 388.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=342000, episode_reward=-121.40 +/- 239.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=343000, episode_reward=138.40 +/- 211.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=344000, episode_reward=87.20 +/- 125.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=345000, episode_reward=218.60 +/- 162.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=346000, episode_reward=296.60 +/- 193.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=347000, episode_reward=152.00 +/- 173.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=348000, episode_reward=112.00 +/- 209.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=349000, episode_reward=209.80 +/- 279.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=350000, episode_reward=51.20 +/- 178.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=351000, episode_reward=76.80 +/- 231.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=352000, episode_reward=246.40 +/- 123.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=353000, episode_reward=158.80 +/- 334.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=354000, episode_reward=124.60 +/- 144.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=355000, episode_reward=221.00 +/- 354.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=356000, episode_reward=417.60 +/- 71.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=357000, episode_reward=113.80 +/- 179.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=358000, episode_reward=100.80 +/- 228.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=359000, episode_reward=299.40 +/- 163.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=360000, episode_reward=193.40 +/- 115.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=361000, episode_reward=202.20 +/- 113.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=362000, episode_reward=147.00 +/- 223.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=363000, episode_reward=284.60 +/- 241.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=364000, episode_reward=98.60 +/- 244.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=365000, episode_reward=117.20 +/- 82.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=366000, episode_reward=166.20 +/- 280.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=367000, episode_reward=179.00 +/- 150.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=368000, episode_reward=157.80 +/- 297.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=369000, episode_reward=261.60 +/- 248.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=370000, episode_reward=185.00 +/- 340.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=371000, episode_reward=242.20 +/- 268.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=372000, episode_reward=180.60 +/- 216.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=373000, episode_reward=299.60 +/- 96.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=374000, episode_reward=178.80 +/- 260.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=375000, episode_reward=293.60 +/- 282.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=376000, episode_reward=-40.60 +/- 195.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=377000, episode_reward=408.20 +/- 207.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=378000, episode_reward=192.80 +/- 138.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=379000, episode_reward=97.60 +/- 57.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=380000, episode_reward=35.00 +/- 180.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=381000, episode_reward=176.40 +/- 209.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=382000, episode_reward=150.80 +/- 127.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=383000, episode_reward=350.00 +/- 164.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=384000, episode_reward=171.00 +/- 282.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=385000, episode_reward=37.80 +/- 236.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=386000, episode_reward=217.80 +/- 194.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=387000, episode_reward=138.40 +/- 252.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=388000, episode_reward=474.60 +/- 176.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=389000, episode_reward=148.00 +/- 139.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=390000, episode_reward=249.20 +/- 250.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=391000, episode_reward=136.80 +/- 166.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=392000, episode_reward=342.40 +/- 247.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=393000, episode_reward=295.80 +/- 135.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=394000, episode_reward=96.40 +/- 138.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=395000, episode_reward=278.80 +/- 195.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=396000, episode_reward=200.80 +/- 249.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=397000, episode_reward=348.40 +/- 215.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=398000, episode_reward=350.20 +/- 232.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=399000, episode_reward=123.00 +/- 139.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=400000, episode_reward=351.20 +/- 146.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=401000, episode_reward=-100.20 +/- 309.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=402000, episode_reward=-76.60 +/- 281.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=403000, episode_reward=310.80 +/- 211.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=404000, episode_reward=215.80 +/- 250.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=405000, episode_reward=207.20 +/- 82.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=406000, episode_reward=288.00 +/- 94.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=407000, episode_reward=154.60 +/- 130.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=408000, episode_reward=204.00 +/- 149.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=409000, episode_reward=20.20 +/- 181.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=410000, episode_reward=306.60 +/- 93.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=411000, episode_reward=256.20 +/- 142.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=412000, episode_reward=330.80 +/- 212.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=413000, episode_reward=377.60 +/- 216.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=414000, episode_reward=121.40 +/- 94.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=415000, episode_reward=335.40 +/- 92.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=416000, episode_reward=97.00 +/- 248.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=417000, episode_reward=171.80 +/- 164.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=418000, episode_reward=121.20 +/- 235.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=419000, episode_reward=172.00 +/- 209.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=420000, episode_reward=273.00 +/- 249.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=421000, episode_reward=313.40 +/- 201.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=422000, episode_reward=118.00 +/- 125.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=423000, episode_reward=264.60 +/- 271.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=424000, episode_reward=310.60 +/- 262.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=425000, episode_reward=64.40 +/- 287.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=426000, episode_reward=205.00 +/- 130.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=427000, episode_reward=264.80 +/- 199.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=428000, episode_reward=216.20 +/- 346.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=429000, episode_reward=200.00 +/- 242.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=430000, episode_reward=295.40 +/- 59.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=431000, episode_reward=109.60 +/- 319.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=432000, episode_reward=286.80 +/- 181.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=433000, episode_reward=138.80 +/- 285.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=434000, episode_reward=171.40 +/- 145.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=435000, episode_reward=211.00 +/- 271.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=436000, episode_reward=148.00 +/- 218.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=437000, episode_reward=97.80 +/- 89.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=438000, episode_reward=208.60 +/- 176.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=439000, episode_reward=21.40 +/- 95.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=440000, episode_reward=119.20 +/- 74.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=441000, episode_reward=230.80 +/- 242.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=442000, episode_reward=370.60 +/- 196.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=443000, episode_reward=196.60 +/- 238.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=444000, episode_reward=57.80 +/- 125.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=445000, episode_reward=190.40 +/- 179.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=446000, episode_reward=233.20 +/- 224.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=447000, episode_reward=185.40 +/- 278.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=448000, episode_reward=286.80 +/- 136.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=449000, episode_reward=329.00 +/- 303.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=450000, episode_reward=179.40 +/- 214.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=451000, episode_reward=207.20 +/- 270.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=452000, episode_reward=271.20 +/- 199.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=453000, episode_reward=271.80 +/- 226.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=454000, episode_reward=263.80 +/- 158.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=455000, episode_reward=263.60 +/- 115.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=456000, episode_reward=418.60 +/- 221.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=457000, episode_reward=-25.40 +/- 282.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=458000, episode_reward=82.00 +/- 296.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=459000, episode_reward=-34.00 +/- 410.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=460000, episode_reward=226.40 +/- 179.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=461000, episode_reward=220.60 +/- 321.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=462000, episode_reward=213.80 +/- 418.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=463000, episode_reward=163.20 +/- 149.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=464000, episode_reward=234.20 +/- 169.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=465000, episode_reward=245.80 +/- 258.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=466000, episode_reward=432.00 +/- 294.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=467000, episode_reward=130.60 +/- 119.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=468000, episode_reward=396.60 +/- 178.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=469000, episode_reward=193.00 +/- 135.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=470000, episode_reward=324.40 +/- 80.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=471000, episode_reward=98.00 +/- 334.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=472000, episode_reward=304.00 +/- 303.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=473000, episode_reward=122.80 +/- 166.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=474000, episode_reward=239.80 +/- 210.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=475000, episode_reward=481.60 +/- 171.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=476000, episode_reward=195.40 +/- 298.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=477000, episode_reward=304.80 +/- 107.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=478000, episode_reward=124.60 +/- 76.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=479000, episode_reward=259.00 +/- 123.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=480000, episode_reward=-145.20 +/- 442.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=481000, episode_reward=25.20 +/- 302.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=482000, episode_reward=114.40 +/- 475.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=483000, episode_reward=354.40 +/- 210.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=484000, episode_reward=265.00 +/- 94.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=485000, episode_reward=403.80 +/- 178.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=486000, episode_reward=178.60 +/- 206.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=487000, episode_reward=307.00 +/- 179.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=488000, episode_reward=291.80 +/- 326.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=489000, episode_reward=248.00 +/- 373.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=490000, episode_reward=329.60 +/- 322.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=491000, episode_reward=196.20 +/- 277.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=492000, episode_reward=225.60 +/- 131.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=493000, episode_reward=234.00 +/- 154.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=494000, episode_reward=139.80 +/- 254.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=495000, episode_reward=270.00 +/- 333.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=496000, episode_reward=285.60 +/- 189.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=497000, episode_reward=337.20 +/- 168.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=498000, episode_reward=138.20 +/- 398.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=499000, episode_reward=428.80 +/- 285.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=500000, episode_reward=199.40 +/- 267.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Gamma = 0.8, Learning Rate = 3e-05, Activation Function = Tanh, Neurons = 64, Process noise = 0.1, Measurement noise = 0.0\n",
      "Eval num_timesteps=1000, episode_reward=-1522.00 +/- 50.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=-1503.00 +/- 41.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3000, episode_reward=-1512.00 +/- 103.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=4000, episode_reward=-1424.00 +/- 53.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=5000, episode_reward=-1561.00 +/- 72.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=6000, episode_reward=-1548.00 +/- 112.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=7000, episode_reward=-1505.00 +/- 82.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=8000, episode_reward=-1528.00 +/- 38.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=9000, episode_reward=-1546.00 +/- 64.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=10000, episode_reward=-1525.00 +/- 93.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=11000, episode_reward=-1487.00 +/- 60.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=12000, episode_reward=-1524.00 +/- 92.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=13000, episode_reward=-1565.00 +/- 95.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=14000, episode_reward=-1545.00 +/- 125.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=15000, episode_reward=-1515.00 +/- 85.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=16000, episode_reward=-1538.00 +/- 77.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=17000, episode_reward=-1430.00 +/- 60.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=18000, episode_reward=-1495.00 +/- 72.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=19000, episode_reward=-1526.00 +/- 55.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=20000, episode_reward=-1450.00 +/- 87.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=21000, episode_reward=-1524.00 +/- 77.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=22000, episode_reward=-1478.00 +/- 47.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=23000, episode_reward=-1523.00 +/- 65.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=24000, episode_reward=-1503.00 +/- 75.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=25000, episode_reward=-1427.00 +/- 47.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=26000, episode_reward=-1495.00 +/- 92.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=27000, episode_reward=-1495.00 +/- 121.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=28000, episode_reward=-1464.00 +/- 25.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=29000, episode_reward=-1487.00 +/- 109.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=30000, episode_reward=-1518.00 +/- 72.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=31000, episode_reward=-1474.00 +/- 74.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=32000, episode_reward=-1521.00 +/- 74.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=33000, episode_reward=-1535.00 +/- 116.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=34000, episode_reward=-1512.00 +/- 66.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=35000, episode_reward=-1458.00 +/- 91.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=36000, episode_reward=-1464.00 +/- 81.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=37000, episode_reward=-1431.00 +/- 73.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=38000, episode_reward=-1514.00 +/- 22.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=39000, episode_reward=-1518.00 +/- 74.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=40000, episode_reward=-1499.00 +/- 35.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=41000, episode_reward=-1474.00 +/- 93.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=42000, episode_reward=-1487.00 +/- 69.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=43000, episode_reward=-1538.00 +/- 74.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=44000, episode_reward=-1531.00 +/- 57.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=45000, episode_reward=-1442.00 +/- 154.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=46000, episode_reward=-1524.00 +/- 54.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=47000, episode_reward=-1476.00 +/- 127.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=48000, episode_reward=-1552.00 +/- 68.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=49000, episode_reward=-1548.00 +/- 50.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=50000, episode_reward=-1496.00 +/- 80.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=51000, episode_reward=-881.40 +/- 88.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=52000, episode_reward=-640.60 +/- 142.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=53000, episode_reward=-358.20 +/- 192.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=54000, episode_reward=463.80 +/- 162.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=55000, episode_reward=363.80 +/- 115.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=56000, episode_reward=263.80 +/- 76.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=57000, episode_reward=156.00 +/- 102.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=58000, episode_reward=104.80 +/- 68.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=59000, episode_reward=123.20 +/- 104.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=60000, episode_reward=139.40 +/- 193.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=61000, episode_reward=-170.20 +/- 174.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=62000, episode_reward=-68.60 +/- 301.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=63000, episode_reward=-28.80 +/- 201.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=64000, episode_reward=-464.40 +/- 580.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=65000, episode_reward=-23.00 +/- 304.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=66000, episode_reward=-729.80 +/- 1178.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=67000, episode_reward=-150.20 +/- 759.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=68000, episode_reward=-94.20 +/- 388.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=69000, episode_reward=-356.40 +/- 808.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=70000, episode_reward=101.60 +/- 300.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=71000, episode_reward=-73.60 +/- 653.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=72000, episode_reward=43.80 +/- 74.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=73000, episode_reward=183.60 +/- 294.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=74000, episode_reward=-516.80 +/- 514.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=75000, episode_reward=149.40 +/- 342.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=76000, episode_reward=-624.20 +/- 676.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=77000, episode_reward=-441.40 +/- 718.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=78000, episode_reward=-127.60 +/- 375.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=79000, episode_reward=328.60 +/- 158.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=80000, episode_reward=-842.80 +/- 1134.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=81000, episode_reward=-32.40 +/- 344.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=82000, episode_reward=-542.80 +/- 1317.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=83000, episode_reward=42.60 +/- 266.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=84000, episode_reward=-4.60 +/- 348.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=85000, episode_reward=-509.00 +/- 838.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=86000, episode_reward=178.40 +/- 399.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=87000, episode_reward=70.00 +/- 85.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=88000, episode_reward=-182.80 +/- 389.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=89000, episode_reward=-394.80 +/- 735.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=90000, episode_reward=177.40 +/- 222.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=91000, episode_reward=103.40 +/- 159.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=92000, episode_reward=93.00 +/- 281.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=93000, episode_reward=-74.80 +/- 241.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=94000, episode_reward=307.60 +/- 218.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=95000, episode_reward=-228.00 +/- 448.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=96000, episode_reward=203.40 +/- 293.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=97000, episode_reward=44.60 +/- 341.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=98000, episode_reward=-261.00 +/- 317.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=99000, episode_reward=-53.40 +/- 226.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=100000, episode_reward=-85.60 +/- 215.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=101000, episode_reward=172.60 +/- 349.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=102000, episode_reward=171.20 +/- 215.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=103000, episode_reward=8.20 +/- 352.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=104000, episode_reward=-66.20 +/- 405.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=105000, episode_reward=52.40 +/- 110.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=106000, episode_reward=-58.80 +/- 254.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=107000, episode_reward=35.80 +/- 28.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=108000, episode_reward=14.20 +/- 310.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=109000, episode_reward=-4.80 +/- 272.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=110000, episode_reward=-12.20 +/- 187.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=111000, episode_reward=165.40 +/- 251.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=112000, episode_reward=-79.80 +/- 416.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=113000, episode_reward=-261.80 +/- 359.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=114000, episode_reward=-172.00 +/- 228.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=115000, episode_reward=342.20 +/- 343.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=116000, episode_reward=233.40 +/- 198.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=117000, episode_reward=149.00 +/- 205.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=118000, episode_reward=254.20 +/- 141.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=119000, episode_reward=176.60 +/- 186.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=120000, episode_reward=-368.60 +/- 396.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=121000, episode_reward=114.60 +/- 367.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=122000, episode_reward=-219.60 +/- 295.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=123000, episode_reward=45.80 +/- 271.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=124000, episode_reward=-36.60 +/- 210.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=125000, episode_reward=40.80 +/- 142.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=126000, episode_reward=42.20 +/- 120.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=127000, episode_reward=175.60 +/- 487.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=128000, episode_reward=268.40 +/- 213.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=129000, episode_reward=63.40 +/- 137.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=130000, episode_reward=268.60 +/- 342.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=131000, episode_reward=81.00 +/- 190.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=132000, episode_reward=90.20 +/- 296.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=133000, episode_reward=258.40 +/- 152.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=134000, episode_reward=124.40 +/- 356.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=135000, episode_reward=134.00 +/- 247.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=136000, episode_reward=93.60 +/- 208.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=137000, episode_reward=204.80 +/- 107.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=138000, episode_reward=209.00 +/- 153.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=139000, episode_reward=193.60 +/- 107.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=140000, episode_reward=40.80 +/- 140.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=141000, episode_reward=71.60 +/- 224.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=142000, episode_reward=88.60 +/- 232.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=143000, episode_reward=-124.60 +/- 346.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=144000, episode_reward=85.60 +/- 139.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=145000, episode_reward=-134.40 +/- 497.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=146000, episode_reward=181.00 +/- 229.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=147000, episode_reward=100.60 +/- 185.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=148000, episode_reward=81.40 +/- 277.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=149000, episode_reward=103.40 +/- 403.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=150000, episode_reward=80.40 +/- 264.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=151000, episode_reward=139.40 +/- 257.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=152000, episode_reward=105.00 +/- 259.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=153000, episode_reward=145.00 +/- 236.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=154000, episode_reward=-85.00 +/- 276.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=155000, episode_reward=-63.60 +/- 404.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=156000, episode_reward=30.40 +/- 124.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=157000, episode_reward=192.40 +/- 197.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=158000, episode_reward=199.80 +/- 298.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=159000, episode_reward=310.40 +/- 119.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=160000, episode_reward=188.40 +/- 206.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=161000, episode_reward=129.40 +/- 80.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=162000, episode_reward=113.80 +/- 301.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=163000, episode_reward=-40.40 +/- 190.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=164000, episode_reward=233.40 +/- 361.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=165000, episode_reward=159.80 +/- 238.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=166000, episode_reward=224.60 +/- 217.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=167000, episode_reward=135.20 +/- 159.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=168000, episode_reward=68.00 +/- 221.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=169000, episode_reward=83.60 +/- 193.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=170000, episode_reward=271.20 +/- 166.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=171000, episode_reward=39.00 +/- 131.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=172000, episode_reward=152.20 +/- 142.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=173000, episode_reward=258.80 +/- 253.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=174000, episode_reward=182.80 +/- 334.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=175000, episode_reward=52.00 +/- 152.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=176000, episode_reward=264.40 +/- 66.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=177000, episode_reward=200.00 +/- 146.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=178000, episode_reward=179.80 +/- 192.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=179000, episode_reward=27.80 +/- 206.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=180000, episode_reward=240.80 +/- 282.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=181000, episode_reward=50.20 +/- 206.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=182000, episode_reward=170.60 +/- 180.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=183000, episode_reward=100.40 +/- 232.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=184000, episode_reward=95.80 +/- 293.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=185000, episode_reward=105.60 +/- 242.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=186000, episode_reward=97.60 +/- 100.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=187000, episode_reward=142.00 +/- 210.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=188000, episode_reward=216.60 +/- 180.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=189000, episode_reward=138.60 +/- 256.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=190000, episode_reward=112.40 +/- 207.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=191000, episode_reward=211.60 +/- 123.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=192000, episode_reward=16.60 +/- 174.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=193000, episode_reward=177.80 +/- 234.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=194000, episode_reward=210.60 +/- 301.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=195000, episode_reward=76.60 +/- 132.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=196000, episode_reward=86.80 +/- 226.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=197000, episode_reward=177.60 +/- 193.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=198000, episode_reward=43.20 +/- 249.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=199000, episode_reward=82.80 +/- 268.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=200000, episode_reward=305.20 +/- 128.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=201000, episode_reward=164.20 +/- 288.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=202000, episode_reward=-40.20 +/- 158.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=203000, episode_reward=87.40 +/- 314.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=204000, episode_reward=222.40 +/- 247.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=205000, episode_reward=243.60 +/- 258.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=206000, episode_reward=299.20 +/- 144.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=207000, episode_reward=391.20 +/- 153.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=208000, episode_reward=149.20 +/- 161.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=209000, episode_reward=183.00 +/- 243.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=210000, episode_reward=101.20 +/- 241.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=211000, episode_reward=172.80 +/- 294.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=212000, episode_reward=237.00 +/- 147.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=213000, episode_reward=232.60 +/- 321.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=214000, episode_reward=-33.80 +/- 142.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=215000, episode_reward=219.20 +/- 234.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=216000, episode_reward=111.40 +/- 160.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=217000, episode_reward=-19.20 +/- 140.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=218000, episode_reward=-76.20 +/- 114.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=219000, episode_reward=106.60 +/- 244.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=220000, episode_reward=105.40 +/- 240.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=221000, episode_reward=182.60 +/- 207.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=222000, episode_reward=287.60 +/- 192.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=223000, episode_reward=136.60 +/- 232.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=224000, episode_reward=201.40 +/- 241.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=225000, episode_reward=166.60 +/- 74.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=226000, episode_reward=377.00 +/- 89.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=227000, episode_reward=175.00 +/- 126.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=228000, episode_reward=-52.40 +/- 286.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=229000, episode_reward=185.00 +/- 300.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=230000, episode_reward=101.80 +/- 124.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=231000, episode_reward=205.80 +/- 161.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=232000, episode_reward=55.20 +/- 301.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=233000, episode_reward=238.00 +/- 177.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=234000, episode_reward=301.20 +/- 313.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=235000, episode_reward=112.20 +/- 196.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=236000, episode_reward=42.00 +/- 68.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=237000, episode_reward=152.80 +/- 189.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=238000, episode_reward=260.40 +/- 273.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=239000, episode_reward=168.00 +/- 201.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=240000, episode_reward=204.40 +/- 158.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=241000, episode_reward=162.80 +/- 72.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=242000, episode_reward=289.80 +/- 119.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=243000, episode_reward=218.40 +/- 222.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=244000, episode_reward=335.80 +/- 163.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=245000, episode_reward=5.40 +/- 321.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=246000, episode_reward=321.60 +/- 215.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=247000, episode_reward=151.80 +/- 180.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=248000, episode_reward=221.20 +/- 113.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=249000, episode_reward=217.80 +/- 173.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=250000, episode_reward=362.40 +/- 234.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=251000, episode_reward=-75.00 +/- 233.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=252000, episode_reward=64.40 +/- 262.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=253000, episode_reward=95.40 +/- 155.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=254000, episode_reward=169.00 +/- 156.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=255000, episode_reward=83.60 +/- 157.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=256000, episode_reward=188.80 +/- 318.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=257000, episode_reward=121.60 +/- 244.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=258000, episode_reward=152.40 +/- 244.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=259000, episode_reward=287.60 +/- 211.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=260000, episode_reward=353.00 +/- 169.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=261000, episode_reward=276.80 +/- 172.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=262000, episode_reward=186.80 +/- 116.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=263000, episode_reward=88.20 +/- 284.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=264000, episode_reward=172.20 +/- 360.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=265000, episode_reward=322.80 +/- 186.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=266000, episode_reward=-67.80 +/- 149.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=267000, episode_reward=155.20 +/- 130.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=268000, episode_reward=155.20 +/- 263.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=269000, episode_reward=370.80 +/- 207.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=270000, episode_reward=234.20 +/- 186.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=271000, episode_reward=6.60 +/- 126.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=272000, episode_reward=13.60 +/- 158.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=273000, episode_reward=149.20 +/- 100.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=274000, episode_reward=206.20 +/- 155.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=275000, episode_reward=126.40 +/- 322.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=276000, episode_reward=210.00 +/- 180.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=277000, episode_reward=197.00 +/- 278.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=278000, episode_reward=226.20 +/- 255.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=279000, episode_reward=328.00 +/- 134.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=280000, episode_reward=282.00 +/- 66.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=281000, episode_reward=-54.00 +/- 159.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=282000, episode_reward=216.80 +/- 224.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=283000, episode_reward=150.60 +/- 177.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=284000, episode_reward=185.40 +/- 99.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=285000, episode_reward=134.00 +/- 184.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=286000, episode_reward=160.00 +/- 183.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=287000, episode_reward=271.00 +/- 172.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=288000, episode_reward=33.60 +/- 179.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=289000, episode_reward=139.20 +/- 155.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=290000, episode_reward=166.40 +/- 210.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=291000, episode_reward=213.20 +/- 196.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=292000, episode_reward=141.40 +/- 93.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=293000, episode_reward=270.40 +/- 179.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=294000, episode_reward=314.00 +/- 203.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=295000, episode_reward=260.60 +/- 171.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=296000, episode_reward=183.00 +/- 274.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=297000, episode_reward=251.60 +/- 129.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=298000, episode_reward=335.80 +/- 227.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=299000, episode_reward=216.80 +/- 135.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=300000, episode_reward=120.00 +/- 113.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=301000, episode_reward=152.20 +/- 122.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=302000, episode_reward=124.20 +/- 188.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=303000, episode_reward=58.00 +/- 347.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=304000, episode_reward=279.00 +/- 218.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=305000, episode_reward=151.80 +/- 249.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=306000, episode_reward=179.00 +/- 89.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=307000, episode_reward=62.00 +/- 135.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=308000, episode_reward=263.20 +/- 212.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=309000, episode_reward=-0.20 +/- 211.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=310000, episode_reward=190.20 +/- 286.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=311000, episode_reward=266.80 +/- 180.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=312000, episode_reward=154.60 +/- 175.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=313000, episode_reward=61.40 +/- 246.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=314000, episode_reward=309.60 +/- 109.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=315000, episode_reward=111.20 +/- 181.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=316000, episode_reward=294.60 +/- 161.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=317000, episode_reward=64.80 +/- 178.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=318000, episode_reward=148.20 +/- 156.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=319000, episode_reward=278.60 +/- 243.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=320000, episode_reward=65.60 +/- 96.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=321000, episode_reward=236.40 +/- 166.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=322000, episode_reward=88.60 +/- 238.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=323000, episode_reward=104.60 +/- 260.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=324000, episode_reward=-84.40 +/- 202.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=325000, episode_reward=168.60 +/- 164.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=326000, episode_reward=316.60 +/- 211.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=327000, episode_reward=218.80 +/- 174.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=328000, episode_reward=157.60 +/- 165.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=329000, episode_reward=215.40 +/- 199.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=330000, episode_reward=143.60 +/- 299.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=331000, episode_reward=162.20 +/- 87.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=332000, episode_reward=346.40 +/- 238.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=333000, episode_reward=310.80 +/- 136.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=334000, episode_reward=226.20 +/- 58.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=335000, episode_reward=407.40 +/- 180.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=336000, episode_reward=191.80 +/- 56.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=337000, episode_reward=96.20 +/- 76.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=338000, episode_reward=86.80 +/- 179.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=339000, episode_reward=227.80 +/- 99.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=340000, episode_reward=199.60 +/- 166.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=341000, episode_reward=117.80 +/- 38.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=342000, episode_reward=286.60 +/- 211.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=343000, episode_reward=269.80 +/- 178.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=344000, episode_reward=355.80 +/- 153.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=345000, episode_reward=307.60 +/- 117.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=346000, episode_reward=218.60 +/- 150.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=347000, episode_reward=210.20 +/- 142.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=348000, episode_reward=214.20 +/- 252.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=349000, episode_reward=191.00 +/- 213.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=350000, episode_reward=259.00 +/- 164.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=351000, episode_reward=482.80 +/- 534.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=352000, episode_reward=179.80 +/- 51.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=353000, episode_reward=441.80 +/- 279.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=354000, episode_reward=378.80 +/- 161.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=355000, episode_reward=172.20 +/- 133.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=356000, episode_reward=526.60 +/- 213.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=357000, episode_reward=162.20 +/- 187.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=358000, episode_reward=384.80 +/- 316.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=359000, episode_reward=422.80 +/- 139.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=360000, episode_reward=601.20 +/- 156.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=361000, episode_reward=491.20 +/- 182.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=362000, episode_reward=427.60 +/- 293.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=363000, episode_reward=300.40 +/- 72.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=364000, episode_reward=319.60 +/- 290.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=365000, episode_reward=417.80 +/- 153.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=366000, episode_reward=582.00 +/- 507.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=367000, episode_reward=442.20 +/- 230.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=368000, episode_reward=381.20 +/- 133.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=369000, episode_reward=481.80 +/- 313.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=370000, episode_reward=321.60 +/- 316.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=371000, episode_reward=692.40 +/- 304.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=372000, episode_reward=388.40 +/- 97.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=373000, episode_reward=164.20 +/- 224.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=374000, episode_reward=489.60 +/- 75.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=375000, episode_reward=286.20 +/- 525.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=376000, episode_reward=738.20 +/- 270.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=377000, episode_reward=407.40 +/- 182.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=378000, episode_reward=517.60 +/- 348.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=379000, episode_reward=496.20 +/- 265.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=380000, episode_reward=361.60 +/- 292.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=381000, episode_reward=623.00 +/- 241.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=382000, episode_reward=400.80 +/- 429.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=383000, episode_reward=400.00 +/- 193.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=384000, episode_reward=540.40 +/- 371.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=385000, episode_reward=417.40 +/- 422.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=386000, episode_reward=315.40 +/- 242.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=387000, episode_reward=517.20 +/- 395.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=388000, episode_reward=375.00 +/- 233.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=389000, episode_reward=423.60 +/- 330.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=390000, episode_reward=381.20 +/- 278.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=391000, episode_reward=668.00 +/- 227.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=392000, episode_reward=739.00 +/- 154.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=393000, episode_reward=619.40 +/- 175.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=394000, episode_reward=803.00 +/- 247.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=395000, episode_reward=-1316.60 +/- 4006.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=396000, episode_reward=407.00 +/- 186.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=397000, episode_reward=424.00 +/- 269.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=398000, episode_reward=322.40 +/- 129.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=399000, episode_reward=385.00 +/- 118.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=400000, episode_reward=901.60 +/- 146.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=401000, episode_reward=562.60 +/- 316.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=402000, episode_reward=446.20 +/- 130.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=403000, episode_reward=440.80 +/- 160.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=404000, episode_reward=624.60 +/- 258.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=405000, episode_reward=422.80 +/- 123.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=406000, episode_reward=561.60 +/- 243.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=407000, episode_reward=317.20 +/- 339.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=408000, episode_reward=651.00 +/- 248.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=409000, episode_reward=475.80 +/- 187.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=410000, episode_reward=415.80 +/- 237.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=411000, episode_reward=452.40 +/- 192.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=412000, episode_reward=622.20 +/- 296.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=413000, episode_reward=601.40 +/- 256.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=414000, episode_reward=572.40 +/- 223.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=415000, episode_reward=636.80 +/- 228.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=416000, episode_reward=576.60 +/- 317.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=417000, episode_reward=-192.80 +/- 2298.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=418000, episode_reward=418.40 +/- 132.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=419000, episode_reward=-2177.20 +/- 5882.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=420000, episode_reward=563.00 +/- 200.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=421000, episode_reward=488.60 +/- 88.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=422000, episode_reward=612.20 +/- 238.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=423000, episode_reward=643.60 +/- 299.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=424000, episode_reward=576.40 +/- 233.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=425000, episode_reward=762.00 +/- 137.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=426000, episode_reward=686.20 +/- 110.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=427000, episode_reward=751.00 +/- 289.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=428000, episode_reward=546.00 +/- 244.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=429000, episode_reward=837.60 +/- 259.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=430000, episode_reward=806.80 +/- 140.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=431000, episode_reward=395.80 +/- 233.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=432000, episode_reward=485.20 +/- 198.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=433000, episode_reward=808.40 +/- 135.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=434000, episode_reward=241.60 +/- 373.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=435000, episode_reward=555.00 +/- 195.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=436000, episode_reward=830.40 +/- 190.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=437000, episode_reward=450.20 +/- 307.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=438000, episode_reward=625.40 +/- 324.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=439000, episode_reward=685.60 +/- 286.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=440000, episode_reward=551.00 +/- 289.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=441000, episode_reward=640.80 +/- 273.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=442000, episode_reward=617.20 +/- 309.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=443000, episode_reward=605.00 +/- 433.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=444000, episode_reward=745.40 +/- 255.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=445000, episode_reward=603.00 +/- 343.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=446000, episode_reward=574.80 +/- 230.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=447000, episode_reward=707.20 +/- 258.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=448000, episode_reward=794.80 +/- 250.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=449000, episode_reward=727.60 +/- 275.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=450000, episode_reward=757.60 +/- 207.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=451000, episode_reward=541.80 +/- 373.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=452000, episode_reward=524.20 +/- 164.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=453000, episode_reward=609.20 +/- 213.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=454000, episode_reward=603.80 +/- 134.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=455000, episode_reward=598.80 +/- 304.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=456000, episode_reward=871.40 +/- 170.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=457000, episode_reward=915.20 +/- 161.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=458000, episode_reward=718.40 +/- 312.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=459000, episode_reward=609.20 +/- 128.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=460000, episode_reward=554.00 +/- 172.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=461000, episode_reward=607.00 +/- 392.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=462000, episode_reward=905.00 +/- 195.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=463000, episode_reward=-2387.40 +/- 6345.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=464000, episode_reward=903.20 +/- 326.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=465000, episode_reward=473.80 +/- 138.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=466000, episode_reward=614.80 +/- 140.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=467000, episode_reward=795.00 +/- 322.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=468000, episode_reward=427.20 +/- 110.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=469000, episode_reward=712.40 +/- 173.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=470000, episode_reward=662.00 +/- 162.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=471000, episode_reward=779.80 +/- 163.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=472000, episode_reward=664.00 +/- 187.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=473000, episode_reward=571.20 +/- 174.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=474000, episode_reward=-1544.60 +/- 4422.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=475000, episode_reward=701.20 +/- 236.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=476000, episode_reward=659.00 +/- 260.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=477000, episode_reward=711.80 +/- 177.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=478000, episode_reward=666.80 +/- 398.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=479000, episode_reward=546.60 +/- 280.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=480000, episode_reward=656.80 +/- 162.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=481000, episode_reward=373.40 +/- 294.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=482000, episode_reward=526.40 +/- 291.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=483000, episode_reward=798.80 +/- 185.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=484000, episode_reward=622.80 +/- 58.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=485000, episode_reward=435.20 +/- 305.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=486000, episode_reward=633.00 +/- 179.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=487000, episode_reward=532.00 +/- 125.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=488000, episode_reward=870.00 +/- 96.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=489000, episode_reward=653.60 +/- 171.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=490000, episode_reward=715.60 +/- 374.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=491000, episode_reward=373.40 +/- 180.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=492000, episode_reward=480.60 +/- 87.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=493000, episode_reward=694.40 +/- 62.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=494000, episode_reward=465.80 +/- 237.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=495000, episode_reward=369.20 +/- 142.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=496000, episode_reward=692.80 +/- 500.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=497000, episode_reward=743.40 +/- 280.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=498000, episode_reward=490.00 +/- 144.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=499000, episode_reward=617.80 +/- 154.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=500000, episode_reward=657.00 +/- 130.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Gamma = 0.8, Learning Rate = 3e-05, Activation Function = ReLU, Neurons = 8, Process noise = 0.1, Measurement noise = 0.0\n",
      "Eval num_timesteps=1000, episode_reward=-1468.00 +/- 111.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=-1551.00 +/- 77.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=3000, episode_reward=-1476.00 +/- 66.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=4000, episode_reward=-1526.00 +/- 94.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=5000, episode_reward=-1505.00 +/- 66.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=6000, episode_reward=-1492.00 +/- 78.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=7000, episode_reward=-1531.00 +/- 79.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=8000, episode_reward=-1468.00 +/- 85.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=9000, episode_reward=-1481.00 +/- 63.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=10000, episode_reward=-1508.00 +/- 69.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=11000, episode_reward=-1538.00 +/- 121.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=12000, episode_reward=-1449.00 +/- 114.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=13000, episode_reward=-1430.00 +/- 94.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=14000, episode_reward=-1526.00 +/- 84.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=15000, episode_reward=-1462.00 +/- 62.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=16000, episode_reward=-1478.00 +/- 102.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=17000, episode_reward=-1505.00 +/- 77.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=18000, episode_reward=-1481.00 +/- 72.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=19000, episode_reward=-1547.00 +/- 45.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=20000, episode_reward=-1448.00 +/- 72.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=21000, episode_reward=-1478.00 +/- 52.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=22000, episode_reward=-1559.00 +/- 64.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=23000, episode_reward=-1529.00 +/- 157.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=24000, episode_reward=-1555.00 +/- 52.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=25000, episode_reward=-1505.00 +/- 66.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=26000, episode_reward=-1576.00 +/- 70.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=27000, episode_reward=-1545.00 +/- 94.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=28000, episode_reward=-1509.00 +/- 69.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=29000, episode_reward=-1411.00 +/- 89.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=30000, episode_reward=-1475.00 +/- 79.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=31000, episode_reward=-1521.00 +/- 105.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=32000, episode_reward=-1497.00 +/- 96.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=33000, episode_reward=-1467.00 +/- 58.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=34000, episode_reward=-1484.00 +/- 71.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=35000, episode_reward=-1470.00 +/- 80.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=36000, episode_reward=-1392.00 +/- 98.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=37000, episode_reward=-1526.00 +/- 72.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=38000, episode_reward=-1534.00 +/- 32.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=39000, episode_reward=-1540.00 +/- 29.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=40000, episode_reward=-1526.00 +/- 83.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=41000, episode_reward=-1450.00 +/- 62.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=42000, episode_reward=-1489.00 +/- 45.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=43000, episode_reward=-1481.00 +/- 38.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=44000, episode_reward=-1463.00 +/- 71.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=45000, episode_reward=-1398.00 +/- 109.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=46000, episode_reward=-1518.00 +/- 42.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=47000, episode_reward=-1507.00 +/- 22.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=48000, episode_reward=-1537.00 +/- 58.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=49000, episode_reward=-1535.00 +/- 74.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=50000, episode_reward=-1534.00 +/- 81.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=51000, episode_reward=-1543.00 +/- 57.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=52000, episode_reward=-1390.00 +/- 70.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=53000, episode_reward=-1530.00 +/- 55.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=54000, episode_reward=-2378.60 +/- 59.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=55000, episode_reward=-2448.20 +/- 51.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=56000, episode_reward=-1532.80 +/- 68.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=57000, episode_reward=-1335.00 +/- 85.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=58000, episode_reward=-941.20 +/- 64.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=59000, episode_reward=-255.20 +/- 137.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=60000, episode_reward=32.40 +/- 170.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=61000, episode_reward=127.80 +/- 64.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=62000, episode_reward=-78.80 +/- 159.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=63000, episode_reward=143.60 +/- 86.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=64000, episode_reward=-47.00 +/- 129.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=65000, episode_reward=44.40 +/- 177.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=66000, episode_reward=15.00 +/- 147.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=67000, episode_reward=131.00 +/- 131.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=68000, episode_reward=7.20 +/- 116.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=69000, episode_reward=26.20 +/- 187.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=70000, episode_reward=73.80 +/- 142.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=71000, episode_reward=-73.80 +/- 117.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=72000, episode_reward=-38.20 +/- 106.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=73000, episode_reward=34.00 +/- 157.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=74000, episode_reward=-86.60 +/- 153.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=75000, episode_reward=-1.00 +/- 208.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=76000, episode_reward=84.60 +/- 156.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=77000, episode_reward=43.60 +/- 167.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=78000, episode_reward=85.60 +/- 157.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=79000, episode_reward=148.20 +/- 155.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=80000, episode_reward=-139.80 +/- 51.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=81000, episode_reward=96.40 +/- 162.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=82000, episode_reward=124.80 +/- 125.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=83000, episode_reward=66.80 +/- 150.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=84000, episode_reward=78.40 +/- 125.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=85000, episode_reward=-122.20 +/- 93.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=86000, episode_reward=102.00 +/- 99.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=87000, episode_reward=107.00 +/- 145.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=88000, episode_reward=-44.20 +/- 135.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=89000, episode_reward=134.00 +/- 149.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=90000, episode_reward=-17.20 +/- 184.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=91000, episode_reward=46.00 +/- 130.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=92000, episode_reward=1.80 +/- 116.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=93000, episode_reward=34.00 +/- 153.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=94000, episode_reward=49.60 +/- 202.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=95000, episode_reward=102.60 +/- 145.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=96000, episode_reward=-25.40 +/- 146.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=97000, episode_reward=66.60 +/- 176.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=98000, episode_reward=110.20 +/- 164.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=99000, episode_reward=84.00 +/- 61.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=100000, episode_reward=32.40 +/- 224.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=101000, episode_reward=-2.00 +/- 135.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=102000, episode_reward=28.00 +/- 110.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=103000, episode_reward=54.80 +/- 165.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=104000, episode_reward=8.20 +/- 128.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=105000, episode_reward=-17.00 +/- 152.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=106000, episode_reward=46.80 +/- 157.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=107000, episode_reward=-56.00 +/- 117.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=108000, episode_reward=46.80 +/- 122.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=109000, episode_reward=-47.20 +/- 144.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=110000, episode_reward=131.00 +/- 46.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=111000, episode_reward=145.60 +/- 38.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=112000, episode_reward=187.80 +/- 66.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=113000, episode_reward=53.20 +/- 166.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=114000, episode_reward=-93.00 +/- 83.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=115000, episode_reward=92.80 +/- 83.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=116000, episode_reward=118.60 +/- 121.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=117000, episode_reward=108.00 +/- 138.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=118000, episode_reward=39.00 +/- 172.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=119000, episode_reward=-6.00 +/- 136.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=120000, episode_reward=57.60 +/- 159.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=121000, episode_reward=133.40 +/- 125.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=122000, episode_reward=36.00 +/- 156.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=123000, episode_reward=-29.80 +/- 212.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=124000, episode_reward=77.60 +/- 138.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=125000, episode_reward=-29.00 +/- 98.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=126000, episode_reward=-106.00 +/- 132.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=127000, episode_reward=97.00 +/- 113.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=128000, episode_reward=168.60 +/- 152.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=129000, episode_reward=129.00 +/- 101.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=130000, episode_reward=136.80 +/- 143.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=131000, episode_reward=159.60 +/- 48.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=132000, episode_reward=-26.40 +/- 129.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=133000, episode_reward=-64.40 +/- 122.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=134000, episode_reward=168.80 +/- 40.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=135000, episode_reward=-80.80 +/- 199.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=136000, episode_reward=-48.20 +/- 162.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=137000, episode_reward=4.60 +/- 160.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=138000, episode_reward=97.20 +/- 64.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=139000, episode_reward=41.00 +/- 131.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=140000, episode_reward=60.80 +/- 141.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=141000, episode_reward=-33.80 +/- 115.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=142000, episode_reward=111.60 +/- 198.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=143000, episode_reward=-28.20 +/- 164.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=144000, episode_reward=111.40 +/- 182.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=145000, episode_reward=-82.60 +/- 168.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=146000, episode_reward=47.80 +/- 203.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=147000, episode_reward=50.60 +/- 193.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=148000, episode_reward=100.60 +/- 187.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=149000, episode_reward=-156.20 +/- 78.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=150000, episode_reward=-49.60 +/- 144.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=151000, episode_reward=126.40 +/- 171.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=152000, episode_reward=-16.80 +/- 210.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=153000, episode_reward=115.40 +/- 156.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=154000, episode_reward=-78.80 +/- 142.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=155000, episode_reward=80.60 +/- 145.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=156000, episode_reward=34.40 +/- 166.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=157000, episode_reward=-5.00 +/- 186.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=158000, episode_reward=104.80 +/- 128.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=159000, episode_reward=-75.20 +/- 20.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=160000, episode_reward=48.80 +/- 154.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=161000, episode_reward=22.60 +/- 124.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=162000, episode_reward=133.80 +/- 88.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=163000, episode_reward=95.40 +/- 145.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=164000, episode_reward=-27.80 +/- 166.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=165000, episode_reward=-89.60 +/- 102.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=166000, episode_reward=-53.80 +/- 157.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=167000, episode_reward=98.20 +/- 133.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=168000, episode_reward=45.00 +/- 96.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=169000, episode_reward=152.80 +/- 124.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=170000, episode_reward=-57.00 +/- 190.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=171000, episode_reward=130.80 +/- 142.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=172000, episode_reward=195.40 +/- 39.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=173000, episode_reward=108.80 +/- 119.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=174000, episode_reward=-54.60 +/- 151.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=175000, episode_reward=55.80 +/- 177.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=176000, episode_reward=154.60 +/- 68.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=177000, episode_reward=-109.80 +/- 131.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=178000, episode_reward=144.80 +/- 87.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=179000, episode_reward=111.00 +/- 144.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=180000, episode_reward=-2.20 +/- 228.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=181000, episode_reward=56.00 +/- 159.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=182000, episode_reward=185.20 +/- 118.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=183000, episode_reward=18.60 +/- 178.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=184000, episode_reward=-14.60 +/- 143.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=185000, episode_reward=13.60 +/- 197.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=186000, episode_reward=37.00 +/- 185.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=187000, episode_reward=137.80 +/- 163.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=188000, episode_reward=13.00 +/- 127.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=189000, episode_reward=64.20 +/- 178.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=190000, episode_reward=176.80 +/- 151.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=191000, episode_reward=111.40 +/- 125.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=192000, episode_reward=99.00 +/- 128.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=193000, episode_reward=93.80 +/- 198.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=194000, episode_reward=54.40 +/- 182.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=195000, episode_reward=78.00 +/- 147.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=196000, episode_reward=106.80 +/- 128.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=197000, episode_reward=193.20 +/- 28.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=198000, episode_reward=81.20 +/- 162.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=199000, episode_reward=82.00 +/- 139.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=200000, episode_reward=96.60 +/- 180.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=201000, episode_reward=46.00 +/- 149.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=202000, episode_reward=-57.40 +/- 94.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=203000, episode_reward=33.00 +/- 159.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=204000, episode_reward=88.20 +/- 124.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=205000, episode_reward=191.80 +/- 37.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=206000, episode_reward=94.20 +/- 91.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=207000, episode_reward=28.60 +/- 205.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=208000, episode_reward=24.20 +/- 155.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=209000, episode_reward=-8.80 +/- 99.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=210000, episode_reward=5.20 +/- 142.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=211000, episode_reward=104.60 +/- 149.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=212000, episode_reward=100.40 +/- 139.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=213000, episode_reward=122.80 +/- 249.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=214000, episode_reward=-69.20 +/- 161.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=215000, episode_reward=-31.60 +/- 194.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=216000, episode_reward=-612.20 +/- 1431.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=217000, episode_reward=-6392.80 +/- 1717.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=218000, episode_reward=-4118.00 +/- 1837.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=219000, episode_reward=-4994.00 +/- 2110.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=220000, episode_reward=283.00 +/- 284.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=221000, episode_reward=9.60 +/- 239.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=222000, episode_reward=-570.80 +/- 569.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=223000, episode_reward=-183.40 +/- 497.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=224000, episode_reward=121.60 +/- 267.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=225000, episode_reward=-50.20 +/- 297.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=226000, episode_reward=89.80 +/- 311.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=227000, episode_reward=-139.80 +/- 677.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=228000, episode_reward=-252.60 +/- 455.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=229000, episode_reward=92.00 +/- 547.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=230000, episode_reward=-437.40 +/- 617.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=231000, episode_reward=-327.20 +/- 906.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=232000, episode_reward=-233.00 +/- 610.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=233000, episode_reward=-285.40 +/- 347.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=234000, episode_reward=102.20 +/- 131.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=235000, episode_reward=-47.00 +/- 437.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=236000, episode_reward=-224.80 +/- 657.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=237000, episode_reward=-212.60 +/- 395.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=238000, episode_reward=122.00 +/- 351.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=239000, episode_reward=130.20 +/- 126.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=240000, episode_reward=-494.40 +/- 369.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=241000, episode_reward=-439.80 +/- 285.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=242000, episode_reward=-43.60 +/- 478.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=243000, episode_reward=-16.40 +/- 273.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=244000, episode_reward=98.80 +/- 391.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=245000, episode_reward=275.40 +/- 97.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=246000, episode_reward=-45.60 +/- 460.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=247000, episode_reward=-39.00 +/- 187.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=248000, episode_reward=39.20 +/- 172.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=249000, episode_reward=-187.60 +/- 385.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=250000, episode_reward=206.40 +/- 147.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=251000, episode_reward=-218.60 +/- 435.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=252000, episode_reward=179.20 +/- 169.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=253000, episode_reward=-217.60 +/- 238.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=254000, episode_reward=50.40 +/- 256.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=255000, episode_reward=-18.00 +/- 653.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=256000, episode_reward=-281.60 +/- 686.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=257000, episode_reward=-185.40 +/- 507.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=258000, episode_reward=-284.20 +/- 358.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=259000, episode_reward=-97.00 +/- 302.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=260000, episode_reward=59.00 +/- 332.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=261000, episode_reward=75.40 +/- 291.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=262000, episode_reward=100.80 +/- 378.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=263000, episode_reward=-349.40 +/- 433.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=264000, episode_reward=232.20 +/- 209.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=265000, episode_reward=-407.60 +/- 667.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=266000, episode_reward=122.20 +/- 339.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=267000, episode_reward=231.20 +/- 280.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=268000, episode_reward=-614.20 +/- 745.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=269000, episode_reward=238.20 +/- 324.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=270000, episode_reward=-76.60 +/- 369.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=271000, episode_reward=-93.80 +/- 401.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=272000, episode_reward=151.20 +/- 176.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=273000, episode_reward=-92.20 +/- 463.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=274000, episode_reward=-63.20 +/- 476.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=275000, episode_reward=-87.20 +/- 212.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=276000, episode_reward=151.80 +/- 159.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=277000, episode_reward=-682.40 +/- 1059.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=278000, episode_reward=-266.80 +/- 976.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=279000, episode_reward=-226.40 +/- 566.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=280000, episode_reward=-99.80 +/- 136.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=281000, episode_reward=-329.60 +/- 771.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=282000, episode_reward=-522.60 +/- 1134.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=283000, episode_reward=-10.80 +/- 207.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=284000, episode_reward=-182.20 +/- 398.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=285000, episode_reward=-199.20 +/- 592.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=286000, episode_reward=110.40 +/- 179.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=287000, episode_reward=-566.60 +/- 924.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=288000, episode_reward=-252.20 +/- 193.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=289000, episode_reward=93.00 +/- 166.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=290000, episode_reward=-292.60 +/- 1042.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=291000, episode_reward=-204.60 +/- 368.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=292000, episode_reward=125.20 +/- 259.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=293000, episode_reward=-222.60 +/- 547.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=294000, episode_reward=-445.40 +/- 598.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=295000, episode_reward=-1.80 +/- 534.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=296000, episode_reward=-50.00 +/- 336.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=297000, episode_reward=-177.00 +/- 302.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=298000, episode_reward=-0.40 +/- 514.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=299000, episode_reward=-158.00 +/- 379.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=300000, episode_reward=-256.60 +/- 779.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=301000, episode_reward=-73.40 +/- 242.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=302000, episode_reward=4.00 +/- 343.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=303000, episode_reward=-124.80 +/- 448.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=304000, episode_reward=-43.40 +/- 264.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=305000, episode_reward=-70.60 +/- 382.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=306000, episode_reward=85.80 +/- 260.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=307000, episode_reward=32.40 +/- 319.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=308000, episode_reward=-91.20 +/- 629.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=309000, episode_reward=52.00 +/- 163.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=310000, episode_reward=-4.80 +/- 256.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=311000, episode_reward=-341.40 +/- 405.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=312000, episode_reward=174.80 +/- 270.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=313000, episode_reward=-42.00 +/- 364.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=314000, episode_reward=222.20 +/- 232.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=315000, episode_reward=-184.00 +/- 239.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=316000, episode_reward=-422.40 +/- 281.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=317000, episode_reward=-214.80 +/- 251.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=318000, episode_reward=-417.00 +/- 526.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=319000, episode_reward=33.80 +/- 239.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=320000, episode_reward=-184.20 +/- 335.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=321000, episode_reward=-74.60 +/- 167.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=322000, episode_reward=-42.20 +/- 357.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=323000, episode_reward=-35.80 +/- 243.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=324000, episode_reward=-173.60 +/- 295.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=325000, episode_reward=-136.20 +/- 396.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=326000, episode_reward=137.80 +/- 247.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=327000, episode_reward=81.40 +/- 352.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=328000, episode_reward=-88.60 +/- 221.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=329000, episode_reward=182.60 +/- 148.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=330000, episode_reward=210.00 +/- 204.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=331000, episode_reward=-107.60 +/- 297.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=332000, episode_reward=59.80 +/- 305.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=333000, episode_reward=-79.00 +/- 241.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=334000, episode_reward=111.20 +/- 220.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=335000, episode_reward=-96.80 +/- 411.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=336000, episode_reward=-167.60 +/- 316.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=337000, episode_reward=-345.00 +/- 352.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=338000, episode_reward=1.40 +/- 152.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=339000, episode_reward=107.40 +/- 92.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=340000, episode_reward=-87.00 +/- 351.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=341000, episode_reward=-37.20 +/- 251.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=342000, episode_reward=328.80 +/- 135.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=343000, episode_reward=-291.20 +/- 165.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=344000, episode_reward=-60.40 +/- 244.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=345000, episode_reward=178.80 +/- 282.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=346000, episode_reward=-324.60 +/- 456.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=347000, episode_reward=-241.80 +/- 322.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=348000, episode_reward=189.20 +/- 274.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=349000, episode_reward=62.40 +/- 47.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=350000, episode_reward=-142.80 +/- 534.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=351000, episode_reward=-6.80 +/- 175.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=352000, episode_reward=-164.80 +/- 105.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=353000, episode_reward=-226.60 +/- 267.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=354000, episode_reward=178.80 +/- 212.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=355000, episode_reward=47.00 +/- 329.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=356000, episode_reward=-14.40 +/- 391.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=357000, episode_reward=201.40 +/- 207.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=358000, episode_reward=107.20 +/- 328.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=359000, episode_reward=2.60 +/- 152.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=360000, episode_reward=125.40 +/- 299.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=361000, episode_reward=207.00 +/- 78.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=362000, episode_reward=125.40 +/- 159.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=363000, episode_reward=252.00 +/- 185.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=364000, episode_reward=160.40 +/- 294.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=365000, episode_reward=235.40 +/- 199.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=366000, episode_reward=-126.00 +/- 130.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=367000, episode_reward=214.80 +/- 170.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=368000, episode_reward=188.00 +/- 237.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=369000, episode_reward=55.00 +/- 244.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=370000, episode_reward=-75.20 +/- 364.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=371000, episode_reward=47.00 +/- 255.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=372000, episode_reward=231.40 +/- 197.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=373000, episode_reward=40.00 +/- 353.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=374000, episode_reward=365.40 +/- 124.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=375000, episode_reward=32.00 +/- 233.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=376000, episode_reward=-57.80 +/- 136.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=377000, episode_reward=216.40 +/- 305.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=378000, episode_reward=10.40 +/- 187.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=379000, episode_reward=69.40 +/- 190.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=380000, episode_reward=91.80 +/- 253.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=381000, episode_reward=-56.00 +/- 339.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=382000, episode_reward=251.20 +/- 340.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=383000, episode_reward=126.00 +/- 168.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=384000, episode_reward=29.80 +/- 270.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=385000, episode_reward=57.80 +/- 346.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=386000, episode_reward=13.60 +/- 252.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=387000, episode_reward=-90.80 +/- 610.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=388000, episode_reward=159.00 +/- 66.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=389000, episode_reward=326.60 +/- 151.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=390000, episode_reward=148.60 +/- 354.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=391000, episode_reward=144.20 +/- 271.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=392000, episode_reward=-171.00 +/- 178.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=393000, episode_reward=-17.80 +/- 241.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=394000, episode_reward=152.00 +/- 123.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=395000, episode_reward=-69.20 +/- 272.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=396000, episode_reward=-89.80 +/- 336.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=397000, episode_reward=100.00 +/- 443.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=398000, episode_reward=138.60 +/- 361.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=399000, episode_reward=250.00 +/- 176.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=400000, episode_reward=51.60 +/- 332.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=401000, episode_reward=110.80 +/- 200.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=402000, episode_reward=-88.60 +/- 131.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=403000, episode_reward=226.80 +/- 44.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=404000, episode_reward=133.20 +/- 85.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=405000, episode_reward=191.60 +/- 112.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=406000, episode_reward=-119.80 +/- 333.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=407000, episode_reward=-118.80 +/- 251.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=408000, episode_reward=-14.40 +/- 153.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=409000, episode_reward=220.00 +/- 211.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=410000, episode_reward=-73.40 +/- 292.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=411000, episode_reward=85.80 +/- 221.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=412000, episode_reward=-296.40 +/- 259.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=413000, episode_reward=82.40 +/- 227.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=414000, episode_reward=-199.20 +/- 359.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=415000, episode_reward=128.60 +/- 356.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=416000, episode_reward=148.40 +/- 237.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=417000, episode_reward=-58.60 +/- 320.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=418000, episode_reward=-97.40 +/- 281.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=419000, episode_reward=143.20 +/- 72.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=420000, episode_reward=-38.60 +/- 64.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=421000, episode_reward=94.80 +/- 295.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=422000, episode_reward=-15.60 +/- 326.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=423000, episode_reward=148.40 +/- 231.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=424000, episode_reward=122.00 +/- 152.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=425000, episode_reward=-13.80 +/- 257.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=426000, episode_reward=204.20 +/- 169.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=427000, episode_reward=-182.40 +/- 234.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=428000, episode_reward=-56.00 +/- 255.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=429000, episode_reward=207.40 +/- 218.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=430000, episode_reward=104.40 +/- 390.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=431000, episode_reward=43.20 +/- 354.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=432000, episode_reward=308.60 +/- 187.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=433000, episode_reward=4.60 +/- 153.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=434000, episode_reward=-92.60 +/- 280.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=435000, episode_reward=-99.40 +/- 215.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=436000, episode_reward=199.00 +/- 182.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=437000, episode_reward=-93.40 +/- 186.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=438000, episode_reward=-49.20 +/- 196.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=439000, episode_reward=142.80 +/- 252.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=440000, episode_reward=92.40 +/- 179.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=441000, episode_reward=246.40 +/- 269.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=442000, episode_reward=240.80 +/- 448.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=443000, episode_reward=234.60 +/- 294.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=444000, episode_reward=200.80 +/- 298.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=445000, episode_reward=-71.80 +/- 277.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=446000, episode_reward=236.40 +/- 155.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=447000, episode_reward=-3.60 +/- 285.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=448000, episode_reward=-108.40 +/- 189.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=449000, episode_reward=228.40 +/- 244.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=450000, episode_reward=-295.80 +/- 192.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=451000, episode_reward=236.80 +/- 113.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=452000, episode_reward=191.40 +/- 402.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=453000, episode_reward=261.40 +/- 212.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=454000, episode_reward=158.80 +/- 361.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=455000, episode_reward=-99.40 +/- 211.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=456000, episode_reward=88.40 +/- 323.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=457000, episode_reward=18.00 +/- 462.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=458000, episode_reward=68.00 +/- 348.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=459000, episode_reward=134.00 +/- 193.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=460000, episode_reward=140.40 +/- 204.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=461000, episode_reward=43.00 +/- 269.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=462000, episode_reward=-125.60 +/- 429.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=463000, episode_reward=-285.20 +/- 436.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=464000, episode_reward=0.40 +/- 217.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=465000, episode_reward=81.00 +/- 390.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=466000, episode_reward=283.60 +/- 259.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=467000, episode_reward=-146.80 +/- 244.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=468000, episode_reward=-5.00 +/- 189.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=469000, episode_reward=-102.60 +/- 363.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=470000, episode_reward=-86.20 +/- 195.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=471000, episode_reward=65.80 +/- 130.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=472000, episode_reward=-135.80 +/- 334.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=473000, episode_reward=173.40 +/- 224.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=474000, episode_reward=53.20 +/- 284.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=475000, episode_reward=216.20 +/- 240.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=476000, episode_reward=-58.00 +/- 128.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=477000, episode_reward=-64.00 +/- 248.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=478000, episode_reward=-82.60 +/- 99.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=479000, episode_reward=341.20 +/- 159.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=480000, episode_reward=-14.80 +/- 295.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=481000, episode_reward=142.20 +/- 200.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=482000, episode_reward=156.40 +/- 276.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=483000, episode_reward=-46.60 +/- 243.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=484000, episode_reward=79.00 +/- 347.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=485000, episode_reward=-13.80 +/- 271.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=486000, episode_reward=49.80 +/- 171.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=487000, episode_reward=80.00 +/- 290.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=488000, episode_reward=15.00 +/- 317.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=489000, episode_reward=-166.60 +/- 330.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=490000, episode_reward=348.80 +/- 183.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=491000, episode_reward=115.60 +/- 200.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=492000, episode_reward=-21.00 +/- 491.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=493000, episode_reward=32.40 +/- 178.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=494000, episode_reward=-4.80 +/- 382.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=495000, episode_reward=-133.80 +/- 613.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=496000, episode_reward=-211.40 +/- 379.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=497000, episode_reward=-141.40 +/- 408.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=498000, episode_reward=-124.20 +/- 365.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=499000, episode_reward=160.00 +/- 295.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=500000, episode_reward=143.80 +/- 163.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Gamma = 0.8, Learning Rate = 3e-05, Activation Function = ReLU, Neurons = 32, Process noise = 0.1, Measurement noise = 0.0\n",
      "Eval num_timesteps=1000, episode_reward=-573.20 +/- 32.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=-600.60 +/- 69.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=3000, episode_reward=-592.80 +/- 64.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=4000, episode_reward=-606.60 +/- 60.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=5000, episode_reward=-532.60 +/- 51.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=6000, episode_reward=-605.20 +/- 49.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=7000, episode_reward=-634.40 +/- 84.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=8000, episode_reward=-638.00 +/- 23.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=9000, episode_reward=-601.00 +/- 49.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=10000, episode_reward=-529.40 +/- 77.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=11000, episode_reward=-598.00 +/- 46.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=12000, episode_reward=-618.00 +/- 64.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=13000, episode_reward=-560.40 +/- 116.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=14000, episode_reward=-599.20 +/- 59.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=15000, episode_reward=-574.80 +/- 87.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=16000, episode_reward=-587.20 +/- 71.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=17000, episode_reward=-653.00 +/- 113.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=18000, episode_reward=-601.80 +/- 57.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=19000, episode_reward=-542.20 +/- 68.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=20000, episode_reward=-588.00 +/- 50.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=21000, episode_reward=-551.80 +/- 49.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=22000, episode_reward=-574.40 +/- 85.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=23000, episode_reward=-581.40 +/- 96.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=24000, episode_reward=-624.20 +/- 65.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=25000, episode_reward=-594.80 +/- 93.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=26000, episode_reward=-547.40 +/- 48.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=27000, episode_reward=-513.60 +/- 67.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=28000, episode_reward=-611.00 +/- 44.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=29000, episode_reward=-588.20 +/- 78.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=30000, episode_reward=-567.60 +/- 101.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=31000, episode_reward=-560.60 +/- 123.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=32000, episode_reward=-642.80 +/- 77.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=33000, episode_reward=-596.60 +/- 89.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=34000, episode_reward=-587.00 +/- 22.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=35000, episode_reward=-568.80 +/- 68.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=36000, episode_reward=-617.20 +/- 73.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=37000, episode_reward=-669.60 +/- 66.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=38000, episode_reward=-566.00 +/- 95.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=39000, episode_reward=-554.60 +/- 74.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=40000, episode_reward=-612.60 +/- 45.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=41000, episode_reward=-567.80 +/- 75.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=42000, episode_reward=-599.00 +/- 37.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=43000, episode_reward=-609.40 +/- 58.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=44000, episode_reward=-489.40 +/- 144.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=45000, episode_reward=-531.20 +/- 77.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=46000, episode_reward=-638.20 +/- 96.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=47000, episode_reward=-608.20 +/- 113.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=48000, episode_reward=-619.00 +/- 88.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=49000, episode_reward=-640.00 +/- 114.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=50000, episode_reward=-570.40 +/- 63.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=51000, episode_reward=-589.60 +/- 79.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=52000, episode_reward=-517.60 +/- 56.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=53000, episode_reward=-537.40 +/- 66.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=54000, episode_reward=-7375.60 +/- 2032.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=55000, episode_reward=204.00 +/- 159.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=56000, episode_reward=405.40 +/- 221.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=57000, episode_reward=244.00 +/- 299.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=58000, episode_reward=215.20 +/- 80.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=59000, episode_reward=139.20 +/- 224.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=60000, episode_reward=-37.20 +/- 173.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=61000, episode_reward=-310.00 +/- 259.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=62000, episode_reward=-4916.20 +/- 929.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=63000, episode_reward=-7132.00 +/- 2709.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=64000, episode_reward=-3654.00 +/- 885.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=65000, episode_reward=178.80 +/- 86.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=66000, episode_reward=157.00 +/- 301.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=67000, episode_reward=-71.00 +/- 182.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=68000, episode_reward=49.80 +/- 106.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=69000, episode_reward=39.40 +/- 164.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=70000, episode_reward=67.00 +/- 198.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=71000, episode_reward=260.60 +/- 130.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=72000, episode_reward=227.40 +/- 149.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=73000, episode_reward=-40.80 +/- 171.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=74000, episode_reward=26.60 +/- 187.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=75000, episode_reward=54.00 +/- 186.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=76000, episode_reward=110.40 +/- 277.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=77000, episode_reward=162.20 +/- 224.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=78000, episode_reward=10.40 +/- 234.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=79000, episode_reward=-18.00 +/- 128.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=80000, episode_reward=-121.60 +/- 198.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=81000, episode_reward=-51.80 +/- 216.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=82000, episode_reward=-118.00 +/- 79.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=83000, episode_reward=-136.80 +/- 301.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=84000, episode_reward=-67.20 +/- 410.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=85000, episode_reward=-175.40 +/- 220.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=86000, episode_reward=-42.20 +/- 265.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=87000, episode_reward=-227.60 +/- 189.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=88000, episode_reward=-190.20 +/- 212.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=89000, episode_reward=-140.00 +/- 257.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=90000, episode_reward=-302.40 +/- 167.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=91000, episode_reward=-290.20 +/- 187.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=92000, episode_reward=-309.80 +/- 406.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=93000, episode_reward=-340.00 +/- 329.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=94000, episode_reward=-403.40 +/- 251.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=95000, episode_reward=-522.80 +/- 243.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=96000, episode_reward=-400.00 +/- 224.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=97000, episode_reward=-650.40 +/- 260.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=98000, episode_reward=-497.20 +/- 374.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=99000, episode_reward=-481.80 +/- 188.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=100000, episode_reward=-631.00 +/- 118.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=101000, episode_reward=-451.20 +/- 181.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=102000, episode_reward=-672.20 +/- 311.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=103000, episode_reward=-602.40 +/- 216.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=104000, episode_reward=-524.20 +/- 329.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=105000, episode_reward=-928.80 +/- 508.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=106000, episode_reward=-744.40 +/- 205.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=107000, episode_reward=-764.40 +/- 279.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=108000, episode_reward=-938.00 +/- 221.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=109000, episode_reward=-944.20 +/- 220.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=110000, episode_reward=-591.20 +/- 272.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=111000, episode_reward=-784.80 +/- 262.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=112000, episode_reward=-1102.60 +/- 185.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=113000, episode_reward=-942.00 +/- 523.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=114000, episode_reward=-758.60 +/- 348.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=115000, episode_reward=-629.00 +/- 235.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=116000, episode_reward=-558.40 +/- 259.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=117000, episode_reward=-548.40 +/- 298.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=118000, episode_reward=-506.80 +/- 310.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=119000, episode_reward=-358.20 +/- 260.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=120000, episode_reward=-469.80 +/- 411.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=121000, episode_reward=-458.20 +/- 382.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=122000, episode_reward=-382.40 +/- 296.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=123000, episode_reward=-600.40 +/- 179.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=124000, episode_reward=-621.80 +/- 198.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=125000, episode_reward=-572.60 +/- 148.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=126000, episode_reward=-327.60 +/- 233.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=127000, episode_reward=-172.00 +/- 231.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=128000, episode_reward=-365.60 +/- 194.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=129000, episode_reward=-491.20 +/- 213.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=130000, episode_reward=-119.40 +/- 291.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=131000, episode_reward=-307.40 +/- 301.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=132000, episode_reward=-31.20 +/- 129.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=133000, episode_reward=-328.00 +/- 300.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=134000, episode_reward=-230.40 +/- 274.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=135000, episode_reward=-170.20 +/- 91.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=136000, episode_reward=-268.60 +/- 142.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=137000, episode_reward=-329.00 +/- 146.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=138000, episode_reward=-280.20 +/- 182.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=139000, episode_reward=-66.40 +/- 129.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=140000, episode_reward=-176.80 +/- 320.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=141000, episode_reward=-443.00 +/- 177.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=142000, episode_reward=-272.20 +/- 182.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=143000, episode_reward=-108.20 +/- 178.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=144000, episode_reward=-155.00 +/- 288.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=145000, episode_reward=-105.00 +/- 62.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=146000, episode_reward=-125.20 +/- 210.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=147000, episode_reward=6.40 +/- 87.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=148000, episode_reward=-115.60 +/- 211.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=149000, episode_reward=-16.60 +/- 172.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=150000, episode_reward=-82.60 +/- 119.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=151000, episode_reward=51.60 +/- 147.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=152000, episode_reward=-48.00 +/- 157.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=153000, episode_reward=-93.80 +/- 332.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=154000, episode_reward=-15.60 +/- 111.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=155000, episode_reward=-184.20 +/- 120.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=156000, episode_reward=-87.00 +/- 297.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=157000, episode_reward=-2.80 +/- 256.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=158000, episode_reward=-125.20 +/- 461.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=159000, episode_reward=-26.60 +/- 129.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=160000, episode_reward=-7.80 +/- 214.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=161000, episode_reward=32.00 +/- 107.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=162000, episode_reward=-257.60 +/- 198.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=163000, episode_reward=112.60 +/- 336.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=164000, episode_reward=-164.60 +/- 46.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=165000, episode_reward=-50.00 +/- 254.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=166000, episode_reward=-72.80 +/- 225.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=167000, episode_reward=-51.40 +/- 145.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=168000, episode_reward=111.60 +/- 168.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=169000, episode_reward=-34.80 +/- 299.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=170000, episode_reward=38.80 +/- 61.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=171000, episode_reward=10.40 +/- 236.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=172000, episode_reward=82.40 +/- 231.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=173000, episode_reward=-14.40 +/- 181.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=174000, episode_reward=-16.60 +/- 95.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=175000, episode_reward=161.20 +/- 262.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=176000, episode_reward=-77.00 +/- 91.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=177000, episode_reward=49.80 +/- 180.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=178000, episode_reward=36.60 +/- 159.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=179000, episode_reward=-77.40 +/- 218.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=180000, episode_reward=6.80 +/- 224.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=181000, episode_reward=-60.60 +/- 165.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=182000, episode_reward=-34.20 +/- 219.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=183000, episode_reward=72.40 +/- 120.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=184000, episode_reward=-83.20 +/- 306.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=185000, episode_reward=264.40 +/- 175.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=186000, episode_reward=-34.40 +/- 83.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=187000, episode_reward=-13.00 +/- 118.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=188000, episode_reward=-29.60 +/- 127.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=189000, episode_reward=49.40 +/- 84.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=190000, episode_reward=104.80 +/- 219.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=191000, episode_reward=-123.80 +/- 176.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=192000, episode_reward=-225.60 +/- 144.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=193000, episode_reward=30.60 +/- 221.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=194000, episode_reward=107.20 +/- 194.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=195000, episode_reward=-67.20 +/- 116.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=196000, episode_reward=41.60 +/- 238.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=197000, episode_reward=45.40 +/- 70.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=198000, episode_reward=64.60 +/- 203.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=199000, episode_reward=-111.20 +/- 285.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=200000, episode_reward=62.20 +/- 162.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=201000, episode_reward=256.80 +/- 202.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=202000, episode_reward=74.60 +/- 119.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=203000, episode_reward=98.40 +/- 286.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=204000, episode_reward=84.40 +/- 191.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=205000, episode_reward=179.00 +/- 129.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=206000, episode_reward=262.00 +/- 288.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=207000, episode_reward=74.60 +/- 345.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=208000, episode_reward=35.00 +/- 278.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=209000, episode_reward=162.40 +/- 118.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=210000, episode_reward=-55.20 +/- 157.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=211000, episode_reward=31.80 +/- 240.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=212000, episode_reward=127.20 +/- 224.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=213000, episode_reward=238.40 +/- 138.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=214000, episode_reward=23.60 +/- 190.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=215000, episode_reward=40.60 +/- 129.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=216000, episode_reward=-110.80 +/- 286.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=217000, episode_reward=98.00 +/- 115.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=218000, episode_reward=25.80 +/- 236.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=219000, episode_reward=87.20 +/- 259.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=220000, episode_reward=-108.20 +/- 339.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=221000, episode_reward=6.40 +/- 196.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=222000, episode_reward=95.40 +/- 48.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=223000, episode_reward=102.60 +/- 221.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=224000, episode_reward=167.20 +/- 149.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=225000, episode_reward=8.20 +/- 117.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=226000, episode_reward=154.60 +/- 227.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=227000, episode_reward=179.40 +/- 201.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=228000, episode_reward=35.20 +/- 251.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=229000, episode_reward=26.60 +/- 210.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=230000, episode_reward=96.00 +/- 260.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=231000, episode_reward=81.20 +/- 195.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=232000, episode_reward=-87.20 +/- 266.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=233000, episode_reward=13.80 +/- 213.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=234000, episode_reward=123.60 +/- 122.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=235000, episode_reward=212.40 +/- 199.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=236000, episode_reward=112.40 +/- 239.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=237000, episode_reward=197.40 +/- 125.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=238000, episode_reward=182.00 +/- 131.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=239000, episode_reward=-58.40 +/- 168.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=240000, episode_reward=-9.60 +/- 82.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=241000, episode_reward=27.80 +/- 107.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=242000, episode_reward=206.00 +/- 193.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=243000, episode_reward=139.80 +/- 190.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=244000, episode_reward=-50.00 +/- 263.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=245000, episode_reward=138.80 +/- 368.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=246000, episode_reward=-25.20 +/- 88.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=247000, episode_reward=194.20 +/- 199.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=248000, episode_reward=191.00 +/- 165.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=249000, episode_reward=40.20 +/- 61.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=250000, episode_reward=154.40 +/- 364.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=251000, episode_reward=74.40 +/- 248.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=252000, episode_reward=-10.00 +/- 281.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=253000, episode_reward=26.00 +/- 250.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=254000, episode_reward=12.00 +/- 212.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=255000, episode_reward=167.80 +/- 205.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=256000, episode_reward=206.40 +/- 107.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=257000, episode_reward=303.20 +/- 194.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=258000, episode_reward=155.40 +/- 88.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=259000, episode_reward=254.20 +/- 161.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=260000, episode_reward=101.80 +/- 202.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=261000, episode_reward=180.80 +/- 142.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=262000, episode_reward=94.60 +/- 153.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=263000, episode_reward=135.00 +/- 211.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=264000, episode_reward=-36.80 +/- 192.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=265000, episode_reward=263.60 +/- 97.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=266000, episode_reward=147.00 +/- 207.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=267000, episode_reward=46.80 +/- 184.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=268000, episode_reward=30.00 +/- 60.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=269000, episode_reward=-64.60 +/- 232.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=270000, episode_reward=292.00 +/- 146.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=271000, episode_reward=164.00 +/- 192.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=272000, episode_reward=102.20 +/- 160.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=273000, episode_reward=159.60 +/- 347.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=274000, episode_reward=190.60 +/- 252.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=275000, episode_reward=155.80 +/- 242.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=276000, episode_reward=124.00 +/- 193.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=277000, episode_reward=78.00 +/- 98.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=278000, episode_reward=197.20 +/- 93.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=279000, episode_reward=-8.20 +/- 165.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=280000, episode_reward=5.20 +/- 233.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=281000, episode_reward=198.80 +/- 373.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=282000, episode_reward=60.00 +/- 363.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=283000, episode_reward=179.60 +/- 141.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=284000, episode_reward=25.60 +/- 178.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=285000, episode_reward=70.80 +/- 129.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=286000, episode_reward=9.80 +/- 227.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=287000, episode_reward=17.20 +/- 196.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=288000, episode_reward=-8.60 +/- 210.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=289000, episode_reward=66.80 +/- 90.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=290000, episode_reward=138.40 +/- 134.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=291000, episode_reward=-0.60 +/- 57.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=292000, episode_reward=-42.40 +/- 241.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=293000, episode_reward=-8.60 +/- 87.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=294000, episode_reward=54.60 +/- 204.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=295000, episode_reward=68.20 +/- 237.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=296000, episode_reward=-26.00 +/- 198.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=297000, episode_reward=135.00 +/- 246.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=298000, episode_reward=211.00 +/- 279.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=299000, episode_reward=229.80 +/- 64.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=300000, episode_reward=85.80 +/- 117.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=301000, episode_reward=14.00 +/- 159.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=302000, episode_reward=109.40 +/- 160.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=303000, episode_reward=33.40 +/- 175.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=304000, episode_reward=127.80 +/- 198.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=305000, episode_reward=59.00 +/- 214.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=306000, episode_reward=-111.00 +/- 95.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=307000, episode_reward=-22.00 +/- 164.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=308000, episode_reward=304.00 +/- 78.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=309000, episode_reward=16.80 +/- 189.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=310000, episode_reward=1.00 +/- 209.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=311000, episode_reward=12.40 +/- 195.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=312000, episode_reward=171.00 +/- 171.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=313000, episode_reward=175.00 +/- 84.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=314000, episode_reward=-0.20 +/- 174.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=315000, episode_reward=151.40 +/- 204.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=316000, episode_reward=87.80 +/- 123.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=317000, episode_reward=82.60 +/- 141.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=318000, episode_reward=19.40 +/- 192.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=319000, episode_reward=122.40 +/- 158.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=320000, episode_reward=98.80 +/- 197.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=321000, episode_reward=120.40 +/- 214.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=322000, episode_reward=-4.20 +/- 399.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=323000, episode_reward=155.80 +/- 353.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=324000, episode_reward=82.40 +/- 182.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=325000, episode_reward=114.80 +/- 66.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=326000, episode_reward=-88.60 +/- 223.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=327000, episode_reward=65.00 +/- 186.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=328000, episode_reward=-51.20 +/- 77.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=329000, episode_reward=237.60 +/- 144.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=330000, episode_reward=103.20 +/- 140.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=331000, episode_reward=101.20 +/- 373.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=332000, episode_reward=-40.00 +/- 325.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=333000, episode_reward=44.40 +/- 144.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=334000, episode_reward=190.80 +/- 147.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=335000, episode_reward=191.40 +/- 61.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=336000, episode_reward=15.40 +/- 194.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=337000, episode_reward=156.40 +/- 221.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=338000, episode_reward=77.80 +/- 207.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=339000, episode_reward=201.00 +/- 186.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=340000, episode_reward=99.40 +/- 249.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=341000, episode_reward=14.00 +/- 283.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=342000, episode_reward=63.80 +/- 337.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=343000, episode_reward=260.60 +/- 147.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=344000, episode_reward=35.40 +/- 80.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=345000, episode_reward=-63.00 +/- 85.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=346000, episode_reward=-30.60 +/- 163.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=347000, episode_reward=111.00 +/- 371.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=348000, episode_reward=87.20 +/- 120.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=349000, episode_reward=427.60 +/- 117.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=350000, episode_reward=210.20 +/- 235.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=351000, episode_reward=120.60 +/- 243.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=352000, episode_reward=124.80 +/- 272.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=353000, episode_reward=134.00 +/- 300.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=354000, episode_reward=219.80 +/- 102.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=355000, episode_reward=227.80 +/- 52.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=356000, episode_reward=127.20 +/- 269.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=357000, episode_reward=207.60 +/- 178.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=358000, episode_reward=-48.00 +/- 261.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=359000, episode_reward=6.80 +/- 80.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=360000, episode_reward=299.60 +/- 157.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=361000, episode_reward=255.80 +/- 200.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=362000, episode_reward=109.80 +/- 212.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=363000, episode_reward=120.20 +/- 186.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=364000, episode_reward=132.80 +/- 168.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=365000, episode_reward=221.20 +/- 133.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=366000, episode_reward=-168.20 +/- 235.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=367000, episode_reward=-42.60 +/- 329.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=368000, episode_reward=142.40 +/- 334.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=369000, episode_reward=361.80 +/- 237.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=370000, episode_reward=224.20 +/- 237.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=371000, episode_reward=142.60 +/- 95.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=372000, episode_reward=259.00 +/- 161.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=373000, episode_reward=60.60 +/- 178.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=374000, episode_reward=150.40 +/- 236.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=375000, episode_reward=224.60 +/- 273.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=376000, episode_reward=123.00 +/- 287.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=377000, episode_reward=35.40 +/- 273.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=378000, episode_reward=88.60 +/- 199.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=379000, episode_reward=75.80 +/- 322.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=380000, episode_reward=70.80 +/- 212.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=381000, episode_reward=116.20 +/- 186.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=382000, episode_reward=56.00 +/- 166.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=383000, episode_reward=133.60 +/- 271.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=384000, episode_reward=254.80 +/- 287.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=385000, episode_reward=159.00 +/- 105.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=386000, episode_reward=138.80 +/- 239.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=387000, episode_reward=43.00 +/- 145.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=388000, episode_reward=231.40 +/- 147.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=389000, episode_reward=181.60 +/- 147.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=390000, episode_reward=147.80 +/- 124.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=391000, episode_reward=89.00 +/- 207.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=392000, episode_reward=130.80 +/- 91.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=393000, episode_reward=300.00 +/- 140.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=394000, episode_reward=160.40 +/- 224.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=395000, episode_reward=112.80 +/- 149.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=396000, episode_reward=103.60 +/- 151.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=397000, episode_reward=232.40 +/- 195.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=398000, episode_reward=-143.40 +/- 205.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=399000, episode_reward=179.60 +/- 234.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=400000, episode_reward=66.60 +/- 104.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=401000, episode_reward=-88.80 +/- 286.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=402000, episode_reward=119.80 +/- 266.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=403000, episode_reward=317.40 +/- 224.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=404000, episode_reward=226.80 +/- 230.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=405000, episode_reward=97.80 +/- 299.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=406000, episode_reward=96.80 +/- 208.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=407000, episode_reward=-0.60 +/- 261.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=408000, episode_reward=260.00 +/- 248.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=409000, episode_reward=175.20 +/- 46.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=410000, episode_reward=74.40 +/- 184.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=411000, episode_reward=-55.40 +/- 183.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=412000, episode_reward=136.20 +/- 86.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=413000, episode_reward=13.00 +/- 287.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=414000, episode_reward=232.20 +/- 170.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=415000, episode_reward=32.80 +/- 235.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=416000, episode_reward=75.00 +/- 256.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=417000, episode_reward=135.20 +/- 206.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=418000, episode_reward=212.20 +/- 198.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=419000, episode_reward=130.20 +/- 120.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=420000, episode_reward=198.20 +/- 97.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=421000, episode_reward=219.60 +/- 154.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=422000, episode_reward=259.00 +/- 76.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=423000, episode_reward=-73.40 +/- 151.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=424000, episode_reward=184.80 +/- 236.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=425000, episode_reward=71.00 +/- 136.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=426000, episode_reward=184.00 +/- 182.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=427000, episode_reward=62.00 +/- 227.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=428000, episode_reward=210.60 +/- 213.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=429000, episode_reward=242.20 +/- 136.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=430000, episode_reward=63.60 +/- 290.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=431000, episode_reward=-55.20 +/- 125.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=432000, episode_reward=-34.00 +/- 121.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=433000, episode_reward=231.40 +/- 124.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=434000, episode_reward=192.40 +/- 280.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=435000, episode_reward=221.60 +/- 262.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=436000, episode_reward=353.40 +/- 172.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=437000, episode_reward=42.20 +/- 257.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=438000, episode_reward=16.20 +/- 111.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=439000, episode_reward=113.40 +/- 273.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=440000, episode_reward=187.60 +/- 81.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=441000, episode_reward=221.40 +/- 332.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=442000, episode_reward=229.40 +/- 184.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=443000, episode_reward=100.80 +/- 203.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=444000, episode_reward=78.00 +/- 150.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=445000, episode_reward=-1.60 +/- 184.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=446000, episode_reward=254.80 +/- 82.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=447000, episode_reward=155.60 +/- 243.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=448000, episode_reward=291.40 +/- 185.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=449000, episode_reward=305.20 +/- 156.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=450000, episode_reward=458.40 +/- 235.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=451000, episode_reward=214.00 +/- 237.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=452000, episode_reward=201.80 +/- 205.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=453000, episode_reward=146.20 +/- 120.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=454000, episode_reward=260.00 +/- 234.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=455000, episode_reward=352.80 +/- 209.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=456000, episode_reward=235.20 +/- 106.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=457000, episode_reward=233.00 +/- 215.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=458000, episode_reward=122.20 +/- 276.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=459000, episode_reward=226.00 +/- 136.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=460000, episode_reward=317.00 +/- 158.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=461000, episode_reward=212.20 +/- 149.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=462000, episode_reward=266.80 +/- 152.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=463000, episode_reward=253.80 +/- 289.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=464000, episode_reward=250.80 +/- 299.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=465000, episode_reward=217.60 +/- 208.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=466000, episode_reward=207.60 +/- 137.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=467000, episode_reward=251.20 +/- 232.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=468000, episode_reward=309.00 +/- 269.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=469000, episode_reward=156.60 +/- 171.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=470000, episode_reward=169.80 +/- 222.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=471000, episode_reward=91.80 +/- 243.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=472000, episode_reward=101.80 +/- 164.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=473000, episode_reward=367.80 +/- 89.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=474000, episode_reward=215.60 +/- 200.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=475000, episode_reward=199.60 +/- 152.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=476000, episode_reward=438.80 +/- 263.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=477000, episode_reward=174.80 +/- 239.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=478000, episode_reward=273.20 +/- 169.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=479000, episode_reward=246.40 +/- 139.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=480000, episode_reward=435.00 +/- 194.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=481000, episode_reward=246.20 +/- 88.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=482000, episode_reward=282.40 +/- 257.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=483000, episode_reward=40.80 +/- 297.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=484000, episode_reward=180.80 +/- 80.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=485000, episode_reward=187.60 +/- 224.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=486000, episode_reward=269.20 +/- 96.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=487000, episode_reward=211.20 +/- 197.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=488000, episode_reward=305.00 +/- 162.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=489000, episode_reward=210.00 +/- 70.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=490000, episode_reward=257.00 +/- 291.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=491000, episode_reward=256.80 +/- 156.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=492000, episode_reward=225.40 +/- 78.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=493000, episode_reward=327.40 +/- 198.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=494000, episode_reward=123.40 +/- 139.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=495000, episode_reward=482.40 +/- 169.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=496000, episode_reward=93.40 +/- 167.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=497000, episode_reward=42.20 +/- 110.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=498000, episode_reward=266.60 +/- 151.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=499000, episode_reward=235.00 +/- 237.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=500000, episode_reward=278.20 +/- 148.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Gamma = 0.8, Learning Rate = 3e-05, Activation Function = ReLU, Neurons = 64, Process noise = 0.1, Measurement noise = 0.0\n",
      "Eval num_timesteps=1000, episode_reward=-1577.60 +/- 151.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=-1571.20 +/- 184.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3000, episode_reward=-1498.80 +/- 101.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4000, episode_reward=-1599.60 +/- 149.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=5000, episode_reward=-1590.20 +/- 90.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=6000, episode_reward=-1533.40 +/- 130.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=7000, episode_reward=-1695.00 +/- 116.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=8000, episode_reward=-1476.20 +/- 73.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=9000, episode_reward=-1585.60 +/- 48.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=10000, episode_reward=-1532.20 +/- 126.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=11000, episode_reward=-1678.20 +/- 174.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=12000, episode_reward=-1641.80 +/- 80.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=13000, episode_reward=-1582.40 +/- 124.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=14000, episode_reward=-1548.40 +/- 131.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=15000, episode_reward=-1636.60 +/- 134.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=16000, episode_reward=-1516.80 +/- 133.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=17000, episode_reward=-1535.60 +/- 55.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=18000, episode_reward=-1616.20 +/- 120.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=19000, episode_reward=-1640.60 +/- 166.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=20000, episode_reward=-1545.80 +/- 145.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=21000, episode_reward=-1564.20 +/- 91.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=22000, episode_reward=-1499.20 +/- 146.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=23000, episode_reward=-1559.60 +/- 161.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=24000, episode_reward=-1478.40 +/- 97.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=25000, episode_reward=-1479.40 +/- 33.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=26000, episode_reward=-1592.80 +/- 129.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=27000, episode_reward=-1544.20 +/- 105.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=28000, episode_reward=-1647.40 +/- 136.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=29000, episode_reward=-1506.40 +/- 104.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=30000, episode_reward=-1592.60 +/- 152.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=31000, episode_reward=-1584.00 +/- 62.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=32000, episode_reward=-1552.80 +/- 88.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=33000, episode_reward=-1502.80 +/- 58.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=34000, episode_reward=-1519.00 +/- 104.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=35000, episode_reward=-1542.20 +/- 63.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=36000, episode_reward=-1624.20 +/- 131.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=37000, episode_reward=-1620.20 +/- 127.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=38000, episode_reward=-1513.40 +/- 91.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=39000, episode_reward=-1576.40 +/- 96.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=40000, episode_reward=-1634.40 +/- 63.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=41000, episode_reward=-1609.60 +/- 169.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=42000, episode_reward=-1624.60 +/- 116.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=43000, episode_reward=-1550.00 +/- 179.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=44000, episode_reward=-1590.60 +/- 103.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=45000, episode_reward=-1571.00 +/- 83.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=46000, episode_reward=-1529.80 +/- 95.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=47000, episode_reward=-1610.20 +/- 92.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=48000, episode_reward=-1621.60 +/- 92.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=49000, episode_reward=-1594.00 +/- 61.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=50000, episode_reward=-1558.20 +/- 167.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=51000, episode_reward=-878.00 +/- 155.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=52000, episode_reward=-560.40 +/- 346.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=53000, episode_reward=-222.60 +/- 220.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=54000, episode_reward=-357.00 +/- 507.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=55000, episode_reward=-4.20 +/- 187.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=56000, episode_reward=-515.40 +/- 255.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=57000, episode_reward=-610.00 +/- 645.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=58000, episode_reward=-740.60 +/- 657.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=59000, episode_reward=-905.60 +/- 777.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=60000, episode_reward=155.20 +/- 395.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=61000, episode_reward=-105.00 +/- 458.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=62000, episode_reward=-65.60 +/- 419.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=63000, episode_reward=-174.60 +/- 636.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=64000, episode_reward=-192.20 +/- 260.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=65000, episode_reward=-49.80 +/- 344.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=66000, episode_reward=-327.40 +/- 1126.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=67000, episode_reward=192.60 +/- 157.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=68000, episode_reward=0.00 +/- 222.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=69000, episode_reward=159.00 +/- 349.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=70000, episode_reward=-21.40 +/- 215.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=71000, episode_reward=167.40 +/- 272.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=72000, episode_reward=-64.20 +/- 479.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=73000, episode_reward=-449.60 +/- 636.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=74000, episode_reward=0.40 +/- 291.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=75000, episode_reward=-596.60 +/- 778.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=76000, episode_reward=158.40 +/- 271.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=77000, episode_reward=-148.00 +/- 235.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=78000, episode_reward=-99.80 +/- 250.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=79000, episode_reward=145.40 +/- 218.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=80000, episode_reward=60.20 +/- 268.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=81000, episode_reward=43.60 +/- 234.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=82000, episode_reward=162.80 +/- 284.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=83000, episode_reward=120.00 +/- 99.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=84000, episode_reward=226.20 +/- 205.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=85000, episode_reward=268.60 +/- 145.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=86000, episode_reward=190.00 +/- 185.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=87000, episode_reward=76.80 +/- 107.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=88000, episode_reward=247.60 +/- 187.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=89000, episode_reward=233.40 +/- 147.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=90000, episode_reward=199.40 +/- 208.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=91000, episode_reward=172.60 +/- 181.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=92000, episode_reward=262.40 +/- 271.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=93000, episode_reward=402.00 +/- 188.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=94000, episode_reward=311.40 +/- 132.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=95000, episode_reward=226.80 +/- 160.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=96000, episode_reward=377.60 +/- 148.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=97000, episode_reward=215.60 +/- 147.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=98000, episode_reward=432.80 +/- 134.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=99000, episode_reward=328.80 +/- 195.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=100000, episode_reward=240.00 +/- 104.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=101000, episode_reward=115.80 +/- 76.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=102000, episode_reward=155.40 +/- 253.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=103000, episode_reward=152.40 +/- 130.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=104000, episode_reward=269.00 +/- 142.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=105000, episode_reward=375.40 +/- 137.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=106000, episode_reward=245.60 +/- 163.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=107000, episode_reward=233.60 +/- 233.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=108000, episode_reward=161.20 +/- 230.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=109000, episode_reward=247.80 +/- 189.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=110000, episode_reward=181.60 +/- 263.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=111000, episode_reward=286.40 +/- 211.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=112000, episode_reward=180.00 +/- 153.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=113000, episode_reward=54.20 +/- 269.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=114000, episode_reward=132.80 +/- 219.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=115000, episode_reward=221.60 +/- 158.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=116000, episode_reward=158.00 +/- 252.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=117000, episode_reward=211.80 +/- 143.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=118000, episode_reward=194.40 +/- 232.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=119000, episode_reward=232.80 +/- 157.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=120000, episode_reward=-29.00 +/- 233.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=121000, episode_reward=248.40 +/- 114.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=122000, episode_reward=233.80 +/- 241.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=123000, episode_reward=204.80 +/- 166.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=124000, episode_reward=109.60 +/- 90.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=125000, episode_reward=290.40 +/- 148.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=126000, episode_reward=260.60 +/- 225.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=127000, episode_reward=188.00 +/- 140.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=128000, episode_reward=153.80 +/- 158.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=129000, episode_reward=310.20 +/- 89.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=130000, episode_reward=225.00 +/- 145.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=131000, episode_reward=243.20 +/- 163.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=132000, episode_reward=160.00 +/- 242.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=133000, episode_reward=215.00 +/- 172.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=134000, episode_reward=196.60 +/- 89.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=135000, episode_reward=223.20 +/- 134.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=136000, episode_reward=268.40 +/- 123.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=137000, episode_reward=77.00 +/- 171.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=138000, episode_reward=324.20 +/- 310.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=139000, episode_reward=82.80 +/- 93.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=140000, episode_reward=292.00 +/- 291.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=141000, episode_reward=143.80 +/- 164.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=142000, episode_reward=297.60 +/- 192.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=143000, episode_reward=266.80 +/- 155.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=144000, episode_reward=155.60 +/- 138.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=145000, episode_reward=350.20 +/- 162.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=146000, episode_reward=394.80 +/- 257.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=147000, episode_reward=171.80 +/- 112.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=148000, episode_reward=124.80 +/- 176.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=149000, episode_reward=55.80 +/- 143.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=150000, episode_reward=271.60 +/- 205.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=151000, episode_reward=181.60 +/- 83.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=152000, episode_reward=132.80 +/- 71.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=153000, episode_reward=224.00 +/- 261.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=154000, episode_reward=234.80 +/- 294.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=155000, episode_reward=270.60 +/- 217.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=156000, episode_reward=322.80 +/- 269.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=157000, episode_reward=337.20 +/- 153.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=158000, episode_reward=316.00 +/- 98.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=159000, episode_reward=11.80 +/- 281.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=160000, episode_reward=322.00 +/- 234.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=161000, episode_reward=300.20 +/- 208.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=162000, episode_reward=178.00 +/- 184.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=163000, episode_reward=323.00 +/- 124.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=164000, episode_reward=122.40 +/- 94.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=165000, episode_reward=157.80 +/- 154.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=166000, episode_reward=246.40 +/- 240.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=167000, episode_reward=76.40 +/- 94.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=168000, episode_reward=144.80 +/- 85.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=169000, episode_reward=-97.20 +/- 210.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=170000, episode_reward=231.80 +/- 102.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=171000, episode_reward=247.60 +/- 147.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=172000, episode_reward=112.80 +/- 74.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=173000, episode_reward=181.60 +/- 217.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=174000, episode_reward=286.00 +/- 107.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=175000, episode_reward=288.60 +/- 265.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=176000, episode_reward=197.60 +/- 127.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=177000, episode_reward=272.60 +/- 268.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=178000, episode_reward=109.80 +/- 200.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=179000, episode_reward=19.40 +/- 79.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=180000, episode_reward=253.00 +/- 111.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=181000, episode_reward=381.60 +/- 162.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=182000, episode_reward=189.60 +/- 100.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=183000, episode_reward=182.20 +/- 209.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=184000, episode_reward=320.80 +/- 177.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=185000, episode_reward=324.40 +/- 240.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=186000, episode_reward=297.80 +/- 201.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=187000, episode_reward=470.80 +/- 180.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=188000, episode_reward=463.60 +/- 96.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=189000, episode_reward=314.20 +/- 190.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=190000, episode_reward=201.20 +/- 190.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=191000, episode_reward=350.40 +/- 168.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=192000, episode_reward=187.80 +/- 254.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=193000, episode_reward=144.40 +/- 153.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=194000, episode_reward=106.40 +/- 153.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=195000, episode_reward=212.20 +/- 268.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=196000, episode_reward=234.00 +/- 205.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=197000, episode_reward=246.20 +/- 199.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=198000, episode_reward=344.20 +/- 138.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=199000, episode_reward=120.80 +/- 161.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=200000, episode_reward=271.40 +/- 206.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=201000, episode_reward=110.00 +/- 261.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=202000, episode_reward=89.80 +/- 139.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=203000, episode_reward=209.60 +/- 83.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=204000, episode_reward=269.80 +/- 247.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=205000, episode_reward=154.40 +/- 231.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=206000, episode_reward=307.80 +/- 291.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=207000, episode_reward=128.80 +/- 168.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=208000, episode_reward=218.40 +/- 193.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=209000, episode_reward=101.40 +/- 119.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=210000, episode_reward=207.00 +/- 145.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=211000, episode_reward=212.80 +/- 302.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=212000, episode_reward=229.00 +/- 212.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=213000, episode_reward=221.40 +/- 105.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=214000, episode_reward=247.20 +/- 200.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=215000, episode_reward=303.80 +/- 245.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=216000, episode_reward=223.40 +/- 128.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=217000, episode_reward=257.60 +/- 177.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=218000, episode_reward=239.00 +/- 228.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=219000, episode_reward=273.00 +/- 63.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=220000, episode_reward=217.60 +/- 121.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=221000, episode_reward=157.00 +/- 158.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=222000, episode_reward=118.00 +/- 203.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=223000, episode_reward=234.00 +/- 117.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=224000, episode_reward=309.20 +/- 188.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=225000, episode_reward=206.40 +/- 188.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=226000, episode_reward=218.00 +/- 159.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=227000, episode_reward=227.80 +/- 224.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=228000, episode_reward=254.80 +/- 206.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=229000, episode_reward=-18.60 +/- 189.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=230000, episode_reward=109.40 +/- 119.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=231000, episode_reward=307.20 +/- 144.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=232000, episode_reward=190.00 +/- 286.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=233000, episode_reward=183.40 +/- 178.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=234000, episode_reward=242.20 +/- 225.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=235000, episode_reward=156.40 +/- 191.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=236000, episode_reward=505.60 +/- 233.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=237000, episode_reward=274.20 +/- 216.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=238000, episode_reward=264.00 +/- 154.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=239000, episode_reward=200.80 +/- 165.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=240000, episode_reward=204.80 +/- 150.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=241000, episode_reward=284.40 +/- 142.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=242000, episode_reward=174.40 +/- 290.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=243000, episode_reward=287.40 +/- 249.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=244000, episode_reward=102.00 +/- 115.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=245000, episode_reward=172.80 +/- 228.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=246000, episode_reward=310.20 +/- 246.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=247000, episode_reward=282.40 +/- 231.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=248000, episode_reward=221.00 +/- 332.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=249000, episode_reward=240.00 +/- 174.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=250000, episode_reward=297.80 +/- 217.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=251000, episode_reward=192.80 +/- 202.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=252000, episode_reward=50.40 +/- 268.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=253000, episode_reward=349.60 +/- 24.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=254000, episode_reward=179.00 +/- 163.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=255000, episode_reward=327.40 +/- 387.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=256000, episode_reward=91.80 +/- 275.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=257000, episode_reward=180.00 +/- 113.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=258000, episode_reward=230.40 +/- 270.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=259000, episode_reward=269.60 +/- 245.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=260000, episode_reward=124.40 +/- 197.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=261000, episode_reward=186.20 +/- 284.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=262000, episode_reward=310.60 +/- 188.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=263000, episode_reward=133.80 +/- 254.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=264000, episode_reward=158.60 +/- 149.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=265000, episode_reward=215.00 +/- 116.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=266000, episode_reward=322.40 +/- 130.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=267000, episode_reward=287.80 +/- 286.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=268000, episode_reward=251.60 +/- 217.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=269000, episode_reward=252.20 +/- 238.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=270000, episode_reward=328.00 +/- 178.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=271000, episode_reward=323.00 +/- 102.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=272000, episode_reward=424.40 +/- 85.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=273000, episode_reward=208.20 +/- 256.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=274000, episode_reward=256.20 +/- 322.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=275000, episode_reward=208.80 +/- 79.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=276000, episode_reward=169.40 +/- 161.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=277000, episode_reward=258.80 +/- 230.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=278000, episode_reward=245.40 +/- 96.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=279000, episode_reward=255.80 +/- 119.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=280000, episode_reward=303.40 +/- 121.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=281000, episode_reward=314.20 +/- 144.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=282000, episode_reward=196.60 +/- 120.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=283000, episode_reward=393.20 +/- 104.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=284000, episode_reward=213.20 +/- 153.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=285000, episode_reward=256.40 +/- 124.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=286000, episode_reward=345.00 +/- 167.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=287000, episode_reward=286.00 +/- 158.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=288000, episode_reward=173.00 +/- 81.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=289000, episode_reward=276.40 +/- 186.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=290000, episode_reward=297.80 +/- 203.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=291000, episode_reward=321.40 +/- 275.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=292000, episode_reward=96.60 +/- 210.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=293000, episode_reward=282.20 +/- 226.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=294000, episode_reward=328.00 +/- 257.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=295000, episode_reward=225.60 +/- 237.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=296000, episode_reward=294.40 +/- 204.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=297000, episode_reward=269.80 +/- 112.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=298000, episode_reward=247.80 +/- 205.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=299000, episode_reward=452.00 +/- 143.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=300000, episode_reward=165.60 +/- 123.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=301000, episode_reward=238.80 +/- 174.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=302000, episode_reward=310.60 +/- 79.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=303000, episode_reward=296.80 +/- 261.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=304000, episode_reward=272.60 +/- 121.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=305000, episode_reward=194.60 +/- 86.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=306000, episode_reward=323.60 +/- 141.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=307000, episode_reward=200.80 +/- 133.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=308000, episode_reward=169.40 +/- 170.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=309000, episode_reward=265.80 +/- 209.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=310000, episode_reward=153.40 +/- 213.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=311000, episode_reward=521.20 +/- 110.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=312000, episode_reward=170.00 +/- 312.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=313000, episode_reward=87.40 +/- 174.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=314000, episode_reward=316.20 +/- 119.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=315000, episode_reward=399.20 +/- 208.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=316000, episode_reward=186.40 +/- 305.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=317000, episode_reward=380.40 +/- 117.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=318000, episode_reward=186.20 +/- 131.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=319000, episode_reward=212.40 +/- 245.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=320000, episode_reward=235.00 +/- 205.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=321000, episode_reward=258.40 +/- 284.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=322000, episode_reward=123.80 +/- 342.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=323000, episode_reward=325.80 +/- 154.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=324000, episode_reward=278.80 +/- 159.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=325000, episode_reward=219.40 +/- 232.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=326000, episode_reward=211.00 +/- 189.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=327000, episode_reward=289.80 +/- 178.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=328000, episode_reward=186.00 +/- 185.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=329000, episode_reward=223.00 +/- 144.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=330000, episode_reward=135.00 +/- 89.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=331000, episode_reward=133.00 +/- 188.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=332000, episode_reward=391.20 +/- 170.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=333000, episode_reward=432.20 +/- 248.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=334000, episode_reward=193.40 +/- 173.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=335000, episode_reward=195.00 +/- 309.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=336000, episode_reward=296.00 +/- 45.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=337000, episode_reward=300.40 +/- 162.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=338000, episode_reward=356.20 +/- 158.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=339000, episode_reward=270.40 +/- 194.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=340000, episode_reward=209.20 +/- 244.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=341000, episode_reward=170.80 +/- 124.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=342000, episode_reward=265.60 +/- 170.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=343000, episode_reward=208.40 +/- 98.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=344000, episode_reward=205.20 +/- 201.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=345000, episode_reward=270.20 +/- 221.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=346000, episode_reward=129.60 +/- 332.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=347000, episode_reward=398.80 +/- 183.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=348000, episode_reward=607.00 +/- 170.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=349000, episode_reward=231.80 +/- 252.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=350000, episode_reward=267.40 +/- 184.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=351000, episode_reward=415.00 +/- 234.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=352000, episode_reward=191.20 +/- 256.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=353000, episode_reward=414.20 +/- 128.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=354000, episode_reward=260.40 +/- 197.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=355000, episode_reward=389.60 +/- 145.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=356000, episode_reward=223.80 +/- 74.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=357000, episode_reward=184.60 +/- 243.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=358000, episode_reward=300.20 +/- 140.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=359000, episode_reward=283.20 +/- 209.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=360000, episode_reward=258.80 +/- 321.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=361000, episode_reward=174.60 +/- 303.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=362000, episode_reward=274.80 +/- 112.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=363000, episode_reward=361.20 +/- 83.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=364000, episode_reward=342.60 +/- 211.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=365000, episode_reward=349.80 +/- 155.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=366000, episode_reward=287.20 +/- 133.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=367000, episode_reward=353.00 +/- 63.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=368000, episode_reward=242.60 +/- 174.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=369000, episode_reward=367.20 +/- 220.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=370000, episode_reward=294.80 +/- 191.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=371000, episode_reward=211.20 +/- 236.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=372000, episode_reward=328.80 +/- 190.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=373000, episode_reward=334.80 +/- 169.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=374000, episode_reward=279.80 +/- 154.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=375000, episode_reward=349.20 +/- 157.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=376000, episode_reward=152.60 +/- 187.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=377000, episode_reward=285.40 +/- 194.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=378000, episode_reward=216.80 +/- 254.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=379000, episode_reward=159.80 +/- 85.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=380000, episode_reward=277.80 +/- 134.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=381000, episode_reward=355.40 +/- 189.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=382000, episode_reward=420.20 +/- 95.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=383000, episode_reward=290.00 +/- 166.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=384000, episode_reward=244.60 +/- 241.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=385000, episode_reward=268.20 +/- 201.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=386000, episode_reward=258.00 +/- 108.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=387000, episode_reward=333.00 +/- 204.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=388000, episode_reward=293.00 +/- 290.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=389000, episode_reward=405.40 +/- 197.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=390000, episode_reward=227.60 +/- 199.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=391000, episode_reward=313.00 +/- 82.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=392000, episode_reward=319.20 +/- 216.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=393000, episode_reward=351.00 +/- 213.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=394000, episode_reward=300.20 +/- 261.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=395000, episode_reward=357.80 +/- 345.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=396000, episode_reward=419.00 +/- 118.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=397000, episode_reward=309.20 +/- 76.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=398000, episode_reward=210.00 +/- 199.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=399000, episode_reward=435.20 +/- 203.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=400000, episode_reward=139.20 +/- 92.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=401000, episode_reward=387.60 +/- 286.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=402000, episode_reward=218.00 +/- 105.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=403000, episode_reward=153.60 +/- 156.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=404000, episode_reward=445.80 +/- 188.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=405000, episode_reward=406.20 +/- 47.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=406000, episode_reward=364.20 +/- 104.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=407000, episode_reward=202.40 +/- 45.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=408000, episode_reward=290.00 +/- 255.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=409000, episode_reward=209.40 +/- 138.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=410000, episode_reward=299.40 +/- 173.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=411000, episode_reward=362.60 +/- 108.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=412000, episode_reward=200.40 +/- 159.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=413000, episode_reward=340.60 +/- 232.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=414000, episode_reward=370.20 +/- 105.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=415000, episode_reward=274.00 +/- 269.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=416000, episode_reward=246.00 +/- 320.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=417000, episode_reward=348.00 +/- 187.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=418000, episode_reward=360.60 +/- 340.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=419000, episode_reward=290.80 +/- 80.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=420000, episode_reward=317.40 +/- 100.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=421000, episode_reward=325.20 +/- 119.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=422000, episode_reward=377.00 +/- 191.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=423000, episode_reward=456.00 +/- 142.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=424000, episode_reward=205.00 +/- 121.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=425000, episode_reward=354.60 +/- 113.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=426000, episode_reward=248.20 +/- 292.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=427000, episode_reward=225.40 +/- 181.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=428000, episode_reward=243.20 +/- 164.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=429000, episode_reward=265.60 +/- 201.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=430000, episode_reward=324.00 +/- 260.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=431000, episode_reward=323.20 +/- 138.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=432000, episode_reward=295.60 +/- 128.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=433000, episode_reward=282.60 +/- 109.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=434000, episode_reward=219.20 +/- 196.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=435000, episode_reward=436.80 +/- 229.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=436000, episode_reward=287.60 +/- 97.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=437000, episode_reward=120.40 +/- 286.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=438000, episode_reward=290.40 +/- 157.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=439000, episode_reward=167.60 +/- 279.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=440000, episode_reward=256.40 +/- 126.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=441000, episode_reward=253.60 +/- 219.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=442000, episode_reward=159.60 +/- 170.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=443000, episode_reward=433.80 +/- 195.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=444000, episode_reward=252.60 +/- 183.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=445000, episode_reward=228.80 +/- 232.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=446000, episode_reward=186.00 +/- 296.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=447000, episode_reward=183.40 +/- 162.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=448000, episode_reward=324.80 +/- 252.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=449000, episode_reward=91.60 +/- 233.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=450000, episode_reward=251.20 +/- 172.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=451000, episode_reward=333.00 +/- 209.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=452000, episode_reward=460.00 +/- 155.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=453000, episode_reward=382.40 +/- 205.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=454000, episode_reward=360.40 +/- 95.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=455000, episode_reward=367.60 +/- 70.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=456000, episode_reward=297.20 +/- 150.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=457000, episode_reward=293.20 +/- 279.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=458000, episode_reward=189.80 +/- 189.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=459000, episode_reward=408.40 +/- 50.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=460000, episode_reward=297.20 +/- 213.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=461000, episode_reward=359.60 +/- 156.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=462000, episode_reward=245.60 +/- 315.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=463000, episode_reward=367.40 +/- 203.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=464000, episode_reward=119.60 +/- 290.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=465000, episode_reward=327.20 +/- 213.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=466000, episode_reward=256.80 +/- 207.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=467000, episode_reward=286.00 +/- 260.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=468000, episode_reward=203.20 +/- 200.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=469000, episode_reward=373.60 +/- 185.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=470000, episode_reward=401.20 +/- 135.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=471000, episode_reward=291.00 +/- 178.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=472000, episode_reward=301.00 +/- 300.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=473000, episode_reward=252.80 +/- 111.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=474000, episode_reward=203.40 +/- 162.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=475000, episode_reward=349.80 +/- 280.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=476000, episode_reward=214.40 +/- 158.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=477000, episode_reward=111.80 +/- 196.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=478000, episode_reward=288.80 +/- 106.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=479000, episode_reward=268.00 +/- 147.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=480000, episode_reward=134.40 +/- 164.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=481000, episode_reward=260.00 +/- 176.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=482000, episode_reward=245.40 +/- 167.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=483000, episode_reward=124.60 +/- 155.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=484000, episode_reward=353.60 +/- 308.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=485000, episode_reward=94.20 +/- 147.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=486000, episode_reward=307.20 +/- 159.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=487000, episode_reward=461.80 +/- 161.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=488000, episode_reward=580.80 +/- 57.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=489000, episode_reward=402.00 +/- 150.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=490000, episode_reward=310.20 +/- 155.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=491000, episode_reward=354.00 +/- 93.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=492000, episode_reward=196.60 +/- 93.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=493000, episode_reward=249.00 +/- 198.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=494000, episode_reward=179.20 +/- 87.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=495000, episode_reward=371.00 +/- 195.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=496000, episode_reward=355.20 +/- 256.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=497000, episode_reward=242.20 +/- 184.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=498000, episode_reward=381.60 +/- 154.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=499000, episode_reward=390.60 +/- 215.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=500000, episode_reward=503.20 +/- 115.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Gamma = 0.9, Learning Rate = 0.003, Activation Function = Tanh, Neurons = 8, Process noise = 0.1, Measurement noise = 0.0\n",
      "Eval num_timesteps=1000, episode_reward=-19020.00 +/- 91.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=-18986.00 +/- 39.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3000, episode_reward=-19002.00 +/- 86.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=4000, episode_reward=-18971.00 +/- 105.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=5000, episode_reward=-18982.00 +/- 58.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=6000, episode_reward=-19003.00 +/- 72.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=7000, episode_reward=-19099.00 +/- 77.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=8000, episode_reward=-19034.00 +/- 27.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=9000, episode_reward=-19002.00 +/- 64.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=10000, episode_reward=-19014.00 +/- 67.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=11000, episode_reward=-18960.00 +/- 49.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=12000, episode_reward=-19005.00 +/- 148.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=13000, episode_reward=-19034.00 +/- 36.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=14000, episode_reward=-19067.00 +/- 95.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=15000, episode_reward=-19013.00 +/- 98.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=16000, episode_reward=-19030.00 +/- 50.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=17000, episode_reward=-18986.00 +/- 90.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=18000, episode_reward=-18949.00 +/- 43.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=19000, episode_reward=-18978.00 +/- 55.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=20000, episode_reward=-18991.00 +/- 28.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=21000, episode_reward=-19013.00 +/- 91.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=22000, episode_reward=-18951.00 +/- 83.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=23000, episode_reward=-19072.00 +/- 59.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=24000, episode_reward=-18936.00 +/- 64.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=25000, episode_reward=-18995.00 +/- 61.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=26000, episode_reward=-18992.00 +/- 44.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=27000, episode_reward=-19069.00 +/- 46.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=28000, episode_reward=-19013.00 +/- 130.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=29000, episode_reward=-19070.00 +/- 63.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=30000, episode_reward=-19044.00 +/- 83.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=31000, episode_reward=-19050.00 +/- 74.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=32000, episode_reward=-19064.00 +/- 99.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=33000, episode_reward=-19000.00 +/- 44.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=34000, episode_reward=-18963.00 +/- 75.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=35000, episode_reward=-18989.00 +/- 65.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=36000, episode_reward=-19039.00 +/- 94.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=37000, episode_reward=-19047.00 +/- 87.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=38000, episode_reward=-19005.00 +/- 47.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=39000, episode_reward=-18969.00 +/- 117.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=40000, episode_reward=-19042.00 +/- 38.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=41000, episode_reward=-19034.00 +/- 95.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=42000, episode_reward=-19022.00 +/- 83.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=43000, episode_reward=-18942.00 +/- 131.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=44000, episode_reward=-19007.00 +/- 60.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=45000, episode_reward=-18934.00 +/- 68.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=46000, episode_reward=-19002.00 +/- 105.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=47000, episode_reward=-19006.00 +/- 62.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=48000, episode_reward=-18976.00 +/- 71.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=49000, episode_reward=-19052.00 +/- 95.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=50000, episode_reward=-19016.00 +/- 80.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=51000, episode_reward=-1135.00 +/- 223.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=52000, episode_reward=-969.40 +/- 162.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=53000, episode_reward=-328.40 +/- 419.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=54000, episode_reward=-670.80 +/- 413.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=55000, episode_reward=-1145.00 +/- 248.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=56000, episode_reward=-584.80 +/- 181.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=57000, episode_reward=-946.20 +/- 292.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=58000, episode_reward=-1513.20 +/- 345.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=59000, episode_reward=-136.40 +/- 139.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=60000, episode_reward=-603.00 +/- 220.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=61000, episode_reward=-1146.40 +/- 238.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=62000, episode_reward=-580.60 +/- 250.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=63000, episode_reward=-354.00 +/- 203.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=64000, episode_reward=-373.40 +/- 345.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=65000, episode_reward=-285.00 +/- 239.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=66000, episode_reward=-332.80 +/- 276.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=67000, episode_reward=90.40 +/- 96.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=68000, episode_reward=-188.00 +/- 272.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=69000, episode_reward=294.60 +/- 215.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=70000, episode_reward=115.40 +/- 275.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=71000, episode_reward=-189.80 +/- 338.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=72000, episode_reward=-228.00 +/- 446.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=73000, episode_reward=-145.40 +/- 339.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=74000, episode_reward=-36.80 +/- 182.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=75000, episode_reward=123.80 +/- 189.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=76000, episode_reward=-183.80 +/- 192.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=77000, episode_reward=86.80 +/- 106.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=78000, episode_reward=220.80 +/- 50.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=79000, episode_reward=237.20 +/- 292.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=80000, episode_reward=136.00 +/- 29.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=81000, episode_reward=89.20 +/- 196.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=82000, episode_reward=-207.00 +/- 269.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=83000, episode_reward=-8.20 +/- 328.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=84000, episode_reward=130.60 +/- 244.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=85000, episode_reward=196.40 +/- 246.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=86000, episode_reward=211.80 +/- 248.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=87000, episode_reward=58.40 +/- 172.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=88000, episode_reward=337.40 +/- 111.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=89000, episode_reward=-30.60 +/- 152.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=90000, episode_reward=411.20 +/- 135.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=91000, episode_reward=-155.60 +/- 279.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=92000, episode_reward=18.60 +/- 160.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=93000, episode_reward=159.80 +/- 162.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=94000, episode_reward=361.20 +/- 161.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=95000, episode_reward=142.40 +/- 226.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=96000, episode_reward=39.20 +/- 198.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=97000, episode_reward=21.20 +/- 153.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=98000, episode_reward=156.40 +/- 191.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=99000, episode_reward=6.80 +/- 92.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=100000, episode_reward=188.40 +/- 229.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=101000, episode_reward=12.60 +/- 250.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=102000, episode_reward=-85.80 +/- 299.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=103000, episode_reward=221.20 +/- 217.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=104000, episode_reward=414.00 +/- 252.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=105000, episode_reward=884.60 +/- 193.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=106000, episode_reward=666.60 +/- 252.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=107000, episode_reward=1034.80 +/- 347.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=108000, episode_reward=1047.60 +/- 255.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=109000, episode_reward=762.40 +/- 257.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=110000, episode_reward=1053.80 +/- 303.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=111000, episode_reward=711.40 +/- 471.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=112000, episode_reward=833.00 +/- 244.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=113000, episode_reward=-2441.20 +/- 6974.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=114000, episode_reward=936.80 +/- 293.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=115000, episode_reward=894.00 +/- 102.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=116000, episode_reward=788.20 +/- 159.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=117000, episode_reward=-1837.40 +/- 5470.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=118000, episode_reward=1061.40 +/- 194.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=119000, episode_reward=936.80 +/- 238.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=120000, episode_reward=603.40 +/- 318.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=121000, episode_reward=571.80 +/- 226.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=122000, episode_reward=749.20 +/- 252.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=123000, episode_reward=530.60 +/- 363.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=124000, episode_reward=532.60 +/- 162.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=125000, episode_reward=442.80 +/- 214.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=126000, episode_reward=290.80 +/- 245.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=127000, episode_reward=225.00 +/- 251.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=128000, episode_reward=427.40 +/- 275.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=129000, episode_reward=240.00 +/- 331.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=130000, episode_reward=209.00 +/- 127.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=131000, episode_reward=197.40 +/- 419.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=132000, episode_reward=391.00 +/- 182.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=133000, episode_reward=400.80 +/- 100.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=134000, episode_reward=466.40 +/- 77.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=135000, episode_reward=287.00 +/- 256.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=136000, episode_reward=38.20 +/- 134.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=137000, episode_reward=156.20 +/- 331.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=138000, episode_reward=398.00 +/- 218.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=139000, episode_reward=184.80 +/- 285.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=140000, episode_reward=327.40 +/- 225.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=141000, episode_reward=70.00 +/- 195.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=142000, episode_reward=315.20 +/- 273.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=143000, episode_reward=369.20 +/- 389.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=144000, episode_reward=139.80 +/- 102.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=145000, episode_reward=9.80 +/- 247.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=146000, episode_reward=309.00 +/- 125.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=147000, episode_reward=342.60 +/- 173.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=148000, episode_reward=321.20 +/- 229.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=149000, episode_reward=187.80 +/- 133.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=150000, episode_reward=77.40 +/- 303.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=151000, episode_reward=159.00 +/- 113.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=152000, episode_reward=-831.80 +/- 2063.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=153000, episode_reward=203.60 +/- 271.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=154000, episode_reward=342.20 +/- 451.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=155000, episode_reward=387.80 +/- 284.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=156000, episode_reward=121.40 +/- 197.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=157000, episode_reward=534.80 +/- 322.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=158000, episode_reward=125.60 +/- 353.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=159000, episode_reward=74.00 +/- 340.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=160000, episode_reward=10.40 +/- 396.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=161000, episode_reward=-418.60 +/- 1118.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=162000, episode_reward=323.00 +/- 268.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=163000, episode_reward=353.60 +/- 211.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=164000, episode_reward=-2224.60 +/- 5357.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=165000, episode_reward=297.20 +/- 116.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=166000, episode_reward=-1409.80 +/- 4098.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=167000, episode_reward=-1641.80 +/- 3946.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=168000, episode_reward=265.60 +/- 276.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=169000, episode_reward=387.60 +/- 272.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=170000, episode_reward=515.80 +/- 488.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=171000, episode_reward=268.40 +/- 296.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=172000, episode_reward=367.20 +/- 254.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=173000, episode_reward=493.80 +/- 155.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=174000, episode_reward=418.60 +/- 212.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=175000, episode_reward=238.20 +/- 56.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=176000, episode_reward=404.00 +/- 257.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=177000, episode_reward=643.20 +/- 241.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=178000, episode_reward=-1690.80 +/- 4331.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=179000, episode_reward=572.60 +/- 354.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=180000, episode_reward=605.00 +/- 359.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=181000, episode_reward=-2182.60 +/- 5234.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=182000, episode_reward=895.00 +/- 211.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=183000, episode_reward=619.20 +/- 270.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=184000, episode_reward=244.60 +/- 316.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=185000, episode_reward=683.60 +/- 364.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=186000, episode_reward=-687.20 +/- 2262.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=187000, episode_reward=832.80 +/- 292.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=188000, episode_reward=771.80 +/- 263.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=189000, episode_reward=915.00 +/- 348.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=190000, episode_reward=167.20 +/- 868.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=191000, episode_reward=712.00 +/- 90.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=192000, episode_reward=343.00 +/- 864.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=193000, episode_reward=820.60 +/- 304.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=194000, episode_reward=436.60 +/- 92.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=195000, episode_reward=411.20 +/- 231.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=196000, episode_reward=560.80 +/- 238.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=197000, episode_reward=362.20 +/- 313.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=198000, episode_reward=433.40 +/- 128.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=199000, episode_reward=587.60 +/- 276.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=200000, episode_reward=549.40 +/- 351.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=201000, episode_reward=637.20 +/- 186.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=202000, episode_reward=678.40 +/- 191.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=203000, episode_reward=829.80 +/- 276.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=204000, episode_reward=609.20 +/- 174.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=205000, episode_reward=657.20 +/- 507.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=206000, episode_reward=414.80 +/- 373.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=207000, episode_reward=400.80 +/- 117.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=208000, episode_reward=-1663.20 +/- 4078.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=209000, episode_reward=679.40 +/- 274.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=210000, episode_reward=653.60 +/- 274.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=211000, episode_reward=572.20 +/- 215.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=212000, episode_reward=628.80 +/- 275.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=213000, episode_reward=657.40 +/- 238.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=214000, episode_reward=721.00 +/- 248.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=215000, episode_reward=488.00 +/- 454.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=216000, episode_reward=498.60 +/- 208.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=217000, episode_reward=548.60 +/- 283.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=218000, episode_reward=432.60 +/- 435.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=219000, episode_reward=833.00 +/- 412.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=220000, episode_reward=764.80 +/- 146.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=221000, episode_reward=859.00 +/- 302.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=222000, episode_reward=593.60 +/- 312.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=223000, episode_reward=689.40 +/- 324.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=224000, episode_reward=595.60 +/- 185.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=225000, episode_reward=763.40 +/- 229.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=226000, episode_reward=467.20 +/- 120.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=227000, episode_reward=643.80 +/- 250.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=228000, episode_reward=-554.20 +/- 2144.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=229000, episode_reward=-1065.00 +/- 2880.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=230000, episode_reward=346.20 +/- 98.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=231000, episode_reward=970.80 +/- 217.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=232000, episode_reward=925.80 +/- 115.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=233000, episode_reward=799.20 +/- 425.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=234000, episode_reward=950.40 +/- 455.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=235000, episode_reward=812.60 +/- 280.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=236000, episode_reward=703.80 +/- 179.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=237000, episode_reward=630.20 +/- 272.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=238000, episode_reward=563.60 +/- 164.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=239000, episode_reward=860.00 +/- 219.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=240000, episode_reward=733.80 +/- 275.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=241000, episode_reward=799.80 +/- 220.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=242000, episode_reward=608.00 +/- 368.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=243000, episode_reward=697.00 +/- 367.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=244000, episode_reward=904.20 +/- 150.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=245000, episode_reward=779.00 +/- 257.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=246000, episode_reward=981.00 +/- 257.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=247000, episode_reward=-1609.20 +/- 4518.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=248000, episode_reward=938.80 +/- 205.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=249000, episode_reward=881.00 +/- 153.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=250000, episode_reward=1021.40 +/- 108.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=251000, episode_reward=821.00 +/- 62.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=252000, episode_reward=882.80 +/- 245.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=253000, episode_reward=384.80 +/- 488.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=254000, episode_reward=883.60 +/- 166.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=255000, episode_reward=688.40 +/- 277.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=256000, episode_reward=806.40 +/- 299.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=257000, episode_reward=653.40 +/- 309.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=258000, episode_reward=676.00 +/- 197.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=259000, episode_reward=684.20 +/- 284.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=260000, episode_reward=742.40 +/- 276.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=261000, episode_reward=737.60 +/- 260.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=262000, episode_reward=676.00 +/- 230.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=263000, episode_reward=591.80 +/- 236.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=264000, episode_reward=781.20 +/- 218.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=265000, episode_reward=-2944.00 +/- 6730.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=266000, episode_reward=855.80 +/- 279.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=267000, episode_reward=775.40 +/- 268.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=268000, episode_reward=744.20 +/- 199.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=269000, episode_reward=868.80 +/- 376.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=270000, episode_reward=982.00 +/- 393.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=271000, episode_reward=560.20 +/- 328.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=272000, episode_reward=783.60 +/- 259.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=273000, episode_reward=839.00 +/- 177.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=274000, episode_reward=804.40 +/- 250.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=275000, episode_reward=780.00 +/- 330.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=276000, episode_reward=518.80 +/- 224.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=277000, episode_reward=799.60 +/- 105.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=278000, episode_reward=738.60 +/- 295.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=279000, episode_reward=819.20 +/- 185.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=280000, episode_reward=721.40 +/- 200.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=281000, episode_reward=-2660.20 +/- 6306.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=282000, episode_reward=866.00 +/- 263.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=283000, episode_reward=667.80 +/- 141.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=284000, episode_reward=480.60 +/- 265.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=285000, episode_reward=818.20 +/- 233.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=286000, episode_reward=634.20 +/- 320.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=287000, episode_reward=873.80 +/- 246.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=288000, episode_reward=572.40 +/- 202.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=289000, episode_reward=939.60 +/- 192.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=290000, episode_reward=932.20 +/- 184.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=291000, episode_reward=497.80 +/- 178.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=292000, episode_reward=838.80 +/- 291.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=293000, episode_reward=815.80 +/- 306.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=294000, episode_reward=1095.20 +/- 144.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=295000, episode_reward=835.80 +/- 410.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=296000, episode_reward=628.60 +/- 133.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=297000, episode_reward=901.20 +/- 407.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=298000, episode_reward=884.60 +/- 186.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=299000, episode_reward=702.80 +/- 255.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=300000, episode_reward=591.60 +/- 247.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=301000, episode_reward=897.40 +/- 219.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=302000, episode_reward=744.40 +/- 179.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=303000, episode_reward=883.00 +/- 83.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=304000, episode_reward=1030.20 +/- 217.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=305000, episode_reward=622.60 +/- 52.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=306000, episode_reward=697.40 +/- 320.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=307000, episode_reward=675.00 +/- 345.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=308000, episode_reward=968.80 +/- 184.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=309000, episode_reward=733.20 +/- 321.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=310000, episode_reward=1050.40 +/- 175.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=311000, episode_reward=773.60 +/- 152.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=312000, episode_reward=-1562.60 +/- 5227.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=313000, episode_reward=702.60 +/- 283.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=314000, episode_reward=905.80 +/- 133.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=315000, episode_reward=768.00 +/- 183.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=316000, episode_reward=681.00 +/- 252.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=317000, episode_reward=610.40 +/- 357.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=318000, episode_reward=429.40 +/- 301.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=319000, episode_reward=-1104.80 +/- 3592.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=320000, episode_reward=1203.00 +/- 381.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=321000, episode_reward=834.60 +/- 99.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=322000, episode_reward=905.00 +/- 220.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=323000, episode_reward=783.80 +/- 120.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=324000, episode_reward=817.40 +/- 165.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=325000, episode_reward=783.00 +/- 200.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=326000, episode_reward=145.40 +/- 1067.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=327000, episode_reward=314.20 +/- 686.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=328000, episode_reward=783.00 +/- 256.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=329000, episode_reward=1063.00 +/- 98.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=330000, episode_reward=575.80 +/- 131.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=331000, episode_reward=976.20 +/- 290.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=332000, episode_reward=684.00 +/- 181.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=333000, episode_reward=788.80 +/- 220.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=334000, episode_reward=1047.80 +/- 267.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=335000, episode_reward=1133.20 +/- 323.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=336000, episode_reward=792.60 +/- 281.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=337000, episode_reward=875.80 +/- 135.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=338000, episode_reward=-2770.80 +/- 4824.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=339000, episode_reward=835.40 +/- 306.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=340000, episode_reward=732.00 +/- 483.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=341000, episode_reward=622.00 +/- 531.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=342000, episode_reward=838.80 +/- 272.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=343000, episode_reward=1007.20 +/- 387.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=344000, episode_reward=740.20 +/- 230.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=345000, episode_reward=919.80 +/- 159.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=346000, episode_reward=700.00 +/- 275.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=347000, episode_reward=1080.80 +/- 271.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=348000, episode_reward=603.00 +/- 351.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=349000, episode_reward=888.60 +/- 241.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=350000, episode_reward=922.60 +/- 362.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=351000, episode_reward=922.20 +/- 319.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=352000, episode_reward=937.80 +/- 280.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=353000, episode_reward=720.60 +/- 327.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=354000, episode_reward=1108.80 +/- 142.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=355000, episode_reward=913.00 +/- 352.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=356000, episode_reward=712.20 +/- 197.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=357000, episode_reward=882.60 +/- 207.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=358000, episode_reward=904.80 +/- 268.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=359000, episode_reward=467.00 +/- 93.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=360000, episode_reward=857.40 +/- 371.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=361000, episode_reward=623.60 +/- 233.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=362000, episode_reward=740.20 +/- 334.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=363000, episode_reward=1013.60 +/- 216.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=364000, episode_reward=854.20 +/- 222.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=365000, episode_reward=979.20 +/- 385.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=366000, episode_reward=836.20 +/- 316.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=367000, episode_reward=954.80 +/- 291.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=368000, episode_reward=799.40 +/- 280.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=369000, episode_reward=558.60 +/- 178.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=370000, episode_reward=733.60 +/- 207.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=371000, episode_reward=959.20 +/- 390.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=372000, episode_reward=813.00 +/- 221.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=373000, episode_reward=-756.40 +/- 2587.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=374000, episode_reward=1008.60 +/- 229.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=375000, episode_reward=676.20 +/- 170.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=376000, episode_reward=669.20 +/- 313.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=377000, episode_reward=919.00 +/- 270.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=378000, episode_reward=896.40 +/- 327.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=379000, episode_reward=819.00 +/- 241.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=380000, episode_reward=724.60 +/- 145.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=381000, episode_reward=733.00 +/- 84.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=382000, episode_reward=731.80 +/- 297.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=383000, episode_reward=922.40 +/- 467.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=384000, episode_reward=733.20 +/- 346.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=385000, episode_reward=701.80 +/- 395.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=386000, episode_reward=719.80 +/- 428.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=387000, episode_reward=1011.80 +/- 241.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=388000, episode_reward=1026.60 +/- 216.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=389000, episode_reward=635.60 +/- 127.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=390000, episode_reward=619.80 +/- 145.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=391000, episode_reward=750.00 +/- 318.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=392000, episode_reward=881.00 +/- 309.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=393000, episode_reward=799.00 +/- 274.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=394000, episode_reward=902.40 +/- 267.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=395000, episode_reward=572.20 +/- 163.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=396000, episode_reward=820.20 +/- 354.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=397000, episode_reward=1019.60 +/- 241.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=398000, episode_reward=780.60 +/- 222.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=399000, episode_reward=850.00 +/- 349.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=400000, episode_reward=947.80 +/- 311.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=401000, episode_reward=489.80 +/- 394.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=402000, episode_reward=1000.00 +/- 109.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=403000, episode_reward=821.20 +/- 198.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=404000, episode_reward=982.40 +/- 369.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=405000, episode_reward=882.40 +/- 195.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=406000, episode_reward=1196.60 +/- 292.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=407000, episode_reward=786.80 +/- 191.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=408000, episode_reward=1031.00 +/- 267.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=409000, episode_reward=868.60 +/- 329.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=410000, episode_reward=1054.00 +/- 405.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=411000, episode_reward=682.20 +/- 268.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=412000, episode_reward=655.80 +/- 117.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=413000, episode_reward=982.40 +/- 340.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=414000, episode_reward=870.60 +/- 341.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=415000, episode_reward=1095.40 +/- 178.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=416000, episode_reward=922.40 +/- 202.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=417000, episode_reward=673.80 +/- 293.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=418000, episode_reward=1035.60 +/- 134.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=419000, episode_reward=659.20 +/- 293.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=420000, episode_reward=1139.80 +/- 131.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=421000, episode_reward=906.60 +/- 229.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=422000, episode_reward=-1664.60 +/- 4729.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=423000, episode_reward=897.80 +/- 300.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=424000, episode_reward=897.20 +/- 124.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=425000, episode_reward=883.60 +/- 138.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=426000, episode_reward=830.40 +/- 176.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=427000, episode_reward=939.60 +/- 460.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=428000, episode_reward=1022.20 +/- 427.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=429000, episode_reward=1008.00 +/- 338.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=430000, episode_reward=947.40 +/- 318.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=431000, episode_reward=835.00 +/- 182.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=432000, episode_reward=890.20 +/- 151.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=433000, episode_reward=840.20 +/- 250.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=434000, episode_reward=939.20 +/- 246.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=435000, episode_reward=812.00 +/- 226.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=436000, episode_reward=753.40 +/- 268.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=437000, episode_reward=-838.00 +/- 2661.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=438000, episode_reward=704.80 +/- 413.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=439000, episode_reward=1071.20 +/- 99.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=440000, episode_reward=999.60 +/- 106.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=441000, episode_reward=853.40 +/- 264.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=442000, episode_reward=977.40 +/- 314.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=443000, episode_reward=884.40 +/- 245.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=444000, episode_reward=859.60 +/- 420.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=445000, episode_reward=908.40 +/- 284.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=446000, episode_reward=1073.80 +/- 287.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=447000, episode_reward=1306.20 +/- 124.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=448000, episode_reward=995.00 +/- 218.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=449000, episode_reward=1119.20 +/- 107.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=450000, episode_reward=986.20 +/- 230.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=451000, episode_reward=1073.00 +/- 295.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=452000, episode_reward=1281.80 +/- 289.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=453000, episode_reward=1004.80 +/- 394.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=454000, episode_reward=1185.80 +/- 340.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=455000, episode_reward=957.20 +/- 273.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=456000, episode_reward=1086.80 +/- 244.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=457000, episode_reward=1050.00 +/- 137.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=458000, episode_reward=1102.60 +/- 162.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=459000, episode_reward=1054.00 +/- 365.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=460000, episode_reward=1109.00 +/- 253.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=461000, episode_reward=1279.40 +/- 135.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=462000, episode_reward=1064.20 +/- 246.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=463000, episode_reward=1209.60 +/- 376.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=464000, episode_reward=-1170.80 +/- 4428.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=465000, episode_reward=1120.00 +/- 182.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=466000, episode_reward=1092.00 +/- 517.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=467000, episode_reward=960.00 +/- 227.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=468000, episode_reward=1053.80 +/- 187.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=469000, episode_reward=1458.40 +/- 214.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=470000, episode_reward=1344.60 +/- 189.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=471000, episode_reward=1139.80 +/- 433.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=472000, episode_reward=1058.60 +/- 201.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=473000, episode_reward=1217.00 +/- 242.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=474000, episode_reward=1111.60 +/- 251.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=475000, episode_reward=1229.00 +/- 44.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=476000, episode_reward=1398.60 +/- 359.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=477000, episode_reward=1106.00 +/- 257.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=478000, episode_reward=928.60 +/- 231.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=479000, episode_reward=1094.40 +/- 350.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=480000, episode_reward=1275.80 +/- 206.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=481000, episode_reward=1324.00 +/- 454.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=482000, episode_reward=1014.40 +/- 234.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=483000, episode_reward=1096.40 +/- 205.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=484000, episode_reward=1382.80 +/- 371.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=485000, episode_reward=1039.80 +/- 118.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=486000, episode_reward=1148.80 +/- 309.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=487000, episode_reward=1249.20 +/- 260.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=488000, episode_reward=1168.00 +/- 483.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=489000, episode_reward=1296.60 +/- 310.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=490000, episode_reward=1041.80 +/- 213.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=491000, episode_reward=1308.00 +/- 348.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=492000, episode_reward=1208.60 +/- 406.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=493000, episode_reward=1289.00 +/- 234.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=494000, episode_reward=1356.00 +/- 188.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=495000, episode_reward=1486.20 +/- 175.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=496000, episode_reward=1401.40 +/- 379.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=497000, episode_reward=1087.20 +/- 221.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=498000, episode_reward=1266.80 +/- 356.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=499000, episode_reward=1401.00 +/- 188.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=500000, episode_reward=1209.00 +/- 264.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Gamma = 0.9, Learning Rate = 0.003, Activation Function = Tanh, Neurons = 32, Process noise = 0.1, Measurement noise = 0.0\n",
      "Eval num_timesteps=1000, episode_reward=-693.60 +/- 138.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=-836.40 +/- 327.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=3000, episode_reward=-845.60 +/- 490.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=4000, episode_reward=-635.00 +/- 77.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=5000, episode_reward=-709.40 +/- 305.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=6000, episode_reward=-630.20 +/- 37.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=7000, episode_reward=-679.00 +/- 66.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=8000, episode_reward=-613.60 +/- 60.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=9000, episode_reward=-763.00 +/- 423.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=10000, episode_reward=-750.20 +/- 276.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=11000, episode_reward=-826.00 +/- 448.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=12000, episode_reward=-865.80 +/- 467.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=13000, episode_reward=-595.00 +/- 128.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=14000, episode_reward=-1346.00 +/- 596.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=15000, episode_reward=-898.80 +/- 405.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=16000, episode_reward=-746.80 +/- 185.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=17000, episode_reward=-936.40 +/- 710.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=18000, episode_reward=-767.80 +/- 169.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=19000, episode_reward=-603.60 +/- 102.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=20000, episode_reward=-967.00 +/- 679.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=21000, episode_reward=-920.00 +/- 476.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=22000, episode_reward=-618.60 +/- 58.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=23000, episode_reward=-664.60 +/- 213.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=24000, episode_reward=-964.00 +/- 396.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=25000, episode_reward=-581.00 +/- 56.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=26000, episode_reward=-1176.00 +/- 723.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=27000, episode_reward=-599.80 +/- 70.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=28000, episode_reward=-616.40 +/- 70.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=29000, episode_reward=-850.60 +/- 380.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=30000, episode_reward=-584.40 +/- 75.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=31000, episode_reward=-681.20 +/- 250.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=32000, episode_reward=-683.40 +/- 141.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=33000, episode_reward=-713.80 +/- 163.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=34000, episode_reward=-766.20 +/- 72.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=35000, episode_reward=-564.20 +/- 73.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=36000, episode_reward=-595.20 +/- 190.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=37000, episode_reward=-1193.00 +/- 689.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=38000, episode_reward=-691.20 +/- 225.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=39000, episode_reward=-785.00 +/- 381.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=40000, episode_reward=-767.20 +/- 425.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=41000, episode_reward=-1143.20 +/- 704.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=42000, episode_reward=-609.00 +/- 92.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=43000, episode_reward=-558.80 +/- 77.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=44000, episode_reward=-873.60 +/- 415.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=45000, episode_reward=-609.00 +/- 73.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=46000, episode_reward=-563.60 +/- 99.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=47000, episode_reward=-589.40 +/- 113.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=48000, episode_reward=-581.60 +/- 76.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=49000, episode_reward=-605.20 +/- 39.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=50000, episode_reward=-914.80 +/- 388.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=51000, episode_reward=3.80 +/- 224.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=52000, episode_reward=-508.80 +/- 241.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=53000, episode_reward=-1296.20 +/- 299.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=54000, episode_reward=-46.80 +/- 313.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=55000, episode_reward=-109.00 +/- 196.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=56000, episode_reward=-102.20 +/- 200.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=57000, episode_reward=-217.60 +/- 308.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=58000, episode_reward=180.40 +/- 179.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=59000, episode_reward=243.40 +/- 260.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=60000, episode_reward=210.20 +/- 85.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=61000, episode_reward=-4548.60 +/- 731.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=62000, episode_reward=-723.60 +/- 596.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=63000, episode_reward=-5593.00 +/- 2407.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=64000, episode_reward=-5212.20 +/- 1559.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=65000, episode_reward=-2330.60 +/- 2446.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=66000, episode_reward=-5798.60 +/- 2663.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=67000, episode_reward=136.00 +/- 87.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=68000, episode_reward=-1144.80 +/- 2097.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=69000, episode_reward=-1855.60 +/- 2744.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=70000, episode_reward=-2872.80 +/- 3008.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=71000, episode_reward=52.00 +/- 302.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=72000, episode_reward=177.80 +/- 76.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=73000, episode_reward=194.20 +/- 154.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=74000, episode_reward=69.60 +/- 346.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=75000, episode_reward=347.40 +/- 199.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=76000, episode_reward=452.20 +/- 130.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=77000, episode_reward=263.20 +/- 221.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=78000, episode_reward=360.20 +/- 236.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=79000, episode_reward=386.00 +/- 97.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=80000, episode_reward=316.20 +/- 315.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=81000, episode_reward=-135.60 +/- 144.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=82000, episode_reward=337.00 +/- 133.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=83000, episode_reward=420.80 +/- 233.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=84000, episode_reward=583.20 +/- 245.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=85000, episode_reward=917.00 +/- 172.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=86000, episode_reward=956.40 +/- 471.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=87000, episode_reward=389.80 +/- 323.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=88000, episode_reward=730.20 +/- 194.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=89000, episode_reward=784.80 +/- 355.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=90000, episode_reward=1004.20 +/- 175.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=91000, episode_reward=-2365.00 +/- 6226.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=92000, episode_reward=874.80 +/- 241.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=93000, episode_reward=938.40 +/- 393.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=94000, episode_reward=1112.80 +/- 381.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=95000, episode_reward=806.00 +/- 149.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=96000, episode_reward=815.00 +/- 405.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=97000, episode_reward=809.00 +/- 121.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=98000, episode_reward=726.80 +/- 100.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=99000, episode_reward=800.00 +/- 354.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=100000, episode_reward=841.20 +/- 221.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=101000, episode_reward=1040.00 +/- 379.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=102000, episode_reward=754.20 +/- 226.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=103000, episode_reward=769.20 +/- 625.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=104000, episode_reward=872.60 +/- 317.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=105000, episode_reward=980.20 +/- 297.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=106000, episode_reward=841.60 +/- 213.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=107000, episode_reward=989.00 +/- 216.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=108000, episode_reward=892.00 +/- 231.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=109000, episode_reward=999.40 +/- 252.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=110000, episode_reward=960.40 +/- 320.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=111000, episode_reward=435.20 +/- 160.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=112000, episode_reward=709.20 +/- 112.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=113000, episode_reward=764.80 +/- 342.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=114000, episode_reward=760.60 +/- 126.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=115000, episode_reward=679.40 +/- 302.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=116000, episode_reward=758.60 +/- 183.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=117000, episode_reward=896.60 +/- 354.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=118000, episode_reward=831.20 +/- 301.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=119000, episode_reward=919.80 +/- 221.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=120000, episode_reward=797.80 +/- 211.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=121000, episode_reward=625.20 +/- 383.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=122000, episode_reward=736.00 +/- 200.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=123000, episode_reward=951.20 +/- 173.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=124000, episode_reward=454.60 +/- 323.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=125000, episode_reward=671.80 +/- 274.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=126000, episode_reward=722.00 +/- 201.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=127000, episode_reward=620.60 +/- 317.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=128000, episode_reward=675.60 +/- 291.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=129000, episode_reward=586.60 +/- 190.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=130000, episode_reward=849.60 +/- 143.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=131000, episode_reward=663.40 +/- 158.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=132000, episode_reward=893.40 +/- 109.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=133000, episode_reward=1015.20 +/- 220.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=134000, episode_reward=1002.60 +/- 206.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=135000, episode_reward=763.80 +/- 262.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=136000, episode_reward=813.00 +/- 240.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=137000, episode_reward=882.20 +/- 131.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=138000, episode_reward=640.00 +/- 421.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=139000, episode_reward=910.00 +/- 134.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=140000, episode_reward=1012.40 +/- 172.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=141000, episode_reward=725.00 +/- 298.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=142000, episode_reward=917.20 +/- 342.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=143000, episode_reward=1038.00 +/- 246.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=144000, episode_reward=705.00 +/- 447.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=145000, episode_reward=837.80 +/- 297.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=146000, episode_reward=809.20 +/- 138.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=147000, episode_reward=860.60 +/- 184.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=148000, episode_reward=1077.60 +/- 392.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=149000, episode_reward=956.80 +/- 206.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=150000, episode_reward=1073.00 +/- 102.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=151000, episode_reward=828.40 +/- 256.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=152000, episode_reward=932.80 +/- 221.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=153000, episode_reward=390.60 +/- 1490.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=154000, episode_reward=849.60 +/- 232.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=155000, episode_reward=778.60 +/- 244.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=156000, episode_reward=1021.20 +/- 209.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=157000, episode_reward=1074.40 +/- 411.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=158000, episode_reward=800.00 +/- 448.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=159000, episode_reward=867.20 +/- 219.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=160000, episode_reward=626.00 +/- 186.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=161000, episode_reward=814.00 +/- 254.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=162000, episode_reward=590.80 +/- 264.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=163000, episode_reward=901.20 +/- 185.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=164000, episode_reward=732.00 +/- 300.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=165000, episode_reward=745.20 +/- 285.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=166000, episode_reward=1050.60 +/- 332.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=167000, episode_reward=862.20 +/- 169.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=168000, episode_reward=725.00 +/- 268.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=169000, episode_reward=865.60 +/- 353.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=170000, episode_reward=589.40 +/- 148.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=171000, episode_reward=1026.40 +/- 219.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=172000, episode_reward=910.40 +/- 289.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=173000, episode_reward=767.00 +/- 260.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=174000, episode_reward=818.00 +/- 205.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=175000, episode_reward=1002.60 +/- 366.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=176000, episode_reward=954.60 +/- 291.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=177000, episode_reward=811.00 +/- 352.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=178000, episode_reward=645.20 +/- 340.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=179000, episode_reward=972.80 +/- 241.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=180000, episode_reward=1016.40 +/- 249.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=181000, episode_reward=980.00 +/- 219.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=182000, episode_reward=814.20 +/- 376.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=183000, episode_reward=1062.40 +/- 293.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=184000, episode_reward=795.80 +/- 254.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=185000, episode_reward=819.60 +/- 457.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=186000, episode_reward=953.80 +/- 131.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=187000, episode_reward=765.00 +/- 140.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=188000, episode_reward=1114.00 +/- 226.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=189000, episode_reward=881.80 +/- 299.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=190000, episode_reward=983.00 +/- 289.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=191000, episode_reward=766.80 +/- 164.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=192000, episode_reward=874.00 +/- 130.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=193000, episode_reward=696.80 +/- 221.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=194000, episode_reward=815.80 +/- 502.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=195000, episode_reward=1038.40 +/- 258.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=196000, episode_reward=741.00 +/- 437.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=197000, episode_reward=879.40 +/- 214.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=198000, episode_reward=704.20 +/- 129.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=199000, episode_reward=986.20 +/- 71.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=200000, episode_reward=820.00 +/- 150.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=201000, episode_reward=1032.40 +/- 195.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=202000, episode_reward=821.40 +/- 110.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=203000, episode_reward=901.40 +/- 393.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=204000, episode_reward=1097.00 +/- 221.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=205000, episode_reward=978.20 +/- 153.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=206000, episode_reward=668.40 +/- 426.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=207000, episode_reward=895.60 +/- 249.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=208000, episode_reward=1150.00 +/- 201.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=209000, episode_reward=932.20 +/- 155.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=210000, episode_reward=966.60 +/- 272.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=211000, episode_reward=1010.20 +/- 100.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=212000, episode_reward=945.60 +/- 246.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=213000, episode_reward=837.20 +/- 272.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=214000, episode_reward=931.40 +/- 282.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=215000, episode_reward=919.80 +/- 331.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=216000, episode_reward=-2284.00 +/- 6305.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=217000, episode_reward=971.40 +/- 330.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=218000, episode_reward=1182.80 +/- 252.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=219000, episode_reward=1168.40 +/- 345.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=220000, episode_reward=945.20 +/- 321.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=221000, episode_reward=1123.80 +/- 71.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=222000, episode_reward=830.60 +/- 333.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=223000, episode_reward=1133.20 +/- 346.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=224000, episode_reward=1025.40 +/- 124.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=225000, episode_reward=1081.20 +/- 271.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=226000, episode_reward=798.40 +/- 100.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=227000, episode_reward=943.40 +/- 183.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=228000, episode_reward=-1234.20 +/- 5082.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=229000, episode_reward=863.00 +/- 499.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=230000, episode_reward=1057.00 +/- 444.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=231000, episode_reward=937.00 +/- 94.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=232000, episode_reward=1146.20 +/- 314.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=233000, episode_reward=773.20 +/- 333.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=234000, episode_reward=789.80 +/- 203.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=235000, episode_reward=912.40 +/- 354.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=236000, episode_reward=1120.20 +/- 191.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=237000, episode_reward=1205.60 +/- 315.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=238000, episode_reward=-1857.00 +/- 5673.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=239000, episode_reward=1035.60 +/- 317.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=240000, episode_reward=954.20 +/- 106.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=241000, episode_reward=834.60 +/- 216.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=242000, episode_reward=1008.20 +/- 443.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=243000, episode_reward=1015.60 +/- 243.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=244000, episode_reward=1109.80 +/- 268.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=245000, episode_reward=1143.40 +/- 391.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=246000, episode_reward=864.40 +/- 349.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=247000, episode_reward=1081.00 +/- 381.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=248000, episode_reward=1287.60 +/- 249.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=249000, episode_reward=1061.00 +/- 296.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=250000, episode_reward=1106.00 +/- 327.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=251000, episode_reward=1131.60 +/- 164.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=252000, episode_reward=1160.80 +/- 348.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=253000, episode_reward=1320.20 +/- 329.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=254000, episode_reward=963.40 +/- 254.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=255000, episode_reward=1045.40 +/- 193.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=256000, episode_reward=1078.80 +/- 167.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=257000, episode_reward=1292.20 +/- 230.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=258000, episode_reward=890.80 +/- 320.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=259000, episode_reward=1302.00 +/- 176.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=260000, episode_reward=1248.60 +/- 410.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=261000, episode_reward=1195.00 +/- 203.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=262000, episode_reward=1524.20 +/- 172.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=263000, episode_reward=1274.60 +/- 177.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=264000, episode_reward=1255.60 +/- 343.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=265000, episode_reward=969.40 +/- 309.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=266000, episode_reward=1290.60 +/- 223.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=267000, episode_reward=1274.40 +/- 285.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=268000, episode_reward=1266.40 +/- 155.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=269000, episode_reward=1075.80 +/- 224.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=270000, episode_reward=1258.80 +/- 199.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=271000, episode_reward=1217.80 +/- 290.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=272000, episode_reward=1170.20 +/- 194.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=273000, episode_reward=1472.80 +/- 164.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=274000, episode_reward=973.20 +/- 243.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=275000, episode_reward=1376.40 +/- 316.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=276000, episode_reward=1219.20 +/- 217.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=277000, episode_reward=1001.60 +/- 258.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=278000, episode_reward=820.40 +/- 396.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=279000, episode_reward=1412.60 +/- 121.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=280000, episode_reward=1229.20 +/- 310.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=281000, episode_reward=1098.80 +/- 395.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=282000, episode_reward=1039.40 +/- 195.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=283000, episode_reward=973.40 +/- 262.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=284000, episode_reward=943.20 +/- 214.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=285000, episode_reward=-1027.60 +/- 3927.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=286000, episode_reward=1065.00 +/- 273.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=287000, episode_reward=1183.80 +/- 184.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=288000, episode_reward=96.40 +/- 2184.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=289000, episode_reward=1012.60 +/- 156.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=290000, episode_reward=815.00 +/- 267.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=291000, episode_reward=1103.40 +/- 192.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=292000, episode_reward=1328.60 +/- 257.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=293000, episode_reward=1088.00 +/- 339.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=294000, episode_reward=1086.60 +/- 278.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=295000, episode_reward=1035.40 +/- 203.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=296000, episode_reward=1285.00 +/- 185.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=297000, episode_reward=1367.20 +/- 270.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=298000, episode_reward=995.80 +/- 212.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=299000, episode_reward=1048.20 +/- 230.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=300000, episode_reward=1088.40 +/- 196.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=301000, episode_reward=1053.00 +/- 348.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=302000, episode_reward=520.80 +/- 1373.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=303000, episode_reward=1040.20 +/- 200.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=304000, episode_reward=1418.60 +/- 172.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=305000, episode_reward=1098.80 +/- 50.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=306000, episode_reward=1045.20 +/- 190.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=307000, episode_reward=1245.20 +/- 220.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=308000, episode_reward=1382.60 +/- 308.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=309000, episode_reward=1211.80 +/- 228.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=310000, episode_reward=1297.80 +/- 217.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=311000, episode_reward=968.00 +/- 379.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=312000, episode_reward=988.80 +/- 221.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=313000, episode_reward=1027.20 +/- 109.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=314000, episode_reward=1354.60 +/- 250.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=315000, episode_reward=1363.40 +/- 360.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=316000, episode_reward=987.00 +/- 277.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=317000, episode_reward=1449.20 +/- 187.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=318000, episode_reward=1188.20 +/- 347.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=319000, episode_reward=1223.20 +/- 286.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=320000, episode_reward=1338.00 +/- 210.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=321000, episode_reward=1283.40 +/- 208.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=322000, episode_reward=1398.60 +/- 219.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=323000, episode_reward=1092.20 +/- 178.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=324000, episode_reward=1091.80 +/- 333.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=325000, episode_reward=1013.40 +/- 200.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=326000, episode_reward=1102.60 +/- 246.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=327000, episode_reward=911.80 +/- 284.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=328000, episode_reward=1177.80 +/- 244.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=329000, episode_reward=1170.20 +/- 450.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=330000, episode_reward=1084.40 +/- 408.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=331000, episode_reward=1031.00 +/- 394.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=332000, episode_reward=1197.20 +/- 254.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=333000, episode_reward=1191.60 +/- 259.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=334000, episode_reward=977.60 +/- 195.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=335000, episode_reward=1175.60 +/- 291.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=336000, episode_reward=781.00 +/- 209.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=337000, episode_reward=1444.80 +/- 227.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=338000, episode_reward=912.80 +/- 183.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=339000, episode_reward=885.00 +/- 241.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=340000, episode_reward=935.60 +/- 179.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=341000, episode_reward=1051.00 +/- 308.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=342000, episode_reward=1045.00 +/- 222.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=343000, episode_reward=1001.20 +/- 165.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=344000, episode_reward=959.60 +/- 179.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=345000, episode_reward=1154.00 +/- 277.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=346000, episode_reward=1036.40 +/- 301.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=347000, episode_reward=1135.60 +/- 271.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=348000, episode_reward=999.00 +/- 201.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=349000, episode_reward=1480.20 +/- 249.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=350000, episode_reward=1435.80 +/- 151.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=351000, episode_reward=1118.20 +/- 297.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=352000, episode_reward=1133.80 +/- 459.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=353000, episode_reward=1021.00 +/- 135.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=354000, episode_reward=-1461.80 +/- 5283.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=355000, episode_reward=1258.40 +/- 186.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=356000, episode_reward=1209.80 +/- 125.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=357000, episode_reward=1275.80 +/- 192.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=358000, episode_reward=1139.20 +/- 436.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=359000, episode_reward=1089.60 +/- 150.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=360000, episode_reward=937.00 +/- 210.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=361000, episode_reward=1257.40 +/- 424.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=362000, episode_reward=1145.40 +/- 187.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=363000, episode_reward=1202.00 +/- 208.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=364000, episode_reward=1289.40 +/- 220.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=365000, episode_reward=1112.20 +/- 241.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=366000, episode_reward=1134.00 +/- 322.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=367000, episode_reward=1069.00 +/- 338.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=368000, episode_reward=1154.00 +/- 279.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=369000, episode_reward=-1689.80 +/- 5105.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=370000, episode_reward=-1605.00 +/- 4888.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=371000, episode_reward=1227.20 +/- 447.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=372000, episode_reward=1001.20 +/- 187.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=373000, episode_reward=1242.60 +/- 219.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=374000, episode_reward=533.80 +/- 1409.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=375000, episode_reward=991.20 +/- 373.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=376000, episode_reward=1015.20 +/- 323.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=377000, episode_reward=318.40 +/- 1331.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=378000, episode_reward=932.20 +/- 170.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=379000, episode_reward=1094.60 +/- 268.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=380000, episode_reward=456.00 +/- 1621.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=381000, episode_reward=1325.40 +/- 144.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=382000, episode_reward=1275.00 +/- 272.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=383000, episode_reward=1134.80 +/- 128.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=384000, episode_reward=1283.40 +/- 319.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=385000, episode_reward=970.60 +/- 216.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=386000, episode_reward=-2267.80 +/- 7170.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=387000, episode_reward=1305.40 +/- 275.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=388000, episode_reward=1023.00 +/- 314.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=389000, episode_reward=1052.60 +/- 238.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=390000, episode_reward=1161.40 +/- 323.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=391000, episode_reward=1326.60 +/- 139.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=392000, episode_reward=1099.40 +/- 148.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=393000, episode_reward=1270.60 +/- 89.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=394000, episode_reward=1082.40 +/- 135.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=395000, episode_reward=1164.60 +/- 364.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=396000, episode_reward=1147.20 +/- 267.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=397000, episode_reward=1115.40 +/- 270.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=398000, episode_reward=995.40 +/- 107.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=399000, episode_reward=1168.60 +/- 240.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=400000, episode_reward=1110.40 +/- 108.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=401000, episode_reward=-672.80 +/- 3827.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=402000, episode_reward=834.40 +/- 580.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=403000, episode_reward=1289.00 +/- 220.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=404000, episode_reward=1305.80 +/- 192.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=405000, episode_reward=1231.80 +/- 324.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=406000, episode_reward=-1050.40 +/- 4820.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=407000, episode_reward=1056.60 +/- 288.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=408000, episode_reward=820.60 +/- 490.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=409000, episode_reward=1019.20 +/- 134.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=410000, episode_reward=1243.40 +/- 219.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=411000, episode_reward=1235.20 +/- 358.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=412000, episode_reward=1507.60 +/- 338.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=413000, episode_reward=1478.20 +/- 230.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=414000, episode_reward=1103.20 +/- 169.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=415000, episode_reward=1046.40 +/- 220.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=416000, episode_reward=923.80 +/- 366.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=417000, episode_reward=1498.80 +/- 170.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=418000, episode_reward=-363.40 +/- 3327.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=419000, episode_reward=1274.20 +/- 342.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=420000, episode_reward=992.20 +/- 263.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=421000, episode_reward=1318.60 +/- 276.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=422000, episode_reward=959.40 +/- 277.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=423000, episode_reward=38.80 +/- 1737.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=424000, episode_reward=1230.80 +/- 277.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=425000, episode_reward=1353.80 +/- 149.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=426000, episode_reward=1232.00 +/- 199.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=427000, episode_reward=746.60 +/- 251.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=428000, episode_reward=967.40 +/- 189.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=429000, episode_reward=-1440.40 +/- 5026.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=430000, episode_reward=1214.20 +/- 256.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=431000, episode_reward=1039.20 +/- 271.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=432000, episode_reward=1342.00 +/- 196.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=433000, episode_reward=1412.80 +/- 194.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=434000, episode_reward=1178.00 +/- 168.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=435000, episode_reward=1306.80 +/- 248.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=436000, episode_reward=762.80 +/- 363.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=437000, episode_reward=1543.40 +/- 192.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=438000, episode_reward=1090.40 +/- 217.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=439000, episode_reward=1147.00 +/- 195.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=440000, episode_reward=1335.80 +/- 255.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=441000, episode_reward=1089.60 +/- 172.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=442000, episode_reward=1113.00 +/- 131.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=443000, episode_reward=1360.40 +/- 212.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=444000, episode_reward=1275.60 +/- 174.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=445000, episode_reward=1165.80 +/- 348.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=446000, episode_reward=1284.20 +/- 228.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=447000, episode_reward=1316.60 +/- 196.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=448000, episode_reward=-37.40 +/- 2169.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=449000, episode_reward=974.80 +/- 263.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=450000, episode_reward=-1863.80 +/- 6013.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=451000, episode_reward=1119.80 +/- 434.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=452000, episode_reward=1077.80 +/- 269.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=453000, episode_reward=1280.20 +/- 176.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=454000, episode_reward=1175.00 +/- 219.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=455000, episode_reward=1238.20 +/- 99.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=456000, episode_reward=568.00 +/- 957.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=457000, episode_reward=914.80 +/- 332.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=458000, episode_reward=1109.80 +/- 202.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=459000, episode_reward=1002.80 +/- 303.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=460000, episode_reward=1287.80 +/- 370.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=461000, episode_reward=1319.00 +/- 175.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=462000, episode_reward=1095.00 +/- 126.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=463000, episode_reward=1450.20 +/- 179.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=464000, episode_reward=1196.20 +/- 202.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=465000, episode_reward=913.60 +/- 165.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=466000, episode_reward=1156.60 +/- 61.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=467000, episode_reward=1034.80 +/- 203.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=468000, episode_reward=301.00 +/- 2245.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=469000, episode_reward=1212.40 +/- 220.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=470000, episode_reward=801.60 +/- 182.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=471000, episode_reward=1084.60 +/- 180.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=472000, episode_reward=1036.00 +/- 216.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=473000, episode_reward=1174.40 +/- 259.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=474000, episode_reward=1042.80 +/- 164.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=475000, episode_reward=1213.60 +/- 312.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=476000, episode_reward=1352.40 +/- 121.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=477000, episode_reward=1056.80 +/- 290.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=478000, episode_reward=1330.60 +/- 350.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=479000, episode_reward=1122.40 +/- 230.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=480000, episode_reward=1018.00 +/- 280.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=481000, episode_reward=1009.40 +/- 207.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=482000, episode_reward=1054.40 +/- 335.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=483000, episode_reward=1086.60 +/- 293.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=484000, episode_reward=1124.60 +/- 305.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=485000, episode_reward=1285.00 +/- 259.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=486000, episode_reward=1148.80 +/- 177.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=487000, episode_reward=1231.40 +/- 171.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=488000, episode_reward=1086.80 +/- 402.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=489000, episode_reward=1260.20 +/- 139.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=490000, episode_reward=1252.80 +/- 208.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=491000, episode_reward=1213.40 +/- 337.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=492000, episode_reward=1156.40 +/- 193.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=493000, episode_reward=1240.80 +/- 308.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=494000, episode_reward=1165.60 +/- 317.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=495000, episode_reward=1107.60 +/- 274.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=496000, episode_reward=1361.80 +/- 316.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=497000, episode_reward=1062.60 +/- 204.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=498000, episode_reward=1370.60 +/- 306.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=499000, episode_reward=1075.20 +/- 207.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=500000, episode_reward=1088.40 +/- 207.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Gamma = 0.9, Learning Rate = 0.003, Activation Function = Tanh, Neurons = 64, Process noise = 0.1, Measurement noise = 0.0\n",
      "Eval num_timesteps=1000, episode_reward=-513.40 +/- 163.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=-467.20 +/- 95.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3000, episode_reward=-497.00 +/- 158.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=4000, episode_reward=-503.40 +/- 196.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=5000, episode_reward=-477.20 +/- 165.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=6000, episode_reward=-453.60 +/- 194.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=7000, episode_reward=-410.80 +/- 166.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=8000, episode_reward=-534.60 +/- 154.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=9000, episode_reward=-632.40 +/- 126.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=10000, episode_reward=-475.80 +/- 179.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=11000, episode_reward=-395.60 +/- 159.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=12000, episode_reward=-534.00 +/- 187.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=13000, episode_reward=-448.40 +/- 153.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=14000, episode_reward=-583.60 +/- 183.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=15000, episode_reward=-389.40 +/- 250.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=16000, episode_reward=-652.20 +/- 703.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=17000, episode_reward=-384.20 +/- 154.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=18000, episode_reward=-446.00 +/- 345.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=19000, episode_reward=-375.00 +/- 86.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=20000, episode_reward=-430.80 +/- 163.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=21000, episode_reward=-551.20 +/- 122.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=22000, episode_reward=-556.40 +/- 182.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=23000, episode_reward=-702.60 +/- 723.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=24000, episode_reward=-468.80 +/- 131.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=25000, episode_reward=-388.80 +/- 152.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=26000, episode_reward=-426.80 +/- 76.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=27000, episode_reward=-554.00 +/- 165.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=28000, episode_reward=-945.20 +/- 551.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=29000, episode_reward=-391.00 +/- 139.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=30000, episode_reward=-461.80 +/- 196.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=31000, episode_reward=-567.60 +/- 82.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=32000, episode_reward=-617.20 +/- 171.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=33000, episode_reward=-512.60 +/- 233.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=34000, episode_reward=-483.60 +/- 434.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=35000, episode_reward=-501.00 +/- 96.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=36000, episode_reward=-633.80 +/- 128.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=37000, episode_reward=-619.20 +/- 228.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=38000, episode_reward=-682.00 +/- 357.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=39000, episode_reward=-390.40 +/- 142.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=40000, episode_reward=-602.60 +/- 172.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=41000, episode_reward=-438.20 +/- 171.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=42000, episode_reward=-407.20 +/- 305.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=43000, episode_reward=-447.60 +/- 59.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=44000, episode_reward=-403.40 +/- 183.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=45000, episode_reward=-415.20 +/- 144.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=46000, episode_reward=-492.40 +/- 165.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=47000, episode_reward=-574.40 +/- 88.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=48000, episode_reward=-511.00 +/- 172.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=49000, episode_reward=-565.80 +/- 179.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=50000, episode_reward=-500.20 +/- 101.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=51000, episode_reward=-1108.60 +/- 220.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=52000, episode_reward=-1937.20 +/- 312.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=53000, episode_reward=280.00 +/- 123.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=54000, episode_reward=53.40 +/- 267.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=55000, episode_reward=152.80 +/- 79.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=56000, episode_reward=190.20 +/- 87.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=57000, episode_reward=172.80 +/- 237.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=58000, episode_reward=43.40 +/- 261.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=59000, episode_reward=-77.40 +/- 232.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=60000, episode_reward=28.40 +/- 180.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=61000, episode_reward=-7071.40 +/- 1652.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=62000, episode_reward=-5717.40 +/- 1442.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=63000, episode_reward=-2061.40 +/- 1513.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=64000, episode_reward=-1603.80 +/- 2434.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=65000, episode_reward=83.40 +/- 83.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=66000, episode_reward=-3104.20 +/- 2832.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=67000, episode_reward=-8.20 +/- 378.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=68000, episode_reward=155.60 +/- 292.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=69000, episode_reward=20.00 +/- 157.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=70000, episode_reward=170.40 +/- 202.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=71000, episode_reward=10.40 +/- 323.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=72000, episode_reward=207.80 +/- 191.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=73000, episode_reward=242.40 +/- 190.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=74000, episode_reward=285.20 +/- 159.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=75000, episode_reward=119.60 +/- 226.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=76000, episode_reward=239.80 +/- 94.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=77000, episode_reward=173.60 +/- 229.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=78000, episode_reward=331.80 +/- 194.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=79000, episode_reward=275.40 +/- 147.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=80000, episode_reward=654.00 +/- 410.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=81000, episode_reward=967.40 +/- 123.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=82000, episode_reward=639.00 +/- 307.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=83000, episode_reward=557.00 +/- 384.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=84000, episode_reward=1020.80 +/- 307.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=85000, episode_reward=829.20 +/- 133.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=86000, episode_reward=519.60 +/- 413.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=87000, episode_reward=-701.20 +/- 3545.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=88000, episode_reward=434.80 +/- 132.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=89000, episode_reward=1069.00 +/- 349.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=90000, episode_reward=764.20 +/- 194.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=91000, episode_reward=696.40 +/- 478.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=92000, episode_reward=438.40 +/- 336.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=93000, episode_reward=1125.40 +/- 246.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=94000, episode_reward=1051.80 +/- 352.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=95000, episode_reward=1218.80 +/- 430.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=96000, episode_reward=875.20 +/- 229.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=97000, episode_reward=950.40 +/- 247.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=98000, episode_reward=1001.40 +/- 354.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=99000, episode_reward=746.20 +/- 406.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=100000, episode_reward=821.20 +/- 174.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=101000, episode_reward=961.60 +/- 369.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=102000, episode_reward=771.00 +/- 73.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=103000, episode_reward=905.60 +/- 235.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=104000, episode_reward=888.60 +/- 369.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=105000, episode_reward=805.40 +/- 430.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=106000, episode_reward=1256.80 +/- 294.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=107000, episode_reward=1173.20 +/- 179.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=108000, episode_reward=766.00 +/- 356.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=109000, episode_reward=920.00 +/- 127.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=110000, episode_reward=968.00 +/- 144.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=111000, episode_reward=895.20 +/- 271.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=112000, episode_reward=1276.60 +/- 373.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=113000, episode_reward=981.40 +/- 194.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=114000, episode_reward=1284.80 +/- 233.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=115000, episode_reward=772.80 +/- 304.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=116000, episode_reward=787.00 +/- 134.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=117000, episode_reward=1071.40 +/- 484.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=118000, episode_reward=887.00 +/- 123.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=119000, episode_reward=1313.80 +/- 312.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=120000, episode_reward=963.40 +/- 269.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=121000, episode_reward=1043.60 +/- 381.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=122000, episode_reward=1035.20 +/- 577.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=123000, episode_reward=824.60 +/- 311.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=124000, episode_reward=823.40 +/- 236.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=125000, episode_reward=1177.80 +/- 269.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=126000, episode_reward=1295.20 +/- 207.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=127000, episode_reward=995.20 +/- 217.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=128000, episode_reward=995.00 +/- 306.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=129000, episode_reward=1183.20 +/- 241.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=130000, episode_reward=1267.20 +/- 166.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=131000, episode_reward=896.60 +/- 81.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=132000, episode_reward=1018.20 +/- 230.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=133000, episode_reward=814.20 +/- 218.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=134000, episode_reward=1088.40 +/- 219.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=135000, episode_reward=966.40 +/- 126.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=136000, episode_reward=1230.00 +/- 336.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=137000, episode_reward=865.80 +/- 367.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=138000, episode_reward=938.00 +/- 348.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=139000, episode_reward=-1639.80 +/- 5800.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=140000, episode_reward=639.00 +/- 188.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=141000, episode_reward=749.60 +/- 294.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=142000, episode_reward=1098.00 +/- 263.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=143000, episode_reward=1108.40 +/- 320.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=144000, episode_reward=915.40 +/- 561.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=145000, episode_reward=951.20 +/- 200.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=146000, episode_reward=972.20 +/- 321.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=147000, episode_reward=945.20 +/- 209.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=148000, episode_reward=1026.80 +/- 251.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=149000, episode_reward=1028.00 +/- 158.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=150000, episode_reward=1109.00 +/- 297.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=151000, episode_reward=1022.20 +/- 125.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=152000, episode_reward=1137.80 +/- 159.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=153000, episode_reward=1108.80 +/- 305.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=154000, episode_reward=1059.00 +/- 308.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=155000, episode_reward=-2220.80 +/- 7021.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=156000, episode_reward=1308.60 +/- 264.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=157000, episode_reward=1360.00 +/- 234.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=158000, episode_reward=1226.40 +/- 240.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=159000, episode_reward=1177.80 +/- 281.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=160000, episode_reward=1239.40 +/- 261.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=161000, episode_reward=1259.60 +/- 423.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=162000, episode_reward=1090.20 +/- 429.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=163000, episode_reward=1486.80 +/- 143.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=164000, episode_reward=1269.80 +/- 215.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=165000, episode_reward=1305.00 +/- 303.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=166000, episode_reward=1071.60 +/- 274.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=167000, episode_reward=1202.40 +/- 219.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=168000, episode_reward=1181.00 +/- 207.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=169000, episode_reward=1101.40 +/- 235.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=170000, episode_reward=-87.40 +/- 1946.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=171000, episode_reward=1378.20 +/- 182.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=172000, episode_reward=1440.00 +/- 251.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=173000, episode_reward=1269.20 +/- 207.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=174000, episode_reward=1145.40 +/- 246.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=175000, episode_reward=1415.60 +/- 265.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=176000, episode_reward=1139.80 +/- 354.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=177000, episode_reward=1236.80 +/- 188.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=178000, episode_reward=776.80 +/- 376.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=179000, episode_reward=1461.80 +/- 344.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=180000, episode_reward=1378.60 +/- 219.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=181000, episode_reward=1379.80 +/- 256.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=182000, episode_reward=1142.80 +/- 291.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=183000, episode_reward=1306.00 +/- 227.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=184000, episode_reward=1277.40 +/- 255.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=185000, episode_reward=1361.00 +/- 148.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=186000, episode_reward=1351.60 +/- 251.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=187000, episode_reward=1565.40 +/- 286.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=188000, episode_reward=1230.20 +/- 275.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=189000, episode_reward=1180.40 +/- 175.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=190000, episode_reward=1288.20 +/- 176.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=191000, episode_reward=1336.80 +/- 273.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=192000, episode_reward=1056.60 +/- 210.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=193000, episode_reward=1049.40 +/- 241.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=194000, episode_reward=893.60 +/- 164.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=195000, episode_reward=1319.60 +/- 222.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=196000, episode_reward=964.80 +/- 206.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=197000, episode_reward=1266.00 +/- 199.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=198000, episode_reward=1279.20 +/- 330.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=199000, episode_reward=1379.80 +/- 84.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=200000, episode_reward=1205.40 +/- 102.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=201000, episode_reward=1150.20 +/- 329.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=202000, episode_reward=945.20 +/- 214.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=203000, episode_reward=1458.20 +/- 206.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=204000, episode_reward=1202.20 +/- 303.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=205000, episode_reward=1358.80 +/- 187.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=206000, episode_reward=1429.40 +/- 166.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=207000, episode_reward=1371.40 +/- 174.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=208000, episode_reward=1109.40 +/- 372.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=209000, episode_reward=1063.80 +/- 160.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=210000, episode_reward=1223.40 +/- 369.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=211000, episode_reward=1116.80 +/- 259.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=212000, episode_reward=1152.20 +/- 88.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=213000, episode_reward=1307.00 +/- 288.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=214000, episode_reward=1217.60 +/- 140.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=215000, episode_reward=1155.80 +/- 218.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=216000, episode_reward=977.00 +/- 361.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=217000, episode_reward=1422.20 +/- 260.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=218000, episode_reward=1056.20 +/- 132.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=219000, episode_reward=1391.00 +/- 149.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=220000, episode_reward=1111.80 +/- 256.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=221000, episode_reward=1316.40 +/- 333.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=222000, episode_reward=1271.60 +/- 225.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=223000, episode_reward=1445.00 +/- 150.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=224000, episode_reward=1225.60 +/- 240.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=225000, episode_reward=1053.80 +/- 116.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=226000, episode_reward=1039.00 +/- 210.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=227000, episode_reward=1321.00 +/- 260.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=228000, episode_reward=1015.40 +/- 228.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=229000, episode_reward=1155.40 +/- 219.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=230000, episode_reward=1123.40 +/- 300.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=231000, episode_reward=1389.20 +/- 171.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=232000, episode_reward=1388.00 +/- 277.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=233000, episode_reward=1442.20 +/- 371.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=234000, episode_reward=1407.00 +/- 191.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=235000, episode_reward=1427.40 +/- 151.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=236000, episode_reward=1250.80 +/- 363.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=237000, episode_reward=1166.20 +/- 338.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=238000, episode_reward=929.60 +/- 191.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=239000, episode_reward=1524.00 +/- 220.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=240000, episode_reward=1406.80 +/- 217.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=241000, episode_reward=1216.00 +/- 160.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=242000, episode_reward=1177.60 +/- 299.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=243000, episode_reward=1198.40 +/- 169.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=244000, episode_reward=1070.40 +/- 349.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=245000, episode_reward=1280.20 +/- 272.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=246000, episode_reward=1094.40 +/- 329.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=247000, episode_reward=984.60 +/- 208.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=248000, episode_reward=1101.20 +/- 341.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=249000, episode_reward=1159.00 +/- 241.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=250000, episode_reward=1134.20 +/- 351.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=251000, episode_reward=1520.80 +/- 164.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=252000, episode_reward=1218.20 +/- 164.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=253000, episode_reward=1170.60 +/- 353.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=254000, episode_reward=1084.80 +/- 330.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=255000, episode_reward=1418.40 +/- 203.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=256000, episode_reward=1158.60 +/- 308.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=257000, episode_reward=1152.80 +/- 205.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=258000, episode_reward=1064.80 +/- 90.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=259000, episode_reward=1286.80 +/- 359.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=260000, episode_reward=1362.40 +/- 404.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=261000, episode_reward=1244.00 +/- 296.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=262000, episode_reward=1081.20 +/- 249.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=263000, episode_reward=1630.60 +/- 132.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=264000, episode_reward=1040.00 +/- 295.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=265000, episode_reward=846.40 +/- 388.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=266000, episode_reward=1331.00 +/- 169.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=267000, episode_reward=1393.60 +/- 371.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=268000, episode_reward=1093.20 +/- 155.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=269000, episode_reward=1237.00 +/- 129.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=270000, episode_reward=1538.60 +/- 279.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=271000, episode_reward=960.20 +/- 415.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=272000, episode_reward=1265.40 +/- 289.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=273000, episode_reward=1131.40 +/- 451.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=274000, episode_reward=1327.60 +/- 124.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=275000, episode_reward=1184.60 +/- 332.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=276000, episode_reward=1071.80 +/- 196.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=277000, episode_reward=1213.60 +/- 215.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=278000, episode_reward=940.80 +/- 169.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=279000, episode_reward=1067.20 +/- 195.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=280000, episode_reward=1284.60 +/- 141.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=281000, episode_reward=882.20 +/- 258.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=282000, episode_reward=1230.00 +/- 208.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=283000, episode_reward=1120.00 +/- 235.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=284000, episode_reward=1113.60 +/- 537.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=285000, episode_reward=1373.40 +/- 185.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=286000, episode_reward=1249.00 +/- 176.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=287000, episode_reward=1441.60 +/- 232.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=288000, episode_reward=1475.80 +/- 321.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=289000, episode_reward=1172.80 +/- 305.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=290000, episode_reward=1295.40 +/- 151.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=291000, episode_reward=1056.00 +/- 295.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=292000, episode_reward=1010.00 +/- 309.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=293000, episode_reward=1204.20 +/- 239.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=294000, episode_reward=1232.00 +/- 195.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=295000, episode_reward=774.60 +/- 199.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=296000, episode_reward=1009.00 +/- 305.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=297000, episode_reward=900.20 +/- 371.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=298000, episode_reward=1190.20 +/- 155.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=299000, episode_reward=924.00 +/- 188.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=300000, episode_reward=1257.60 +/- 383.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=301000, episode_reward=1078.20 +/- 278.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=302000, episode_reward=1116.00 +/- 223.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=303000, episode_reward=996.20 +/- 158.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=304000, episode_reward=1214.40 +/- 275.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=305000, episode_reward=1221.40 +/- 288.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=306000, episode_reward=1442.20 +/- 461.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=307000, episode_reward=1311.60 +/- 454.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=308000, episode_reward=1282.20 +/- 250.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=309000, episode_reward=1205.60 +/- 269.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=310000, episode_reward=1121.00 +/- 285.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=311000, episode_reward=1215.00 +/- 286.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=312000, episode_reward=1160.40 +/- 512.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=313000, episode_reward=890.00 +/- 263.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=314000, episode_reward=1024.20 +/- 321.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=315000, episode_reward=1313.60 +/- 92.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=316000, episode_reward=987.80 +/- 296.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=317000, episode_reward=910.00 +/- 199.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=318000, episode_reward=1153.80 +/- 214.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=319000, episode_reward=1312.20 +/- 312.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=320000, episode_reward=1017.40 +/- 274.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=321000, episode_reward=1184.40 +/- 234.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=322000, episode_reward=1261.20 +/- 289.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=323000, episode_reward=778.20 +/- 292.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=324000, episode_reward=1335.80 +/- 459.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=325000, episode_reward=1285.40 +/- 255.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=326000, episode_reward=1007.60 +/- 250.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=327000, episode_reward=989.20 +/- 421.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=328000, episode_reward=1070.40 +/- 383.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=329000, episode_reward=985.40 +/- 373.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=330000, episode_reward=1307.00 +/- 171.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=331000, episode_reward=1291.40 +/- 166.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=332000, episode_reward=1287.80 +/- 183.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=333000, episode_reward=1183.80 +/- 423.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=334000, episode_reward=1117.40 +/- 322.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=335000, episode_reward=1162.00 +/- 374.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=336000, episode_reward=1112.40 +/- 318.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=337000, episode_reward=1349.40 +/- 72.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=338000, episode_reward=1407.40 +/- 335.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=339000, episode_reward=818.00 +/- 334.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=340000, episode_reward=1167.60 +/- 444.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=341000, episode_reward=1262.80 +/- 130.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=342000, episode_reward=1241.40 +/- 324.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=343000, episode_reward=1423.20 +/- 300.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=344000, episode_reward=1000.00 +/- 83.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=345000, episode_reward=1075.20 +/- 300.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=346000, episode_reward=1115.00 +/- 209.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=347000, episode_reward=1315.00 +/- 223.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=348000, episode_reward=1205.20 +/- 291.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=349000, episode_reward=1368.00 +/- 231.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=350000, episode_reward=1035.00 +/- 128.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=351000, episode_reward=956.00 +/- 192.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=352000, episode_reward=979.00 +/- 170.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=353000, episode_reward=714.40 +/- 308.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=354000, episode_reward=1025.60 +/- 273.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=355000, episode_reward=1027.80 +/- 335.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=356000, episode_reward=1281.20 +/- 155.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=357000, episode_reward=1131.40 +/- 216.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=358000, episode_reward=1173.80 +/- 247.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=359000, episode_reward=1021.60 +/- 151.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=360000, episode_reward=886.00 +/- 360.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=361000, episode_reward=1199.60 +/- 221.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=362000, episode_reward=1248.80 +/- 286.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=363000, episode_reward=1266.60 +/- 473.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=364000, episode_reward=768.40 +/- 99.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=365000, episode_reward=821.40 +/- 199.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=366000, episode_reward=818.60 +/- 483.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=367000, episode_reward=1058.20 +/- 209.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=368000, episode_reward=675.60 +/- 208.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=369000, episode_reward=903.80 +/- 265.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=370000, episode_reward=911.20 +/- 141.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=371000, episode_reward=978.20 +/- 100.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=372000, episode_reward=974.00 +/- 264.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=373000, episode_reward=1345.80 +/- 379.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=374000, episode_reward=1217.60 +/- 137.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=375000, episode_reward=1069.60 +/- 261.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=376000, episode_reward=1405.80 +/- 187.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=377000, episode_reward=1122.80 +/- 159.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=378000, episode_reward=998.60 +/- 223.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=379000, episode_reward=1359.80 +/- 246.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=380000, episode_reward=1248.80 +/- 434.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=381000, episode_reward=998.40 +/- 162.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=382000, episode_reward=1206.00 +/- 100.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=383000, episode_reward=964.80 +/- 367.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=384000, episode_reward=889.40 +/- 270.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=385000, episode_reward=1422.00 +/- 330.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=386000, episode_reward=1204.20 +/- 217.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=387000, episode_reward=1119.00 +/- 205.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=388000, episode_reward=1322.20 +/- 289.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=389000, episode_reward=923.40 +/- 213.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=390000, episode_reward=1166.80 +/- 275.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=391000, episode_reward=1375.60 +/- 257.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=392000, episode_reward=1282.40 +/- 316.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=393000, episode_reward=1200.60 +/- 137.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=394000, episode_reward=1359.60 +/- 264.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=395000, episode_reward=1111.60 +/- 266.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=396000, episode_reward=628.00 +/- 144.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=397000, episode_reward=1007.40 +/- 206.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=398000, episode_reward=832.60 +/- 134.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=399000, episode_reward=990.60 +/- 189.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=400000, episode_reward=857.40 +/- 139.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=401000, episode_reward=1187.40 +/- 254.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=402000, episode_reward=925.80 +/- 247.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=403000, episode_reward=1297.60 +/- 242.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=404000, episode_reward=1303.80 +/- 361.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=405000, episode_reward=1142.60 +/- 204.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=406000, episode_reward=1415.60 +/- 222.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=407000, episode_reward=1312.40 +/- 333.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=408000, episode_reward=1224.60 +/- 330.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=409000, episode_reward=1348.00 +/- 382.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=410000, episode_reward=1444.00 +/- 190.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=411000, episode_reward=905.80 +/- 323.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=412000, episode_reward=1369.20 +/- 381.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=413000, episode_reward=1156.00 +/- 302.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=414000, episode_reward=900.40 +/- 231.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=415000, episode_reward=1416.20 +/- 333.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=416000, episode_reward=1117.80 +/- 247.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=417000, episode_reward=1341.80 +/- 180.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=418000, episode_reward=1226.60 +/- 342.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=419000, episode_reward=920.60 +/- 365.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=420000, episode_reward=1079.60 +/- 292.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=421000, episode_reward=828.20 +/- 175.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=422000, episode_reward=1058.00 +/- 269.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=423000, episode_reward=1133.20 +/- 309.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=424000, episode_reward=1046.40 +/- 271.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=425000, episode_reward=1095.80 +/- 212.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=426000, episode_reward=1374.40 +/- 257.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=427000, episode_reward=924.80 +/- 161.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=428000, episode_reward=1239.80 +/- 236.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=429000, episode_reward=1205.00 +/- 288.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=430000, episode_reward=1156.80 +/- 171.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=431000, episode_reward=1050.00 +/- 113.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=432000, episode_reward=1506.20 +/- 244.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=433000, episode_reward=697.40 +/- 187.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=434000, episode_reward=1047.80 +/- 310.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=435000, episode_reward=990.00 +/- 215.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=436000, episode_reward=1220.20 +/- 214.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=437000, episode_reward=1003.20 +/- 215.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=438000, episode_reward=930.00 +/- 432.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=439000, episode_reward=1109.40 +/- 284.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=440000, episode_reward=931.80 +/- 226.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=441000, episode_reward=1393.40 +/- 158.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=442000, episode_reward=1240.60 +/- 316.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=443000, episode_reward=1294.60 +/- 219.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=444000, episode_reward=1077.80 +/- 378.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=445000, episode_reward=835.60 +/- 236.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=446000, episode_reward=917.00 +/- 406.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=447000, episode_reward=838.00 +/- 274.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=448000, episode_reward=1103.80 +/- 367.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=449000, episode_reward=987.00 +/- 321.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=450000, episode_reward=1077.40 +/- 117.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=451000, episode_reward=943.40 +/- 404.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=452000, episode_reward=1147.40 +/- 187.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=453000, episode_reward=1307.40 +/- 123.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=454000, episode_reward=1158.00 +/- 272.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=455000, episode_reward=922.80 +/- 215.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=456000, episode_reward=1005.60 +/- 464.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=457000, episode_reward=939.80 +/- 305.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=458000, episode_reward=954.60 +/- 309.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=459000, episode_reward=1206.20 +/- 286.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=460000, episode_reward=1239.80 +/- 172.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=461000, episode_reward=1166.40 +/- 271.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=462000, episode_reward=1066.40 +/- 270.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=463000, episode_reward=887.40 +/- 288.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=464000, episode_reward=1182.20 +/- 217.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=465000, episode_reward=1121.20 +/- 212.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=466000, episode_reward=1121.40 +/- 188.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=467000, episode_reward=1510.40 +/- 182.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=468000, episode_reward=1319.20 +/- 282.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=469000, episode_reward=1101.60 +/- 142.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=470000, episode_reward=1149.00 +/- 247.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=471000, episode_reward=992.60 +/- 605.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=472000, episode_reward=1070.20 +/- 249.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=473000, episode_reward=801.40 +/- 410.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=474000, episode_reward=996.80 +/- 250.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=475000, episode_reward=766.40 +/- 464.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=476000, episode_reward=714.80 +/- 233.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=477000, episode_reward=966.80 +/- 283.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=478000, episode_reward=836.40 +/- 251.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=479000, episode_reward=1165.00 +/- 452.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=480000, episode_reward=1093.80 +/- 180.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=481000, episode_reward=993.60 +/- 279.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=482000, episode_reward=1154.00 +/- 242.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=483000, episode_reward=1192.60 +/- 449.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=484000, episode_reward=1129.40 +/- 293.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=485000, episode_reward=942.00 +/- 469.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=486000, episode_reward=991.60 +/- 329.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=487000, episode_reward=1099.40 +/- 241.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=488000, episode_reward=1113.60 +/- 232.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=489000, episode_reward=893.00 +/- 89.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=490000, episode_reward=1027.60 +/- 91.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=491000, episode_reward=1180.00 +/- 187.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=492000, episode_reward=1393.80 +/- 188.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=493000, episode_reward=1066.40 +/- 283.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=494000, episode_reward=1135.00 +/- 516.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=495000, episode_reward=932.00 +/- 320.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=496000, episode_reward=717.20 +/- 459.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=497000, episode_reward=804.80 +/- 261.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=498000, episode_reward=1387.40 +/- 360.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=499000, episode_reward=1177.00 +/- 300.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=500000, episode_reward=962.80 +/- 175.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Gamma = 0.9, Learning Rate = 0.003, Activation Function = ReLU, Neurons = 8, Process noise = 0.1, Measurement noise = 0.0\n",
      "Eval num_timesteps=1000, episode_reward=-411.20 +/- 142.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=-465.40 +/- 273.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=3000, episode_reward=-517.80 +/- 249.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=4000, episode_reward=-414.60 +/- 324.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=5000, episode_reward=-402.20 +/- 106.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=6000, episode_reward=-441.20 +/- 140.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=7000, episode_reward=-321.60 +/- 184.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=8000, episode_reward=-580.80 +/- 179.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=9000, episode_reward=-528.80 +/- 161.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=10000, episode_reward=-628.40 +/- 224.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=11000, episode_reward=-388.80 +/- 185.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=12000, episode_reward=-497.40 +/- 147.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=13000, episode_reward=-568.40 +/- 159.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=14000, episode_reward=-510.40 +/- 141.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=15000, episode_reward=-500.00 +/- 162.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=16000, episode_reward=-335.60 +/- 159.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=17000, episode_reward=-449.60 +/- 227.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=18000, episode_reward=-573.60 +/- 205.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=19000, episode_reward=-470.80 +/- 237.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=20000, episode_reward=-324.60 +/- 125.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=21000, episode_reward=-422.00 +/- 276.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=22000, episode_reward=-346.00 +/- 233.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=23000, episode_reward=-391.00 +/- 129.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=24000, episode_reward=-525.60 +/- 86.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=25000, episode_reward=-408.20 +/- 140.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=26000, episode_reward=-400.40 +/- 139.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=27000, episode_reward=-393.60 +/- 122.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=28000, episode_reward=-622.00 +/- 160.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=29000, episode_reward=-345.80 +/- 189.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=30000, episode_reward=-477.80 +/- 141.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=31000, episode_reward=-513.40 +/- 101.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=32000, episode_reward=-438.60 +/- 165.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=33000, episode_reward=-470.20 +/- 154.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=34000, episode_reward=-502.40 +/- 104.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=35000, episode_reward=-342.40 +/- 284.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=36000, episode_reward=-483.40 +/- 98.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=37000, episode_reward=-532.00 +/- 111.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=38000, episode_reward=-371.60 +/- 197.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=39000, episode_reward=-499.80 +/- 179.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=40000, episode_reward=-361.20 +/- 171.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=41000, episode_reward=-455.60 +/- 222.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=42000, episode_reward=-447.20 +/- 155.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=43000, episode_reward=-396.80 +/- 111.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=44000, episode_reward=-540.40 +/- 235.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=45000, episode_reward=-421.20 +/- 148.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=46000, episode_reward=-466.20 +/- 194.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=47000, episode_reward=-441.40 +/- 132.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=48000, episode_reward=-457.60 +/- 196.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=49000, episode_reward=-347.60 +/- 122.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=50000, episode_reward=-475.80 +/- 145.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=51000, episode_reward=-6428.80 +/- 2343.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=52000, episode_reward=518.00 +/- 178.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=53000, episode_reward=435.80 +/- 248.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=54000, episode_reward=453.80 +/- 57.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=55000, episode_reward=230.60 +/- 173.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=56000, episode_reward=607.80 +/- 78.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=57000, episode_reward=443.40 +/- 165.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=58000, episode_reward=442.40 +/- 135.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=59000, episode_reward=294.80 +/- 105.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=60000, episode_reward=115.40 +/- 496.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=61000, episode_reward=493.00 +/- 151.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=62000, episode_reward=446.60 +/- 176.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=63000, episode_reward=154.00 +/- 319.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=64000, episode_reward=312.00 +/- 152.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=65000, episode_reward=499.00 +/- 159.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=66000, episode_reward=430.00 +/- 210.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=67000, episode_reward=308.60 +/- 167.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=68000, episode_reward=252.20 +/- 107.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=69000, episode_reward=-333.20 +/- 324.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=70000, episode_reward=82.60 +/- 108.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=71000, episode_reward=504.80 +/- 110.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=72000, episode_reward=280.20 +/- 334.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=73000, episode_reward=367.60 +/- 248.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=74000, episode_reward=413.80 +/- 191.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=75000, episode_reward=502.20 +/- 136.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=76000, episode_reward=349.20 +/- 215.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=77000, episode_reward=498.40 +/- 224.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=78000, episode_reward=363.60 +/- 77.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=79000, episode_reward=287.80 +/- 224.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=80000, episode_reward=435.40 +/- 222.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=81000, episode_reward=443.80 +/- 299.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=82000, episode_reward=292.60 +/- 114.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=83000, episode_reward=524.20 +/- 98.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=84000, episode_reward=310.60 +/- 119.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=85000, episode_reward=425.60 +/- 126.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=86000, episode_reward=382.20 +/- 123.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=87000, episode_reward=297.40 +/- 85.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=88000, episode_reward=154.00 +/- 264.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=89000, episode_reward=317.00 +/- 276.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=90000, episode_reward=511.60 +/- 60.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=91000, episode_reward=413.80 +/- 36.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=92000, episode_reward=290.60 +/- 85.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=93000, episode_reward=503.60 +/- 171.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=94000, episode_reward=492.20 +/- 71.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=95000, episode_reward=357.20 +/- 177.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=96000, episode_reward=424.40 +/- 168.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=97000, episode_reward=363.00 +/- 153.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=98000, episode_reward=159.60 +/- 103.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=99000, episode_reward=324.40 +/- 141.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=100000, episode_reward=497.60 +/- 153.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=101000, episode_reward=324.00 +/- 140.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=102000, episode_reward=404.40 +/- 135.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=103000, episode_reward=408.60 +/- 157.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=104000, episode_reward=340.60 +/- 117.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=105000, episode_reward=367.20 +/- 160.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=106000, episode_reward=310.20 +/- 203.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=107000, episode_reward=263.60 +/- 139.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=108000, episode_reward=367.20 +/- 205.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=109000, episode_reward=558.40 +/- 57.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=110000, episode_reward=297.60 +/- 173.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=111000, episode_reward=324.20 +/- 270.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=112000, episode_reward=401.20 +/- 140.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=113000, episode_reward=431.00 +/- 212.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=114000, episode_reward=246.40 +/- 200.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=115000, episode_reward=280.80 +/- 71.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=116000, episode_reward=282.20 +/- 114.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=117000, episode_reward=146.00 +/- 282.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=118000, episode_reward=138.00 +/- 321.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=119000, episode_reward=339.40 +/- 36.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=120000, episode_reward=458.80 +/- 254.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=121000, episode_reward=529.00 +/- 136.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=122000, episode_reward=331.20 +/- 146.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=123000, episode_reward=287.80 +/- 226.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=124000, episode_reward=377.40 +/- 105.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=125000, episode_reward=416.20 +/- 293.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=126000, episode_reward=511.80 +/- 167.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=127000, episode_reward=322.20 +/- 187.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=128000, episode_reward=352.60 +/- 104.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=129000, episode_reward=437.60 +/- 121.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=130000, episode_reward=395.40 +/- 91.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=131000, episode_reward=343.80 +/- 325.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=132000, episode_reward=342.40 +/- 156.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=133000, episode_reward=420.00 +/- 134.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=134000, episode_reward=218.40 +/- 254.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=135000, episode_reward=356.20 +/- 120.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=136000, episode_reward=187.80 +/- 118.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=137000, episode_reward=227.80 +/- 171.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=138000, episode_reward=185.80 +/- 235.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=139000, episode_reward=204.40 +/- 182.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=140000, episode_reward=411.00 +/- 173.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=141000, episode_reward=235.80 +/- 169.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=142000, episode_reward=285.60 +/- 244.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=143000, episode_reward=511.40 +/- 150.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=144000, episode_reward=447.00 +/- 68.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=145000, episode_reward=337.20 +/- 97.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=146000, episode_reward=347.40 +/- 148.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=147000, episode_reward=439.60 +/- 141.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=148000, episode_reward=238.20 +/- 231.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=149000, episode_reward=530.00 +/- 143.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=150000, episode_reward=379.40 +/- 160.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=151000, episode_reward=405.60 +/- 123.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=152000, episode_reward=348.80 +/- 156.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=153000, episode_reward=353.60 +/- 83.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=154000, episode_reward=361.00 +/- 178.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=155000, episode_reward=426.00 +/- 173.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=156000, episode_reward=438.00 +/- 163.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=157000, episode_reward=188.60 +/- 123.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=158000, episode_reward=419.00 +/- 140.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=159000, episode_reward=408.80 +/- 288.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=160000, episode_reward=317.80 +/- 141.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=161000, episode_reward=549.80 +/- 95.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=162000, episode_reward=350.40 +/- 110.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=163000, episode_reward=274.20 +/- 110.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=164000, episode_reward=371.00 +/- 206.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=165000, episode_reward=336.00 +/- 287.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=166000, episode_reward=412.00 +/- 288.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=167000, episode_reward=475.40 +/- 182.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=168000, episode_reward=611.40 +/- 192.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=169000, episode_reward=298.40 +/- 164.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=170000, episode_reward=420.40 +/- 195.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=171000, episode_reward=397.60 +/- 210.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=172000, episode_reward=375.60 +/- 149.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=173000, episode_reward=285.00 +/- 239.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=174000, episode_reward=373.40 +/- 372.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=175000, episode_reward=419.60 +/- 111.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=176000, episode_reward=336.80 +/- 246.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=177000, episode_reward=377.00 +/- 171.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=178000, episode_reward=435.60 +/- 117.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=179000, episode_reward=277.00 +/- 158.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=180000, episode_reward=462.20 +/- 278.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=181000, episode_reward=276.60 +/- 240.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=182000, episode_reward=369.60 +/- 98.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=183000, episode_reward=164.80 +/- 162.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=184000, episode_reward=291.80 +/- 168.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=185000, episode_reward=225.80 +/- 253.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=186000, episode_reward=366.60 +/- 109.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=187000, episode_reward=396.20 +/- 232.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=188000, episode_reward=432.60 +/- 150.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=189000, episode_reward=479.40 +/- 147.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=190000, episode_reward=145.00 +/- 226.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=191000, episode_reward=288.80 +/- 154.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=192000, episode_reward=420.80 +/- 193.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=193000, episode_reward=352.60 +/- 99.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=194000, episode_reward=429.20 +/- 99.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=195000, episode_reward=384.60 +/- 282.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=196000, episode_reward=465.20 +/- 289.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=197000, episode_reward=181.00 +/- 180.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=198000, episode_reward=447.00 +/- 62.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=199000, episode_reward=399.60 +/- 241.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=200000, episode_reward=529.60 +/- 184.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=201000, episode_reward=417.80 +/- 226.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=202000, episode_reward=250.00 +/- 129.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=203000, episode_reward=449.00 +/- 102.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=204000, episode_reward=293.20 +/- 94.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=205000, episode_reward=340.80 +/- 146.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=206000, episode_reward=264.20 +/- 64.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=207000, episode_reward=329.60 +/- 168.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=208000, episode_reward=274.60 +/- 136.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=209000, episode_reward=327.20 +/- 202.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=210000, episode_reward=438.80 +/- 145.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=211000, episode_reward=342.00 +/- 76.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=212000, episode_reward=463.20 +/- 91.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=213000, episode_reward=245.40 +/- 241.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=214000, episode_reward=407.40 +/- 209.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=215000, episode_reward=253.40 +/- 218.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=216000, episode_reward=441.00 +/- 127.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=217000, episode_reward=364.20 +/- 75.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=218000, episode_reward=570.80 +/- 241.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=219000, episode_reward=313.80 +/- 120.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=220000, episode_reward=403.60 +/- 66.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=221000, episode_reward=142.00 +/- 178.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=222000, episode_reward=320.60 +/- 249.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=223000, episode_reward=512.40 +/- 310.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=224000, episode_reward=375.40 +/- 157.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=225000, episode_reward=433.80 +/- 269.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=226000, episode_reward=501.00 +/- 194.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=227000, episode_reward=408.60 +/- 215.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=228000, episode_reward=320.60 +/- 66.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=229000, episode_reward=433.40 +/- 247.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=230000, episode_reward=286.00 +/- 203.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=231000, episode_reward=190.60 +/- 240.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=232000, episode_reward=326.00 +/- 242.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=233000, episode_reward=571.00 +/- 50.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=234000, episode_reward=445.80 +/- 254.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=235000, episode_reward=249.20 +/- 154.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=236000, episode_reward=441.00 +/- 241.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=237000, episode_reward=498.00 +/- 208.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=238000, episode_reward=429.00 +/- 212.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=239000, episode_reward=301.60 +/- 190.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=240000, episode_reward=485.40 +/- 175.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=241000, episode_reward=405.60 +/- 169.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=242000, episode_reward=488.80 +/- 155.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=243000, episode_reward=267.60 +/- 111.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=244000, episode_reward=359.60 +/- 78.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=245000, episode_reward=319.60 +/- 105.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=246000, episode_reward=301.80 +/- 209.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=247000, episode_reward=464.80 +/- 255.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=248000, episode_reward=502.20 +/- 117.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=249000, episode_reward=480.00 +/- 215.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=250000, episode_reward=386.80 +/- 234.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=251000, episode_reward=439.20 +/- 113.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=252000, episode_reward=199.80 +/- 175.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=253000, episode_reward=365.40 +/- 198.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=254000, episode_reward=452.80 +/- 296.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=255000, episode_reward=265.20 +/- 175.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=256000, episode_reward=390.20 +/- 181.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=257000, episode_reward=334.20 +/- 174.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=258000, episode_reward=269.00 +/- 166.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=259000, episode_reward=144.60 +/- 209.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=260000, episode_reward=273.20 +/- 219.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=261000, episode_reward=426.20 +/- 136.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=262000, episode_reward=391.60 +/- 193.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=263000, episode_reward=238.40 +/- 203.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=264000, episode_reward=540.00 +/- 130.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=265000, episode_reward=335.60 +/- 148.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=266000, episode_reward=464.80 +/- 196.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=267000, episode_reward=205.20 +/- 112.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=268000, episode_reward=396.00 +/- 164.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=269000, episode_reward=281.40 +/- 252.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=270000, episode_reward=542.20 +/- 214.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=271000, episode_reward=261.80 +/- 60.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=272000, episode_reward=265.60 +/- 220.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=273000, episode_reward=359.40 +/- 136.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=274000, episode_reward=410.20 +/- 175.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=275000, episode_reward=342.40 +/- 148.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=276000, episode_reward=511.60 +/- 153.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=277000, episode_reward=354.40 +/- 222.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=278000, episode_reward=430.60 +/- 38.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=279000, episode_reward=323.20 +/- 102.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=280000, episode_reward=437.00 +/- 212.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=281000, episode_reward=499.40 +/- 218.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=282000, episode_reward=514.40 +/- 223.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=283000, episode_reward=498.20 +/- 201.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=284000, episode_reward=436.40 +/- 153.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=285000, episode_reward=414.00 +/- 232.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=286000, episode_reward=353.20 +/- 143.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=287000, episode_reward=211.00 +/- 296.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=288000, episode_reward=561.00 +/- 117.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=289000, episode_reward=274.40 +/- 303.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=290000, episode_reward=431.80 +/- 86.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=291000, episode_reward=462.40 +/- 122.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=292000, episode_reward=328.00 +/- 159.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=293000, episode_reward=284.20 +/- 200.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=294000, episode_reward=290.00 +/- 112.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=295000, episode_reward=435.80 +/- 195.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=296000, episode_reward=264.20 +/- 128.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=297000, episode_reward=296.80 +/- 217.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=298000, episode_reward=198.60 +/- 91.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=299000, episode_reward=268.20 +/- 356.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=300000, episode_reward=357.60 +/- 96.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=301000, episode_reward=388.20 +/- 220.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=302000, episode_reward=161.20 +/- 297.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=303000, episode_reward=330.20 +/- 218.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=304000, episode_reward=401.20 +/- 173.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=305000, episode_reward=197.60 +/- 221.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=306000, episode_reward=368.60 +/- 117.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=307000, episode_reward=447.40 +/- 178.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=308000, episode_reward=419.80 +/- 150.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=309000, episode_reward=413.20 +/- 269.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=310000, episode_reward=281.60 +/- 183.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=311000, episode_reward=398.40 +/- 158.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=312000, episode_reward=322.60 +/- 75.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=313000, episode_reward=173.60 +/- 279.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=314000, episode_reward=542.60 +/- 118.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=315000, episode_reward=285.40 +/- 175.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=316000, episode_reward=285.20 +/- 256.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=317000, episode_reward=464.20 +/- 147.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=318000, episode_reward=493.20 +/- 124.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=319000, episode_reward=409.60 +/- 145.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=320000, episode_reward=344.00 +/- 154.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=321000, episode_reward=338.00 +/- 281.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=322000, episode_reward=210.00 +/- 274.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=323000, episode_reward=391.60 +/- 112.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=324000, episode_reward=364.40 +/- 174.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=325000, episode_reward=288.40 +/- 164.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=326000, episode_reward=293.00 +/- 192.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=327000, episode_reward=300.60 +/- 193.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=328000, episode_reward=381.80 +/- 117.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=329000, episode_reward=272.80 +/- 127.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=330000, episode_reward=281.00 +/- 66.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=331000, episode_reward=332.40 +/- 89.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=332000, episode_reward=303.20 +/- 157.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=333000, episode_reward=254.60 +/- 195.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=334000, episode_reward=286.20 +/- 178.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=335000, episode_reward=411.80 +/- 259.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=336000, episode_reward=289.20 +/- 165.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=337000, episode_reward=301.80 +/- 257.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=338000, episode_reward=377.60 +/- 133.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=339000, episode_reward=187.00 +/- 238.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=340000, episode_reward=347.40 +/- 149.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=341000, episode_reward=266.40 +/- 153.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=342000, episode_reward=457.60 +/- 81.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=343000, episode_reward=461.20 +/- 233.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=344000, episode_reward=417.00 +/- 297.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=345000, episode_reward=440.40 +/- 206.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=346000, episode_reward=352.60 +/- 200.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=347000, episode_reward=305.60 +/- 175.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=348000, episode_reward=395.80 +/- 101.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=349000, episode_reward=465.00 +/- 103.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=350000, episode_reward=407.40 +/- 165.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=351000, episode_reward=294.00 +/- 269.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=352000, episode_reward=380.20 +/- 219.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=353000, episode_reward=350.60 +/- 247.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=354000, episode_reward=278.60 +/- 190.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=355000, episode_reward=312.40 +/- 93.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=356000, episode_reward=370.00 +/- 313.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=357000, episode_reward=353.80 +/- 138.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=358000, episode_reward=359.00 +/- 182.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=359000, episode_reward=298.00 +/- 223.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=360000, episode_reward=340.20 +/- 180.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=361000, episode_reward=321.40 +/- 227.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=362000, episode_reward=321.20 +/- 177.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=363000, episode_reward=238.40 +/- 218.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=364000, episode_reward=242.40 +/- 135.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=365000, episode_reward=223.40 +/- 240.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=366000, episode_reward=298.80 +/- 263.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=367000, episode_reward=246.40 +/- 158.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=368000, episode_reward=378.20 +/- 188.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=369000, episode_reward=456.60 +/- 180.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=370000, episode_reward=454.20 +/- 169.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=371000, episode_reward=297.40 +/- 107.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=372000, episode_reward=362.60 +/- 153.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=373000, episode_reward=237.00 +/- 169.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=374000, episode_reward=330.60 +/- 193.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=375000, episode_reward=487.60 +/- 187.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=376000, episode_reward=387.00 +/- 207.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=377000, episode_reward=354.80 +/- 109.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=378000, episode_reward=308.20 +/- 197.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=379000, episode_reward=614.40 +/- 113.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=380000, episode_reward=292.80 +/- 220.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=381000, episode_reward=148.60 +/- 326.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=382000, episode_reward=215.40 +/- 279.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=383000, episode_reward=146.00 +/- 187.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=384000, episode_reward=365.00 +/- 235.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=385000, episode_reward=339.20 +/- 129.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=386000, episode_reward=536.40 +/- 100.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=387000, episode_reward=215.40 +/- 128.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=388000, episode_reward=300.40 +/- 202.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=389000, episode_reward=297.20 +/- 194.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=390000, episode_reward=381.20 +/- 160.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=391000, episode_reward=496.60 +/- 145.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=392000, episode_reward=280.40 +/- 230.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=393000, episode_reward=131.20 +/- 147.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=394000, episode_reward=325.80 +/- 116.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=395000, episode_reward=311.60 +/- 263.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=396000, episode_reward=372.60 +/- 166.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=397000, episode_reward=508.00 +/- 186.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=398000, episode_reward=343.00 +/- 85.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=399000, episode_reward=469.60 +/- 173.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=400000, episode_reward=330.20 +/- 136.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=401000, episode_reward=393.20 +/- 208.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=402000, episode_reward=215.20 +/- 213.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=403000, episode_reward=443.20 +/- 284.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=404000, episode_reward=402.80 +/- 119.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=405000, episode_reward=327.20 +/- 128.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=406000, episode_reward=280.80 +/- 198.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=407000, episode_reward=225.20 +/- 158.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=408000, episode_reward=307.00 +/- 230.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=409000, episode_reward=274.80 +/- 176.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=410000, episode_reward=342.00 +/- 213.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=411000, episode_reward=233.80 +/- 80.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=412000, episode_reward=278.00 +/- 122.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=413000, episode_reward=445.40 +/- 132.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=414000, episode_reward=499.80 +/- 239.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=415000, episode_reward=425.60 +/- 144.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=416000, episode_reward=331.40 +/- 202.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=417000, episode_reward=467.80 +/- 85.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=418000, episode_reward=235.40 +/- 311.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=419000, episode_reward=313.60 +/- 101.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=420000, episode_reward=429.80 +/- 225.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=421000, episode_reward=378.20 +/- 155.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=422000, episode_reward=511.00 +/- 100.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=423000, episode_reward=399.20 +/- 140.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=424000, episode_reward=362.00 +/- 275.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=425000, episode_reward=414.60 +/- 263.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=426000, episode_reward=272.80 +/- 281.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=427000, episode_reward=433.80 +/- 124.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=428000, episode_reward=411.60 +/- 135.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=429000, episode_reward=371.00 +/- 149.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=430000, episode_reward=412.60 +/- 137.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=431000, episode_reward=146.20 +/- 253.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=432000, episode_reward=346.60 +/- 288.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=433000, episode_reward=119.40 +/- 33.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=434000, episode_reward=297.60 +/- 212.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=435000, episode_reward=327.60 +/- 135.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=436000, episode_reward=394.00 +/- 77.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=437000, episode_reward=501.60 +/- 175.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=438000, episode_reward=122.80 +/- 148.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=439000, episode_reward=398.60 +/- 50.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=440000, episode_reward=223.80 +/- 266.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=441000, episode_reward=189.80 +/- 234.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=442000, episode_reward=370.40 +/- 187.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=443000, episode_reward=366.60 +/- 190.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=444000, episode_reward=388.00 +/- 222.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=445000, episode_reward=423.60 +/- 112.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=446000, episode_reward=293.80 +/- 116.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=447000, episode_reward=338.00 +/- 256.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=448000, episode_reward=504.40 +/- 72.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=449000, episode_reward=230.60 +/- 158.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=450000, episode_reward=408.20 +/- 113.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=451000, episode_reward=317.20 +/- 104.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=452000, episode_reward=239.40 +/- 83.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=453000, episode_reward=251.20 +/- 307.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=454000, episode_reward=342.00 +/- 290.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=455000, episode_reward=182.60 +/- 98.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=456000, episode_reward=308.20 +/- 73.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=457000, episode_reward=347.60 +/- 219.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=458000, episode_reward=352.20 +/- 192.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=459000, episode_reward=348.20 +/- 259.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=460000, episode_reward=462.60 +/- 217.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=461000, episode_reward=298.20 +/- 147.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=462000, episode_reward=410.80 +/- 92.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=463000, episode_reward=366.60 +/- 210.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=464000, episode_reward=333.00 +/- 181.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=465000, episode_reward=322.80 +/- 121.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=466000, episode_reward=353.60 +/- 159.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=467000, episode_reward=395.60 +/- 123.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=468000, episode_reward=255.40 +/- 232.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=469000, episode_reward=378.40 +/- 92.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=470000, episode_reward=448.40 +/- 216.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=471000, episode_reward=369.20 +/- 191.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=472000, episode_reward=412.40 +/- 218.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=473000, episode_reward=454.20 +/- 104.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=474000, episode_reward=424.40 +/- 210.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=475000, episode_reward=343.00 +/- 230.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=476000, episode_reward=401.40 +/- 112.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=477000, episode_reward=453.20 +/- 182.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=478000, episode_reward=326.60 +/- 321.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=479000, episode_reward=444.40 +/- 161.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=480000, episode_reward=238.40 +/- 147.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=481000, episode_reward=369.80 +/- 95.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=482000, episode_reward=382.20 +/- 142.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=483000, episode_reward=194.00 +/- 179.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=484000, episode_reward=432.00 +/- 234.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=485000, episode_reward=401.20 +/- 231.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=486000, episode_reward=442.60 +/- 255.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=487000, episode_reward=410.20 +/- 144.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=488000, episode_reward=443.80 +/- 102.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=489000, episode_reward=351.40 +/- 91.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=490000, episode_reward=249.00 +/- 79.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=491000, episode_reward=446.00 +/- 166.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=492000, episode_reward=388.20 +/- 170.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=493000, episode_reward=185.40 +/- 328.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=494000, episode_reward=275.60 +/- 102.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=495000, episode_reward=424.80 +/- 232.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=496000, episode_reward=334.40 +/- 273.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=497000, episode_reward=348.40 +/- 165.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=498000, episode_reward=351.60 +/- 206.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=499000, episode_reward=485.80 +/- 145.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=500000, episode_reward=435.80 +/- 151.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Gamma = 0.9, Learning Rate = 0.003, Activation Function = ReLU, Neurons = 32, Process noise = 0.1, Measurement noise = 0.0\n",
      "Eval num_timesteps=1000, episode_reward=-157.80 +/- 89.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=-46.20 +/- 91.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3000, episode_reward=-150.00 +/- 157.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=4000, episode_reward=84.80 +/- 175.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=5000, episode_reward=4.60 +/- 205.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=6000, episode_reward=5.00 +/- 102.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=7000, episode_reward=170.00 +/- 68.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=8000, episode_reward=27.20 +/- 204.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=9000, episode_reward=152.40 +/- 110.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=10000, episode_reward=34.00 +/- 99.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=11000, episode_reward=184.20 +/- 150.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=12000, episode_reward=33.40 +/- 141.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=13000, episode_reward=49.60 +/- 267.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=14000, episode_reward=35.00 +/- 201.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=15000, episode_reward=-138.60 +/- 178.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=16000, episode_reward=-38.40 +/- 159.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=17000, episode_reward=119.40 +/- 92.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=18000, episode_reward=117.60 +/- 150.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=19000, episode_reward=77.80 +/- 179.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=20000, episode_reward=-55.20 +/- 92.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=21000, episode_reward=83.60 +/- 84.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=22000, episode_reward=89.60 +/- 166.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=23000, episode_reward=54.00 +/- 196.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=24000, episode_reward=117.40 +/- 108.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=25000, episode_reward=19.00 +/- 142.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=26000, episode_reward=-102.20 +/- 232.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=27000, episode_reward=91.60 +/- 172.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=28000, episode_reward=-42.20 +/- 130.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=29000, episode_reward=-20.80 +/- 186.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=30000, episode_reward=-107.80 +/- 117.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=31000, episode_reward=15.20 +/- 245.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=32000, episode_reward=-27.80 +/- 141.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=33000, episode_reward=87.20 +/- 156.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=34000, episode_reward=-108.20 +/- 199.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=35000, episode_reward=-77.00 +/- 168.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=36000, episode_reward=71.80 +/- 183.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=37000, episode_reward=148.20 +/- 126.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=38000, episode_reward=63.80 +/- 197.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=39000, episode_reward=73.40 +/- 138.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=40000, episode_reward=94.20 +/- 325.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=41000, episode_reward=69.60 +/- 184.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=42000, episode_reward=0.80 +/- 156.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=43000, episode_reward=-8.60 +/- 158.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=44000, episode_reward=102.80 +/- 215.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=45000, episode_reward=39.80 +/- 223.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=46000, episode_reward=-81.40 +/- 249.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=47000, episode_reward=-64.00 +/- 208.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=48000, episode_reward=122.20 +/- 86.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=49000, episode_reward=47.20 +/- 213.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=50000, episode_reward=121.00 +/- 74.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=51000, episode_reward=390.20 +/- 279.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=52000, episode_reward=470.80 +/- 193.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=53000, episode_reward=411.60 +/- 100.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=54000, episode_reward=428.80 +/- 91.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=55000, episode_reward=269.60 +/- 181.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=56000, episode_reward=360.20 +/- 146.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=57000, episode_reward=177.00 +/- 269.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=58000, episode_reward=183.00 +/- 197.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=59000, episode_reward=236.60 +/- 233.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=60000, episode_reward=316.40 +/- 319.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=61000, episode_reward=156.40 +/- 138.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=62000, episode_reward=283.60 +/- 279.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=63000, episode_reward=437.20 +/- 214.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=64000, episode_reward=243.60 +/- 266.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=65000, episode_reward=491.40 +/- 127.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=66000, episode_reward=292.60 +/- 268.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=67000, episode_reward=227.80 +/- 232.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=68000, episode_reward=113.60 +/- 144.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=69000, episode_reward=4.40 +/- 251.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=70000, episode_reward=436.20 +/- 198.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=71000, episode_reward=71.80 +/- 244.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=72000, episode_reward=143.00 +/- 236.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=73000, episode_reward=421.00 +/- 181.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=74000, episode_reward=235.60 +/- 224.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=75000, episode_reward=377.60 +/- 53.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=76000, episode_reward=400.40 +/- 187.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=77000, episode_reward=447.00 +/- 222.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=78000, episode_reward=408.00 +/- 258.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=79000, episode_reward=251.80 +/- 165.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=80000, episode_reward=459.00 +/- 120.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=81000, episode_reward=394.20 +/- 297.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=82000, episode_reward=377.00 +/- 151.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=83000, episode_reward=267.00 +/- 152.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=84000, episode_reward=485.20 +/- 174.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=85000, episode_reward=267.00 +/- 145.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=86000, episode_reward=358.00 +/- 214.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=87000, episode_reward=412.40 +/- 194.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=88000, episode_reward=312.20 +/- 139.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=89000, episode_reward=389.00 +/- 152.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=90000, episode_reward=365.40 +/- 241.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=91000, episode_reward=373.40 +/- 241.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=92000, episode_reward=379.20 +/- 276.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=93000, episode_reward=364.80 +/- 160.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=94000, episode_reward=271.20 +/- 66.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=95000, episode_reward=250.80 +/- 233.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=96000, episode_reward=513.60 +/- 182.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=97000, episode_reward=320.00 +/- 174.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=98000, episode_reward=259.00 +/- 59.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=99000, episode_reward=259.00 +/- 190.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=100000, episode_reward=355.00 +/- 145.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=101000, episode_reward=231.40 +/- 259.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=102000, episode_reward=267.00 +/- 133.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=103000, episode_reward=449.80 +/- 269.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=104000, episode_reward=422.20 +/- 194.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=105000, episode_reward=194.40 +/- 82.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=106000, episode_reward=381.40 +/- 249.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=107000, episode_reward=562.00 +/- 137.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=108000, episode_reward=482.40 +/- 210.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=109000, episode_reward=336.20 +/- 210.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=110000, episode_reward=296.20 +/- 241.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=111000, episode_reward=415.80 +/- 210.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=112000, episode_reward=321.80 +/- 243.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=113000, episode_reward=366.20 +/- 84.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=114000, episode_reward=520.00 +/- 79.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=115000, episode_reward=237.40 +/- 139.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=116000, episode_reward=325.80 +/- 103.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=117000, episode_reward=376.20 +/- 84.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=118000, episode_reward=334.60 +/- 160.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=119000, episode_reward=428.40 +/- 260.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=120000, episode_reward=231.60 +/- 98.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=121000, episode_reward=382.80 +/- 238.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=122000, episode_reward=299.00 +/- 340.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=123000, episode_reward=485.80 +/- 99.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=124000, episode_reward=386.60 +/- 221.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=125000, episode_reward=245.20 +/- 199.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=126000, episode_reward=403.60 +/- 76.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=127000, episode_reward=444.20 +/- 153.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=128000, episode_reward=286.20 +/- 104.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=129000, episode_reward=369.40 +/- 87.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=130000, episode_reward=374.00 +/- 203.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=131000, episode_reward=255.20 +/- 274.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=132000, episode_reward=242.00 +/- 247.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=133000, episode_reward=349.00 +/- 144.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=134000, episode_reward=349.00 +/- 215.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=135000, episode_reward=374.80 +/- 121.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=136000, episode_reward=302.60 +/- 173.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=137000, episode_reward=387.40 +/- 310.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=138000, episode_reward=343.80 +/- 82.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=139000, episode_reward=375.00 +/- 141.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=140000, episode_reward=568.20 +/- 155.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=141000, episode_reward=307.80 +/- 131.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=142000, episode_reward=427.80 +/- 105.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=143000, episode_reward=337.60 +/- 257.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=144000, episode_reward=557.40 +/- 185.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=145000, episode_reward=363.40 +/- 181.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=146000, episode_reward=355.00 +/- 269.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=147000, episode_reward=243.40 +/- 297.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=148000, episode_reward=318.00 +/- 245.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=149000, episode_reward=276.60 +/- 95.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=150000, episode_reward=390.80 +/- 246.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=151000, episode_reward=393.00 +/- 105.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=152000, episode_reward=465.80 +/- 145.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=153000, episode_reward=291.20 +/- 256.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=154000, episode_reward=256.80 +/- 126.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=155000, episode_reward=342.20 +/- 135.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=156000, episode_reward=198.80 +/- 281.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=157000, episode_reward=311.00 +/- 104.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=158000, episode_reward=231.20 +/- 131.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=159000, episode_reward=367.20 +/- 135.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=160000, episode_reward=486.60 +/- 302.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=161000, episode_reward=534.20 +/- 261.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=162000, episode_reward=249.60 +/- 111.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=163000, episode_reward=246.80 +/- 117.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=164000, episode_reward=393.80 +/- 357.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=165000, episode_reward=356.60 +/- 257.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=166000, episode_reward=245.40 +/- 237.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=167000, episode_reward=401.40 +/- 199.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=168000, episode_reward=395.40 +/- 162.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=169000, episode_reward=237.40 +/- 144.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=170000, episode_reward=364.20 +/- 184.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=171000, episode_reward=173.00 +/- 184.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=172000, episode_reward=350.00 +/- 102.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=173000, episode_reward=230.40 +/- 219.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=174000, episode_reward=439.00 +/- 263.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=175000, episode_reward=413.00 +/- 137.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=176000, episode_reward=233.00 +/- 160.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=177000, episode_reward=535.00 +/- 228.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=178000, episode_reward=515.00 +/- 152.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=179000, episode_reward=407.20 +/- 197.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=180000, episode_reward=535.00 +/- 88.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=181000, episode_reward=103.40 +/- 109.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=182000, episode_reward=410.60 +/- 152.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=183000, episode_reward=232.00 +/- 272.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=184000, episode_reward=525.80 +/- 213.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=185000, episode_reward=451.60 +/- 156.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=186000, episode_reward=199.60 +/- 98.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=187000, episode_reward=247.40 +/- 235.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=188000, episode_reward=331.80 +/- 214.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=189000, episode_reward=362.40 +/- 92.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=190000, episode_reward=568.20 +/- 148.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=191000, episode_reward=558.40 +/- 241.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=192000, episode_reward=446.00 +/- 167.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=193000, episode_reward=376.40 +/- 104.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=194000, episode_reward=482.40 +/- 206.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=195000, episode_reward=455.60 +/- 239.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=196000, episode_reward=368.20 +/- 228.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=197000, episode_reward=675.80 +/- 119.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=198000, episode_reward=376.20 +/- 360.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=199000, episode_reward=908.00 +/- 320.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=200000, episode_reward=749.60 +/- 263.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=201000, episode_reward=557.40 +/- 215.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=202000, episode_reward=487.60 +/- 149.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=203000, episode_reward=1008.40 +/- 206.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=204000, episode_reward=783.80 +/- 86.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=205000, episode_reward=863.20 +/- 305.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=206000, episode_reward=871.60 +/- 382.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=207000, episode_reward=911.00 +/- 295.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=208000, episode_reward=1005.40 +/- 301.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=209000, episode_reward=728.80 +/- 239.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=210000, episode_reward=1333.20 +/- 145.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=211000, episode_reward=1325.20 +/- 213.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=212000, episode_reward=813.20 +/- 183.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=213000, episode_reward=1307.20 +/- 58.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=214000, episode_reward=1269.20 +/- 293.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=215000, episode_reward=1307.40 +/- 178.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=216000, episode_reward=957.40 +/- 248.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=217000, episode_reward=1061.20 +/- 392.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=218000, episode_reward=1130.20 +/- 84.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=219000, episode_reward=984.60 +/- 486.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=220000, episode_reward=1059.20 +/- 208.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=221000, episode_reward=1129.40 +/- 410.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=222000, episode_reward=1273.80 +/- 251.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=223000, episode_reward=1468.20 +/- 227.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=224000, episode_reward=1175.60 +/- 201.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=225000, episode_reward=859.00 +/- 340.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=226000, episode_reward=1172.40 +/- 117.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=227000, episode_reward=1006.20 +/- 172.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=228000, episode_reward=1124.00 +/- 326.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=229000, episode_reward=1073.00 +/- 263.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=230000, episode_reward=987.20 +/- 196.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=231000, episode_reward=1191.00 +/- 220.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=232000, episode_reward=1326.60 +/- 465.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=233000, episode_reward=1213.60 +/- 187.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=234000, episode_reward=948.00 +/- 442.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=235000, episode_reward=1347.80 +/- 331.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=236000, episode_reward=1249.20 +/- 223.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=237000, episode_reward=1204.20 +/- 236.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=238000, episode_reward=1051.20 +/- 117.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=239000, episode_reward=1148.20 +/- 186.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=240000, episode_reward=978.00 +/- 247.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=241000, episode_reward=1402.60 +/- 161.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=242000, episode_reward=1204.60 +/- 220.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=243000, episode_reward=1285.40 +/- 342.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=244000, episode_reward=1063.00 +/- 326.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=245000, episode_reward=1322.60 +/- 350.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=246000, episode_reward=1379.00 +/- 369.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=247000, episode_reward=1217.80 +/- 258.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=248000, episode_reward=1186.40 +/- 522.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=249000, episode_reward=1295.40 +/- 221.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=250000, episode_reward=1475.40 +/- 292.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=251000, episode_reward=1110.40 +/- 265.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=252000, episode_reward=1012.40 +/- 191.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=253000, episode_reward=1255.20 +/- 404.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=254000, episode_reward=1243.60 +/- 172.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=255000, episode_reward=995.80 +/- 333.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=256000, episode_reward=1160.80 +/- 312.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=257000, episode_reward=1424.60 +/- 369.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=258000, episode_reward=1415.00 +/- 183.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=259000, episode_reward=741.80 +/- 273.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=260000, episode_reward=916.40 +/- 220.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=261000, episode_reward=1262.20 +/- 243.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=262000, episode_reward=1161.60 +/- 323.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=263000, episode_reward=1522.00 +/- 192.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=264000, episode_reward=1295.40 +/- 250.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=265000, episode_reward=1085.60 +/- 336.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=266000, episode_reward=1375.40 +/- 283.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=267000, episode_reward=1074.40 +/- 444.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=268000, episode_reward=1020.40 +/- 394.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=269000, episode_reward=930.00 +/- 231.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=270000, episode_reward=1263.60 +/- 337.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=271000, episode_reward=1102.60 +/- 281.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=272000, episode_reward=1114.00 +/- 287.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=273000, episode_reward=1054.60 +/- 316.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=274000, episode_reward=1322.80 +/- 396.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=275000, episode_reward=951.20 +/- 178.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=276000, episode_reward=1244.00 +/- 156.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=277000, episode_reward=1179.80 +/- 244.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=278000, episode_reward=1074.20 +/- 465.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=279000, episode_reward=1186.20 +/- 313.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=280000, episode_reward=944.40 +/- 242.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=281000, episode_reward=985.80 +/- 324.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=282000, episode_reward=1121.60 +/- 389.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=283000, episode_reward=1429.40 +/- 312.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=284000, episode_reward=1310.20 +/- 366.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=285000, episode_reward=1038.40 +/- 454.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=286000, episode_reward=1513.00 +/- 253.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=287000, episode_reward=1280.00 +/- 265.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=288000, episode_reward=1206.40 +/- 260.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=289000, episode_reward=892.60 +/- 451.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=290000, episode_reward=1015.60 +/- 295.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=291000, episode_reward=1127.80 +/- 270.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=292000, episode_reward=1095.00 +/- 493.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=293000, episode_reward=1063.00 +/- 219.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=294000, episode_reward=1288.40 +/- 194.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=295000, episode_reward=1263.40 +/- 560.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=296000, episode_reward=1615.00 +/- 166.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=297000, episode_reward=1293.20 +/- 119.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=298000, episode_reward=1289.60 +/- 376.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=299000, episode_reward=1136.20 +/- 234.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=300000, episode_reward=1240.80 +/- 387.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=301000, episode_reward=1312.40 +/- 376.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=302000, episode_reward=1248.00 +/- 136.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=303000, episode_reward=1173.40 +/- 188.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=304000, episode_reward=974.20 +/- 65.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=305000, episode_reward=766.20 +/- 349.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=306000, episode_reward=867.40 +/- 349.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=307000, episode_reward=956.80 +/- 171.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=308000, episode_reward=1468.00 +/- 141.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=309000, episode_reward=1275.40 +/- 410.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=310000, episode_reward=799.00 +/- 268.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=311000, episode_reward=995.40 +/- 279.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=312000, episode_reward=1047.80 +/- 232.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=313000, episode_reward=926.20 +/- 272.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=314000, episode_reward=1083.40 +/- 422.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=315000, episode_reward=1103.60 +/- 403.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=316000, episode_reward=1190.20 +/- 319.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=317000, episode_reward=1203.00 +/- 118.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=318000, episode_reward=1490.20 +/- 346.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=319000, episode_reward=908.00 +/- 407.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=320000, episode_reward=1046.00 +/- 329.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=321000, episode_reward=1157.40 +/- 134.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=322000, episode_reward=1271.00 +/- 211.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=323000, episode_reward=816.00 +/- 271.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=324000, episode_reward=1241.60 +/- 291.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=325000, episode_reward=1186.80 +/- 425.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=326000, episode_reward=1128.00 +/- 235.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=327000, episode_reward=1365.40 +/- 177.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=328000, episode_reward=927.40 +/- 226.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=329000, episode_reward=1365.20 +/- 291.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=330000, episode_reward=1184.20 +/- 103.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=331000, episode_reward=913.80 +/- 278.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=332000, episode_reward=1059.20 +/- 258.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=333000, episode_reward=1382.60 +/- 292.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=334000, episode_reward=986.40 +/- 321.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=335000, episode_reward=1213.80 +/- 276.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=336000, episode_reward=1442.60 +/- 344.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=337000, episode_reward=1356.20 +/- 193.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=338000, episode_reward=1117.80 +/- 313.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=339000, episode_reward=1008.00 +/- 221.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=340000, episode_reward=1250.00 +/- 411.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=341000, episode_reward=1335.20 +/- 222.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=342000, episode_reward=870.40 +/- 306.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=343000, episode_reward=806.80 +/- 372.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=344000, episode_reward=1307.00 +/- 279.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=345000, episode_reward=989.80 +/- 194.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=346000, episode_reward=986.40 +/- 304.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=347000, episode_reward=1433.80 +/- 288.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=348000, episode_reward=1195.60 +/- 460.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=349000, episode_reward=1312.60 +/- 212.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=350000, episode_reward=1164.80 +/- 222.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=351000, episode_reward=651.80 +/- 144.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=352000, episode_reward=995.60 +/- 330.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=353000, episode_reward=981.40 +/- 369.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=354000, episode_reward=1594.00 +/- 244.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=355000, episode_reward=1046.40 +/- 205.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=356000, episode_reward=1193.80 +/- 235.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=357000, episode_reward=1100.80 +/- 179.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=358000, episode_reward=1056.60 +/- 339.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=359000, episode_reward=1208.80 +/- 320.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=360000, episode_reward=896.20 +/- 292.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=361000, episode_reward=1059.60 +/- 210.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=362000, episode_reward=1310.00 +/- 196.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=363000, episode_reward=940.80 +/- 221.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=364000, episode_reward=1116.60 +/- 282.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=365000, episode_reward=1457.20 +/- 380.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=366000, episode_reward=654.20 +/- 203.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=367000, episode_reward=927.00 +/- 386.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=368000, episode_reward=1080.20 +/- 504.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=369000, episode_reward=774.00 +/- 401.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=370000, episode_reward=1263.20 +/- 288.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=371000, episode_reward=1587.60 +/- 120.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=372000, episode_reward=1739.80 +/- 134.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=373000, episode_reward=1296.20 +/- 184.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=374000, episode_reward=1483.40 +/- 387.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=375000, episode_reward=1408.60 +/- 240.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=376000, episode_reward=1620.60 +/- 305.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=377000, episode_reward=909.00 +/- 307.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=378000, episode_reward=1240.00 +/- 250.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=379000, episode_reward=980.80 +/- 422.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=380000, episode_reward=1342.80 +/- 320.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=381000, episode_reward=952.80 +/- 437.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=382000, episode_reward=1038.60 +/- 319.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=383000, episode_reward=1290.60 +/- 193.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=384000, episode_reward=1401.80 +/- 381.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=385000, episode_reward=1130.00 +/- 159.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=386000, episode_reward=1091.40 +/- 94.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=387000, episode_reward=1009.00 +/- 371.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=388000, episode_reward=1001.80 +/- 206.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=389000, episode_reward=1386.20 +/- 243.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=390000, episode_reward=1230.60 +/- 297.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=391000, episode_reward=1060.80 +/- 237.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=392000, episode_reward=1025.20 +/- 189.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=393000, episode_reward=1253.40 +/- 412.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=394000, episode_reward=1080.20 +/- 137.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=395000, episode_reward=1021.00 +/- 454.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=396000, episode_reward=1171.40 +/- 242.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=397000, episode_reward=1422.60 +/- 374.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=398000, episode_reward=1101.40 +/- 206.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=399000, episode_reward=1410.80 +/- 85.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=400000, episode_reward=1079.40 +/- 199.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=401000, episode_reward=1250.60 +/- 355.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=402000, episode_reward=1170.00 +/- 255.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=403000, episode_reward=1324.60 +/- 343.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=404000, episode_reward=1010.20 +/- 375.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=405000, episode_reward=1448.60 +/- 198.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=406000, episode_reward=1080.60 +/- 361.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=407000, episode_reward=1479.40 +/- 186.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=408000, episode_reward=1455.40 +/- 224.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=409000, episode_reward=1525.40 +/- 286.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=410000, episode_reward=1391.00 +/- 248.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=411000, episode_reward=1375.00 +/- 332.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=412000, episode_reward=1230.00 +/- 270.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=413000, episode_reward=1381.40 +/- 413.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=414000, episode_reward=950.20 +/- 289.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=415000, episode_reward=1270.60 +/- 188.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=416000, episode_reward=1300.00 +/- 151.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=417000, episode_reward=1383.00 +/- 175.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=418000, episode_reward=1405.00 +/- 231.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=419000, episode_reward=1228.60 +/- 213.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=420000, episode_reward=1225.80 +/- 96.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=421000, episode_reward=1346.20 +/- 380.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=422000, episode_reward=1014.60 +/- 234.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=423000, episode_reward=965.20 +/- 202.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=424000, episode_reward=1318.00 +/- 192.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=425000, episode_reward=879.60 +/- 435.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=426000, episode_reward=1158.60 +/- 170.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=427000, episode_reward=1095.00 +/- 268.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=428000, episode_reward=1350.00 +/- 141.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=429000, episode_reward=1198.40 +/- 226.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=430000, episode_reward=1340.20 +/- 275.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=431000, episode_reward=1445.20 +/- 205.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=432000, episode_reward=1376.20 +/- 284.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=433000, episode_reward=1078.60 +/- 362.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=434000, episode_reward=1382.20 +/- 174.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=435000, episode_reward=1014.60 +/- 303.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=436000, episode_reward=1480.80 +/- 225.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=437000, episode_reward=487.40 +/- 249.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=438000, episode_reward=956.20 +/- 357.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=439000, episode_reward=966.60 +/- 249.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=440000, episode_reward=1224.20 +/- 352.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=441000, episode_reward=970.20 +/- 365.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=442000, episode_reward=1541.60 +/- 167.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=443000, episode_reward=1261.40 +/- 280.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=444000, episode_reward=992.40 +/- 335.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=445000, episode_reward=1110.80 +/- 260.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=446000, episode_reward=900.40 +/- 247.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=447000, episode_reward=1138.40 +/- 257.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=448000, episode_reward=1357.40 +/- 359.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=449000, episode_reward=762.40 +/- 118.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=450000, episode_reward=1119.40 +/- 198.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=451000, episode_reward=1217.40 +/- 370.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=452000, episode_reward=1387.00 +/- 252.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=453000, episode_reward=1334.60 +/- 222.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=454000, episode_reward=977.60 +/- 450.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=455000, episode_reward=1288.80 +/- 299.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=456000, episode_reward=1454.80 +/- 170.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=457000, episode_reward=949.40 +/- 249.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=458000, episode_reward=1629.00 +/- 354.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=459000, episode_reward=1073.60 +/- 133.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=460000, episode_reward=1344.80 +/- 510.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=461000, episode_reward=1521.00 +/- 240.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=462000, episode_reward=1245.80 +/- 217.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=463000, episode_reward=1042.60 +/- 172.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=464000, episode_reward=1283.00 +/- 274.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=465000, episode_reward=798.20 +/- 384.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=466000, episode_reward=914.00 +/- 232.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=467000, episode_reward=1627.00 +/- 246.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=468000, episode_reward=1169.80 +/- 202.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=469000, episode_reward=1147.20 +/- 268.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=470000, episode_reward=1111.60 +/- 250.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=471000, episode_reward=1179.00 +/- 306.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=472000, episode_reward=1123.20 +/- 206.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=473000, episode_reward=1269.20 +/- 144.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=474000, episode_reward=1124.20 +/- 266.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=475000, episode_reward=1198.80 +/- 357.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=476000, episode_reward=1201.60 +/- 291.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=477000, episode_reward=975.60 +/- 342.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=478000, episode_reward=1258.20 +/- 158.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=479000, episode_reward=1154.80 +/- 267.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=480000, episode_reward=1230.20 +/- 287.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=481000, episode_reward=1585.80 +/- 175.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=482000, episode_reward=1479.40 +/- 295.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=483000, episode_reward=1164.60 +/- 51.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=484000, episode_reward=1098.60 +/- 603.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=485000, episode_reward=1245.40 +/- 476.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=486000, episode_reward=1168.80 +/- 388.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=487000, episode_reward=1121.00 +/- 306.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=488000, episode_reward=1412.00 +/- 393.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=489000, episode_reward=1356.20 +/- 155.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=490000, episode_reward=1547.00 +/- 181.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=491000, episode_reward=868.20 +/- 345.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=492000, episode_reward=1264.40 +/- 240.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=493000, episode_reward=898.20 +/- 307.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=494000, episode_reward=1461.60 +/- 137.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=495000, episode_reward=1433.60 +/- 393.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=496000, episode_reward=1412.40 +/- 302.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=497000, episode_reward=697.60 +/- 478.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=498000, episode_reward=970.80 +/- 188.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=499000, episode_reward=1404.40 +/- 425.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=500000, episode_reward=829.40 +/- 302.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Gamma = 0.9, Learning Rate = 0.003, Activation Function = ReLU, Neurons = 64, Process noise = 0.1, Measurement noise = 0.0\n",
      "Eval num_timesteps=1000, episode_reward=-896.60 +/- 120.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=-944.80 +/- 98.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=3000, episode_reward=-1008.20 +/- 196.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=4000, episode_reward=-998.00 +/- 162.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=5000, episode_reward=-1010.00 +/- 191.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=6000, episode_reward=-893.00 +/- 127.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=7000, episode_reward=-903.80 +/- 43.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=8000, episode_reward=-1020.00 +/- 143.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=9000, episode_reward=-829.60 +/- 117.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=10000, episode_reward=-1042.60 +/- 181.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=11000, episode_reward=-942.60 +/- 107.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=12000, episode_reward=-1061.80 +/- 107.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=13000, episode_reward=-1042.60 +/- 122.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=14000, episode_reward=-886.00 +/- 88.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=15000, episode_reward=-1034.80 +/- 132.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=16000, episode_reward=-800.00 +/- 121.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=17000, episode_reward=-994.40 +/- 121.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=18000, episode_reward=-920.60 +/- 176.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=19000, episode_reward=-1118.40 +/- 171.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=20000, episode_reward=-1000.60 +/- 70.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=21000, episode_reward=-1007.80 +/- 86.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=22000, episode_reward=-1038.20 +/- 139.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=23000, episode_reward=-924.60 +/- 133.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=24000, episode_reward=-952.60 +/- 172.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=25000, episode_reward=-959.80 +/- 90.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=26000, episode_reward=-1013.80 +/- 83.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=27000, episode_reward=-1097.80 +/- 179.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=28000, episode_reward=-981.20 +/- 123.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=29000, episode_reward=-894.60 +/- 199.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=30000, episode_reward=-899.80 +/- 115.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=31000, episode_reward=-989.20 +/- 141.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=32000, episode_reward=-1005.60 +/- 83.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=33000, episode_reward=-925.00 +/- 84.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=34000, episode_reward=-1070.20 +/- 96.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=35000, episode_reward=-997.40 +/- 118.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=36000, episode_reward=-973.00 +/- 138.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=37000, episode_reward=-1022.60 +/- 146.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=38000, episode_reward=-962.60 +/- 157.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=39000, episode_reward=-1045.40 +/- 102.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=40000, episode_reward=-1144.80 +/- 126.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=41000, episode_reward=-1020.60 +/- 79.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=42000, episode_reward=-1016.60 +/- 115.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=43000, episode_reward=-988.00 +/- 99.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=44000, episode_reward=-1068.60 +/- 113.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=45000, episode_reward=-1068.80 +/- 112.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=46000, episode_reward=-924.20 +/- 189.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=47000, episode_reward=-996.80 +/- 98.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=48000, episode_reward=-996.60 +/- 190.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=49000, episode_reward=-1095.40 +/- 138.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=50000, episode_reward=-1020.80 +/- 98.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=51000, episode_reward=68.60 +/- 201.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=52000, episode_reward=296.00 +/- 75.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=53000, episode_reward=354.20 +/- 201.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=54000, episode_reward=308.20 +/- 208.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=55000, episode_reward=52.20 +/- 271.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=56000, episode_reward=-15.20 +/- 160.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=57000, episode_reward=301.80 +/- 35.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=58000, episode_reward=448.00 +/- 345.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=59000, episode_reward=256.80 +/- 196.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=60000, episode_reward=177.00 +/- 119.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=61000, episode_reward=269.20 +/- 292.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=62000, episode_reward=333.00 +/- 153.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=63000, episode_reward=490.80 +/- 104.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=64000, episode_reward=272.40 +/- 201.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=65000, episode_reward=512.20 +/- 152.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=66000, episode_reward=255.20 +/- 127.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=67000, episode_reward=443.40 +/- 254.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=68000, episode_reward=427.00 +/- 96.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=69000, episode_reward=172.00 +/- 185.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=70000, episode_reward=251.80 +/- 198.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=71000, episode_reward=210.60 +/- 247.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=72000, episode_reward=511.80 +/- 121.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=73000, episode_reward=396.80 +/- 198.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=74000, episode_reward=437.20 +/- 151.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=75000, episode_reward=295.00 +/- 173.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=76000, episode_reward=378.00 +/- 89.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=77000, episode_reward=304.00 +/- 55.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=78000, episode_reward=349.40 +/- 101.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=79000, episode_reward=403.60 +/- 240.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=80000, episode_reward=335.60 +/- 174.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=81000, episode_reward=407.20 +/- 173.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=82000, episode_reward=366.40 +/- 204.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=83000, episode_reward=366.40 +/- 146.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=84000, episode_reward=530.20 +/- 140.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=85000, episode_reward=411.80 +/- 131.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=86000, episode_reward=347.60 +/- 179.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=87000, episode_reward=335.20 +/- 101.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=88000, episode_reward=399.20 +/- 161.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=89000, episode_reward=383.80 +/- 171.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=90000, episode_reward=379.00 +/- 214.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=91000, episode_reward=564.80 +/- 90.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=92000, episode_reward=364.60 +/- 90.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=93000, episode_reward=296.60 +/- 193.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=94000, episode_reward=326.80 +/- 193.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=95000, episode_reward=283.60 +/- 108.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=96000, episode_reward=322.80 +/- 178.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=97000, episode_reward=476.40 +/- 140.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=98000, episode_reward=278.60 +/- 326.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=99000, episode_reward=278.20 +/- 281.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=100000, episode_reward=404.00 +/- 128.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=101000, episode_reward=368.00 +/- 151.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=102000, episode_reward=403.40 +/- 105.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=103000, episode_reward=405.60 +/- 217.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=104000, episode_reward=392.80 +/- 126.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=105000, episode_reward=461.00 +/- 193.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=106000, episode_reward=383.40 +/- 84.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=107000, episode_reward=227.40 +/- 240.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=108000, episode_reward=391.20 +/- 142.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=109000, episode_reward=447.80 +/- 141.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=110000, episode_reward=497.80 +/- 293.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=111000, episode_reward=899.00 +/- 106.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=112000, episode_reward=930.20 +/- 226.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=113000, episode_reward=936.20 +/- 198.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=114000, episode_reward=553.60 +/- 265.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=115000, episode_reward=868.60 +/- 405.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=116000, episode_reward=1083.80 +/- 402.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=117000, episode_reward=812.40 +/- 338.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=118000, episode_reward=280.80 +/- 141.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=119000, episode_reward=920.60 +/- 413.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=120000, episode_reward=652.80 +/- 203.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=121000, episode_reward=873.60 +/- 275.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=122000, episode_reward=1086.00 +/- 205.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=123000, episode_reward=1184.20 +/- 316.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=124000, episode_reward=1064.40 +/- 130.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=125000, episode_reward=450.00 +/- 262.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=126000, episode_reward=759.00 +/- 160.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=127000, episode_reward=623.60 +/- 253.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=128000, episode_reward=1039.40 +/- 295.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=129000, episode_reward=745.60 +/- 268.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=130000, episode_reward=716.60 +/- 170.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=131000, episode_reward=972.80 +/- 295.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=132000, episode_reward=944.80 +/- 377.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=133000, episode_reward=1283.80 +/- 257.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=134000, episode_reward=856.00 +/- 374.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=135000, episode_reward=959.40 +/- 223.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=136000, episode_reward=1111.40 +/- 358.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=137000, episode_reward=958.20 +/- 233.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=138000, episode_reward=897.20 +/- 423.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=139000, episode_reward=1198.80 +/- 354.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=140000, episode_reward=1159.20 +/- 312.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=141000, episode_reward=1106.20 +/- 190.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=142000, episode_reward=931.20 +/- 269.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=143000, episode_reward=1271.80 +/- 332.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=144000, episode_reward=1307.20 +/- 111.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=145000, episode_reward=1597.20 +/- 313.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=146000, episode_reward=1163.40 +/- 261.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=147000, episode_reward=1254.60 +/- 132.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=148000, episode_reward=1019.20 +/- 273.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=149000, episode_reward=975.00 +/- 92.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=150000, episode_reward=1102.20 +/- 285.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=151000, episode_reward=920.20 +/- 270.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=152000, episode_reward=1322.20 +/- 160.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=153000, episode_reward=1317.40 +/- 248.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=154000, episode_reward=1451.80 +/- 146.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=155000, episode_reward=1482.40 +/- 174.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=156000, episode_reward=1207.40 +/- 251.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=157000, episode_reward=1169.20 +/- 316.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=158000, episode_reward=1383.20 +/- 291.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=159000, episode_reward=1573.40 +/- 131.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=160000, episode_reward=1074.80 +/- 188.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=161000, episode_reward=1517.00 +/- 302.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=162000, episode_reward=1333.60 +/- 301.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=163000, episode_reward=1354.80 +/- 211.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=164000, episode_reward=1186.60 +/- 418.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=165000, episode_reward=1671.80 +/- 106.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=166000, episode_reward=1455.00 +/- 292.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=167000, episode_reward=1317.40 +/- 279.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=168000, episode_reward=1620.20 +/- 223.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=169000, episode_reward=1563.40 +/- 214.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=170000, episode_reward=1062.60 +/- 89.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=171000, episode_reward=1278.60 +/- 120.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=172000, episode_reward=1189.80 +/- 167.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=173000, episode_reward=849.40 +/- 189.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=174000, episode_reward=1109.20 +/- 178.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=175000, episode_reward=1630.40 +/- 204.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=176000, episode_reward=1443.20 +/- 303.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=177000, episode_reward=1447.40 +/- 335.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=178000, episode_reward=1561.20 +/- 299.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=179000, episode_reward=749.60 +/- 282.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=180000, episode_reward=1270.20 +/- 447.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=181000, episode_reward=1324.00 +/- 244.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=182000, episode_reward=1283.40 +/- 429.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=183000, episode_reward=1777.80 +/- 365.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=184000, episode_reward=1667.20 +/- 99.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=185000, episode_reward=1320.20 +/- 278.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=186000, episode_reward=1459.40 +/- 151.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=187000, episode_reward=945.40 +/- 335.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=188000, episode_reward=1133.40 +/- 471.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=189000, episode_reward=1447.80 +/- 270.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=190000, episode_reward=1401.20 +/- 203.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=191000, episode_reward=1534.80 +/- 216.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=192000, episode_reward=916.20 +/- 311.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=193000, episode_reward=1205.80 +/- 431.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=194000, episode_reward=1026.60 +/- 388.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=195000, episode_reward=1125.00 +/- 130.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=196000, episode_reward=1154.00 +/- 628.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=197000, episode_reward=1271.60 +/- 369.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=198000, episode_reward=1293.60 +/- 293.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=199000, episode_reward=1317.80 +/- 244.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=200000, episode_reward=1389.60 +/- 315.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=201000, episode_reward=1209.20 +/- 278.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=202000, episode_reward=1541.60 +/- 296.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=203000, episode_reward=1122.00 +/- 261.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=204000, episode_reward=1219.40 +/- 174.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=205000, episode_reward=1449.40 +/- 274.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=206000, episode_reward=1380.00 +/- 257.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=207000, episode_reward=1649.40 +/- 263.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=208000, episode_reward=1367.40 +/- 168.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=209000, episode_reward=1055.20 +/- 194.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=210000, episode_reward=1344.80 +/- 353.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=211000, episode_reward=1272.60 +/- 304.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=212000, episode_reward=898.20 +/- 151.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=213000, episode_reward=1272.20 +/- 300.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=214000, episode_reward=861.80 +/- 145.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=215000, episode_reward=1250.40 +/- 221.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=216000, episode_reward=1051.20 +/- 102.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=217000, episode_reward=964.00 +/- 151.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=218000, episode_reward=1386.00 +/- 329.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=219000, episode_reward=1538.40 +/- 281.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=220000, episode_reward=1108.20 +/- 155.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=221000, episode_reward=947.40 +/- 281.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=222000, episode_reward=889.60 +/- 315.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=223000, episode_reward=1194.00 +/- 291.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=224000, episode_reward=1185.80 +/- 282.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=225000, episode_reward=1135.20 +/- 197.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=226000, episode_reward=626.00 +/- 291.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=227000, episode_reward=1254.20 +/- 428.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=228000, episode_reward=915.80 +/- 278.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=229000, episode_reward=1416.80 +/- 314.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=230000, episode_reward=1124.20 +/- 465.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=231000, episode_reward=1226.80 +/- 414.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=232000, episode_reward=1422.00 +/- 349.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=233000, episode_reward=959.20 +/- 373.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=234000, episode_reward=1216.80 +/- 164.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=235000, episode_reward=1374.80 +/- 209.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=236000, episode_reward=1369.20 +/- 186.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=237000, episode_reward=578.00 +/- 281.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=238000, episode_reward=1298.60 +/- 273.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=239000, episode_reward=1308.20 +/- 156.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=240000, episode_reward=1076.60 +/- 154.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=241000, episode_reward=1460.60 +/- 199.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=242000, episode_reward=1549.20 +/- 277.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=243000, episode_reward=1595.60 +/- 255.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=244000, episode_reward=1144.80 +/- 172.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=245000, episode_reward=1117.80 +/- 418.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=246000, episode_reward=1576.60 +/- 382.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=247000, episode_reward=1086.40 +/- 374.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=248000, episode_reward=1527.40 +/- 274.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=249000, episode_reward=1177.00 +/- 204.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=250000, episode_reward=1298.00 +/- 238.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=251000, episode_reward=1225.60 +/- 200.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=252000, episode_reward=1393.60 +/- 387.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=253000, episode_reward=1309.00 +/- 184.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=254000, episode_reward=1303.20 +/- 239.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=255000, episode_reward=1091.40 +/- 268.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=256000, episode_reward=1298.20 +/- 364.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=257000, episode_reward=1589.40 +/- 472.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=258000, episode_reward=1143.40 +/- 472.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=259000, episode_reward=1214.80 +/- 262.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=260000, episode_reward=1532.00 +/- 310.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=261000, episode_reward=1131.60 +/- 276.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=262000, episode_reward=1560.60 +/- 96.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=263000, episode_reward=1239.80 +/- 337.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=264000, episode_reward=1372.80 +/- 378.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=265000, episode_reward=1332.00 +/- 283.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=266000, episode_reward=704.80 +/- 389.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=267000, episode_reward=1435.40 +/- 333.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=268000, episode_reward=966.20 +/- 298.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=269000, episode_reward=1354.40 +/- 381.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=270000, episode_reward=1354.80 +/- 115.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=271000, episode_reward=960.00 +/- 334.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=272000, episode_reward=1083.80 +/- 321.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=273000, episode_reward=1436.60 +/- 159.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=274000, episode_reward=1173.60 +/- 333.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=275000, episode_reward=884.20 +/- 248.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=276000, episode_reward=1619.20 +/- 226.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=277000, episode_reward=1120.20 +/- 261.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=278000, episode_reward=803.00 +/- 258.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=279000, episode_reward=1551.60 +/- 328.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=280000, episode_reward=1369.20 +/- 197.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=281000, episode_reward=1256.00 +/- 300.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=282000, episode_reward=1340.00 +/- 314.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=283000, episode_reward=1411.20 +/- 401.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=284000, episode_reward=1106.80 +/- 287.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=285000, episode_reward=1605.60 +/- 198.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=286000, episode_reward=1432.60 +/- 343.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=287000, episode_reward=1027.80 +/- 194.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=288000, episode_reward=1253.00 +/- 215.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=289000, episode_reward=1640.00 +/- 221.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=290000, episode_reward=1298.20 +/- 287.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=291000, episode_reward=1407.20 +/- 367.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=292000, episode_reward=1546.20 +/- 290.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=293000, episode_reward=1365.00 +/- 365.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=294000, episode_reward=1383.40 +/- 220.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=295000, episode_reward=1216.20 +/- 297.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=296000, episode_reward=1336.60 +/- 483.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=297000, episode_reward=1434.40 +/- 236.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=298000, episode_reward=1187.40 +/- 426.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=299000, episode_reward=1518.20 +/- 229.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=300000, episode_reward=1393.00 +/- 413.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=301000, episode_reward=1320.20 +/- 117.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=302000, episode_reward=1537.00 +/- 211.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=303000, episode_reward=1161.00 +/- 300.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=304000, episode_reward=1677.40 +/- 222.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=305000, episode_reward=1118.40 +/- 369.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=306000, episode_reward=980.60 +/- 137.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=307000, episode_reward=1232.00 +/- 395.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=308000, episode_reward=1039.60 +/- 239.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=309000, episode_reward=1323.40 +/- 227.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=310000, episode_reward=1379.80 +/- 251.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=311000, episode_reward=855.40 +/- 333.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=312000, episode_reward=1377.60 +/- 179.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=313000, episode_reward=1136.00 +/- 282.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=314000, episode_reward=1530.20 +/- 65.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=315000, episode_reward=1306.80 +/- 324.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=316000, episode_reward=1540.00 +/- 146.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=317000, episode_reward=1492.00 +/- 227.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=318000, episode_reward=1444.60 +/- 355.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=319000, episode_reward=1304.20 +/- 227.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=320000, episode_reward=816.40 +/- 209.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=321000, episode_reward=1085.00 +/- 361.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=322000, episode_reward=1154.40 +/- 259.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=323000, episode_reward=1163.40 +/- 145.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=324000, episode_reward=1142.80 +/- 140.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=325000, episode_reward=1050.80 +/- 278.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=326000, episode_reward=790.40 +/- 248.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=327000, episode_reward=828.20 +/- 217.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=328000, episode_reward=1474.00 +/- 459.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=329000, episode_reward=1438.40 +/- 274.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=330000, episode_reward=1272.80 +/- 276.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=331000, episode_reward=1384.80 +/- 412.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=332000, episode_reward=998.00 +/- 433.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=333000, episode_reward=683.00 +/- 285.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=334000, episode_reward=1253.80 +/- 193.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=335000, episode_reward=1028.80 +/- 93.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=336000, episode_reward=1463.00 +/- 494.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=337000, episode_reward=1098.60 +/- 255.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=338000, episode_reward=1343.60 +/- 285.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=339000, episode_reward=912.40 +/- 307.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=340000, episode_reward=1008.40 +/- 149.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=341000, episode_reward=1598.80 +/- 239.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=342000, episode_reward=1496.80 +/- 276.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=343000, episode_reward=1606.00 +/- 241.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=344000, episode_reward=1438.20 +/- 305.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=345000, episode_reward=1249.20 +/- 317.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=346000, episode_reward=1367.20 +/- 246.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=347000, episode_reward=1280.00 +/- 207.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=348000, episode_reward=1495.80 +/- 392.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=349000, episode_reward=1240.80 +/- 124.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=350000, episode_reward=1506.80 +/- 444.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=351000, episode_reward=1341.20 +/- 127.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=352000, episode_reward=1365.80 +/- 295.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=353000, episode_reward=1272.60 +/- 439.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=354000, episode_reward=1619.40 +/- 166.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=355000, episode_reward=1542.80 +/- 206.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=356000, episode_reward=1498.40 +/- 288.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=357000, episode_reward=1074.60 +/- 371.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=358000, episode_reward=1398.40 +/- 369.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=359000, episode_reward=1339.80 +/- 243.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=360000, episode_reward=1377.00 +/- 149.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=361000, episode_reward=1172.00 +/- 399.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=362000, episode_reward=965.60 +/- 396.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=363000, episode_reward=1456.60 +/- 341.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=364000, episode_reward=1306.00 +/- 405.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=365000, episode_reward=1458.80 +/- 297.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=366000, episode_reward=1259.20 +/- 477.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=367000, episode_reward=1396.40 +/- 201.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=368000, episode_reward=1191.80 +/- 230.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=369000, episode_reward=1373.60 +/- 279.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=370000, episode_reward=1388.00 +/- 280.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=371000, episode_reward=1365.40 +/- 185.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=372000, episode_reward=1389.60 +/- 170.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=373000, episode_reward=891.00 +/- 335.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=374000, episode_reward=1373.40 +/- 223.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=375000, episode_reward=1582.20 +/- 137.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=376000, episode_reward=1500.00 +/- 212.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=377000, episode_reward=858.00 +/- 170.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=378000, episode_reward=1083.00 +/- 373.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=379000, episode_reward=1486.40 +/- 438.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=380000, episode_reward=1610.60 +/- 94.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=381000, episode_reward=1528.00 +/- 265.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=382000, episode_reward=1150.40 +/- 259.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=383000, episode_reward=1168.20 +/- 174.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=384000, episode_reward=1506.00 +/- 260.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=385000, episode_reward=1417.40 +/- 332.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=386000, episode_reward=1299.20 +/- 168.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=387000, episode_reward=1336.00 +/- 166.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=388000, episode_reward=1068.20 +/- 379.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=389000, episode_reward=1318.00 +/- 233.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=390000, episode_reward=1469.40 +/- 328.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=391000, episode_reward=1448.20 +/- 301.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=392000, episode_reward=774.00 +/- 248.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=393000, episode_reward=791.60 +/- 77.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=394000, episode_reward=1139.80 +/- 418.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=395000, episode_reward=944.60 +/- 175.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=396000, episode_reward=1381.40 +/- 199.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=397000, episode_reward=1576.00 +/- 250.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=398000, episode_reward=1227.40 +/- 242.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=399000, episode_reward=834.60 +/- 331.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=400000, episode_reward=656.40 +/- 120.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=401000, episode_reward=901.60 +/- 190.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=402000, episode_reward=1363.40 +/- 432.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=403000, episode_reward=917.20 +/- 274.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=404000, episode_reward=1035.40 +/- 143.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=405000, episode_reward=1096.60 +/- 160.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=406000, episode_reward=952.00 +/- 204.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=407000, episode_reward=1076.40 +/- 398.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=408000, episode_reward=1209.80 +/- 143.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=409000, episode_reward=1470.80 +/- 358.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=410000, episode_reward=1119.40 +/- 132.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=411000, episode_reward=1458.80 +/- 312.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=412000, episode_reward=1400.40 +/- 370.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=413000, episode_reward=1315.40 +/- 189.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=414000, episode_reward=1397.60 +/- 317.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=415000, episode_reward=1179.80 +/- 350.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=416000, episode_reward=1481.00 +/- 340.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=417000, episode_reward=1363.00 +/- 269.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=418000, episode_reward=1608.00 +/- 296.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=419000, episode_reward=1260.40 +/- 229.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=420000, episode_reward=1123.60 +/- 328.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=421000, episode_reward=1263.00 +/- 310.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=422000, episode_reward=1361.80 +/- 429.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=423000, episode_reward=1017.40 +/- 245.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=424000, episode_reward=1235.80 +/- 424.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=425000, episode_reward=1221.40 +/- 219.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=426000, episode_reward=1545.00 +/- 134.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=427000, episode_reward=1661.60 +/- 222.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=428000, episode_reward=1314.80 +/- 186.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=429000, episode_reward=1359.60 +/- 375.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=430000, episode_reward=1198.00 +/- 219.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=431000, episode_reward=1156.80 +/- 430.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=432000, episode_reward=929.20 +/- 373.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=433000, episode_reward=1375.80 +/- 292.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=434000, episode_reward=1030.80 +/- 199.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=435000, episode_reward=762.40 +/- 205.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=436000, episode_reward=952.60 +/- 196.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=437000, episode_reward=1186.40 +/- 274.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=438000, episode_reward=1017.40 +/- 178.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=439000, episode_reward=1061.80 +/- 405.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=440000, episode_reward=1061.60 +/- 213.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=441000, episode_reward=834.40 +/- 249.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=442000, episode_reward=1432.00 +/- 273.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=443000, episode_reward=949.40 +/- 325.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=444000, episode_reward=1440.00 +/- 56.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=445000, episode_reward=905.80 +/- 286.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=446000, episode_reward=1168.00 +/- 313.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=447000, episode_reward=1391.60 +/- 73.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=448000, episode_reward=1220.80 +/- 407.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=449000, episode_reward=1149.20 +/- 397.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=450000, episode_reward=1213.40 +/- 196.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=451000, episode_reward=1168.00 +/- 274.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=452000, episode_reward=1388.00 +/- 317.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=453000, episode_reward=933.00 +/- 401.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=454000, episode_reward=1148.00 +/- 144.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=455000, episode_reward=1225.40 +/- 329.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=456000, episode_reward=854.40 +/- 369.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=457000, episode_reward=1159.80 +/- 144.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=458000, episode_reward=915.00 +/- 392.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=459000, episode_reward=1477.60 +/- 302.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=460000, episode_reward=1031.40 +/- 246.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=461000, episode_reward=1125.80 +/- 297.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=462000, episode_reward=1396.00 +/- 251.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=463000, episode_reward=1467.40 +/- 316.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=464000, episode_reward=1357.40 +/- 132.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=465000, episode_reward=1547.80 +/- 59.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=466000, episode_reward=1199.40 +/- 285.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=467000, episode_reward=1125.40 +/- 201.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=468000, episode_reward=1398.20 +/- 157.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=469000, episode_reward=1083.80 +/- 259.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=470000, episode_reward=1474.80 +/- 409.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=471000, episode_reward=1752.40 +/- 146.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=472000, episode_reward=1486.60 +/- 93.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=473000, episode_reward=948.20 +/- 228.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=474000, episode_reward=1230.40 +/- 97.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=475000, episode_reward=905.40 +/- 162.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=476000, episode_reward=1599.20 +/- 317.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=477000, episode_reward=1237.80 +/- 311.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=478000, episode_reward=1257.80 +/- 193.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=479000, episode_reward=1154.20 +/- 507.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=480000, episode_reward=1138.60 +/- 268.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=481000, episode_reward=1026.60 +/- 227.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=482000, episode_reward=1107.80 +/- 276.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=483000, episode_reward=1633.00 +/- 176.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=484000, episode_reward=1251.00 +/- 271.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=485000, episode_reward=1094.80 +/- 325.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=486000, episode_reward=1564.80 +/- 148.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=487000, episode_reward=1099.40 +/- 273.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=488000, episode_reward=1402.20 +/- 118.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=489000, episode_reward=1109.40 +/- 302.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=490000, episode_reward=1434.40 +/- 284.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=491000, episode_reward=798.60 +/- 208.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=492000, episode_reward=1165.60 +/- 368.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=493000, episode_reward=1723.60 +/- 201.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=494000, episode_reward=1326.00 +/- 462.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=495000, episode_reward=1175.40 +/- 237.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=496000, episode_reward=1264.00 +/- 342.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=497000, episode_reward=978.00 +/- 350.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=498000, episode_reward=1262.40 +/- 353.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=499000, episode_reward=1309.00 +/- 511.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=500000, episode_reward=1541.80 +/- 335.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Gamma = 0.9, Learning Rate = 0.0003, Activation Function = Tanh, Neurons = 8, Process noise = 0.1, Measurement noise = 0.0\n",
      "Eval num_timesteps=1000, episode_reward=-5060.80 +/- 861.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=-2611.20 +/- 1186.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3000, episode_reward=-4575.80 +/- 1579.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=4000, episode_reward=-1234.60 +/- 980.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=5000, episode_reward=-5624.60 +/- 959.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=6000, episode_reward=-4001.80 +/- 2685.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=7000, episode_reward=-4309.80 +/- 1485.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=8000, episode_reward=-5160.00 +/- 1952.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=9000, episode_reward=-4857.00 +/- 1271.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=10000, episode_reward=-4596.60 +/- 2253.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=11000, episode_reward=-5386.20 +/- 1363.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=12000, episode_reward=-5443.20 +/- 1594.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=13000, episode_reward=-3797.80 +/- 2479.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=14000, episode_reward=-4719.60 +/- 2665.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=15000, episode_reward=-5092.60 +/- 1924.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=16000, episode_reward=-5117.00 +/- 1625.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=17000, episode_reward=-4652.40 +/- 2050.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=18000, episode_reward=-5169.40 +/- 1380.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=19000, episode_reward=-4057.80 +/- 2841.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=20000, episode_reward=-5090.60 +/- 1993.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=21000, episode_reward=-5215.40 +/- 1519.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=22000, episode_reward=-5926.00 +/- 1764.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=23000, episode_reward=-5469.20 +/- 1745.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=24000, episode_reward=-5397.80 +/- 1285.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=25000, episode_reward=-4910.40 +/- 2225.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=26000, episode_reward=-3909.60 +/- 2448.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=27000, episode_reward=-5100.20 +/- 2284.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=28000, episode_reward=-5369.80 +/- 1392.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=29000, episode_reward=-4753.40 +/- 933.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=30000, episode_reward=-4834.60 +/- 1851.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=31000, episode_reward=-5721.60 +/- 1771.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=32000, episode_reward=-4546.00 +/- 1724.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=33000, episode_reward=-5524.60 +/- 2331.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=34000, episode_reward=-4930.20 +/- 1927.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=35000, episode_reward=-4041.80 +/- 3345.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=36000, episode_reward=-3808.20 +/- 2928.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=37000, episode_reward=-3530.00 +/- 2447.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=38000, episode_reward=-3747.60 +/- 2447.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=39000, episode_reward=-4665.20 +/- 2458.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=40000, episode_reward=-5685.20 +/- 1336.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=41000, episode_reward=-6067.60 +/- 1246.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=42000, episode_reward=-4262.00 +/- 1732.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=43000, episode_reward=-4475.40 +/- 1034.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=44000, episode_reward=-4684.60 +/- 971.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=45000, episode_reward=-3248.20 +/- 1882.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=46000, episode_reward=-4906.40 +/- 995.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=47000, episode_reward=-3299.20 +/- 613.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=48000, episode_reward=-3809.80 +/- 1735.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=49000, episode_reward=-4559.60 +/- 2560.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=50000, episode_reward=-5946.00 +/- 1731.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=51000, episode_reward=-5484.60 +/- 1940.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=52000, episode_reward=-4431.60 +/- 1569.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=53000, episode_reward=-795.20 +/- 732.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=54000, episode_reward=-625.80 +/- 26.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=55000, episode_reward=-863.60 +/- 195.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=56000, episode_reward=-1108.20 +/- 327.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=57000, episode_reward=-967.40 +/- 399.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=58000, episode_reward=-939.00 +/- 393.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=59000, episode_reward=-2416.20 +/- 679.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=60000, episode_reward=-1246.80 +/- 1087.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=61000, episode_reward=-1999.20 +/- 866.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=62000, episode_reward=-1730.60 +/- 420.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=63000, episode_reward=-2804.00 +/- 459.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=64000, episode_reward=-1519.00 +/- 571.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=65000, episode_reward=-1063.00 +/- 684.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=66000, episode_reward=-710.20 +/- 841.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=67000, episode_reward=-584.20 +/- 219.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=68000, episode_reward=-556.60 +/- 450.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=69000, episode_reward=-794.40 +/- 208.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=70000, episode_reward=-649.20 +/- 177.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=71000, episode_reward=-904.80 +/- 92.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=72000, episode_reward=-750.40 +/- 130.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=73000, episode_reward=-1039.80 +/- 108.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=74000, episode_reward=-823.60 +/- 418.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=75000, episode_reward=-1013.80 +/- 211.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=76000, episode_reward=-854.00 +/- 419.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=77000, episode_reward=-1222.00 +/- 467.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=78000, episode_reward=-1054.80 +/- 610.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=79000, episode_reward=-648.20 +/- 236.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=80000, episode_reward=-646.40 +/- 375.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=81000, episode_reward=-1171.60 +/- 384.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=82000, episode_reward=-1332.20 +/- 162.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=83000, episode_reward=-823.60 +/- 406.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=84000, episode_reward=-1244.40 +/- 260.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=85000, episode_reward=-1136.00 +/- 399.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=86000, episode_reward=-749.60 +/- 369.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=87000, episode_reward=-963.80 +/- 387.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=88000, episode_reward=-1059.80 +/- 356.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=89000, episode_reward=-949.20 +/- 164.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=90000, episode_reward=-1001.00 +/- 290.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=91000, episode_reward=-878.80 +/- 179.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=92000, episode_reward=-777.20 +/- 370.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=93000, episode_reward=-872.20 +/- 288.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=94000, episode_reward=-1103.20 +/- 206.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=95000, episode_reward=-864.60 +/- 356.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=96000, episode_reward=-622.60 +/- 378.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=97000, episode_reward=-834.20 +/- 197.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=98000, episode_reward=-708.80 +/- 300.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=99000, episode_reward=-1144.60 +/- 210.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=100000, episode_reward=-842.80 +/- 247.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=101000, episode_reward=-1084.60 +/- 196.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=102000, episode_reward=-821.20 +/- 407.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=103000, episode_reward=-755.40 +/- 186.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=104000, episode_reward=-576.80 +/- 279.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=105000, episode_reward=-1034.40 +/- 418.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=106000, episode_reward=-808.00 +/- 286.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=107000, episode_reward=-761.00 +/- 151.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=108000, episode_reward=-784.80 +/- 168.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=109000, episode_reward=-714.40 +/- 427.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=110000, episode_reward=-914.40 +/- 262.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=111000, episode_reward=-561.20 +/- 107.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=112000, episode_reward=-642.20 +/- 437.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=113000, episode_reward=-531.60 +/- 95.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=114000, episode_reward=-677.80 +/- 358.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=115000, episode_reward=-649.20 +/- 297.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=116000, episode_reward=-743.80 +/- 342.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=117000, episode_reward=-847.60 +/- 136.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=118000, episode_reward=-798.20 +/- 355.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=119000, episode_reward=-641.80 +/- 235.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=120000, episode_reward=-798.20 +/- 350.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=121000, episode_reward=-579.40 +/- 322.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=122000, episode_reward=-844.20 +/- 182.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=123000, episode_reward=-880.60 +/- 131.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=124000, episode_reward=-586.60 +/- 117.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=125000, episode_reward=-740.60 +/- 178.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=126000, episode_reward=-616.00 +/- 380.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=127000, episode_reward=-512.00 +/- 280.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=128000, episode_reward=-306.80 +/- 326.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=129000, episode_reward=-758.40 +/- 215.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=130000, episode_reward=-603.20 +/- 223.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=131000, episode_reward=-605.40 +/- 362.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=132000, episode_reward=-570.00 +/- 399.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=133000, episode_reward=-745.40 +/- 315.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=134000, episode_reward=-571.80 +/- 224.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=135000, episode_reward=-374.00 +/- 272.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=136000, episode_reward=-688.00 +/- 203.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=137000, episode_reward=-471.80 +/- 192.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=138000, episode_reward=-434.20 +/- 251.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=139000, episode_reward=-609.20 +/- 125.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=140000, episode_reward=-460.60 +/- 283.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=141000, episode_reward=-551.40 +/- 190.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=142000, episode_reward=-346.00 +/- 242.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=143000, episode_reward=-270.80 +/- 240.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=144000, episode_reward=-422.20 +/- 151.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=145000, episode_reward=-463.80 +/- 208.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=146000, episode_reward=-439.00 +/- 253.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=147000, episode_reward=-570.00 +/- 308.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=148000, episode_reward=-411.00 +/- 418.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=149000, episode_reward=-511.20 +/- 113.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=150000, episode_reward=-292.40 +/- 450.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=151000, episode_reward=-468.20 +/- 404.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=152000, episode_reward=-278.40 +/- 137.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=153000, episode_reward=-258.60 +/- 129.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=154000, episode_reward=-451.20 +/- 261.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=155000, episode_reward=-704.40 +/- 299.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=156000, episode_reward=-348.40 +/- 341.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=157000, episode_reward=-393.20 +/- 366.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=158000, episode_reward=-489.80 +/- 295.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=159000, episode_reward=-407.40 +/- 297.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=160000, episode_reward=-411.80 +/- 383.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=161000, episode_reward=-640.60 +/- 194.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=162000, episode_reward=-493.80 +/- 289.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=163000, episode_reward=-283.80 +/- 219.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=164000, episode_reward=-310.40 +/- 121.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=165000, episode_reward=-367.60 +/- 181.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=166000, episode_reward=-422.60 +/- 195.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=167000, episode_reward=-521.60 +/- 318.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=168000, episode_reward=-319.40 +/- 510.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=169000, episode_reward=-315.20 +/- 442.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=170000, episode_reward=-369.80 +/- 194.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=171000, episode_reward=-296.00 +/- 214.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=172000, episode_reward=-244.40 +/- 174.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=173000, episode_reward=-584.40 +/- 278.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=174000, episode_reward=-602.60 +/- 325.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=175000, episode_reward=-547.00 +/- 609.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=176000, episode_reward=-397.40 +/- 249.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=177000, episode_reward=-689.20 +/- 142.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=178000, episode_reward=-376.00 +/- 471.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=179000, episode_reward=-332.20 +/- 323.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=180000, episode_reward=-257.80 +/- 399.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=181000, episode_reward=-420.40 +/- 144.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=182000, episode_reward=-167.40 +/- 127.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=183000, episode_reward=-277.20 +/- 471.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=184000, episode_reward=-412.00 +/- 237.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=185000, episode_reward=-88.00 +/- 295.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=186000, episode_reward=-354.60 +/- 263.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=187000, episode_reward=-301.80 +/- 298.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=188000, episode_reward=-399.40 +/- 286.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=189000, episode_reward=-239.80 +/- 295.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=190000, episode_reward=-432.60 +/- 285.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=191000, episode_reward=-482.60 +/- 374.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=192000, episode_reward=-647.40 +/- 284.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=193000, episode_reward=-660.00 +/- 471.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=194000, episode_reward=-521.40 +/- 347.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=195000, episode_reward=-399.60 +/- 266.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=196000, episode_reward=-280.40 +/- 331.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=197000, episode_reward=-457.40 +/- 169.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=198000, episode_reward=-495.40 +/- 150.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=199000, episode_reward=-237.80 +/- 230.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=200000, episode_reward=-336.60 +/- 365.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=201000, episode_reward=-602.20 +/- 317.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=202000, episode_reward=-302.00 +/- 189.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=203000, episode_reward=-195.80 +/- 375.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=204000, episode_reward=-389.60 +/- 331.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=205000, episode_reward=-450.80 +/- 231.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=206000, episode_reward=-427.00 +/- 295.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=207000, episode_reward=-252.80 +/- 325.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=208000, episode_reward=-438.00 +/- 232.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=209000, episode_reward=-304.80 +/- 179.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=210000, episode_reward=-171.00 +/- 314.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=211000, episode_reward=-186.80 +/- 275.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=212000, episode_reward=-572.00 +/- 298.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=213000, episode_reward=-454.20 +/- 197.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=214000, episode_reward=-305.00 +/- 350.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=215000, episode_reward=-421.60 +/- 187.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=216000, episode_reward=-404.40 +/- 498.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=217000, episode_reward=-358.00 +/- 254.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=218000, episode_reward=-271.00 +/- 247.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=219000, episode_reward=-132.20 +/- 192.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=220000, episode_reward=-328.00 +/- 125.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=221000, episode_reward=-206.80 +/- 305.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=222000, episode_reward=-533.60 +/- 88.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=223000, episode_reward=-167.60 +/- 277.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=224000, episode_reward=-624.80 +/- 466.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=225000, episode_reward=-149.00 +/- 147.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=226000, episode_reward=-445.40 +/- 290.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=227000, episode_reward=-313.40 +/- 149.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=228000, episode_reward=-288.00 +/- 299.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=229000, episode_reward=-299.40 +/- 393.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=230000, episode_reward=-26.80 +/- 184.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=231000, episode_reward=5.60 +/- 232.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=232000, episode_reward=-69.80 +/- 250.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=233000, episode_reward=-191.80 +/- 238.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=234000, episode_reward=-92.80 +/- 313.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=235000, episode_reward=-212.00 +/- 302.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=236000, episode_reward=-111.00 +/- 305.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=237000, episode_reward=-81.00 +/- 173.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=238000, episode_reward=-281.20 +/- 126.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=239000, episode_reward=-57.00 +/- 229.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=240000, episode_reward=-130.00 +/- 166.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=241000, episode_reward=-153.40 +/- 236.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=242000, episode_reward=-349.80 +/- 121.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=243000, episode_reward=-347.00 +/- 165.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=244000, episode_reward=-165.20 +/- 177.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=245000, episode_reward=-388.20 +/- 340.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=246000, episode_reward=143.60 +/- 386.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=247000, episode_reward=-470.00 +/- 131.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=248000, episode_reward=20.20 +/- 321.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=249000, episode_reward=-99.60 +/- 274.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=250000, episode_reward=-81.40 +/- 306.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=251000, episode_reward=-512.60 +/- 200.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=252000, episode_reward=-12.80 +/- 277.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=253000, episode_reward=-354.00 +/- 204.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=254000, episode_reward=-310.80 +/- 143.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=255000, episode_reward=-357.20 +/- 177.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=256000, episode_reward=-269.80 +/- 295.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=257000, episode_reward=-147.80 +/- 132.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=258000, episode_reward=-9.60 +/- 87.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=259000, episode_reward=-251.60 +/- 524.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=260000, episode_reward=-266.40 +/- 255.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=261000, episode_reward=-59.40 +/- 187.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=262000, episode_reward=-137.20 +/- 222.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=263000, episode_reward=-307.40 +/- 348.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=264000, episode_reward=-93.00 +/- 219.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=265000, episode_reward=34.00 +/- 168.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=266000, episode_reward=-158.40 +/- 354.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=267000, episode_reward=74.40 +/- 173.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=268000, episode_reward=-258.40 +/- 268.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=269000, episode_reward=-115.00 +/- 299.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=270000, episode_reward=-28.00 +/- 411.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=271000, episode_reward=-293.80 +/- 370.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=272000, episode_reward=26.60 +/- 273.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=273000, episode_reward=96.60 +/- 205.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=274000, episode_reward=291.60 +/- 371.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=275000, episode_reward=-8.00 +/- 168.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=276000, episode_reward=28.80 +/- 176.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=277000, episode_reward=383.80 +/- 123.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=278000, episode_reward=191.40 +/- 325.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=279000, episode_reward=521.00 +/- 395.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=280000, episode_reward=105.80 +/- 344.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=281000, episode_reward=-143.60 +/- 434.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=282000, episode_reward=363.20 +/- 521.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=283000, episode_reward=320.40 +/- 199.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=284000, episode_reward=153.20 +/- 371.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=285000, episode_reward=171.20 +/- 295.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=286000, episode_reward=-11.00 +/- 252.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=287000, episode_reward=639.00 +/- 384.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=288000, episode_reward=518.20 +/- 211.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=289000, episode_reward=494.80 +/- 372.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=290000, episode_reward=413.40 +/- 282.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=291000, episode_reward=254.60 +/- 371.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=292000, episode_reward=3.40 +/- 297.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=293000, episode_reward=531.60 +/- 354.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=294000, episode_reward=113.80 +/- 284.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=295000, episode_reward=174.60 +/- 288.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=296000, episode_reward=945.40 +/- 203.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=297000, episode_reward=328.00 +/- 258.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=298000, episode_reward=343.60 +/- 333.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=299000, episode_reward=487.40 +/- 408.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=300000, episode_reward=513.20 +/- 618.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=301000, episode_reward=163.40 +/- 394.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=302000, episode_reward=480.60 +/- 291.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=303000, episode_reward=-2013.80 +/- 5454.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=304000, episode_reward=505.60 +/- 267.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=305000, episode_reward=539.80 +/- 341.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=306000, episode_reward=362.60 +/- 283.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=307000, episode_reward=622.20 +/- 148.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=308000, episode_reward=463.40 +/- 355.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=309000, episode_reward=359.20 +/- 419.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=310000, episode_reward=539.00 +/- 315.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=311000, episode_reward=404.20 +/- 398.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=312000, episode_reward=671.40 +/- 305.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=313000, episode_reward=481.80 +/- 414.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=314000, episode_reward=505.40 +/- 370.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=315000, episode_reward=149.40 +/- 524.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=316000, episode_reward=712.40 +/- 294.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=317000, episode_reward=397.40 +/- 247.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=318000, episode_reward=408.20 +/- 498.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=319000, episode_reward=355.60 +/- 352.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=320000, episode_reward=363.60 +/- 297.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=321000, episode_reward=444.60 +/- 291.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=322000, episode_reward=354.40 +/- 274.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=323000, episode_reward=709.80 +/- 380.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=324000, episode_reward=709.20 +/- 255.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=325000, episode_reward=524.80 +/- 324.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=326000, episode_reward=562.00 +/- 564.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=327000, episode_reward=411.80 +/- 424.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=328000, episode_reward=69.40 +/- 437.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=329000, episode_reward=494.80 +/- 175.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=330000, episode_reward=464.20 +/- 123.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=331000, episode_reward=-1114.80 +/- 3193.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=332000, episode_reward=450.80 +/- 312.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=333000, episode_reward=387.00 +/- 236.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=334000, episode_reward=439.00 +/- 303.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=335000, episode_reward=313.20 +/- 330.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=336000, episode_reward=367.80 +/- 309.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=337000, episode_reward=588.20 +/- 567.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=338000, episode_reward=666.80 +/- 265.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=339000, episode_reward=619.40 +/- 229.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=340000, episode_reward=623.20 +/- 305.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=341000, episode_reward=642.20 +/- 548.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=342000, episode_reward=572.60 +/- 347.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=343000, episode_reward=783.40 +/- 397.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=344000, episode_reward=238.00 +/- 222.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=345000, episode_reward=703.60 +/- 341.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=346000, episode_reward=640.80 +/- 362.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=347000, episode_reward=222.00 +/- 345.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=348000, episode_reward=994.40 +/- 372.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=349000, episode_reward=718.80 +/- 404.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=350000, episode_reward=618.60 +/- 350.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=351000, episode_reward=584.80 +/- 360.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=352000, episode_reward=455.60 +/- 565.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=353000, episode_reward=780.40 +/- 280.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=354000, episode_reward=730.20 +/- 427.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=355000, episode_reward=723.20 +/- 233.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=356000, episode_reward=452.80 +/- 332.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=357000, episode_reward=821.20 +/- 179.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=358000, episode_reward=572.00 +/- 594.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=359000, episode_reward=643.20 +/- 161.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=360000, episode_reward=309.60 +/- 326.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=361000, episode_reward=598.60 +/- 123.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=362000, episode_reward=839.20 +/- 572.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=363000, episode_reward=395.80 +/- 448.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=364000, episode_reward=475.80 +/- 378.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=365000, episode_reward=533.20 +/- 167.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=366000, episode_reward=374.20 +/- 315.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=367000, episode_reward=590.20 +/- 413.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=368000, episode_reward=408.20 +/- 286.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=369000, episode_reward=621.20 +/- 249.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=370000, episode_reward=593.20 +/- 309.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=371000, episode_reward=607.60 +/- 328.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=372000, episode_reward=628.60 +/- 314.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=373000, episode_reward=359.00 +/- 338.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=374000, episode_reward=838.60 +/- 100.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=375000, episode_reward=272.20 +/- 220.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=376000, episode_reward=486.00 +/- 374.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=377000, episode_reward=346.00 +/- 349.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=378000, episode_reward=698.80 +/- 208.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=379000, episode_reward=725.80 +/- 268.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=380000, episode_reward=542.40 +/- 273.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=381000, episode_reward=374.20 +/- 245.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=382000, episode_reward=454.00 +/- 288.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=383000, episode_reward=565.40 +/- 176.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=384000, episode_reward=605.40 +/- 99.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=385000, episode_reward=-1142.00 +/- 3370.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=386000, episode_reward=658.20 +/- 272.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=387000, episode_reward=585.60 +/- 250.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=388000, episode_reward=429.40 +/- 499.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=389000, episode_reward=552.40 +/- 519.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=390000, episode_reward=579.40 +/- 215.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=391000, episode_reward=575.80 +/- 300.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=392000, episode_reward=698.60 +/- 375.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=393000, episode_reward=253.20 +/- 337.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=394000, episode_reward=574.00 +/- 171.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=395000, episode_reward=154.60 +/- 357.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=396000, episode_reward=308.40 +/- 355.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=397000, episode_reward=669.80 +/- 284.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=398000, episode_reward=724.00 +/- 229.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=399000, episode_reward=300.60 +/- 529.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=400000, episode_reward=446.40 +/- 215.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=401000, episode_reward=474.00 +/- 294.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=402000, episode_reward=460.40 +/- 95.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=403000, episode_reward=365.40 +/- 266.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=404000, episode_reward=597.20 +/- 554.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=405000, episode_reward=410.40 +/- 403.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=406000, episode_reward=154.20 +/- 76.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=407000, episode_reward=588.20 +/- 246.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=408000, episode_reward=378.00 +/- 293.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=409000, episode_reward=688.80 +/- 342.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=410000, episode_reward=682.60 +/- 291.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=411000, episode_reward=788.60 +/- 549.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=412000, episode_reward=516.80 +/- 271.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=413000, episode_reward=470.20 +/- 277.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=414000, episode_reward=637.00 +/- 291.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=415000, episode_reward=315.80 +/- 236.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=416000, episode_reward=428.20 +/- 341.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=417000, episode_reward=538.40 +/- 283.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=418000, episode_reward=495.80 +/- 197.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=419000, episode_reward=249.20 +/- 266.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=420000, episode_reward=554.00 +/- 439.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=421000, episode_reward=406.60 +/- 294.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=422000, episode_reward=637.40 +/- 312.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=423000, episode_reward=555.00 +/- 245.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=424000, episode_reward=616.40 +/- 353.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=425000, episode_reward=739.00 +/- 195.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=426000, episode_reward=595.80 +/- 199.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=427000, episode_reward=701.20 +/- 374.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=428000, episode_reward=635.80 +/- 505.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=429000, episode_reward=680.20 +/- 203.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=430000, episode_reward=465.40 +/- 411.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=431000, episode_reward=583.60 +/- 323.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=432000, episode_reward=407.00 +/- 309.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=433000, episode_reward=707.60 +/- 458.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=434000, episode_reward=319.80 +/- 515.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=435000, episode_reward=340.80 +/- 309.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=436000, episode_reward=685.40 +/- 380.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=437000, episode_reward=543.20 +/- 167.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=438000, episode_reward=706.20 +/- 219.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=439000, episode_reward=336.40 +/- 211.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=440000, episode_reward=891.60 +/- 208.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=441000, episode_reward=729.80 +/- 196.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=442000, episode_reward=720.20 +/- 126.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=443000, episode_reward=707.40 +/- 121.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=444000, episode_reward=517.00 +/- 359.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=445000, episode_reward=508.00 +/- 103.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=446000, episode_reward=578.60 +/- 213.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=447000, episode_reward=811.20 +/- 369.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=448000, episode_reward=454.00 +/- 356.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=449000, episode_reward=679.80 +/- 169.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=450000, episode_reward=613.20 +/- 263.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=451000, episode_reward=415.20 +/- 359.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=452000, episode_reward=568.80 +/- 302.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=453000, episode_reward=943.00 +/- 214.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=454000, episode_reward=639.60 +/- 195.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=455000, episode_reward=772.60 +/- 168.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=456000, episode_reward=207.80 +/- 260.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=457000, episode_reward=468.80 +/- 281.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=458000, episode_reward=662.20 +/- 211.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=459000, episode_reward=698.60 +/- 350.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=460000, episode_reward=403.40 +/- 304.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=461000, episode_reward=582.60 +/- 375.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=462000, episode_reward=810.20 +/- 349.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=463000, episode_reward=705.80 +/- 361.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=464000, episode_reward=637.00 +/- 342.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=465000, episode_reward=815.40 +/- 241.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=466000, episode_reward=726.00 +/- 378.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=467000, episode_reward=355.00 +/- 123.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=468000, episode_reward=682.80 +/- 480.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=469000, episode_reward=506.20 +/- 218.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=470000, episode_reward=632.40 +/- 244.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=471000, episode_reward=644.60 +/- 227.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=472000, episode_reward=444.20 +/- 202.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=473000, episode_reward=437.40 +/- 540.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=474000, episode_reward=415.40 +/- 309.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=475000, episode_reward=621.40 +/- 222.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=476000, episode_reward=504.40 +/- 385.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=477000, episode_reward=382.80 +/- 290.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=478000, episode_reward=665.80 +/- 413.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=479000, episode_reward=389.40 +/- 434.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=480000, episode_reward=898.80 +/- 163.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=481000, episode_reward=661.60 +/- 182.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=482000, episode_reward=756.20 +/- 324.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=483000, episode_reward=467.60 +/- 331.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=484000, episode_reward=881.40 +/- 497.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=485000, episode_reward=647.00 +/- 114.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=486000, episode_reward=568.60 +/- 274.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=487000, episode_reward=584.40 +/- 511.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=488000, episode_reward=604.00 +/- 358.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=489000, episode_reward=293.40 +/- 237.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=490000, episode_reward=424.00 +/- 97.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=491000, episode_reward=491.80 +/- 223.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=492000, episode_reward=484.00 +/- 176.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=493000, episode_reward=388.20 +/- 310.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=494000, episode_reward=300.80 +/- 351.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=495000, episode_reward=823.00 +/- 362.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=496000, episode_reward=574.80 +/- 337.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=497000, episode_reward=561.80 +/- 295.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=498000, episode_reward=674.40 +/- 439.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=499000, episode_reward=675.80 +/- 465.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=500000, episode_reward=595.40 +/- 409.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Gamma = 0.9, Learning Rate = 0.0003, Activation Function = Tanh, Neurons = 32, Process noise = 0.1, Measurement noise = 0.0\n",
      "Eval num_timesteps=1000, episode_reward=-1852.20 +/- 388.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=-2054.20 +/- 321.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=3000, episode_reward=-2110.00 +/- 254.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=4000, episode_reward=-2041.00 +/- 468.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=5000, episode_reward=-2368.80 +/- 343.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=6000, episode_reward=-1602.60 +/- 491.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=7000, episode_reward=-1826.80 +/- 512.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=8000, episode_reward=-1843.40 +/- 380.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=9000, episode_reward=-1760.80 +/- 798.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=10000, episode_reward=-2223.80 +/- 397.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=11000, episode_reward=-2182.80 +/- 875.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=12000, episode_reward=-2546.00 +/- 553.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=13000, episode_reward=-1995.60 +/- 190.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=14000, episode_reward=-2123.00 +/- 789.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=15000, episode_reward=-1794.20 +/- 546.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=16000, episode_reward=-2124.00 +/- 358.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=17000, episode_reward=-1845.20 +/- 380.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=18000, episode_reward=-2015.80 +/- 484.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=19000, episode_reward=-1691.60 +/- 91.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=20000, episode_reward=-2183.80 +/- 188.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=21000, episode_reward=-1718.00 +/- 639.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=22000, episode_reward=-1381.60 +/- 422.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=23000, episode_reward=-1817.00 +/- 575.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=24000, episode_reward=-1876.40 +/- 595.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=25000, episode_reward=-2031.80 +/- 175.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=26000, episode_reward=-2095.40 +/- 506.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=27000, episode_reward=-2295.80 +/- 697.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=28000, episode_reward=-1234.60 +/- 515.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=29000, episode_reward=-2017.80 +/- 695.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=30000, episode_reward=-1716.60 +/- 306.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=31000, episode_reward=-1691.60 +/- 536.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=32000, episode_reward=-1854.60 +/- 749.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=33000, episode_reward=-2259.20 +/- 819.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=34000, episode_reward=-2193.40 +/- 447.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=35000, episode_reward=-1682.00 +/- 435.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=36000, episode_reward=-2291.40 +/- 712.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=37000, episode_reward=-1644.40 +/- 380.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=38000, episode_reward=-1864.40 +/- 694.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=39000, episode_reward=-1561.40 +/- 265.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=40000, episode_reward=-1586.20 +/- 775.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=41000, episode_reward=-2107.00 +/- 754.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=42000, episode_reward=-2015.60 +/- 166.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=43000, episode_reward=-1850.80 +/- 543.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=44000, episode_reward=-1929.80 +/- 627.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=45000, episode_reward=-2321.00 +/- 199.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=46000, episode_reward=-2132.20 +/- 538.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=47000, episode_reward=-1750.40 +/- 506.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=48000, episode_reward=-1845.20 +/- 646.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=49000, episode_reward=-1445.60 +/- 801.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=50000, episode_reward=-1819.40 +/- 532.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=51000, episode_reward=360.60 +/- 241.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=52000, episode_reward=-6606.60 +/- 1919.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=53000, episode_reward=-451.60 +/- 262.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=54000, episode_reward=-450.40 +/- 242.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=55000, episode_reward=-338.00 +/- 233.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=56000, episode_reward=-745.40 +/- 294.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=57000, episode_reward=-735.20 +/- 84.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=58000, episode_reward=-640.40 +/- 209.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=59000, episode_reward=-768.00 +/- 454.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=60000, episode_reward=-730.60 +/- 292.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=61000, episode_reward=-876.20 +/- 369.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=62000, episode_reward=-610.80 +/- 229.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=63000, episode_reward=-610.00 +/- 256.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=64000, episode_reward=-542.00 +/- 190.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=65000, episode_reward=-455.60 +/- 171.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=66000, episode_reward=-682.00 +/- 131.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=67000, episode_reward=-504.60 +/- 284.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=68000, episode_reward=-663.20 +/- 209.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=69000, episode_reward=-532.00 +/- 354.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=70000, episode_reward=-358.00 +/- 213.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=71000, episode_reward=-518.40 +/- 313.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=72000, episode_reward=-697.20 +/- 226.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=73000, episode_reward=-599.60 +/- 135.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=74000, episode_reward=-486.00 +/- 366.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=75000, episode_reward=-687.20 +/- 165.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=76000, episode_reward=-585.00 +/- 209.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=77000, episode_reward=-327.20 +/- 145.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=78000, episode_reward=-368.20 +/- 306.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=79000, episode_reward=-254.00 +/- 381.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=80000, episode_reward=-211.80 +/- 235.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=81000, episode_reward=-242.00 +/- 266.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=82000, episode_reward=-517.40 +/- 133.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=83000, episode_reward=-223.00 +/- 260.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=84000, episode_reward=-220.80 +/- 203.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=85000, episode_reward=-558.20 +/- 186.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=86000, episode_reward=-423.00 +/- 279.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=87000, episode_reward=-387.20 +/- 108.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=88000, episode_reward=-234.60 +/- 142.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=89000, episode_reward=-387.00 +/- 238.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=90000, episode_reward=-358.60 +/- 170.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=91000, episode_reward=-201.40 +/- 355.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=92000, episode_reward=-371.60 +/- 211.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=93000, episode_reward=-172.00 +/- 241.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=94000, episode_reward=-117.60 +/- 220.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=95000, episode_reward=-125.20 +/- 287.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=96000, episode_reward=-220.60 +/- 152.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=97000, episode_reward=-208.60 +/- 154.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=98000, episode_reward=148.20 +/- 195.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=99000, episode_reward=-258.60 +/- 234.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=100000, episode_reward=-316.60 +/- 251.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=101000, episode_reward=-86.80 +/- 154.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=102000, episode_reward=-147.40 +/- 136.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=103000, episode_reward=-56.40 +/- 251.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=104000, episode_reward=-27.00 +/- 271.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=105000, episode_reward=-51.20 +/- 169.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=106000, episode_reward=-96.80 +/- 227.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=107000, episode_reward=-118.00 +/- 264.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=108000, episode_reward=87.80 +/- 82.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=109000, episode_reward=39.80 +/- 174.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=110000, episode_reward=181.40 +/- 185.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=111000, episode_reward=152.20 +/- 120.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=112000, episode_reward=129.00 +/- 184.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=113000, episode_reward=25.00 +/- 201.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=114000, episode_reward=56.60 +/- 194.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=115000, episode_reward=13.80 +/- 164.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=116000, episode_reward=15.20 +/- 219.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=117000, episode_reward=54.20 +/- 240.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=118000, episode_reward=-377.60 +/- 217.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=119000, episode_reward=22.00 +/- 258.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=120000, episode_reward=57.80 +/- 144.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=121000, episode_reward=-65.00 +/- 121.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=122000, episode_reward=12.00 +/- 247.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=123000, episode_reward=122.80 +/- 172.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=124000, episode_reward=240.40 +/- 129.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=125000, episode_reward=-4.20 +/- 305.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=126000, episode_reward=-6.60 +/- 217.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=127000, episode_reward=68.80 +/- 158.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=128000, episode_reward=136.40 +/- 160.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=129000, episode_reward=164.60 +/- 252.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=130000, episode_reward=210.20 +/- 138.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=131000, episode_reward=123.00 +/- 291.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=132000, episode_reward=90.60 +/- 102.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=133000, episode_reward=180.80 +/- 253.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=134000, episode_reward=89.80 +/- 182.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=135000, episode_reward=167.20 +/- 240.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=136000, episode_reward=122.60 +/- 157.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=137000, episode_reward=-86.40 +/- 148.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=138000, episode_reward=97.00 +/- 305.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=139000, episode_reward=79.80 +/- 288.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=140000, episode_reward=294.00 +/- 236.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=141000, episode_reward=56.60 +/- 136.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=142000, episode_reward=-148.60 +/- 339.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=143000, episode_reward=164.80 +/- 156.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=144000, episode_reward=160.60 +/- 183.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=145000, episode_reward=137.00 +/- 198.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=146000, episode_reward=121.00 +/- 221.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=147000, episode_reward=239.60 +/- 272.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=148000, episode_reward=309.60 +/- 91.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=149000, episode_reward=52.00 +/- 189.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=150000, episode_reward=349.80 +/- 288.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=151000, episode_reward=208.20 +/- 285.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=152000, episode_reward=506.80 +/- 326.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=153000, episode_reward=285.40 +/- 215.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=154000, episode_reward=475.40 +/- 198.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=155000, episode_reward=288.60 +/- 293.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=156000, episode_reward=219.40 +/- 275.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=157000, episode_reward=335.80 +/- 389.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=158000, episode_reward=371.20 +/- 280.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=159000, episode_reward=265.00 +/- 292.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=160000, episode_reward=360.40 +/- 321.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=161000, episode_reward=454.00 +/- 352.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=162000, episode_reward=124.60 +/- 376.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=163000, episode_reward=503.20 +/- 99.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=164000, episode_reward=352.20 +/- 325.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=165000, episode_reward=429.20 +/- 167.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=166000, episode_reward=233.20 +/- 380.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=167000, episode_reward=660.00 +/- 178.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=168000, episode_reward=669.00 +/- 122.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=169000, episode_reward=358.60 +/- 96.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=170000, episode_reward=380.20 +/- 305.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=171000, episode_reward=582.00 +/- 236.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=172000, episode_reward=486.20 +/- 349.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=173000, episode_reward=315.40 +/- 363.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=174000, episode_reward=763.20 +/- 109.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=175000, episode_reward=472.20 +/- 293.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=176000, episode_reward=301.60 +/- 414.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=177000, episode_reward=666.40 +/- 409.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=178000, episode_reward=501.20 +/- 320.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=179000, episode_reward=552.40 +/- 382.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=180000, episode_reward=490.40 +/- 162.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=181000, episode_reward=606.40 +/- 467.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=182000, episode_reward=492.00 +/- 427.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=183000, episode_reward=466.20 +/- 252.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=184000, episode_reward=678.20 +/- 303.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=185000, episode_reward=596.00 +/- 424.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=186000, episode_reward=474.20 +/- 186.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=187000, episode_reward=535.20 +/- 301.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=188000, episode_reward=362.20 +/- 297.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=189000, episode_reward=816.00 +/- 390.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=190000, episode_reward=378.00 +/- 269.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=191000, episode_reward=340.00 +/- 380.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=192000, episode_reward=375.20 +/- 299.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=193000, episode_reward=620.60 +/- 271.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=194000, episode_reward=707.60 +/- 330.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=195000, episode_reward=831.60 +/- 206.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=196000, episode_reward=690.00 +/- 343.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=197000, episode_reward=684.20 +/- 373.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=198000, episode_reward=812.40 +/- 210.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=199000, episode_reward=519.20 +/- 253.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=200000, episode_reward=675.20 +/- 188.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=201000, episode_reward=548.00 +/- 180.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=202000, episode_reward=401.40 +/- 382.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=203000, episode_reward=530.20 +/- 317.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=204000, episode_reward=751.20 +/- 175.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=205000, episode_reward=371.80 +/- 319.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=206000, episode_reward=777.20 +/- 200.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=207000, episode_reward=890.60 +/- 323.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=208000, episode_reward=619.20 +/- 323.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=209000, episode_reward=-1490.60 +/- 4509.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=210000, episode_reward=690.20 +/- 461.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=211000, episode_reward=473.80 +/- 201.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=212000, episode_reward=933.80 +/- 217.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=213000, episode_reward=783.20 +/- 431.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=214000, episode_reward=837.00 +/- 209.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=215000, episode_reward=967.00 +/- 169.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=216000, episode_reward=716.00 +/- 430.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=217000, episode_reward=407.00 +/- 268.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=218000, episode_reward=827.20 +/- 188.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=219000, episode_reward=712.20 +/- 313.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=220000, episode_reward=652.40 +/- 542.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=221000, episode_reward=797.80 +/- 205.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=222000, episode_reward=544.40 +/- 59.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=223000, episode_reward=613.80 +/- 242.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=224000, episode_reward=656.00 +/- 364.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=225000, episode_reward=589.80 +/- 242.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=226000, episode_reward=794.60 +/- 458.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=227000, episode_reward=902.60 +/- 269.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=228000, episode_reward=726.00 +/- 313.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=229000, episode_reward=833.40 +/- 170.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=230000, episode_reward=1118.80 +/- 130.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=231000, episode_reward=889.20 +/- 385.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=232000, episode_reward=706.60 +/- 166.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=233000, episode_reward=1108.80 +/- 147.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=234000, episode_reward=411.60 +/- 180.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=235000, episode_reward=705.80 +/- 372.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=236000, episode_reward=909.60 +/- 296.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=237000, episode_reward=716.40 +/- 463.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=238000, episode_reward=1228.40 +/- 255.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=239000, episode_reward=934.80 +/- 289.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=240000, episode_reward=817.00 +/- 362.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=241000, episode_reward=538.40 +/- 365.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=242000, episode_reward=640.80 +/- 220.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=243000, episode_reward=774.00 +/- 321.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=244000, episode_reward=783.00 +/- 228.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=245000, episode_reward=611.00 +/- 451.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=246000, episode_reward=997.80 +/- 258.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=247000, episode_reward=-601.00 +/- 2755.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=248000, episode_reward=696.00 +/- 289.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=249000, episode_reward=848.40 +/- 299.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=250000, episode_reward=885.60 +/- 318.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=251000, episode_reward=726.60 +/- 144.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=252000, episode_reward=964.00 +/- 102.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=253000, episode_reward=807.00 +/- 279.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=254000, episode_reward=521.40 +/- 142.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=255000, episode_reward=836.20 +/- 286.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=256000, episode_reward=744.00 +/- 154.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=257000, episode_reward=-2019.80 +/- 5484.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=258000, episode_reward=832.20 +/- 428.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=259000, episode_reward=781.60 +/- 338.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=260000, episode_reward=947.20 +/- 263.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=261000, episode_reward=811.60 +/- 187.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=262000, episode_reward=688.40 +/- 406.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=263000, episode_reward=741.20 +/- 413.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=264000, episode_reward=697.40 +/- 264.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=265000, episode_reward=986.40 +/- 342.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=266000, episode_reward=777.40 +/- 182.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=267000, episode_reward=618.20 +/- 223.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=268000, episode_reward=678.80 +/- 329.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=269000, episode_reward=590.60 +/- 137.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=270000, episode_reward=863.40 +/- 243.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=271000, episode_reward=943.20 +/- 284.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=272000, episode_reward=711.00 +/- 221.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=273000, episode_reward=914.80 +/- 430.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=274000, episode_reward=873.80 +/- 156.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=275000, episode_reward=536.80 +/- 233.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=276000, episode_reward=594.40 +/- 205.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=277000, episode_reward=929.00 +/- 220.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=278000, episode_reward=802.00 +/- 192.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=279000, episode_reward=731.60 +/- 361.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=280000, episode_reward=404.20 +/- 345.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=281000, episode_reward=61.20 +/- 1540.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=282000, episode_reward=767.20 +/- 240.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=283000, episode_reward=866.00 +/- 273.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=284000, episode_reward=-1968.80 +/- 5167.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=285000, episode_reward=890.40 +/- 344.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=286000, episode_reward=1040.20 +/- 81.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=287000, episode_reward=712.40 +/- 366.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=288000, episode_reward=888.40 +/- 519.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=289000, episode_reward=857.80 +/- 342.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=290000, episode_reward=809.80 +/- 249.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=291000, episode_reward=827.80 +/- 284.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=292000, episode_reward=976.00 +/- 298.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=293000, episode_reward=1013.00 +/- 256.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=294000, episode_reward=787.40 +/- 207.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=295000, episode_reward=970.80 +/- 230.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=296000, episode_reward=731.60 +/- 229.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=297000, episode_reward=697.40 +/- 194.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=298000, episode_reward=506.40 +/- 224.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=299000, episode_reward=598.60 +/- 381.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=300000, episode_reward=988.80 +/- 105.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=301000, episode_reward=952.40 +/- 353.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=302000, episode_reward=733.80 +/- 439.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=303000, episode_reward=823.60 +/- 275.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=304000, episode_reward=618.20 +/- 121.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=305000, episode_reward=977.60 +/- 280.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=306000, episode_reward=1011.00 +/- 240.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=307000, episode_reward=936.20 +/- 148.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=308000, episode_reward=666.20 +/- 306.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=309000, episode_reward=916.00 +/- 399.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=310000, episode_reward=849.00 +/- 229.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=311000, episode_reward=876.80 +/- 209.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=312000, episode_reward=631.20 +/- 307.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=313000, episode_reward=812.00 +/- 140.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=314000, episode_reward=563.40 +/- 368.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=315000, episode_reward=926.20 +/- 314.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=316000, episode_reward=873.80 +/- 265.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=317000, episode_reward=930.80 +/- 112.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=318000, episode_reward=830.40 +/- 422.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=319000, episode_reward=982.40 +/- 299.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=320000, episode_reward=970.20 +/- 249.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=321000, episode_reward=874.40 +/- 399.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=322000, episode_reward=687.40 +/- 386.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=323000, episode_reward=730.80 +/- 202.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=324000, episode_reward=1056.20 +/- 257.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=325000, episode_reward=863.60 +/- 382.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=326000, episode_reward=966.20 +/- 317.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=327000, episode_reward=1105.20 +/- 241.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=328000, episode_reward=866.20 +/- 366.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=329000, episode_reward=743.80 +/- 323.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=330000, episode_reward=874.20 +/- 250.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=331000, episode_reward=745.60 +/- 229.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=332000, episode_reward=876.80 +/- 163.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=333000, episode_reward=1131.40 +/- 262.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=334000, episode_reward=865.20 +/- 157.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=335000, episode_reward=964.80 +/- 437.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=336000, episode_reward=667.60 +/- 356.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=337000, episode_reward=986.60 +/- 481.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=338000, episode_reward=527.80 +/- 298.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=339000, episode_reward=835.00 +/- 193.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=340000, episode_reward=873.60 +/- 242.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=341000, episode_reward=-1277.60 +/- 4250.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=342000, episode_reward=804.00 +/- 217.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=343000, episode_reward=1052.60 +/- 228.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=344000, episode_reward=665.60 +/- 361.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=345000, episode_reward=777.20 +/- 317.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=346000, episode_reward=1009.60 +/- 83.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=347000, episode_reward=860.80 +/- 328.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=348000, episode_reward=878.40 +/- 140.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=349000, episode_reward=839.00 +/- 344.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=350000, episode_reward=935.80 +/- 230.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=351000, episode_reward=1186.60 +/- 316.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=352000, episode_reward=653.20 +/- 175.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=353000, episode_reward=818.00 +/- 275.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=354000, episode_reward=746.40 +/- 243.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=355000, episode_reward=896.80 +/- 302.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=356000, episode_reward=1004.40 +/- 195.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=357000, episode_reward=1095.20 +/- 345.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=358000, episode_reward=735.20 +/- 242.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=359000, episode_reward=996.20 +/- 239.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=360000, episode_reward=870.00 +/- 297.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=361000, episode_reward=998.40 +/- 284.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=362000, episode_reward=624.20 +/- 348.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=363000, episode_reward=1124.60 +/- 241.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=364000, episode_reward=736.40 +/- 182.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=365000, episode_reward=709.00 +/- 325.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=366000, episode_reward=456.20 +/- 337.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=367000, episode_reward=758.00 +/- 300.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=368000, episode_reward=-1725.20 +/- 5575.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=369000, episode_reward=703.20 +/- 118.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=370000, episode_reward=756.60 +/- 409.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=371000, episode_reward=659.80 +/- 242.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=372000, episode_reward=961.00 +/- 179.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=373000, episode_reward=748.20 +/- 327.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=374000, episode_reward=834.40 +/- 106.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=375000, episode_reward=633.60 +/- 538.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=376000, episode_reward=791.60 +/- 333.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=377000, episode_reward=1002.60 +/- 374.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=378000, episode_reward=870.80 +/- 343.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=379000, episode_reward=1161.20 +/- 214.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=380000, episode_reward=740.40 +/- 320.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=381000, episode_reward=576.40 +/- 206.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=382000, episode_reward=968.20 +/- 334.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=383000, episode_reward=731.20 +/- 239.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=384000, episode_reward=630.80 +/- 294.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=385000, episode_reward=931.60 +/- 221.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=386000, episode_reward=718.60 +/- 93.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=387000, episode_reward=908.80 +/- 140.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=388000, episode_reward=706.60 +/- 276.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=389000, episode_reward=906.20 +/- 371.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=390000, episode_reward=567.00 +/- 250.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=391000, episode_reward=850.80 +/- 244.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=392000, episode_reward=651.60 +/- 402.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=393000, episode_reward=762.40 +/- 274.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=394000, episode_reward=-2795.00 +/- 6974.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=395000, episode_reward=620.40 +/- 251.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=396000, episode_reward=450.20 +/- 763.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=397000, episode_reward=715.60 +/- 357.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=398000, episode_reward=837.20 +/- 119.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=399000, episode_reward=870.00 +/- 343.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=400000, episode_reward=706.00 +/- 275.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=401000, episode_reward=751.00 +/- 320.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=402000, episode_reward=280.20 +/- 382.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=403000, episode_reward=721.40 +/- 444.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=404000, episode_reward=814.20 +/- 317.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=405000, episode_reward=622.00 +/- 119.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=406000, episode_reward=862.00 +/- 353.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=407000, episode_reward=1064.60 +/- 351.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=408000, episode_reward=895.80 +/- 239.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=409000, episode_reward=931.80 +/- 119.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=410000, episode_reward=910.00 +/- 201.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=411000, episode_reward=862.20 +/- 258.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=412000, episode_reward=732.80 +/- 133.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=413000, episode_reward=785.00 +/- 132.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=414000, episode_reward=957.60 +/- 202.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=415000, episode_reward=906.40 +/- 406.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=416000, episode_reward=981.20 +/- 210.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=417000, episode_reward=723.60 +/- 345.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=418000, episode_reward=873.60 +/- 458.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=419000, episode_reward=730.80 +/- 148.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=420000, episode_reward=787.00 +/- 424.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=421000, episode_reward=635.20 +/- 226.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=422000, episode_reward=902.40 +/- 379.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=423000, episode_reward=972.20 +/- 252.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=424000, episode_reward=-1442.60 +/- 4424.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=425000, episode_reward=689.80 +/- 277.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=426000, episode_reward=875.00 +/- 242.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=427000, episode_reward=886.60 +/- 268.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=428000, episode_reward=831.40 +/- 103.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=429000, episode_reward=844.20 +/- 400.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=430000, episode_reward=965.60 +/- 298.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=431000, episode_reward=1177.80 +/- 244.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=432000, episode_reward=659.80 +/- 835.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=433000, episode_reward=927.20 +/- 369.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=434000, episode_reward=947.60 +/- 284.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=435000, episode_reward=849.60 +/- 443.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=436000, episode_reward=-1893.00 +/- 5582.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=437000, episode_reward=912.00 +/- 421.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=438000, episode_reward=970.60 +/- 369.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=439000, episode_reward=504.00 +/- 375.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=440000, episode_reward=657.80 +/- 396.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=441000, episode_reward=-1153.20 +/- 4012.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=442000, episode_reward=633.40 +/- 309.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=443000, episode_reward=810.80 +/- 187.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=444000, episode_reward=903.60 +/- 106.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=445000, episode_reward=739.40 +/- 446.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=446000, episode_reward=993.60 +/- 391.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=447000, episode_reward=788.00 +/- 292.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=448000, episode_reward=848.60 +/- 204.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=449000, episode_reward=668.20 +/- 151.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=450000, episode_reward=880.00 +/- 223.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=451000, episode_reward=988.80 +/- 183.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=452000, episode_reward=895.00 +/- 346.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=453000, episode_reward=1026.60 +/- 268.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=454000, episode_reward=840.60 +/- 374.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=455000, episode_reward=798.40 +/- 236.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=456000, episode_reward=847.40 +/- 311.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=457000, episode_reward=675.60 +/- 240.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=458000, episode_reward=827.60 +/- 409.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=459000, episode_reward=845.00 +/- 244.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=460000, episode_reward=782.20 +/- 234.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=461000, episode_reward=983.60 +/- 141.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=462000, episode_reward=824.40 +/- 451.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=463000, episode_reward=778.00 +/- 191.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=464000, episode_reward=823.00 +/- 289.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=465000, episode_reward=699.40 +/- 209.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=466000, episode_reward=654.20 +/- 252.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=467000, episode_reward=1045.60 +/- 268.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=468000, episode_reward=625.40 +/- 170.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=469000, episode_reward=784.20 +/- 118.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=470000, episode_reward=933.80 +/- 310.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=471000, episode_reward=895.60 +/- 312.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=472000, episode_reward=664.00 +/- 388.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=473000, episode_reward=868.80 +/- 332.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=474000, episode_reward=843.40 +/- 245.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=475000, episode_reward=616.60 +/- 367.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=476000, episode_reward=1010.00 +/- 391.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=477000, episode_reward=1146.60 +/- 338.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=478000, episode_reward=617.00 +/- 376.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=479000, episode_reward=959.20 +/- 301.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=480000, episode_reward=869.80 +/- 188.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=481000, episode_reward=1226.60 +/- 401.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=482000, episode_reward=911.00 +/- 256.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=483000, episode_reward=842.80 +/- 268.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=484000, episode_reward=700.40 +/- 272.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=485000, episode_reward=1185.60 +/- 169.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=486000, episode_reward=1030.60 +/- 293.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=487000, episode_reward=-1595.80 +/- 3973.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=488000, episode_reward=847.20 +/- 173.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=489000, episode_reward=1182.20 +/- 277.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=490000, episode_reward=826.00 +/- 315.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=491000, episode_reward=888.20 +/- 260.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=492000, episode_reward=875.20 +/- 283.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=493000, episode_reward=967.00 +/- 304.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=494000, episode_reward=914.80 +/- 184.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=495000, episode_reward=913.00 +/- 225.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=496000, episode_reward=805.60 +/- 253.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=497000, episode_reward=1212.80 +/- 308.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=498000, episode_reward=742.00 +/- 277.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=499000, episode_reward=575.40 +/- 260.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=500000, episode_reward=1152.60 +/- 404.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Gamma = 0.9, Learning Rate = 0.0003, Activation Function = Tanh, Neurons = 64, Process noise = 0.1, Measurement noise = 0.0\n",
      "Eval num_timesteps=1000, episode_reward=-3446.00 +/- 26.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=-3474.00 +/- 94.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=3000, episode_reward=-3513.00 +/- 63.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=4000, episode_reward=-3493.00 +/- 81.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=5000, episode_reward=-3477.00 +/- 48.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=6000, episode_reward=-3470.00 +/- 100.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=7000, episode_reward=-3488.00 +/- 109.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=8000, episode_reward=-3521.00 +/- 87.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=9000, episode_reward=-3528.00 +/- 77.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=10000, episode_reward=-3593.00 +/- 129.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=11000, episode_reward=-3537.00 +/- 79.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=12000, episode_reward=-3454.00 +/- 59.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=13000, episode_reward=-3504.00 +/- 69.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=14000, episode_reward=-3466.00 +/- 98.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=15000, episode_reward=-3503.00 +/- 73.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=16000, episode_reward=-3468.00 +/- 85.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=17000, episode_reward=-3503.00 +/- 78.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=18000, episode_reward=-3486.00 +/- 52.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=19000, episode_reward=-3468.00 +/- 48.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=20000, episode_reward=-3496.00 +/- 106.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=21000, episode_reward=-3488.00 +/- 50.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=22000, episode_reward=-3494.00 +/- 71.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=23000, episode_reward=-3503.00 +/- 64.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=24000, episode_reward=-3485.00 +/- 87.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=25000, episode_reward=-3473.00 +/- 26.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=26000, episode_reward=-3457.00 +/- 60.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=27000, episode_reward=-3475.00 +/- 59.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=28000, episode_reward=-3512.00 +/- 61.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=29000, episode_reward=-3478.00 +/- 52.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=30000, episode_reward=-3483.00 +/- 96.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=31000, episode_reward=-3464.00 +/- 73.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=32000, episode_reward=-3471.00 +/- 35.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=33000, episode_reward=-3510.00 +/- 67.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=34000, episode_reward=-3463.00 +/- 68.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=35000, episode_reward=-3476.00 +/- 104.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=36000, episode_reward=-3513.00 +/- 34.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=37000, episode_reward=-3538.00 +/- 42.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=38000, episode_reward=-3491.00 +/- 36.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=39000, episode_reward=-3429.00 +/- 87.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-3436.00 +/- 49.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=41000, episode_reward=-3459.00 +/- 86.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=42000, episode_reward=-3503.00 +/- 30.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=43000, episode_reward=-3417.00 +/- 37.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=44000, episode_reward=-3565.00 +/- 112.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=45000, episode_reward=-3519.00 +/- 54.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=46000, episode_reward=-3515.00 +/- 62.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=47000, episode_reward=-3502.00 +/- 105.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=48000, episode_reward=-3436.00 +/- 77.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=49000, episode_reward=-3491.00 +/- 32.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=50000, episode_reward=-3472.00 +/- 46.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=51000, episode_reward=-6394.60 +/- 1673.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=52000, episode_reward=-440.20 +/- 696.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=53000, episode_reward=134.80 +/- 265.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=54000, episode_reward=269.40 +/- 248.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=55000, episode_reward=159.00 +/- 154.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=56000, episode_reward=190.60 +/- 382.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=57000, episode_reward=9.20 +/- 423.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=58000, episode_reward=-38.20 +/- 159.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=59000, episode_reward=118.60 +/- 305.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=60000, episode_reward=42.20 +/- 379.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=61000, episode_reward=-297.80 +/- 369.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=62000, episode_reward=240.80 +/- 195.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=63000, episode_reward=-169.60 +/- 327.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=64000, episode_reward=45.00 +/- 320.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=65000, episode_reward=84.40 +/- 194.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=66000, episode_reward=-122.40 +/- 324.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=67000, episode_reward=155.00 +/- 316.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=68000, episode_reward=-15.20 +/- 158.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=69000, episode_reward=166.20 +/- 312.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=70000, episode_reward=84.40 +/- 95.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=71000, episode_reward=19.80 +/- 107.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=72000, episode_reward=272.40 +/- 103.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=73000, episode_reward=61.80 +/- 207.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=74000, episode_reward=127.40 +/- 284.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=75000, episode_reward=148.60 +/- 312.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=76000, episode_reward=163.40 +/- 235.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=77000, episode_reward=34.60 +/- 189.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=78000, episode_reward=78.00 +/- 194.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=79000, episode_reward=109.00 +/- 358.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=80000, episode_reward=166.40 +/- 201.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=81000, episode_reward=199.80 +/- 366.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=82000, episode_reward=2.40 +/- 140.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=83000, episode_reward=126.00 +/- 192.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=84000, episode_reward=266.60 +/- 163.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=85000, episode_reward=-24.40 +/- 186.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=86000, episode_reward=182.00 +/- 119.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=87000, episode_reward=-67.00 +/- 136.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=88000, episode_reward=196.40 +/- 274.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=89000, episode_reward=46.00 +/- 198.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=90000, episode_reward=77.00 +/- 180.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=91000, episode_reward=95.80 +/- 191.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=92000, episode_reward=69.20 +/- 252.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=93000, episode_reward=192.40 +/- 207.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=94000, episode_reward=175.20 +/- 219.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=95000, episode_reward=157.40 +/- 256.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=96000, episode_reward=265.80 +/- 289.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=97000, episode_reward=198.20 +/- 101.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=98000, episode_reward=331.40 +/- 146.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=99000, episode_reward=292.80 +/- 270.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=100000, episode_reward=250.80 +/- 208.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=101000, episode_reward=270.40 +/- 243.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=102000, episode_reward=261.60 +/- 223.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=103000, episode_reward=319.60 +/- 218.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=104000, episode_reward=199.00 +/- 319.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=105000, episode_reward=332.60 +/- 185.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=106000, episode_reward=120.60 +/- 225.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=107000, episode_reward=328.40 +/- 44.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=108000, episode_reward=245.40 +/- 259.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=109000, episode_reward=370.80 +/- 180.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=110000, episode_reward=227.60 +/- 203.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=111000, episode_reward=226.80 +/- 182.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=112000, episode_reward=364.00 +/- 245.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=113000, episode_reward=127.00 +/- 185.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=114000, episode_reward=349.40 +/- 218.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=115000, episode_reward=290.20 +/- 112.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=116000, episode_reward=298.60 +/- 73.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=117000, episode_reward=284.40 +/- 323.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=118000, episode_reward=383.20 +/- 112.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=119000, episode_reward=282.80 +/- 317.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=120000, episode_reward=503.60 +/- 179.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=121000, episode_reward=271.80 +/- 217.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=122000, episode_reward=405.20 +/- 355.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=123000, episode_reward=708.00 +/- 323.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=124000, episode_reward=492.00 +/- 407.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=125000, episode_reward=637.80 +/- 392.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=126000, episode_reward=766.60 +/- 135.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=127000, episode_reward=-1564.00 +/- 4377.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=128000, episode_reward=652.00 +/- 360.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=129000, episode_reward=617.40 +/- 270.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=130000, episode_reward=956.60 +/- 253.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=131000, episode_reward=595.40 +/- 128.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=132000, episode_reward=597.40 +/- 263.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=133000, episode_reward=968.20 +/- 194.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=134000, episode_reward=913.40 +/- 202.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=135000, episode_reward=841.80 +/- 281.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=136000, episode_reward=653.40 +/- 107.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=137000, episode_reward=998.80 +/- 409.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=138000, episode_reward=970.00 +/- 151.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=139000, episode_reward=797.60 +/- 290.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=140000, episode_reward=1297.60 +/- 159.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=141000, episode_reward=760.40 +/- 283.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=142000, episode_reward=-1501.60 +/- 2962.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=143000, episode_reward=765.40 +/- 418.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=144000, episode_reward=884.40 +/- 394.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=145000, episode_reward=-153.60 +/- 2298.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=146000, episode_reward=933.00 +/- 408.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=147000, episode_reward=902.40 +/- 178.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=148000, episode_reward=1042.60 +/- 361.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=149000, episode_reward=878.20 +/- 139.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=150000, episode_reward=722.80 +/- 367.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=151000, episode_reward=244.80 +/- 1504.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=152000, episode_reward=1064.00 +/- 219.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=153000, episode_reward=1061.60 +/- 266.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=154000, episode_reward=1052.00 +/- 297.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=155000, episode_reward=827.80 +/- 269.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=156000, episode_reward=815.80 +/- 324.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=157000, episode_reward=854.40 +/- 416.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=158000, episode_reward=883.80 +/- 109.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=159000, episode_reward=746.80 +/- 324.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=160000, episode_reward=971.00 +/- 262.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=161000, episode_reward=1023.60 +/- 157.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=162000, episode_reward=1048.40 +/- 98.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=163000, episode_reward=770.20 +/- 177.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=164000, episode_reward=1038.80 +/- 246.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=165000, episode_reward=1117.60 +/- 290.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=166000, episode_reward=890.80 +/- 210.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=167000, episode_reward=822.40 +/- 154.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=168000, episode_reward=899.20 +/- 302.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=169000, episode_reward=1176.40 +/- 288.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=170000, episode_reward=888.40 +/- 207.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=171000, episode_reward=645.00 +/- 243.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=172000, episode_reward=829.60 +/- 407.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=173000, episode_reward=-1223.80 +/- 5188.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=174000, episode_reward=-1183.20 +/- 4022.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=175000, episode_reward=1112.60 +/- 307.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=176000, episode_reward=1034.20 +/- 161.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=177000, episode_reward=894.80 +/- 152.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=178000, episode_reward=885.40 +/- 277.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=179000, episode_reward=1056.80 +/- 227.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=180000, episode_reward=1127.60 +/- 362.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=181000, episode_reward=1157.20 +/- 203.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=182000, episode_reward=918.80 +/- 271.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=183000, episode_reward=923.40 +/- 249.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=184000, episode_reward=690.40 +/- 279.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=185000, episode_reward=924.40 +/- 329.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=186000, episode_reward=905.40 +/- 301.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=187000, episode_reward=1038.80 +/- 279.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=188000, episode_reward=-1324.40 +/- 4521.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=189000, episode_reward=1080.40 +/- 248.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=190000, episode_reward=1324.60 +/- 129.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=191000, episode_reward=1144.20 +/- 220.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=192000, episode_reward=1091.80 +/- 243.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=193000, episode_reward=1347.60 +/- 300.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=194000, episode_reward=1083.60 +/- 166.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=195000, episode_reward=1048.80 +/- 197.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=196000, episode_reward=1032.20 +/- 220.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=197000, episode_reward=1075.00 +/- 268.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=198000, episode_reward=1031.60 +/- 175.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=199000, episode_reward=724.80 +/- 292.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=200000, episode_reward=1082.80 +/- 270.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=201000, episode_reward=1135.40 +/- 259.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=202000, episode_reward=1260.80 +/- 266.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=203000, episode_reward=1123.00 +/- 234.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=204000, episode_reward=696.60 +/- 479.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=205000, episode_reward=1090.60 +/- 345.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=206000, episode_reward=1041.00 +/- 257.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=207000, episode_reward=937.80 +/- 307.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=208000, episode_reward=1148.40 +/- 199.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=209000, episode_reward=1185.20 +/- 365.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=210000, episode_reward=1048.40 +/- 166.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=211000, episode_reward=1117.80 +/- 149.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=212000, episode_reward=929.80 +/- 209.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=213000, episode_reward=1085.40 +/- 459.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=214000, episode_reward=1098.80 +/- 213.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=215000, episode_reward=1202.60 +/- 315.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=216000, episode_reward=1091.60 +/- 279.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=217000, episode_reward=1218.60 +/- 136.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=218000, episode_reward=1212.00 +/- 204.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=219000, episode_reward=1116.60 +/- 207.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=220000, episode_reward=954.40 +/- 107.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=221000, episode_reward=1040.40 +/- 297.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=222000, episode_reward=1115.40 +/- 288.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=223000, episode_reward=747.60 +/- 138.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=224000, episode_reward=222.40 +/- 2328.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=225000, episode_reward=958.60 +/- 148.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=226000, episode_reward=1149.20 +/- 222.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=227000, episode_reward=849.80 +/- 331.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=228000, episode_reward=966.00 +/- 100.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=229000, episode_reward=526.20 +/- 366.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=230000, episode_reward=894.60 +/- 375.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=231000, episode_reward=798.60 +/- 270.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=232000, episode_reward=1067.00 +/- 380.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=233000, episode_reward=964.80 +/- 287.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=234000, episode_reward=833.40 +/- 194.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=235000, episode_reward=880.40 +/- 368.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=236000, episode_reward=996.20 +/- 140.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=237000, episode_reward=1133.60 +/- 280.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=238000, episode_reward=1175.60 +/- 313.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=239000, episode_reward=711.00 +/- 422.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=240000, episode_reward=1089.80 +/- 294.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=241000, episode_reward=804.80 +/- 299.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=242000, episode_reward=1288.80 +/- 238.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=243000, episode_reward=957.20 +/- 295.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=244000, episode_reward=1115.20 +/- 267.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=245000, episode_reward=1174.00 +/- 117.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=246000, episode_reward=1157.20 +/- 297.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=247000, episode_reward=1249.40 +/- 188.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=248000, episode_reward=1015.80 +/- 100.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=249000, episode_reward=1150.80 +/- 284.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=250000, episode_reward=1040.00 +/- 275.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=251000, episode_reward=1003.60 +/- 197.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=252000, episode_reward=1204.20 +/- 355.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=253000, episode_reward=884.60 +/- 325.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=254000, episode_reward=859.20 +/- 166.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=255000, episode_reward=850.00 +/- 162.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=256000, episode_reward=689.20 +/- 336.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=257000, episode_reward=678.00 +/- 388.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=258000, episode_reward=921.80 +/- 132.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=259000, episode_reward=1253.40 +/- 78.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=260000, episode_reward=1025.80 +/- 218.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=261000, episode_reward=895.80 +/- 285.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=262000, episode_reward=862.40 +/- 134.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=263000, episode_reward=818.60 +/- 328.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=264000, episode_reward=822.20 +/- 202.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=265000, episode_reward=855.00 +/- 209.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=266000, episode_reward=980.60 +/- 167.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=267000, episode_reward=887.80 +/- 411.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=268000, episode_reward=1022.00 +/- 118.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=269000, episode_reward=1005.00 +/- 272.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=270000, episode_reward=999.40 +/- 298.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=271000, episode_reward=1127.80 +/- 387.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=272000, episode_reward=766.80 +/- 182.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=273000, episode_reward=974.60 +/- 142.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=274000, episode_reward=768.20 +/- 195.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=275000, episode_reward=1192.40 +/- 352.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=276000, episode_reward=1314.00 +/- 247.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=277000, episode_reward=1232.40 +/- 333.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=278000, episode_reward=849.00 +/- 306.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=279000, episode_reward=1004.60 +/- 202.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=280000, episode_reward=1028.40 +/- 296.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=281000, episode_reward=962.00 +/- 142.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=282000, episode_reward=950.00 +/- 261.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=283000, episode_reward=803.60 +/- 233.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=284000, episode_reward=1100.80 +/- 219.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=285000, episode_reward=771.60 +/- 358.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=286000, episode_reward=836.00 +/- 172.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=287000, episode_reward=968.00 +/- 216.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=288000, episode_reward=745.40 +/- 294.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=289000, episode_reward=984.00 +/- 283.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=290000, episode_reward=-2295.20 +/- 6714.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=291000, episode_reward=1219.60 +/- 192.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=292000, episode_reward=1196.40 +/- 293.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=293000, episode_reward=1114.20 +/- 300.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=294000, episode_reward=1008.20 +/- 270.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=295000, episode_reward=1161.80 +/- 188.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=296000, episode_reward=1024.80 +/- 335.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=297000, episode_reward=990.20 +/- 307.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=298000, episode_reward=1025.20 +/- 210.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=299000, episode_reward=983.00 +/- 115.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=300000, episode_reward=882.60 +/- 204.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=301000, episode_reward=891.60 +/- 307.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=302000, episode_reward=1195.80 +/- 130.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=303000, episode_reward=1215.20 +/- 323.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=304000, episode_reward=1125.60 +/- 462.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=305000, episode_reward=871.80 +/- 159.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=306000, episode_reward=1192.80 +/- 331.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=307000, episode_reward=1057.00 +/- 240.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=308000, episode_reward=938.80 +/- 377.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=309000, episode_reward=1107.00 +/- 190.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=310000, episode_reward=1194.80 +/- 203.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=311000, episode_reward=846.40 +/- 121.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=312000, episode_reward=1125.20 +/- 280.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=313000, episode_reward=1236.20 +/- 225.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=314000, episode_reward=706.20 +/- 143.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=315000, episode_reward=1303.40 +/- 265.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=316000, episode_reward=1325.00 +/- 143.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=317000, episode_reward=822.80 +/- 272.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=318000, episode_reward=1132.40 +/- 392.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=319000, episode_reward=954.20 +/- 347.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=320000, episode_reward=903.60 +/- 350.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=321000, episode_reward=912.00 +/- 271.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=322000, episode_reward=1175.80 +/- 221.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=323000, episode_reward=1202.20 +/- 285.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=324000, episode_reward=1063.20 +/- 333.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=325000, episode_reward=1181.40 +/- 263.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=326000, episode_reward=1180.60 +/- 256.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=327000, episode_reward=884.20 +/- 352.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=328000, episode_reward=884.00 +/- 147.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=329000, episode_reward=933.20 +/- 331.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=330000, episode_reward=1069.40 +/- 321.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=331000, episode_reward=-2550.60 +/- 6981.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=332000, episode_reward=1222.00 +/- 131.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=333000, episode_reward=1017.00 +/- 133.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=334000, episode_reward=828.80 +/- 359.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=335000, episode_reward=912.40 +/- 319.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=336000, episode_reward=993.20 +/- 319.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=337000, episode_reward=939.00 +/- 203.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=338000, episode_reward=889.40 +/- 136.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=339000, episode_reward=941.00 +/- 228.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=340000, episode_reward=926.40 +/- 101.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=341000, episode_reward=1068.20 +/- 284.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=342000, episode_reward=826.00 +/- 315.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=343000, episode_reward=1202.00 +/- 302.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=344000, episode_reward=1053.20 +/- 237.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=345000, episode_reward=1031.60 +/- 298.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=346000, episode_reward=1129.60 +/- 210.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=347000, episode_reward=990.00 +/- 364.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=348000, episode_reward=744.40 +/- 192.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=349000, episode_reward=-1225.00 +/- 4454.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=350000, episode_reward=1096.60 +/- 256.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=351000, episode_reward=1015.00 +/- 475.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=352000, episode_reward=1027.20 +/- 311.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=353000, episode_reward=1002.80 +/- 279.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=354000, episode_reward=917.20 +/- 235.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=355000, episode_reward=777.20 +/- 219.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=356000, episode_reward=1110.00 +/- 199.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=357000, episode_reward=899.60 +/- 253.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=358000, episode_reward=1257.40 +/- 339.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=359000, episode_reward=923.80 +/- 242.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=360000, episode_reward=834.20 +/- 296.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=361000, episode_reward=1107.00 +/- 165.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=362000, episode_reward=1066.20 +/- 124.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=363000, episode_reward=974.00 +/- 224.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=364000, episode_reward=1195.00 +/- 200.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=365000, episode_reward=1012.20 +/- 279.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=366000, episode_reward=875.40 +/- 140.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=367000, episode_reward=990.20 +/- 314.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=368000, episode_reward=1010.20 +/- 104.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=369000, episode_reward=-2831.00 +/- 7453.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=370000, episode_reward=977.40 +/- 355.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=371000, episode_reward=1078.60 +/- 240.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=372000, episode_reward=1258.40 +/- 238.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=373000, episode_reward=1004.60 +/- 291.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=374000, episode_reward=1043.20 +/- 333.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=375000, episode_reward=1170.00 +/- 192.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=376000, episode_reward=924.00 +/- 325.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=377000, episode_reward=1018.20 +/- 199.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=378000, episode_reward=-1891.20 +/- 5609.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=379000, episode_reward=640.20 +/- 161.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=380000, episode_reward=991.20 +/- 424.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=381000, episode_reward=1079.80 +/- 207.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=382000, episode_reward=1160.60 +/- 370.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=383000, episode_reward=994.20 +/- 172.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=384000, episode_reward=993.80 +/- 322.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=385000, episode_reward=942.20 +/- 215.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=386000, episode_reward=725.40 +/- 111.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=387000, episode_reward=911.80 +/- 285.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=388000, episode_reward=764.00 +/- 281.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=389000, episode_reward=692.40 +/- 282.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=390000, episode_reward=1061.40 +/- 267.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=391000, episode_reward=972.60 +/- 232.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=392000, episode_reward=950.00 +/- 99.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=393000, episode_reward=1042.80 +/- 288.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=394000, episode_reward=952.80 +/- 291.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=395000, episode_reward=984.80 +/- 298.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=396000, episode_reward=1186.80 +/- 124.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=397000, episode_reward=956.20 +/- 169.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=398000, episode_reward=1133.00 +/- 173.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=399000, episode_reward=917.40 +/- 210.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=400000, episode_reward=1011.20 +/- 302.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=401000, episode_reward=1126.00 +/- 117.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=402000, episode_reward=1028.20 +/- 229.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=403000, episode_reward=1055.60 +/- 222.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=404000, episode_reward=1168.00 +/- 295.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=405000, episode_reward=1126.00 +/- 238.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=406000, episode_reward=1099.80 +/- 147.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=407000, episode_reward=1173.60 +/- 287.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=408000, episode_reward=921.40 +/- 291.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=409000, episode_reward=983.20 +/- 183.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=410000, episode_reward=1363.20 +/- 232.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=411000, episode_reward=1034.40 +/- 298.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=412000, episode_reward=1301.40 +/- 257.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=413000, episode_reward=1093.00 +/- 271.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=414000, episode_reward=1177.40 +/- 128.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=415000, episode_reward=995.80 +/- 329.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=416000, episode_reward=1213.80 +/- 204.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=417000, episode_reward=1157.40 +/- 225.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=418000, episode_reward=1147.20 +/- 191.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=419000, episode_reward=958.60 +/- 162.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=420000, episode_reward=1155.40 +/- 292.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=421000, episode_reward=933.40 +/- 410.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=422000, episode_reward=954.40 +/- 521.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=423000, episode_reward=1012.20 +/- 171.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=424000, episode_reward=1120.80 +/- 166.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=425000, episode_reward=812.20 +/- 268.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=426000, episode_reward=824.20 +/- 396.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=427000, episode_reward=1169.80 +/- 169.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=428000, episode_reward=935.20 +/- 265.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=429000, episode_reward=1127.00 +/- 322.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=430000, episode_reward=809.20 +/- 490.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=431000, episode_reward=967.60 +/- 309.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=432000, episode_reward=1207.20 +/- 408.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=433000, episode_reward=924.00 +/- 335.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=434000, episode_reward=1173.00 +/- 357.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=435000, episode_reward=1287.00 +/- 203.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=436000, episode_reward=698.40 +/- 78.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=437000, episode_reward=983.20 +/- 381.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=438000, episode_reward=921.80 +/- 139.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=439000, episode_reward=1080.60 +/- 299.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=440000, episode_reward=1015.60 +/- 195.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=441000, episode_reward=969.60 +/- 318.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=442000, episode_reward=1163.80 +/- 274.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=443000, episode_reward=932.40 +/- 316.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=444000, episode_reward=1192.20 +/- 215.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=445000, episode_reward=1189.60 +/- 243.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=446000, episode_reward=950.20 +/- 131.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=447000, episode_reward=1242.00 +/- 308.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=448000, episode_reward=1314.00 +/- 277.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=449000, episode_reward=1065.60 +/- 132.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=450000, episode_reward=1043.80 +/- 339.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=451000, episode_reward=1113.00 +/- 323.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=452000, episode_reward=942.40 +/- 261.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=453000, episode_reward=1275.60 +/- 99.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=454000, episode_reward=1257.60 +/- 256.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=455000, episode_reward=998.80 +/- 254.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=456000, episode_reward=1068.80 +/- 198.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=457000, episode_reward=901.60 +/- 179.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=458000, episode_reward=887.40 +/- 80.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=459000, episode_reward=1064.80 +/- 206.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=460000, episode_reward=1005.20 +/- 529.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=461000, episode_reward=846.40 +/- 288.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=462000, episode_reward=1007.20 +/- 217.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=463000, episode_reward=1043.80 +/- 196.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=464000, episode_reward=1162.00 +/- 381.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=465000, episode_reward=950.00 +/- 355.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=466000, episode_reward=1107.80 +/- 269.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=467000, episode_reward=1103.00 +/- 140.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=468000, episode_reward=1142.00 +/- 168.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=469000, episode_reward=906.60 +/- 285.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=470000, episode_reward=817.20 +/- 206.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=471000, episode_reward=1201.60 +/- 144.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=472000, episode_reward=1019.60 +/- 197.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=473000, episode_reward=1072.60 +/- 111.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=474000, episode_reward=1073.80 +/- 305.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=475000, episode_reward=951.60 +/- 310.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=476000, episode_reward=1086.60 +/- 482.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=477000, episode_reward=711.20 +/- 193.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=478000, episode_reward=1144.80 +/- 337.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=479000, episode_reward=1175.00 +/- 237.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=480000, episode_reward=943.40 +/- 203.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=481000, episode_reward=1138.80 +/- 147.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=482000, episode_reward=963.40 +/- 223.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=483000, episode_reward=1093.80 +/- 477.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=484000, episode_reward=828.40 +/- 178.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=485000, episode_reward=981.60 +/- 216.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=486000, episode_reward=1096.00 +/- 458.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=487000, episode_reward=958.40 +/- 307.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=488000, episode_reward=1271.40 +/- 364.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=489000, episode_reward=1064.20 +/- 279.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=490000, episode_reward=1231.80 +/- 222.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=491000, episode_reward=877.20 +/- 233.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=492000, episode_reward=1283.00 +/- 293.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=493000, episode_reward=1116.00 +/- 180.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=494000, episode_reward=1452.20 +/- 237.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=495000, episode_reward=1097.80 +/- 380.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=496000, episode_reward=1226.60 +/- 194.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=497000, episode_reward=1312.80 +/- 156.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=498000, episode_reward=803.40 +/- 361.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=499000, episode_reward=1099.80 +/- 334.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=500000, episode_reward=1226.80 +/- 227.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Gamma = 0.9, Learning Rate = 0.0003, Activation Function = ReLU, Neurons = 8, Process noise = 0.1, Measurement noise = 0.0\n",
      "Eval num_timesteps=1000, episode_reward=31.60 +/- 153.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=62.00 +/- 169.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3000, episode_reward=206.00 +/- 42.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4000, episode_reward=100.60 +/- 153.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=5000, episode_reward=1.60 +/- 151.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=6000, episode_reward=53.40 +/- 135.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=7000, episode_reward=118.80 +/- 114.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=8000, episode_reward=230.20 +/- 64.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=9000, episode_reward=-26.40 +/- 156.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=10000, episode_reward=120.60 +/- 249.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=11000, episode_reward=162.40 +/- 32.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=12000, episode_reward=45.40 +/- 145.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=13000, episode_reward=24.60 +/- 169.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=14000, episode_reward=-40.80 +/- 119.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=15000, episode_reward=27.00 +/- 200.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=16000, episode_reward=-7.00 +/- 176.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=17000, episode_reward=121.00 +/- 118.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=18000, episode_reward=103.00 +/- 163.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=19000, episode_reward=56.20 +/- 112.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=20000, episode_reward=-32.80 +/- 174.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=21000, episode_reward=57.40 +/- 159.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=22000, episode_reward=46.60 +/- 134.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=23000, episode_reward=-117.60 +/- 100.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=24000, episode_reward=22.20 +/- 162.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=25000, episode_reward=123.00 +/- 114.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=26000, episode_reward=119.80 +/- 116.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=27000, episode_reward=120.20 +/- 151.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=28000, episode_reward=-9.80 +/- 168.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=29000, episode_reward=70.20 +/- 190.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=30000, episode_reward=-0.20 +/- 126.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=31000, episode_reward=95.60 +/- 139.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=32000, episode_reward=134.20 +/- 171.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=33000, episode_reward=52.60 +/- 187.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=34000, episode_reward=-95.00 +/- 163.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=35000, episode_reward=0.60 +/- 153.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=36000, episode_reward=44.40 +/- 155.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=37000, episode_reward=100.40 +/- 153.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=38000, episode_reward=-95.40 +/- 144.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=39000, episode_reward=-19.60 +/- 131.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=40000, episode_reward=119.40 +/- 93.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=41000, episode_reward=-16.60 +/- 278.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=42000, episode_reward=103.60 +/- 134.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=43000, episode_reward=156.80 +/- 158.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=44000, episode_reward=67.20 +/- 138.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=45000, episode_reward=15.00 +/- 114.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=46000, episode_reward=106.00 +/- 136.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=47000, episode_reward=-77.00 +/- 243.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=48000, episode_reward=102.80 +/- 95.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=49000, episode_reward=16.00 +/- 183.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=50000, episode_reward=80.20 +/- 125.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=51000, episode_reward=56.20 +/- 211.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=52000, episode_reward=67.80 +/- 200.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=53000, episode_reward=59.40 +/- 142.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=54000, episode_reward=-50.60 +/- 223.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=55000, episode_reward=-6.00 +/- 103.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=56000, episode_reward=-156.00 +/- 362.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=57000, episode_reward=-519.60 +/- 1061.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=58000, episode_reward=139.60 +/- 427.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=59000, episode_reward=-6361.60 +/- 825.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=60000, episode_reward=-5099.20 +/- 1936.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=61000, episode_reward=-5135.80 +/- 2172.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=62000, episode_reward=-2176.20 +/- 412.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=63000, episode_reward=-1347.00 +/- 228.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=64000, episode_reward=-978.80 +/- 344.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=65000, episode_reward=-1089.60 +/- 311.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=66000, episode_reward=-1331.00 +/- 451.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=67000, episode_reward=-1655.80 +/- 384.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=68000, episode_reward=-1564.60 +/- 629.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=69000, episode_reward=-1632.60 +/- 462.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=70000, episode_reward=-1383.40 +/- 572.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=71000, episode_reward=-909.80 +/- 197.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=72000, episode_reward=-669.40 +/- 276.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=73000, episode_reward=-534.40 +/- 135.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=74000, episode_reward=-378.00 +/- 391.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=75000, episode_reward=-261.00 +/- 310.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=76000, episode_reward=-1898.20 +/- 1706.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=77000, episode_reward=-3672.00 +/- 1851.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=78000, episode_reward=-3672.60 +/- 2289.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=79000, episode_reward=-736.60 +/- 438.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=80000, episode_reward=-784.60 +/- 236.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=81000, episode_reward=-424.00 +/- 131.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=82000, episode_reward=-476.20 +/- 246.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=83000, episode_reward=-681.00 +/- 132.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=84000, episode_reward=-520.00 +/- 515.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=85000, episode_reward=-164.20 +/- 237.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=86000, episode_reward=-52.00 +/- 348.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=87000, episode_reward=-160.80 +/- 175.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=88000, episode_reward=-174.20 +/- 248.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=89000, episode_reward=-333.60 +/- 289.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=90000, episode_reward=-316.80 +/- 210.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=91000, episode_reward=-620.60 +/- 257.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=92000, episode_reward=-332.00 +/- 315.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=93000, episode_reward=11.40 +/- 134.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=94000, episode_reward=-325.60 +/- 264.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=95000, episode_reward=-207.00 +/- 124.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=96000, episode_reward=-305.80 +/- 185.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=97000, episode_reward=-92.20 +/- 216.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=98000, episode_reward=-369.80 +/- 242.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=99000, episode_reward=111.40 +/- 242.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=100000, episode_reward=-55.00 +/- 79.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=101000, episode_reward=-221.60 +/- 214.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=102000, episode_reward=-69.00 +/- 224.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=103000, episode_reward=75.60 +/- 320.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=104000, episode_reward=-24.20 +/- 245.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=105000, episode_reward=77.60 +/- 308.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=106000, episode_reward=-30.00 +/- 267.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=107000, episode_reward=38.60 +/- 292.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=108000, episode_reward=-39.60 +/- 65.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=109000, episode_reward=-27.20 +/- 141.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=110000, episode_reward=215.40 +/- 293.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=111000, episode_reward=174.40 +/- 212.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=112000, episode_reward=83.60 +/- 254.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=113000, episode_reward=174.60 +/- 151.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=114000, episode_reward=350.60 +/- 147.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=115000, episode_reward=128.20 +/- 177.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=116000, episode_reward=118.40 +/- 330.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=117000, episode_reward=29.20 +/- 81.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=118000, episode_reward=170.00 +/- 301.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=119000, episode_reward=7.60 +/- 343.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=120000, episode_reward=-55.60 +/- 207.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=121000, episode_reward=215.20 +/- 147.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=122000, episode_reward=-56.00 +/- 152.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=123000, episode_reward=206.80 +/- 142.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=124000, episode_reward=288.00 +/- 94.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=125000, episode_reward=121.80 +/- 199.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=126000, episode_reward=282.40 +/- 156.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=127000, episode_reward=170.20 +/- 173.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=128000, episode_reward=138.00 +/- 169.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=129000, episode_reward=154.40 +/- 254.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=130000, episode_reward=207.80 +/- 123.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=131000, episode_reward=177.40 +/- 189.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=132000, episode_reward=96.00 +/- 367.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=133000, episode_reward=300.00 +/- 129.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=134000, episode_reward=90.00 +/- 250.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=135000, episode_reward=62.40 +/- 259.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=136000, episode_reward=274.20 +/- 166.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=137000, episode_reward=122.80 +/- 215.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=138000, episode_reward=319.60 +/- 184.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=139000, episode_reward=317.20 +/- 219.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=140000, episode_reward=148.40 +/- 163.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=141000, episode_reward=209.60 +/- 138.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=142000, episode_reward=270.20 +/- 228.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=143000, episode_reward=191.40 +/- 274.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=144000, episode_reward=308.60 +/- 72.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=145000, episode_reward=54.20 +/- 117.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=146000, episode_reward=243.00 +/- 275.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=147000, episode_reward=311.00 +/- 253.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=148000, episode_reward=229.80 +/- 150.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=149000, episode_reward=182.00 +/- 235.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=150000, episode_reward=289.80 +/- 152.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=151000, episode_reward=351.00 +/- 65.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=152000, episode_reward=9.00 +/- 256.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=153000, episode_reward=241.20 +/- 206.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=154000, episode_reward=108.00 +/- 49.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=155000, episode_reward=212.80 +/- 198.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=156000, episode_reward=280.40 +/- 134.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=157000, episode_reward=157.00 +/- 224.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=158000, episode_reward=267.20 +/- 179.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=159000, episode_reward=132.60 +/- 190.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=160000, episode_reward=345.40 +/- 185.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=161000, episode_reward=228.60 +/- 163.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=162000, episode_reward=319.60 +/- 193.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=163000, episode_reward=214.00 +/- 180.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=164000, episode_reward=92.20 +/- 298.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=165000, episode_reward=223.20 +/- 236.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=166000, episode_reward=334.40 +/- 201.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=167000, episode_reward=352.00 +/- 237.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=168000, episode_reward=274.20 +/- 206.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=169000, episode_reward=244.60 +/- 194.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=170000, episode_reward=158.00 +/- 295.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=171000, episode_reward=194.40 +/- 179.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=172000, episode_reward=33.20 +/- 175.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=173000, episode_reward=358.20 +/- 134.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=174000, episode_reward=266.20 +/- 96.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=175000, episode_reward=340.00 +/- 225.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=176000, episode_reward=34.20 +/- 100.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=177000, episode_reward=151.00 +/- 226.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=178000, episode_reward=198.40 +/- 157.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=179000, episode_reward=131.40 +/- 336.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=180000, episode_reward=181.40 +/- 269.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=181000, episode_reward=404.00 +/- 98.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=182000, episode_reward=98.20 +/- 63.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=183000, episode_reward=22.60 +/- 261.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=184000, episode_reward=353.20 +/- 121.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=185000, episode_reward=353.40 +/- 113.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=186000, episode_reward=120.20 +/- 177.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=187000, episode_reward=288.20 +/- 79.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=188000, episode_reward=254.80 +/- 115.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=189000, episode_reward=156.20 +/- 231.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=190000, episode_reward=276.80 +/- 169.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=191000, episode_reward=428.80 +/- 209.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=192000, episode_reward=253.20 +/- 186.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=193000, episode_reward=458.80 +/- 83.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=194000, episode_reward=287.60 +/- 222.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=195000, episode_reward=336.00 +/- 208.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=196000, episode_reward=136.60 +/- 235.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=197000, episode_reward=274.00 +/- 200.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=198000, episode_reward=409.40 +/- 86.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=199000, episode_reward=25.60 +/- 171.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=200000, episode_reward=241.00 +/- 113.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=201000, episode_reward=285.20 +/- 119.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=202000, episode_reward=260.60 +/- 126.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=203000, episode_reward=300.00 +/- 37.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=204000, episode_reward=408.60 +/- 140.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=205000, episode_reward=55.40 +/- 272.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=206000, episode_reward=246.80 +/- 221.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=207000, episode_reward=170.80 +/- 179.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=208000, episode_reward=236.80 +/- 153.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=209000, episode_reward=214.80 +/- 139.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=210000, episode_reward=361.40 +/- 153.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=211000, episode_reward=-59.40 +/- 180.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=212000, episode_reward=305.40 +/- 171.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=213000, episode_reward=25.40 +/- 164.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=214000, episode_reward=42.80 +/- 269.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=215000, episode_reward=302.20 +/- 261.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=216000, episode_reward=343.00 +/- 166.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=217000, episode_reward=373.40 +/- 124.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=218000, episode_reward=258.00 +/- 178.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=219000, episode_reward=291.80 +/- 104.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=220000, episode_reward=132.00 +/- 177.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=221000, episode_reward=164.80 +/- 114.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=222000, episode_reward=182.00 +/- 119.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=223000, episode_reward=197.20 +/- 166.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=224000, episode_reward=142.60 +/- 119.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=225000, episode_reward=290.00 +/- 262.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=226000, episode_reward=377.80 +/- 156.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=227000, episode_reward=195.60 +/- 136.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=228000, episode_reward=223.20 +/- 293.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=229000, episode_reward=356.20 +/- 209.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=230000, episode_reward=314.60 +/- 234.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=231000, episode_reward=179.20 +/- 82.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=232000, episode_reward=430.60 +/- 84.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=233000, episode_reward=158.20 +/- 175.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=234000, episode_reward=129.40 +/- 70.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=235000, episode_reward=283.80 +/- 231.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=236000, episode_reward=338.00 +/- 168.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=237000, episode_reward=143.20 +/- 189.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=238000, episode_reward=355.80 +/- 190.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=239000, episode_reward=212.40 +/- 246.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=240000, episode_reward=290.20 +/- 196.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=241000, episode_reward=1.40 +/- 56.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=242000, episode_reward=175.40 +/- 238.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=243000, episode_reward=138.60 +/- 203.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=244000, episode_reward=468.80 +/- 250.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=245000, episode_reward=91.00 +/- 154.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=246000, episode_reward=213.80 +/- 145.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=247000, episode_reward=310.00 +/- 201.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=248000, episode_reward=166.00 +/- 223.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=249000, episode_reward=222.20 +/- 229.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=250000, episode_reward=206.60 +/- 163.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=251000, episode_reward=212.00 +/- 134.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=252000, episode_reward=171.20 +/- 279.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=253000, episode_reward=116.80 +/- 99.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=254000, episode_reward=17.00 +/- 198.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=255000, episode_reward=270.00 +/- 218.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=256000, episode_reward=149.20 +/- 243.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=257000, episode_reward=146.40 +/- 168.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=258000, episode_reward=299.60 +/- 182.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=259000, episode_reward=-93.40 +/- 127.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=260000, episode_reward=133.60 +/- 270.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=261000, episode_reward=392.20 +/- 144.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=262000, episode_reward=163.60 +/- 289.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=263000, episode_reward=152.60 +/- 182.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=264000, episode_reward=97.00 +/- 127.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=265000, episode_reward=252.80 +/- 212.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=266000, episode_reward=184.80 +/- 152.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=267000, episode_reward=79.00 +/- 158.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=268000, episode_reward=136.00 +/- 59.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=269000, episode_reward=92.80 +/- 224.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=270000, episode_reward=228.60 +/- 119.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=271000, episode_reward=109.20 +/- 164.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=272000, episode_reward=270.60 +/- 169.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=273000, episode_reward=202.00 +/- 144.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=274000, episode_reward=202.20 +/- 192.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=275000, episode_reward=342.00 +/- 238.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=276000, episode_reward=188.00 +/- 176.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=277000, episode_reward=32.60 +/- 171.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=278000, episode_reward=433.40 +/- 225.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=279000, episode_reward=144.20 +/- 189.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=280000, episode_reward=152.80 +/- 103.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=281000, episode_reward=369.40 +/- 138.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=282000, episode_reward=105.20 +/- 192.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=283000, episode_reward=379.20 +/- 265.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=284000, episode_reward=20.80 +/- 242.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=285000, episode_reward=396.40 +/- 147.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=286000, episode_reward=300.80 +/- 379.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=287000, episode_reward=91.40 +/- 128.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=288000, episode_reward=123.80 +/- 119.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=289000, episode_reward=325.00 +/- 116.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=290000, episode_reward=248.80 +/- 158.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=291000, episode_reward=128.20 +/- 172.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=292000, episode_reward=240.60 +/- 128.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=293000, episode_reward=165.60 +/- 232.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=294000, episode_reward=210.80 +/- 191.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=295000, episode_reward=242.20 +/- 84.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=296000, episode_reward=390.60 +/- 98.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=297000, episode_reward=196.20 +/- 182.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=298000, episode_reward=243.20 +/- 309.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=299000, episode_reward=152.00 +/- 241.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=300000, episode_reward=221.60 +/- 156.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=301000, episode_reward=310.20 +/- 130.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=302000, episode_reward=213.60 +/- 157.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=303000, episode_reward=286.40 +/- 121.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=304000, episode_reward=270.00 +/- 106.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=305000, episode_reward=190.00 +/- 76.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=306000, episode_reward=330.80 +/- 211.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=307000, episode_reward=348.80 +/- 80.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=308000, episode_reward=434.60 +/- 151.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=309000, episode_reward=319.40 +/- 176.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=310000, episode_reward=346.40 +/- 160.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=311000, episode_reward=302.60 +/- 149.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=312000, episode_reward=163.80 +/- 195.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=313000, episode_reward=167.20 +/- 264.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=314000, episode_reward=114.60 +/- 130.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=315000, episode_reward=151.20 +/- 81.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=316000, episode_reward=300.80 +/- 184.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=317000, episode_reward=271.40 +/- 127.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=318000, episode_reward=207.80 +/- 218.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=319000, episode_reward=76.80 +/- 234.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=320000, episode_reward=102.20 +/- 122.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=321000, episode_reward=201.80 +/- 209.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=322000, episode_reward=-26.60 +/- 329.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=323000, episode_reward=339.00 +/- 118.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=324000, episode_reward=170.40 +/- 94.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=325000, episode_reward=356.60 +/- 165.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=326000, episode_reward=459.40 +/- 242.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=327000, episode_reward=228.20 +/- 261.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=328000, episode_reward=187.20 +/- 146.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=329000, episode_reward=364.80 +/- 241.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=330000, episode_reward=229.80 +/- 208.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=331000, episode_reward=101.60 +/- 152.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=332000, episode_reward=156.20 +/- 469.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=333000, episode_reward=408.80 +/- 168.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=334000, episode_reward=367.80 +/- 222.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=335000, episode_reward=269.80 +/- 349.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=336000, episode_reward=357.60 +/- 156.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=337000, episode_reward=187.00 +/- 132.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=338000, episode_reward=392.80 +/- 127.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=339000, episode_reward=106.00 +/- 213.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=340000, episode_reward=375.80 +/- 215.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=341000, episode_reward=221.00 +/- 138.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=342000, episode_reward=211.60 +/- 231.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=343000, episode_reward=188.20 +/- 161.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=344000, episode_reward=266.40 +/- 247.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=345000, episode_reward=423.40 +/- 146.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=346000, episode_reward=108.20 +/- 328.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=347000, episode_reward=342.60 +/- 163.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=348000, episode_reward=369.60 +/- 231.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=349000, episode_reward=340.60 +/- 280.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=350000, episode_reward=172.40 +/- 307.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=351000, episode_reward=189.00 +/- 235.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=352000, episode_reward=295.40 +/- 225.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=353000, episode_reward=179.00 +/- 236.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=354000, episode_reward=331.60 +/- 221.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=355000, episode_reward=289.80 +/- 202.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=356000, episode_reward=148.20 +/- 180.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=357000, episode_reward=278.00 +/- 164.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=358000, episode_reward=285.20 +/- 333.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=359000, episode_reward=572.60 +/- 250.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=360000, episode_reward=399.60 +/- 176.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=361000, episode_reward=237.80 +/- 250.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=362000, episode_reward=439.60 +/- 170.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=363000, episode_reward=278.20 +/- 100.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=364000, episode_reward=657.00 +/- 99.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=365000, episode_reward=285.00 +/- 209.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=366000, episode_reward=131.60 +/- 197.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=367000, episode_reward=205.60 +/- 101.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=368000, episode_reward=449.00 +/- 146.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=369000, episode_reward=527.00 +/- 346.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=370000, episode_reward=448.80 +/- 229.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=371000, episode_reward=345.00 +/- 232.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=372000, episode_reward=300.20 +/- 158.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=373000, episode_reward=366.60 +/- 193.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=374000, episode_reward=560.40 +/- 178.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=375000, episode_reward=494.40 +/- 145.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=376000, episode_reward=246.60 +/- 211.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=377000, episode_reward=374.80 +/- 272.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=378000, episode_reward=103.80 +/- 372.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=379000, episode_reward=408.80 +/- 218.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=380000, episode_reward=461.80 +/- 233.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=381000, episode_reward=388.80 +/- 97.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=382000, episode_reward=290.40 +/- 135.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=383000, episode_reward=323.60 +/- 306.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=384000, episode_reward=452.80 +/- 327.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=385000, episode_reward=360.20 +/- 317.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=386000, episode_reward=426.20 +/- 297.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=387000, episode_reward=362.20 +/- 180.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=388000, episode_reward=318.60 +/- 200.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=389000, episode_reward=455.60 +/- 251.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=390000, episode_reward=327.60 +/- 309.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=391000, episode_reward=231.80 +/- 360.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=392000, episode_reward=442.20 +/- 87.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=393000, episode_reward=188.80 +/- 119.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=394000, episode_reward=187.00 +/- 172.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=395000, episode_reward=453.40 +/- 341.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=396000, episode_reward=205.00 +/- 303.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=397000, episode_reward=504.40 +/- 234.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=398000, episode_reward=292.40 +/- 361.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=399000, episode_reward=185.80 +/- 168.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=400000, episode_reward=174.80 +/- 286.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=401000, episode_reward=254.20 +/- 277.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=402000, episode_reward=445.40 +/- 278.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=403000, episode_reward=322.60 +/- 232.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=404000, episode_reward=270.60 +/- 287.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=405000, episode_reward=48.20 +/- 659.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=406000, episode_reward=452.40 +/- 130.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=407000, episode_reward=533.00 +/- 323.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=408000, episode_reward=186.40 +/- 206.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=409000, episode_reward=274.00 +/- 235.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=410000, episode_reward=331.60 +/- 189.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=411000, episode_reward=524.80 +/- 267.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=412000, episode_reward=-11.80 +/- 307.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=413000, episode_reward=374.80 +/- 293.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=414000, episode_reward=344.20 +/- 302.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=415000, episode_reward=356.20 +/- 351.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=416000, episode_reward=261.80 +/- 254.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=417000, episode_reward=267.00 +/- 291.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=418000, episode_reward=-250.60 +/- 449.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=419000, episode_reward=221.60 +/- 256.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=420000, episode_reward=357.40 +/- 257.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=421000, episode_reward=248.20 +/- 329.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=422000, episode_reward=282.20 +/- 403.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=423000, episode_reward=84.80 +/- 356.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=424000, episode_reward=307.20 +/- 133.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=425000, episode_reward=404.00 +/- 186.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=426000, episode_reward=-75.80 +/- 576.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=427000, episode_reward=523.20 +/- 110.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=428000, episode_reward=385.60 +/- 246.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=429000, episode_reward=568.60 +/- 353.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=430000, episode_reward=162.60 +/- 271.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=431000, episode_reward=228.40 +/- 383.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=432000, episode_reward=607.60 +/- 170.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=433000, episode_reward=325.80 +/- 172.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=434000, episode_reward=438.00 +/- 149.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=435000, episode_reward=376.00 +/- 113.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=436000, episode_reward=385.40 +/- 262.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=437000, episode_reward=-333.60 +/- 516.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=438000, episode_reward=465.60 +/- 175.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=439000, episode_reward=313.00 +/- 293.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=440000, episode_reward=480.20 +/- 193.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=441000, episode_reward=63.40 +/- 180.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=442000, episode_reward=224.40 +/- 311.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=443000, episode_reward=236.00 +/- 239.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=444000, episode_reward=229.60 +/- 146.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=445000, episode_reward=390.40 +/- 102.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=446000, episode_reward=687.00 +/- 143.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=447000, episode_reward=263.60 +/- 182.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=448000, episode_reward=375.20 +/- 148.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=449000, episode_reward=140.20 +/- 610.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=450000, episode_reward=305.20 +/- 580.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=451000, episode_reward=-35.00 +/- 611.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=452000, episode_reward=466.00 +/- 424.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=453000, episode_reward=128.80 +/- 405.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=454000, episode_reward=491.80 +/- 260.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=455000, episode_reward=432.40 +/- 212.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=456000, episode_reward=443.80 +/- 338.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=457000, episode_reward=100.00 +/- 486.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=458000, episode_reward=338.80 +/- 117.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=459000, episode_reward=393.00 +/- 368.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=460000, episode_reward=484.60 +/- 160.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=461000, episode_reward=143.20 +/- 301.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=462000, episode_reward=147.40 +/- 249.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=463000, episode_reward=-152.20 +/- 364.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=464000, episode_reward=271.40 +/- 200.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=465000, episode_reward=178.40 +/- 355.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=466000, episode_reward=-296.20 +/- 665.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=467000, episode_reward=284.00 +/- 424.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=468000, episode_reward=122.40 +/- 263.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=469000, episode_reward=443.20 +/- 158.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=470000, episode_reward=221.20 +/- 263.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=471000, episode_reward=271.80 +/- 404.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=472000, episode_reward=333.00 +/- 295.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=473000, episode_reward=426.60 +/- 145.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=474000, episode_reward=558.00 +/- 143.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=475000, episode_reward=324.40 +/- 191.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=476000, episode_reward=93.20 +/- 331.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=477000, episode_reward=-376.60 +/- 302.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=478000, episode_reward=445.80 +/- 380.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=479000, episode_reward=283.40 +/- 202.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=480000, episode_reward=621.60 +/- 277.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=481000, episode_reward=171.60 +/- 534.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=482000, episode_reward=400.00 +/- 107.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=483000, episode_reward=167.60 +/- 524.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=484000, episode_reward=306.60 +/- 347.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=485000, episode_reward=215.20 +/- 234.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=486000, episode_reward=401.80 +/- 243.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=487000, episode_reward=180.40 +/- 344.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=488000, episode_reward=44.00 +/- 569.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=489000, episode_reward=93.20 +/- 183.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=490000, episode_reward=300.20 +/- 323.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=491000, episode_reward=128.00 +/- 315.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=492000, episode_reward=-55.00 +/- 488.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=493000, episode_reward=516.80 +/- 272.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=494000, episode_reward=336.60 +/- 280.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=495000, episode_reward=240.00 +/- 153.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=496000, episode_reward=383.20 +/- 368.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=497000, episode_reward=360.80 +/- 285.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=498000, episode_reward=217.40 +/- 319.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=499000, episode_reward=229.60 +/- 531.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=500000, episode_reward=196.20 +/- 360.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Gamma = 0.9, Learning Rate = 0.0003, Activation Function = ReLU, Neurons = 32, Process noise = 0.1, Measurement noise = 0.0\n",
      "Eval num_timesteps=1000, episode_reward=-19027.00 +/- 71.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=-19045.00 +/- 78.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=3000, episode_reward=-19020.00 +/- 74.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4000, episode_reward=-18974.00 +/- 119.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=5000, episode_reward=-18988.00 +/- 82.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=6000, episode_reward=-18991.00 +/- 51.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=7000, episode_reward=-19013.00 +/- 42.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=8000, episode_reward=-18922.00 +/- 36.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=9000, episode_reward=-19038.00 +/- 70.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=10000, episode_reward=-19021.00 +/- 99.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=11000, episode_reward=-18978.00 +/- 96.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=12000, episode_reward=-19114.00 +/- 61.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=13000, episode_reward=-18963.00 +/- 42.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=14000, episode_reward=-19035.00 +/- 89.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=15000, episode_reward=-18936.00 +/- 78.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=16000, episode_reward=-18964.00 +/- 26.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=17000, episode_reward=-19026.00 +/- 72.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=18000, episode_reward=-19000.00 +/- 77.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=19000, episode_reward=-19037.00 +/- 71.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=20000, episode_reward=-19015.00 +/- 97.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=21000, episode_reward=-18952.00 +/- 57.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=22000, episode_reward=-18912.00 +/- 80.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=23000, episode_reward=-18981.00 +/- 92.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=24000, episode_reward=-18948.00 +/- 64.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=25000, episode_reward=-19018.00 +/- 43.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=26000, episode_reward=-19042.00 +/- 73.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=27000, episode_reward=-19019.00 +/- 36.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=28000, episode_reward=-18930.00 +/- 45.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=29000, episode_reward=-19107.00 +/- 48.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=30000, episode_reward=-18973.00 +/- 61.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=31000, episode_reward=-19009.00 +/- 79.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=32000, episode_reward=-19024.00 +/- 63.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=33000, episode_reward=-19059.00 +/- 126.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=34000, episode_reward=-18930.00 +/- 49.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=35000, episode_reward=-19037.00 +/- 62.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=36000, episode_reward=-18963.00 +/- 106.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=37000, episode_reward=-19010.00 +/- 109.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=38000, episode_reward=-19019.00 +/- 81.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=39000, episode_reward=-18976.00 +/- 52.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=40000, episode_reward=-19015.00 +/- 49.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=41000, episode_reward=-18994.00 +/- 111.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=42000, episode_reward=-18968.00 +/- 62.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=43000, episode_reward=-19015.00 +/- 56.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=44000, episode_reward=-19047.00 +/- 48.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=45000, episode_reward=-18974.00 +/- 75.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=46000, episode_reward=-18972.00 +/- 116.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=47000, episode_reward=-18999.00 +/- 83.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=48000, episode_reward=-19058.00 +/- 70.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=49000, episode_reward=-18976.00 +/- 60.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=50000, episode_reward=-18977.00 +/- 66.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=51000, episode_reward=134.20 +/- 96.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=52000, episode_reward=-23.00 +/- 351.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=53000, episode_reward=-4770.00 +/- 1965.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=54000, episode_reward=-281.20 +/- 325.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=55000, episode_reward=121.20 +/- 156.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=56000, episode_reward=191.80 +/- 246.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=57000, episode_reward=383.00 +/- 34.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=58000, episode_reward=276.20 +/- 256.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=59000, episode_reward=359.20 +/- 193.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=60000, episode_reward=403.40 +/- 189.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=61000, episode_reward=347.20 +/- 240.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=62000, episode_reward=216.80 +/- 128.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=63000, episode_reward=317.80 +/- 61.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=64000, episode_reward=333.60 +/- 341.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=65000, episode_reward=207.20 +/- 90.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=66000, episode_reward=245.00 +/- 159.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=67000, episode_reward=303.00 +/- 136.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=68000, episode_reward=405.40 +/- 226.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=69000, episode_reward=315.80 +/- 78.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=70000, episode_reward=373.20 +/- 293.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=71000, episode_reward=291.80 +/- 121.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=72000, episode_reward=13.40 +/- 183.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=73000, episode_reward=227.80 +/- 234.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=74000, episode_reward=364.60 +/- 204.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=75000, episode_reward=170.20 +/- 206.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=76000, episode_reward=205.00 +/- 247.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=77000, episode_reward=467.00 +/- 104.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=78000, episode_reward=256.00 +/- 203.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=79000, episode_reward=294.80 +/- 199.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=80000, episode_reward=241.20 +/- 211.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=81000, episode_reward=301.60 +/- 226.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=82000, episode_reward=333.80 +/- 141.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=83000, episode_reward=197.60 +/- 296.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=84000, episode_reward=190.60 +/- 133.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=85000, episode_reward=267.20 +/- 240.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=86000, episode_reward=336.00 +/- 122.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=87000, episode_reward=157.00 +/- 150.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=88000, episode_reward=211.80 +/- 77.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=89000, episode_reward=375.00 +/- 256.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=90000, episode_reward=293.80 +/- 110.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=91000, episode_reward=264.80 +/- 185.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=92000, episode_reward=346.00 +/- 126.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=93000, episode_reward=261.20 +/- 141.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=94000, episode_reward=354.80 +/- 198.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=95000, episode_reward=458.20 +/- 149.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=96000, episode_reward=310.20 +/- 197.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=97000, episode_reward=213.80 +/- 235.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=98000, episode_reward=246.40 +/- 209.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=99000, episode_reward=189.20 +/- 309.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=100000, episode_reward=295.00 +/- 124.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=101000, episode_reward=180.00 +/- 154.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=102000, episode_reward=302.60 +/- 149.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=103000, episode_reward=212.40 +/- 211.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=104000, episode_reward=420.40 +/- 235.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=105000, episode_reward=416.40 +/- 82.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=106000, episode_reward=310.60 +/- 183.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=107000, episode_reward=180.00 +/- 146.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=108000, episode_reward=134.20 +/- 77.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=109000, episode_reward=356.00 +/- 111.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=110000, episode_reward=391.20 +/- 229.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=111000, episode_reward=498.00 +/- 85.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=112000, episode_reward=361.80 +/- 239.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=113000, episode_reward=367.00 +/- 149.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=114000, episode_reward=424.80 +/- 159.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=115000, episode_reward=139.40 +/- 133.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=116000, episode_reward=351.40 +/- 166.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=117000, episode_reward=357.00 +/- 158.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=118000, episode_reward=485.00 +/- 135.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=119000, episode_reward=348.00 +/- 66.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=120000, episode_reward=532.80 +/- 175.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=121000, episode_reward=455.20 +/- 135.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=122000, episode_reward=325.40 +/- 132.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=123000, episode_reward=396.00 +/- 204.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=124000, episode_reward=252.00 +/- 141.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=125000, episode_reward=186.00 +/- 181.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=126000, episode_reward=347.60 +/- 134.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=127000, episode_reward=221.40 +/- 276.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=128000, episode_reward=244.00 +/- 335.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=129000, episode_reward=483.40 +/- 143.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=130000, episode_reward=260.80 +/- 109.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=131000, episode_reward=456.00 +/- 157.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=132000, episode_reward=248.00 +/- 228.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=133000, episode_reward=445.80 +/- 195.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=134000, episode_reward=260.80 +/- 249.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=135000, episode_reward=446.80 +/- 123.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=136000, episode_reward=496.20 +/- 102.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=137000, episode_reward=410.20 +/- 261.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=138000, episode_reward=265.20 +/- 234.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=139000, episode_reward=341.40 +/- 292.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=140000, episode_reward=306.40 +/- 243.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=141000, episode_reward=157.20 +/- 112.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=142000, episode_reward=362.60 +/- 69.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=143000, episode_reward=417.80 +/- 86.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=144000, episode_reward=364.40 +/- 186.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=145000, episode_reward=265.80 +/- 155.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=146000, episode_reward=443.20 +/- 192.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=147000, episode_reward=411.00 +/- 110.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=148000, episode_reward=290.60 +/- 210.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=149000, episode_reward=352.60 +/- 170.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=150000, episode_reward=421.00 +/- 198.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=151000, episode_reward=298.40 +/- 223.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=152000, episode_reward=308.60 +/- 194.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=153000, episode_reward=305.20 +/- 178.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=154000, episode_reward=361.00 +/- 315.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=155000, episode_reward=369.60 +/- 77.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=156000, episode_reward=339.00 +/- 101.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=157000, episode_reward=271.60 +/- 128.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=158000, episode_reward=263.20 +/- 202.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=159000, episode_reward=568.80 +/- 111.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=160000, episode_reward=417.40 +/- 173.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=161000, episode_reward=216.60 +/- 188.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=162000, episode_reward=445.20 +/- 92.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=163000, episode_reward=359.60 +/- 174.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=164000, episode_reward=403.00 +/- 122.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=165000, episode_reward=242.00 +/- 156.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=166000, episode_reward=336.80 +/- 125.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=167000, episode_reward=390.60 +/- 211.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=168000, episode_reward=455.00 +/- 246.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=169000, episode_reward=374.40 +/- 202.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=170000, episode_reward=293.80 +/- 184.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=171000, episode_reward=201.00 +/- 214.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=172000, episode_reward=299.40 +/- 185.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=173000, episode_reward=353.80 +/- 87.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=174000, episode_reward=384.20 +/- 164.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=175000, episode_reward=216.40 +/- 188.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=176000, episode_reward=385.60 +/- 165.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=177000, episode_reward=410.00 +/- 120.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=178000, episode_reward=330.40 +/- 309.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=179000, episode_reward=445.60 +/- 153.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=180000, episode_reward=507.60 +/- 98.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=181000, episode_reward=202.20 +/- 157.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=182000, episode_reward=414.60 +/- 268.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=183000, episode_reward=259.20 +/- 161.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=184000, episode_reward=382.00 +/- 144.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=185000, episode_reward=436.00 +/- 192.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=186000, episode_reward=384.20 +/- 120.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=187000, episode_reward=303.60 +/- 147.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=188000, episode_reward=315.20 +/- 175.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=189000, episode_reward=451.20 +/- 211.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=190000, episode_reward=417.60 +/- 126.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=191000, episode_reward=294.40 +/- 175.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=192000, episode_reward=360.20 +/- 225.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=193000, episode_reward=371.60 +/- 131.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=194000, episode_reward=335.60 +/- 211.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=195000, episode_reward=370.60 +/- 117.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=196000, episode_reward=216.80 +/- 294.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=197000, episode_reward=506.80 +/- 198.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=198000, episode_reward=383.60 +/- 235.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=199000, episode_reward=413.40 +/- 113.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=200000, episode_reward=374.60 +/- 116.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=201000, episode_reward=369.00 +/- 189.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=202000, episode_reward=289.00 +/- 169.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=203000, episode_reward=289.20 +/- 163.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=204000, episode_reward=354.20 +/- 110.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=205000, episode_reward=139.20 +/- 248.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=206000, episode_reward=491.80 +/- 132.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=207000, episode_reward=359.00 +/- 95.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=208000, episode_reward=344.20 +/- 162.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=209000, episode_reward=361.00 +/- 114.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=210000, episode_reward=232.00 +/- 239.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=211000, episode_reward=317.00 +/- 145.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=212000, episode_reward=485.80 +/- 32.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=213000, episode_reward=234.20 +/- 212.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=214000, episode_reward=449.80 +/- 160.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=215000, episode_reward=503.80 +/- 105.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=216000, episode_reward=344.60 +/- 153.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=217000, episode_reward=595.00 +/- 204.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=218000, episode_reward=501.80 +/- 85.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=219000, episode_reward=307.20 +/- 135.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=220000, episode_reward=429.40 +/- 170.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=221000, episode_reward=271.80 +/- 130.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=222000, episode_reward=426.40 +/- 206.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=223000, episode_reward=204.60 +/- 186.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=224000, episode_reward=416.40 +/- 224.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=225000, episode_reward=417.40 +/- 243.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=226000, episode_reward=391.80 +/- 77.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=227000, episode_reward=355.60 +/- 85.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=228000, episode_reward=431.60 +/- 220.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=229000, episode_reward=372.80 +/- 219.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=230000, episode_reward=346.20 +/- 147.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=231000, episode_reward=317.40 +/- 197.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=232000, episode_reward=217.00 +/- 162.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=233000, episode_reward=426.00 +/- 228.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=234000, episode_reward=492.40 +/- 254.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=235000, episode_reward=366.40 +/- 144.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=236000, episode_reward=307.00 +/- 163.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=237000, episode_reward=297.80 +/- 222.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=238000, episode_reward=308.80 +/- 313.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=239000, episode_reward=351.40 +/- 107.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=240000, episode_reward=284.00 +/- 105.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=241000, episode_reward=418.60 +/- 180.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=242000, episode_reward=351.00 +/- 162.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=243000, episode_reward=522.20 +/- 185.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=244000, episode_reward=285.60 +/- 238.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=245000, episode_reward=448.60 +/- 101.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=246000, episode_reward=471.80 +/- 142.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=247000, episode_reward=276.60 +/- 76.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=248000, episode_reward=247.60 +/- 94.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=249000, episode_reward=401.40 +/- 135.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=250000, episode_reward=399.60 +/- 111.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=251000, episode_reward=590.80 +/- 110.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=252000, episode_reward=330.20 +/- 118.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=253000, episode_reward=480.60 +/- 267.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=254000, episode_reward=311.20 +/- 155.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=255000, episode_reward=428.40 +/- 138.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=256000, episode_reward=327.60 +/- 153.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=257000, episode_reward=295.80 +/- 250.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=258000, episode_reward=388.60 +/- 126.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=259000, episode_reward=548.40 +/- 111.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=260000, episode_reward=439.60 +/- 251.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=261000, episode_reward=172.20 +/- 118.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=262000, episode_reward=272.40 +/- 183.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=263000, episode_reward=322.60 +/- 250.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=264000, episode_reward=234.60 +/- 297.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=265000, episode_reward=287.80 +/- 107.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=266000, episode_reward=211.60 +/- 323.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=267000, episode_reward=468.60 +/- 69.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=268000, episode_reward=368.40 +/- 241.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=269000, episode_reward=405.20 +/- 125.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=270000, episode_reward=283.80 +/- 185.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=271000, episode_reward=449.60 +/- 117.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=272000, episode_reward=500.80 +/- 129.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=273000, episode_reward=361.80 +/- 138.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=274000, episode_reward=229.20 +/- 288.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=275000, episode_reward=338.20 +/- 144.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=276000, episode_reward=299.00 +/- 147.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=277000, episode_reward=189.00 +/- 232.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=278000, episode_reward=462.40 +/- 121.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=279000, episode_reward=249.80 +/- 99.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=280000, episode_reward=416.20 +/- 147.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=281000, episode_reward=323.40 +/- 195.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=282000, episode_reward=510.40 +/- 68.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=283000, episode_reward=298.80 +/- 70.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=284000, episode_reward=386.00 +/- 208.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=285000, episode_reward=403.40 +/- 292.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=286000, episode_reward=440.20 +/- 241.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=287000, episode_reward=493.40 +/- 120.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=288000, episode_reward=158.60 +/- 150.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=289000, episode_reward=353.00 +/- 185.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=290000, episode_reward=331.40 +/- 48.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=291000, episode_reward=248.60 +/- 134.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=292000, episode_reward=175.40 +/- 207.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=293000, episode_reward=534.20 +/- 77.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=294000, episode_reward=388.40 +/- 200.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=295000, episode_reward=508.20 +/- 50.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=296000, episode_reward=328.40 +/- 155.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=297000, episode_reward=389.40 +/- 125.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=298000, episode_reward=356.80 +/- 132.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=299000, episode_reward=435.40 +/- 197.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=300000, episode_reward=338.20 +/- 233.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=301000, episode_reward=331.20 +/- 299.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=302000, episode_reward=532.00 +/- 254.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=303000, episode_reward=489.60 +/- 225.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=304000, episode_reward=438.40 +/- 220.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=305000, episode_reward=377.00 +/- 207.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=306000, episode_reward=422.80 +/- 313.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=307000, episode_reward=248.00 +/- 195.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=308000, episode_reward=349.40 +/- 138.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=309000, episode_reward=318.40 +/- 160.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=310000, episode_reward=433.00 +/- 262.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=311000, episode_reward=426.00 +/- 117.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=312000, episode_reward=373.60 +/- 110.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=313000, episode_reward=292.80 +/- 137.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=314000, episode_reward=229.20 +/- 173.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=315000, episode_reward=437.00 +/- 162.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=316000, episode_reward=514.00 +/- 115.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=317000, episode_reward=209.60 +/- 327.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=318000, episode_reward=368.20 +/- 154.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=319000, episode_reward=346.80 +/- 122.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=320000, episode_reward=441.80 +/- 180.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=321000, episode_reward=417.60 +/- 96.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=322000, episode_reward=428.80 +/- 106.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=323000, episode_reward=527.60 +/- 132.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=324000, episode_reward=369.40 +/- 149.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=325000, episode_reward=349.00 +/- 144.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=326000, episode_reward=374.20 +/- 171.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=327000, episode_reward=455.40 +/- 249.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=328000, episode_reward=468.80 +/- 195.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=329000, episode_reward=442.60 +/- 145.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=330000, episode_reward=216.60 +/- 161.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=331000, episode_reward=372.20 +/- 166.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=332000, episode_reward=306.60 +/- 148.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=333000, episode_reward=223.40 +/- 221.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=334000, episode_reward=385.00 +/- 55.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=335000, episode_reward=498.40 +/- 171.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=336000, episode_reward=516.60 +/- 211.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=337000, episode_reward=324.40 +/- 256.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=338000, episode_reward=403.00 +/- 89.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=339000, episode_reward=309.40 +/- 266.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=340000, episode_reward=492.00 +/- 240.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=341000, episode_reward=472.20 +/- 103.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=342000, episode_reward=348.40 +/- 297.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=343000, episode_reward=392.40 +/- 246.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=344000, episode_reward=442.80 +/- 170.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=345000, episode_reward=343.40 +/- 267.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=346000, episode_reward=343.80 +/- 176.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=347000, episode_reward=337.40 +/- 97.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=348000, episode_reward=292.60 +/- 154.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=349000, episode_reward=392.40 +/- 83.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=350000, episode_reward=341.40 +/- 234.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=351000, episode_reward=282.00 +/- 234.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=352000, episode_reward=478.00 +/- 125.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=353000, episode_reward=499.60 +/- 193.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=354000, episode_reward=350.00 +/- 241.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=355000, episode_reward=316.20 +/- 163.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=356000, episode_reward=256.00 +/- 215.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=357000, episode_reward=305.40 +/- 132.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=358000, episode_reward=407.00 +/- 100.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=359000, episode_reward=327.80 +/- 100.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=360000, episode_reward=420.20 +/- 128.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=361000, episode_reward=466.00 +/- 223.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=362000, episode_reward=388.60 +/- 144.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=363000, episode_reward=415.40 +/- 109.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=364000, episode_reward=340.00 +/- 222.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=365000, episode_reward=447.00 +/- 138.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=366000, episode_reward=340.00 +/- 160.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=367000, episode_reward=468.00 +/- 196.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=368000, episode_reward=366.40 +/- 228.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=369000, episode_reward=293.00 +/- 260.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=370000, episode_reward=442.60 +/- 108.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=371000, episode_reward=545.80 +/- 198.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=372000, episode_reward=462.20 +/- 285.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=373000, episode_reward=430.20 +/- 105.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=374000, episode_reward=243.00 +/- 247.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=375000, episode_reward=393.60 +/- 62.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=376000, episode_reward=370.60 +/- 292.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=377000, episode_reward=354.60 +/- 164.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=378000, episode_reward=387.60 +/- 89.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=379000, episode_reward=329.80 +/- 106.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=380000, episode_reward=311.60 +/- 136.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=381000, episode_reward=366.20 +/- 155.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=382000, episode_reward=213.80 +/- 50.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=383000, episode_reward=518.00 +/- 236.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=384000, episode_reward=409.40 +/- 180.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=385000, episode_reward=385.20 +/- 177.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=386000, episode_reward=411.80 +/- 91.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=387000, episode_reward=511.80 +/- 231.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=388000, episode_reward=318.40 +/- 331.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=389000, episode_reward=544.20 +/- 193.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=390000, episode_reward=619.00 +/- 122.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=391000, episode_reward=387.00 +/- 142.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=392000, episode_reward=320.20 +/- 192.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=393000, episode_reward=287.60 +/- 244.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=394000, episode_reward=403.00 +/- 130.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=395000, episode_reward=395.80 +/- 222.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=396000, episode_reward=451.40 +/- 180.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=397000, episode_reward=529.20 +/- 148.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=398000, episode_reward=271.60 +/- 261.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=399000, episode_reward=314.00 +/- 275.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=400000, episode_reward=374.80 +/- 166.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=401000, episode_reward=462.80 +/- 137.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=402000, episode_reward=244.80 +/- 165.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=403000, episode_reward=459.80 +/- 67.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=404000, episode_reward=215.20 +/- 104.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=405000, episode_reward=283.80 +/- 211.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=406000, episode_reward=499.40 +/- 168.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=407000, episode_reward=346.00 +/- 365.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=408000, episode_reward=425.20 +/- 144.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=409000, episode_reward=368.20 +/- 272.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=410000, episode_reward=599.00 +/- 166.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=411000, episode_reward=507.20 +/- 158.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=412000, episode_reward=441.40 +/- 89.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=413000, episode_reward=365.60 +/- 86.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=414000, episode_reward=231.60 +/- 215.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=415000, episode_reward=388.00 +/- 69.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=416000, episode_reward=280.80 +/- 118.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=417000, episode_reward=501.20 +/- 80.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=418000, episode_reward=371.60 +/- 153.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=419000, episode_reward=444.60 +/- 90.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=420000, episode_reward=297.00 +/- 211.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=421000, episode_reward=439.60 +/- 132.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=422000, episode_reward=285.20 +/- 279.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=423000, episode_reward=444.00 +/- 243.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=424000, episode_reward=507.40 +/- 220.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=425000, episode_reward=365.80 +/- 91.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=426000, episode_reward=292.00 +/- 153.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=427000, episode_reward=356.60 +/- 206.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=428000, episode_reward=487.60 +/- 68.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=429000, episode_reward=223.80 +/- 339.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=430000, episode_reward=366.00 +/- 84.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=431000, episode_reward=444.60 +/- 214.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=432000, episode_reward=438.20 +/- 140.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=433000, episode_reward=414.00 +/- 134.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=434000, episode_reward=414.40 +/- 185.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=435000, episode_reward=405.80 +/- 140.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=436000, episode_reward=514.00 +/- 296.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=437000, episode_reward=299.80 +/- 169.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=438000, episode_reward=358.00 +/- 113.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=439000, episode_reward=406.40 +/- 130.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=440000, episode_reward=518.00 +/- 113.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=441000, episode_reward=367.00 +/- 155.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=442000, episode_reward=462.00 +/- 139.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=443000, episode_reward=435.20 +/- 159.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=444000, episode_reward=235.40 +/- 147.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=445000, episode_reward=469.20 +/- 143.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=446000, episode_reward=474.60 +/- 227.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=447000, episode_reward=430.40 +/- 192.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=448000, episode_reward=396.80 +/- 204.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=449000, episode_reward=428.00 +/- 88.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=450000, episode_reward=425.60 +/- 202.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=451000, episode_reward=482.40 +/- 163.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=452000, episode_reward=528.40 +/- 206.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=453000, episode_reward=511.20 +/- 188.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=454000, episode_reward=295.60 +/- 137.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=455000, episode_reward=245.80 +/- 64.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=456000, episode_reward=434.40 +/- 66.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=457000, episode_reward=628.40 +/- 49.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=458000, episode_reward=525.60 +/- 198.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=459000, episode_reward=385.80 +/- 128.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=460000, episode_reward=269.20 +/- 222.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=461000, episode_reward=288.20 +/- 160.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=462000, episode_reward=569.40 +/- 280.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=463000, episode_reward=388.80 +/- 62.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=464000, episode_reward=349.00 +/- 126.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=465000, episode_reward=451.00 +/- 187.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=466000, episode_reward=389.20 +/- 199.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=467000, episode_reward=361.20 +/- 196.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=468000, episode_reward=433.60 +/- 225.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=469000, episode_reward=285.60 +/- 135.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=470000, episode_reward=387.00 +/- 146.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=471000, episode_reward=432.00 +/- 199.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=472000, episode_reward=317.80 +/- 92.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=473000, episode_reward=92.20 +/- 216.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=474000, episode_reward=399.40 +/- 82.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=475000, episode_reward=308.80 +/- 238.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=476000, episode_reward=394.00 +/- 152.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=477000, episode_reward=423.40 +/- 191.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=478000, episode_reward=371.60 +/- 121.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=479000, episode_reward=432.20 +/- 222.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=480000, episode_reward=256.80 +/- 124.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=481000, episode_reward=417.20 +/- 337.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=482000, episode_reward=213.00 +/- 254.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=483000, episode_reward=459.60 +/- 183.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=484000, episode_reward=230.80 +/- 199.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=485000, episode_reward=223.60 +/- 130.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=486000, episode_reward=334.40 +/- 183.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=487000, episode_reward=622.40 +/- 163.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=488000, episode_reward=398.40 +/- 199.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=489000, episode_reward=380.40 +/- 225.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=490000, episode_reward=364.00 +/- 100.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=491000, episode_reward=401.40 +/- 119.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=492000, episode_reward=242.00 +/- 231.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=493000, episode_reward=366.00 +/- 67.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=494000, episode_reward=346.80 +/- 193.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=495000, episode_reward=235.40 +/- 132.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=496000, episode_reward=352.80 +/- 129.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=497000, episode_reward=263.20 +/- 202.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=498000, episode_reward=414.00 +/- 72.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=499000, episode_reward=295.80 +/- 142.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=500000, episode_reward=355.00 +/- 114.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Gamma = 0.9, Learning Rate = 0.0003, Activation Function = ReLU, Neurons = 64, Process noise = 0.1, Measurement noise = 0.0\n",
      "Eval num_timesteps=1000, episode_reward=23.60 +/- 94.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=-191.40 +/- 188.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=3000, episode_reward=-26.40 +/- 48.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=4000, episode_reward=-115.40 +/- 114.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=5000, episode_reward=-48.40 +/- 169.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=6000, episode_reward=-88.60 +/- 61.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=7000, episode_reward=35.00 +/- 136.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=8000, episode_reward=69.80 +/- 159.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=9000, episode_reward=-84.40 +/- 240.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=10000, episode_reward=-90.40 +/- 154.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=11000, episode_reward=-113.80 +/- 148.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=12000, episode_reward=-63.60 +/- 71.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=13000, episode_reward=165.00 +/- 176.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=14000, episode_reward=-45.40 +/- 232.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=15000, episode_reward=-19.20 +/- 54.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=16000, episode_reward=-28.20 +/- 85.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=17000, episode_reward=-166.40 +/- 158.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=18000, episode_reward=16.00 +/- 238.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=19000, episode_reward=-31.80 +/- 132.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=20000, episode_reward=-27.80 +/- 136.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=21000, episode_reward=-94.80 +/- 87.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=22000, episode_reward=12.40 +/- 75.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=23000, episode_reward=-14.60 +/- 93.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=24000, episode_reward=-76.80 +/- 142.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=25000, episode_reward=16.60 +/- 70.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=26000, episode_reward=-15.60 +/- 114.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=27000, episode_reward=46.40 +/- 134.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=28000, episode_reward=-46.80 +/- 125.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=29000, episode_reward=-60.80 +/- 175.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=30000, episode_reward=6.20 +/- 33.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=31000, episode_reward=-51.00 +/- 71.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=32000, episode_reward=-81.80 +/- 173.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=33000, episode_reward=-52.60 +/- 116.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=34000, episode_reward=-92.20 +/- 135.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=35000, episode_reward=-73.80 +/- 66.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=36000, episode_reward=12.20 +/- 79.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=37000, episode_reward=-103.20 +/- 131.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=38000, episode_reward=-14.80 +/- 185.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=39000, episode_reward=-57.00 +/- 44.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=40000, episode_reward=8.40 +/- 133.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=41000, episode_reward=-67.80 +/- 267.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=42000, episode_reward=-106.40 +/- 120.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=43000, episode_reward=-71.60 +/- 204.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=44000, episode_reward=68.40 +/- 91.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=45000, episode_reward=-119.60 +/- 212.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=46000, episode_reward=-113.20 +/- 131.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=47000, episode_reward=-80.60 +/- 134.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=48000, episode_reward=-120.00 +/- 157.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=49000, episode_reward=94.60 +/- 120.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=50000, episode_reward=-60.80 +/- 129.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=51000, episode_reward=-89.60 +/- 278.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=52000, episode_reward=-6549.40 +/- 1754.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=53000, episode_reward=118.60 +/- 185.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=54000, episode_reward=417.80 +/- 349.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=55000, episode_reward=308.40 +/- 245.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=56000, episode_reward=485.60 +/- 234.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=57000, episode_reward=193.20 +/- 286.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=58000, episode_reward=271.20 +/- 74.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=59000, episode_reward=155.60 +/- 205.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=60000, episode_reward=251.00 +/- 298.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=61000, episode_reward=149.40 +/- 144.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=62000, episode_reward=37.60 +/- 170.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=63000, episode_reward=216.20 +/- 97.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=64000, episode_reward=235.80 +/- 180.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=65000, episode_reward=227.00 +/- 282.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=66000, episode_reward=84.80 +/- 89.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=67000, episode_reward=304.00 +/- 153.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=68000, episode_reward=139.60 +/- 182.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=69000, episode_reward=442.80 +/- 188.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=70000, episode_reward=238.40 +/- 216.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=71000, episode_reward=321.20 +/- 255.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=72000, episode_reward=191.20 +/- 166.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=73000, episode_reward=293.80 +/- 182.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=74000, episode_reward=65.40 +/- 155.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=75000, episode_reward=239.20 +/- 200.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=76000, episode_reward=358.60 +/- 254.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=77000, episode_reward=11.40 +/- 202.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=78000, episode_reward=248.20 +/- 137.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=79000, episode_reward=262.60 +/- 124.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=80000, episode_reward=526.20 +/- 175.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=81000, episode_reward=192.80 +/- 151.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=82000, episode_reward=147.40 +/- 178.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=83000, episode_reward=245.80 +/- 223.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=84000, episode_reward=498.20 +/- 217.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=85000, episode_reward=252.20 +/- 164.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=86000, episode_reward=315.40 +/- 232.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=87000, episode_reward=330.80 +/- 79.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=88000, episode_reward=265.00 +/- 92.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=89000, episode_reward=144.60 +/- 134.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=90000, episode_reward=156.80 +/- 286.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=91000, episode_reward=353.60 +/- 255.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=92000, episode_reward=252.40 +/- 259.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=93000, episode_reward=128.80 +/- 298.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=94000, episode_reward=370.60 +/- 152.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=95000, episode_reward=337.40 +/- 271.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=96000, episode_reward=331.00 +/- 108.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=97000, episode_reward=378.60 +/- 147.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=98000, episode_reward=277.00 +/- 125.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=99000, episode_reward=227.80 +/- 127.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=100000, episode_reward=472.60 +/- 352.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=101000, episode_reward=238.40 +/- 106.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=102000, episode_reward=312.80 +/- 150.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=103000, episode_reward=447.20 +/- 298.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=104000, episode_reward=345.20 +/- 129.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=105000, episode_reward=369.60 +/- 145.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=106000, episode_reward=336.20 +/- 141.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=107000, episode_reward=327.40 +/- 126.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=108000, episode_reward=239.60 +/- 296.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=109000, episode_reward=250.20 +/- 228.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=110000, episode_reward=396.20 +/- 333.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=111000, episode_reward=581.80 +/- 65.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=112000, episode_reward=388.80 +/- 210.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=113000, episode_reward=429.60 +/- 127.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=114000, episode_reward=244.00 +/- 239.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=115000, episode_reward=486.80 +/- 135.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=116000, episode_reward=400.80 +/- 87.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=117000, episode_reward=441.20 +/- 67.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=118000, episode_reward=315.20 +/- 303.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=119000, episode_reward=468.40 +/- 197.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=120000, episode_reward=396.00 +/- 140.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=121000, episode_reward=443.40 +/- 139.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=122000, episode_reward=286.60 +/- 108.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=123000, episode_reward=509.20 +/- 170.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=124000, episode_reward=304.80 +/- 267.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=125000, episode_reward=337.80 +/- 148.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=126000, episode_reward=385.40 +/- 129.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=127000, episode_reward=309.80 +/- 226.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=128000, episode_reward=402.80 +/- 235.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=129000, episode_reward=242.40 +/- 142.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=130000, episode_reward=324.20 +/- 136.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=131000, episode_reward=181.60 +/- 195.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=132000, episode_reward=370.40 +/- 170.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=133000, episode_reward=232.60 +/- 214.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=134000, episode_reward=453.80 +/- 140.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=135000, episode_reward=379.40 +/- 263.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=136000, episode_reward=353.40 +/- 138.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=137000, episode_reward=378.60 +/- 159.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=138000, episode_reward=354.80 +/- 55.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=139000, episode_reward=334.80 +/- 253.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=140000, episode_reward=496.00 +/- 131.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=141000, episode_reward=529.00 +/- 73.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=142000, episode_reward=278.40 +/- 121.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=143000, episode_reward=316.00 +/- 164.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=144000, episode_reward=193.60 +/- 238.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=145000, episode_reward=448.60 +/- 230.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=146000, episode_reward=383.40 +/- 167.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=147000, episode_reward=268.20 +/- 88.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=148000, episode_reward=236.40 +/- 274.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=149000, episode_reward=342.40 +/- 213.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=150000, episode_reward=405.80 +/- 198.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=151000, episode_reward=454.40 +/- 117.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=152000, episode_reward=516.80 +/- 59.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=153000, episode_reward=360.20 +/- 164.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=154000, episode_reward=328.00 +/- 190.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=155000, episode_reward=367.80 +/- 151.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=156000, episode_reward=240.40 +/- 179.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=157000, episode_reward=325.20 +/- 210.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=158000, episode_reward=498.60 +/- 236.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=159000, episode_reward=171.20 +/- 153.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=160000, episode_reward=275.80 +/- 315.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=161000, episode_reward=413.20 +/- 217.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=162000, episode_reward=319.40 +/- 168.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=163000, episode_reward=333.60 +/- 102.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=164000, episode_reward=330.20 +/- 95.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=165000, episode_reward=398.60 +/- 209.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=166000, episode_reward=384.20 +/- 258.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=167000, episode_reward=379.80 +/- 99.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=168000, episode_reward=328.20 +/- 137.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=169000, episode_reward=524.20 +/- 183.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=170000, episode_reward=365.60 +/- 88.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=171000, episode_reward=303.00 +/- 60.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=172000, episode_reward=322.60 +/- 265.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=173000, episode_reward=354.40 +/- 92.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=174000, episode_reward=301.20 +/- 241.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=175000, episode_reward=481.60 +/- 241.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=176000, episode_reward=402.20 +/- 144.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=177000, episode_reward=182.00 +/- 221.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=178000, episode_reward=274.00 +/- 96.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=179000, episode_reward=501.80 +/- 77.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=180000, episode_reward=443.60 +/- 131.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=181000, episode_reward=429.60 +/- 145.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=182000, episode_reward=285.40 +/- 190.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=183000, episode_reward=149.60 +/- 209.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=184000, episode_reward=448.40 +/- 162.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=185000, episode_reward=435.80 +/- 59.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=186000, episode_reward=501.60 +/- 109.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=187000, episode_reward=427.20 +/- 224.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=188000, episode_reward=280.60 +/- 342.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=189000, episode_reward=366.00 +/- 98.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=190000, episode_reward=501.20 +/- 173.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=191000, episode_reward=361.40 +/- 62.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=192000, episode_reward=483.60 +/- 172.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=193000, episode_reward=309.20 +/- 152.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=194000, episode_reward=175.40 +/- 142.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=195000, episode_reward=323.20 +/- 166.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=196000, episode_reward=273.60 +/- 165.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=197000, episode_reward=379.60 +/- 167.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=198000, episode_reward=460.60 +/- 184.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=199000, episode_reward=370.20 +/- 205.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=200000, episode_reward=440.60 +/- 58.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=201000, episode_reward=507.60 +/- 313.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=202000, episode_reward=271.00 +/- 176.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=203000, episode_reward=353.60 +/- 178.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=204000, episode_reward=368.20 +/- 139.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=205000, episode_reward=354.20 +/- 147.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=206000, episode_reward=342.00 +/- 200.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=207000, episode_reward=504.60 +/- 168.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=208000, episode_reward=301.80 +/- 25.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=209000, episode_reward=266.40 +/- 197.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=210000, episode_reward=348.80 +/- 105.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=211000, episode_reward=271.20 +/- 300.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=212000, episode_reward=419.20 +/- 56.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=213000, episode_reward=371.40 +/- 102.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=214000, episode_reward=343.20 +/- 90.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=215000, episode_reward=223.20 +/- 285.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=216000, episode_reward=388.00 +/- 149.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=217000, episode_reward=438.00 +/- 118.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=218000, episode_reward=404.80 +/- 294.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=219000, episode_reward=426.40 +/- 136.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=220000, episode_reward=353.60 +/- 218.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=221000, episode_reward=429.20 +/- 108.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=222000, episode_reward=361.80 +/- 99.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=223000, episode_reward=485.00 +/- 24.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=224000, episode_reward=338.80 +/- 126.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=225000, episode_reward=372.40 +/- 272.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=226000, episode_reward=485.60 +/- 182.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=227000, episode_reward=460.00 +/- 144.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=228000, episode_reward=379.80 +/- 260.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=229000, episode_reward=345.40 +/- 199.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=230000, episode_reward=477.40 +/- 275.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=231000, episode_reward=335.20 +/- 205.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=232000, episode_reward=377.80 +/- 148.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=233000, episode_reward=528.60 +/- 171.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=234000, episode_reward=300.60 +/- 67.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=235000, episode_reward=273.60 +/- 179.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=236000, episode_reward=388.80 +/- 68.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=237000, episode_reward=249.40 +/- 184.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=238000, episode_reward=382.20 +/- 111.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=239000, episode_reward=340.20 +/- 256.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=240000, episode_reward=230.00 +/- 132.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=241000, episode_reward=452.20 +/- 280.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=242000, episode_reward=417.20 +/- 47.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=243000, episode_reward=468.00 +/- 196.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=244000, episode_reward=516.00 +/- 137.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=245000, episode_reward=428.60 +/- 70.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=246000, episode_reward=244.80 +/- 233.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=247000, episode_reward=177.60 +/- 186.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=248000, episode_reward=168.20 +/- 175.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=249000, episode_reward=403.20 +/- 162.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=250000, episode_reward=342.20 +/- 92.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=251000, episode_reward=493.20 +/- 160.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=252000, episode_reward=314.20 +/- 163.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=253000, episode_reward=198.20 +/- 162.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=254000, episode_reward=493.20 +/- 65.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=255000, episode_reward=435.20 +/- 204.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=256000, episode_reward=448.00 +/- 142.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=257000, episode_reward=488.20 +/- 178.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=258000, episode_reward=393.40 +/- 101.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=259000, episode_reward=147.60 +/- 239.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=260000, episode_reward=519.80 +/- 202.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=261000, episode_reward=497.60 +/- 206.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=262000, episode_reward=396.40 +/- 203.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=263000, episode_reward=317.60 +/- 163.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=264000, episode_reward=336.20 +/- 209.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=265000, episode_reward=352.40 +/- 148.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=266000, episode_reward=399.80 +/- 155.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=267000, episode_reward=437.40 +/- 74.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=268000, episode_reward=489.20 +/- 151.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=269000, episode_reward=374.20 +/- 103.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=270000, episode_reward=264.40 +/- 230.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=271000, episode_reward=442.00 +/- 199.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=272000, episode_reward=253.00 +/- 274.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=273000, episode_reward=391.80 +/- 143.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=274000, episode_reward=336.40 +/- 316.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=275000, episode_reward=269.20 +/- 133.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=276000, episode_reward=477.20 +/- 109.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=277000, episode_reward=488.20 +/- 91.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=278000, episode_reward=448.40 +/- 160.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=279000, episode_reward=507.40 +/- 83.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=280000, episode_reward=285.00 +/- 121.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=281000, episode_reward=525.80 +/- 195.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=282000, episode_reward=486.20 +/- 273.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=283000, episode_reward=625.40 +/- 136.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=284000, episode_reward=431.80 +/- 164.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=285000, episode_reward=316.20 +/- 239.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=286000, episode_reward=568.20 +/- 239.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=287000, episode_reward=607.80 +/- 68.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=288000, episode_reward=313.00 +/- 304.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=289000, episode_reward=467.00 +/- 257.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=290000, episode_reward=620.00 +/- 137.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=291000, episode_reward=882.60 +/- 59.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=292000, episode_reward=581.20 +/- 236.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=293000, episode_reward=420.40 +/- 207.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=294000, episode_reward=761.40 +/- 149.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=295000, episode_reward=482.40 +/- 312.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=296000, episode_reward=512.40 +/- 260.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=297000, episode_reward=711.00 +/- 183.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=298000, episode_reward=755.60 +/- 325.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=299000, episode_reward=752.60 +/- 247.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=300000, episode_reward=627.20 +/- 72.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=301000, episode_reward=902.40 +/- 171.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=302000, episode_reward=789.00 +/- 334.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=303000, episode_reward=475.60 +/- 64.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=304000, episode_reward=573.60 +/- 349.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=305000, episode_reward=738.80 +/- 331.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=306000, episode_reward=791.40 +/- 164.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=307000, episode_reward=929.60 +/- 217.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=308000, episode_reward=656.00 +/- 214.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=309000, episode_reward=791.60 +/- 161.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=310000, episode_reward=450.80 +/- 343.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=311000, episode_reward=878.80 +/- 177.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=312000, episode_reward=593.00 +/- 141.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=313000, episode_reward=704.60 +/- 292.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=314000, episode_reward=677.60 +/- 255.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=315000, episode_reward=844.00 +/- 154.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=316000, episode_reward=679.40 +/- 316.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=317000, episode_reward=888.40 +/- 246.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=318000, episode_reward=770.80 +/- 296.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=319000, episode_reward=645.80 +/- 250.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=320000, episode_reward=543.20 +/- 131.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=321000, episode_reward=563.00 +/- 257.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=322000, episode_reward=826.20 +/- 317.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=323000, episode_reward=650.80 +/- 181.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=324000, episode_reward=645.60 +/- 128.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=325000, episode_reward=815.60 +/- 291.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=326000, episode_reward=709.40 +/- 45.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=327000, episode_reward=682.00 +/- 193.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=328000, episode_reward=907.40 +/- 272.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=329000, episode_reward=689.00 +/- 149.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=330000, episode_reward=836.00 +/- 230.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=331000, episode_reward=841.00 +/- 294.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=332000, episode_reward=710.60 +/- 230.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=333000, episode_reward=621.20 +/- 233.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=334000, episode_reward=777.60 +/- 313.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=335000, episode_reward=690.80 +/- 151.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=336000, episode_reward=692.60 +/- 285.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=337000, episode_reward=909.60 +/- 140.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=338000, episode_reward=690.00 +/- 159.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=339000, episode_reward=954.60 +/- 194.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=340000, episode_reward=690.00 +/- 257.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=341000, episode_reward=607.00 +/- 214.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=342000, episode_reward=768.20 +/- 170.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=343000, episode_reward=745.00 +/- 294.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=344000, episode_reward=643.20 +/- 103.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=345000, episode_reward=549.00 +/- 306.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=346000, episode_reward=674.80 +/- 342.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=347000, episode_reward=898.00 +/- 158.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=348000, episode_reward=711.00 +/- 173.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=349000, episode_reward=678.20 +/- 303.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=350000, episode_reward=685.00 +/- 190.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=351000, episode_reward=911.00 +/- 245.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=352000, episode_reward=829.40 +/- 299.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=353000, episode_reward=920.80 +/- 67.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=354000, episode_reward=830.20 +/- 321.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=355000, episode_reward=799.60 +/- 249.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=356000, episode_reward=748.40 +/- 276.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=357000, episode_reward=678.00 +/- 142.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=358000, episode_reward=638.20 +/- 359.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=359000, episode_reward=858.40 +/- 437.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=360000, episode_reward=962.20 +/- 261.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=361000, episode_reward=795.60 +/- 91.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=362000, episode_reward=622.20 +/- 279.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=363000, episode_reward=731.00 +/- 308.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=364000, episode_reward=688.80 +/- 236.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=365000, episode_reward=775.80 +/- 128.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=366000, episode_reward=725.40 +/- 238.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=367000, episode_reward=736.20 +/- 200.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=368000, episode_reward=572.80 +/- 213.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=369000, episode_reward=568.40 +/- 229.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=370000, episode_reward=945.80 +/- 167.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=371000, episode_reward=823.40 +/- 256.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=372000, episode_reward=544.60 +/- 256.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=373000, episode_reward=767.80 +/- 322.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=374000, episode_reward=700.40 +/- 399.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=375000, episode_reward=694.60 +/- 248.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=376000, episode_reward=763.80 +/- 177.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=377000, episode_reward=621.00 +/- 229.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=378000, episode_reward=668.00 +/- 439.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=379000, episode_reward=896.00 +/- 294.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=380000, episode_reward=789.80 +/- 225.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=381000, episode_reward=640.40 +/- 200.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=382000, episode_reward=701.60 +/- 260.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=383000, episode_reward=425.40 +/- 335.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=384000, episode_reward=574.40 +/- 240.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=385000, episode_reward=850.00 +/- 144.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=386000, episode_reward=927.20 +/- 161.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=387000, episode_reward=809.40 +/- 326.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=388000, episode_reward=836.40 +/- 369.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=389000, episode_reward=986.80 +/- 323.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=390000, episode_reward=638.60 +/- 286.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=391000, episode_reward=783.40 +/- 237.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=392000, episode_reward=856.00 +/- 249.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=393000, episode_reward=839.20 +/- 347.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=394000, episode_reward=1059.00 +/- 253.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=395000, episode_reward=638.60 +/- 308.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=396000, episode_reward=879.20 +/- 237.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=397000, episode_reward=761.20 +/- 441.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=398000, episode_reward=837.60 +/- 204.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=399000, episode_reward=778.00 +/- 180.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=400000, episode_reward=926.60 +/- 246.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=401000, episode_reward=933.00 +/- 137.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=402000, episode_reward=844.00 +/- 198.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=403000, episode_reward=830.80 +/- 152.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=404000, episode_reward=608.00 +/- 210.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=405000, episode_reward=742.80 +/- 352.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=406000, episode_reward=605.20 +/- 332.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=407000, episode_reward=721.60 +/- 303.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=408000, episode_reward=596.00 +/- 398.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=409000, episode_reward=640.60 +/- 187.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=410000, episode_reward=708.60 +/- 200.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=411000, episode_reward=565.40 +/- 198.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=412000, episode_reward=668.60 +/- 186.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=413000, episode_reward=1026.00 +/- 166.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=414000, episode_reward=607.80 +/- 261.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=415000, episode_reward=651.80 +/- 262.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=416000, episode_reward=698.00 +/- 83.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=417000, episode_reward=728.60 +/- 110.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=418000, episode_reward=865.60 +/- 353.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=419000, episode_reward=816.60 +/- 203.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=420000, episode_reward=971.80 +/- 180.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=421000, episode_reward=733.20 +/- 163.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=422000, episode_reward=862.60 +/- 244.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=423000, episode_reward=563.00 +/- 233.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=424000, episode_reward=-2022.20 +/- 5696.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=425000, episode_reward=1038.00 +/- 199.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=426000, episode_reward=770.20 +/- 127.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=427000, episode_reward=508.80 +/- 338.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=428000, episode_reward=1050.40 +/- 266.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=429000, episode_reward=956.20 +/- 162.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=430000, episode_reward=737.40 +/- 237.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=431000, episode_reward=976.80 +/- 257.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=432000, episode_reward=780.40 +/- 319.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=433000, episode_reward=989.00 +/- 332.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=434000, episode_reward=871.00 +/- 311.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=435000, episode_reward=927.40 +/- 109.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=436000, episode_reward=912.60 +/- 391.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=437000, episode_reward=635.00 +/- 266.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=438000, episode_reward=802.20 +/- 278.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=439000, episode_reward=959.80 +/- 236.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=440000, episode_reward=903.00 +/- 222.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=441000, episode_reward=780.60 +/- 339.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=442000, episode_reward=791.20 +/- 217.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=443000, episode_reward=847.80 +/- 232.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=444000, episode_reward=883.60 +/- 342.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=445000, episode_reward=735.00 +/- 179.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=446000, episode_reward=898.40 +/- 65.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=447000, episode_reward=803.40 +/- 286.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=448000, episode_reward=624.80 +/- 300.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=449000, episode_reward=650.20 +/- 249.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=450000, episode_reward=800.00 +/- 235.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=451000, episode_reward=636.80 +/- 188.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=452000, episode_reward=667.20 +/- 278.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=453000, episode_reward=429.00 +/- 208.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=454000, episode_reward=743.80 +/- 285.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=455000, episode_reward=799.40 +/- 139.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=456000, episode_reward=944.00 +/- 317.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=457000, episode_reward=1037.40 +/- 252.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=458000, episode_reward=711.00 +/- 233.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=459000, episode_reward=863.20 +/- 130.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=460000, episode_reward=930.20 +/- 276.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=461000, episode_reward=839.60 +/- 125.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=462000, episode_reward=885.20 +/- 164.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=463000, episode_reward=794.80 +/- 210.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=464000, episode_reward=750.20 +/- 53.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=465000, episode_reward=941.80 +/- 147.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=466000, episode_reward=884.00 +/- 269.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=467000, episode_reward=691.20 +/- 300.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=468000, episode_reward=700.60 +/- 220.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=469000, episode_reward=956.40 +/- 183.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=470000, episode_reward=739.20 +/- 296.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=471000, episode_reward=653.80 +/- 153.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=472000, episode_reward=985.60 +/- 208.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=473000, episode_reward=896.40 +/- 236.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=474000, episode_reward=663.40 +/- 412.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=475000, episode_reward=641.00 +/- 246.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=476000, episode_reward=802.40 +/- 490.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=477000, episode_reward=728.20 +/- 102.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=478000, episode_reward=864.00 +/- 408.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=479000, episode_reward=636.20 +/- 256.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=480000, episode_reward=913.00 +/- 382.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=481000, episode_reward=608.00 +/- 339.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=482000, episode_reward=929.00 +/- 152.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=483000, episode_reward=812.40 +/- 310.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=484000, episode_reward=678.60 +/- 320.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=485000, episode_reward=723.60 +/- 252.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=486000, episode_reward=877.20 +/- 414.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=487000, episode_reward=909.20 +/- 312.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=488000, episode_reward=1126.80 +/- 358.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=489000, episode_reward=879.20 +/- 144.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=490000, episode_reward=896.40 +/- 149.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=491000, episode_reward=888.60 +/- 259.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=492000, episode_reward=786.60 +/- 125.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=493000, episode_reward=974.00 +/- 342.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=494000, episode_reward=900.00 +/- 347.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=495000, episode_reward=1138.20 +/- 216.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=496000, episode_reward=896.00 +/- 256.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=497000, episode_reward=797.20 +/- 368.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=498000, episode_reward=1196.80 +/- 254.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=499000, episode_reward=974.40 +/- 192.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=500000, episode_reward=425.80 +/- 1119.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Gamma = 0.9, Learning Rate = 3e-05, Activation Function = Tanh, Neurons = 8, Process noise = 0.1, Measurement noise = 0.0\n",
      "Eval num_timesteps=1000, episode_reward=-2387.20 +/- 100.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=-2389.40 +/- 48.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=3000, episode_reward=-2444.20 +/- 20.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=4000, episode_reward=-2431.60 +/- 100.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=5000, episode_reward=-2332.40 +/- 129.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=6000, episode_reward=-2403.00 +/- 50.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=7000, episode_reward=-2456.40 +/- 49.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=8000, episode_reward=-2419.00 +/- 103.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=9000, episode_reward=-2428.60 +/- 77.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=10000, episode_reward=-2418.00 +/- 118.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=11000, episode_reward=-2442.60 +/- 61.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=12000, episode_reward=-2528.20 +/- 108.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=13000, episode_reward=-2415.80 +/- 119.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=14000, episode_reward=-2436.80 +/- 122.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=15000, episode_reward=-2386.60 +/- 73.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=16000, episode_reward=-2481.80 +/- 100.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=17000, episode_reward=-2409.60 +/- 65.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=18000, episode_reward=-2355.80 +/- 108.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=19000, episode_reward=-2431.40 +/- 99.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=20000, episode_reward=-2533.60 +/- 102.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=21000, episode_reward=-2419.40 +/- 62.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=22000, episode_reward=-2465.00 +/- 44.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=23000, episode_reward=-2468.20 +/- 120.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=24000, episode_reward=-2438.20 +/- 81.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=25000, episode_reward=-2391.60 +/- 38.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=26000, episode_reward=-2392.80 +/- 75.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=27000, episode_reward=-2448.80 +/- 48.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=28000, episode_reward=-2451.80 +/- 69.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=29000, episode_reward=-2399.80 +/- 72.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=30000, episode_reward=-2429.60 +/- 32.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=31000, episode_reward=-2359.40 +/- 52.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=32000, episode_reward=-2453.80 +/- 83.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=33000, episode_reward=-2345.20 +/- 88.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=34000, episode_reward=-2461.20 +/- 104.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=35000, episode_reward=-2407.00 +/- 37.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=36000, episode_reward=-2372.40 +/- 54.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=37000, episode_reward=-2431.00 +/- 63.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=38000, episode_reward=-2421.20 +/- 125.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=39000, episode_reward=-2463.00 +/- 31.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=40000, episode_reward=-2430.00 +/- 47.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=41000, episode_reward=-2357.20 +/- 103.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=42000, episode_reward=-2406.60 +/- 52.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=43000, episode_reward=-2463.40 +/- 104.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=44000, episode_reward=-2422.60 +/- 51.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=45000, episode_reward=-2435.40 +/- 47.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=46000, episode_reward=-2414.40 +/- 76.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=47000, episode_reward=-2446.00 +/- 61.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=48000, episode_reward=-2385.20 +/- 74.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=49000, episode_reward=-2424.60 +/- 57.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=50000, episode_reward=-2382.60 +/- 46.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=51000, episode_reward=-1753.20 +/- 291.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=52000, episode_reward=-1228.00 +/- 1159.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=53000, episode_reward=110.20 +/- 281.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=54000, episode_reward=397.40 +/- 91.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=55000, episode_reward=336.60 +/- 196.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=56000, episode_reward=222.00 +/- 177.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=57000, episode_reward=485.40 +/- 161.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=58000, episode_reward=259.20 +/- 136.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=59000, episode_reward=492.00 +/- 132.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=60000, episode_reward=373.20 +/- 131.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=61000, episode_reward=304.60 +/- 248.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=62000, episode_reward=305.00 +/- 85.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=63000, episode_reward=205.80 +/- 204.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=64000, episode_reward=333.80 +/- 215.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=65000, episode_reward=374.60 +/- 131.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=66000, episode_reward=366.20 +/- 197.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=67000, episode_reward=375.40 +/- 90.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=68000, episode_reward=254.40 +/- 161.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=69000, episode_reward=555.60 +/- 179.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=70000, episode_reward=622.80 +/- 166.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=71000, episode_reward=398.60 +/- 260.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=72000, episode_reward=151.20 +/- 217.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=73000, episode_reward=297.80 +/- 145.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=74000, episode_reward=45.00 +/- 79.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=75000, episode_reward=107.80 +/- 165.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=76000, episode_reward=90.80 +/- 150.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=77000, episode_reward=57.20 +/- 156.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=78000, episode_reward=52.80 +/- 102.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=79000, episode_reward=192.60 +/- 35.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=80000, episode_reward=128.80 +/- 108.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=81000, episode_reward=-84.60 +/- 42.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=82000, episode_reward=171.40 +/- 215.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=83000, episode_reward=190.80 +/- 92.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=84000, episode_reward=19.60 +/- 166.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=85000, episode_reward=65.40 +/- 189.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=86000, episode_reward=105.80 +/- 137.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=87000, episode_reward=1.00 +/- 110.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=88000, episode_reward=-32.80 +/- 149.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=89000, episode_reward=-54.80 +/- 195.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=90000, episode_reward=13.60 +/- 183.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=91000, episode_reward=61.80 +/- 139.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=92000, episode_reward=114.60 +/- 120.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=93000, episode_reward=81.80 +/- 147.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=94000, episode_reward=41.40 +/- 164.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=95000, episode_reward=41.80 +/- 176.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=96000, episode_reward=-55.20 +/- 158.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=97000, episode_reward=132.60 +/- 118.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=98000, episode_reward=88.80 +/- 195.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=99000, episode_reward=77.00 +/- 139.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=100000, episode_reward=57.80 +/- 123.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=101000, episode_reward=13.80 +/- 206.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=102000, episode_reward=111.60 +/- 123.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=103000, episode_reward=-128.00 +/- 60.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=104000, episode_reward=115.00 +/- 67.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=105000, episode_reward=85.00 +/- 150.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=106000, episode_reward=153.80 +/- 61.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=107000, episode_reward=-9.20 +/- 85.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=108000, episode_reward=-88.40 +/- 185.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=109000, episode_reward=25.80 +/- 162.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=110000, episode_reward=109.80 +/- 134.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=111000, episode_reward=-70.80 +/- 118.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=112000, episode_reward=136.60 +/- 130.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=113000, episode_reward=87.00 +/- 139.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=114000, episode_reward=29.00 +/- 141.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=115000, episode_reward=50.00 +/- 161.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=116000, episode_reward=12.00 +/- 189.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=117000, episode_reward=110.60 +/- 170.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=118000, episode_reward=-116.60 +/- 75.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=119000, episode_reward=27.40 +/- 120.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=120000, episode_reward=21.80 +/- 185.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=121000, episode_reward=-26.20 +/- 212.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=122000, episode_reward=-43.00 +/- 234.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=123000, episode_reward=21.40 +/- 158.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=124000, episode_reward=-37.00 +/- 181.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=125000, episode_reward=73.40 +/- 149.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=126000, episode_reward=140.60 +/- 35.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=127000, episode_reward=202.80 +/- 30.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=128000, episode_reward=-25.00 +/- 88.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=129000, episode_reward=58.20 +/- 165.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=130000, episode_reward=-26.00 +/- 146.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=131000, episode_reward=-42.80 +/- 200.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=132000, episode_reward=82.00 +/- 85.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=133000, episode_reward=-24.00 +/- 142.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=134000, episode_reward=50.20 +/- 213.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=135000, episode_reward=172.80 +/- 79.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=136000, episode_reward=159.40 +/- 124.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=137000, episode_reward=42.60 +/- 194.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=138000, episode_reward=90.00 +/- 85.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=139000, episode_reward=-69.40 +/- 150.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=140000, episode_reward=100.80 +/- 107.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=141000, episode_reward=192.60 +/- 24.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=142000, episode_reward=146.20 +/- 183.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=143000, episode_reward=95.20 +/- 140.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=144000, episode_reward=68.20 +/- 188.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=145000, episode_reward=109.60 +/- 185.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=146000, episode_reward=45.40 +/- 122.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=147000, episode_reward=62.00 +/- 178.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=148000, episode_reward=148.40 +/- 136.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=149000, episode_reward=164.40 +/- 123.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=150000, episode_reward=211.60 +/- 40.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=151000, episode_reward=151.20 +/- 138.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=152000, episode_reward=-37.40 +/- 123.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=153000, episode_reward=51.40 +/- 137.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=154000, episode_reward=163.40 +/- 12.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=155000, episode_reward=-23.40 +/- 191.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=156000, episode_reward=78.60 +/- 143.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=157000, episode_reward=21.20 +/- 235.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=158000, episode_reward=41.20 +/- 170.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=159000, episode_reward=199.80 +/- 76.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=160000, episode_reward=38.20 +/- 126.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=161000, episode_reward=16.00 +/- 181.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=162000, episode_reward=-29.40 +/- 101.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=163000, episode_reward=17.40 +/- 201.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=164000, episode_reward=-68.20 +/- 101.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=165000, episode_reward=120.20 +/- 144.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=166000, episode_reward=-43.00 +/- 125.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=167000, episode_reward=149.20 +/- 69.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=168000, episode_reward=22.80 +/- 165.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=169000, episode_reward=-97.20 +/- 217.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=170000, episode_reward=39.60 +/- 128.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=171000, episode_reward=70.40 +/- 140.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=172000, episode_reward=-75.20 +/- 169.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=173000, episode_reward=60.20 +/- 191.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=174000, episode_reward=45.20 +/- 177.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=175000, episode_reward=28.20 +/- 154.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=176000, episode_reward=68.40 +/- 183.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=177000, episode_reward=182.80 +/- 59.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=178000, episode_reward=121.80 +/- 107.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=179000, episode_reward=-46.00 +/- 182.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=180000, episode_reward=-57.80 +/- 142.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=181000, episode_reward=56.60 +/- 170.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=182000, episode_reward=24.20 +/- 179.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=183000, episode_reward=-82.60 +/- 142.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=184000, episode_reward=75.40 +/- 174.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=185000, episode_reward=138.00 +/- 84.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=186000, episode_reward=168.40 +/- 137.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=187000, episode_reward=58.80 +/- 185.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=188000, episode_reward=-90.60 +/- 123.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=189000, episode_reward=164.40 +/- 82.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=190000, episode_reward=9.40 +/- 100.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=191000, episode_reward=91.20 +/- 147.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=192000, episode_reward=-59.20 +/- 152.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=193000, episode_reward=155.40 +/- 75.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=194000, episode_reward=54.20 +/- 161.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=195000, episode_reward=85.40 +/- 156.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=196000, episode_reward=-15.00 +/- 145.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=197000, episode_reward=111.00 +/- 207.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=198000, episode_reward=157.80 +/- 28.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=199000, episode_reward=3.20 +/- 84.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=200000, episode_reward=94.80 +/- 148.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=201000, episode_reward=72.60 +/- 129.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=202000, episode_reward=74.80 +/- 133.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=203000, episode_reward=180.80 +/- 40.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=204000, episode_reward=99.20 +/- 178.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=205000, episode_reward=210.80 +/- 196.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=206000, episode_reward=106.00 +/- 290.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=207000, episode_reward=-89.20 +/- 36.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=208000, episode_reward=176.00 +/- 32.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=209000, episode_reward=17.80 +/- 118.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=210000, episode_reward=142.40 +/- 157.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=211000, episode_reward=125.00 +/- 108.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=212000, episode_reward=7.00 +/- 211.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=213000, episode_reward=-50.00 +/- 182.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=214000, episode_reward=199.00 +/- 36.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=215000, episode_reward=89.80 +/- 167.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=216000, episode_reward=7.40 +/- 177.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=217000, episode_reward=85.80 +/- 136.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=218000, episode_reward=-24.00 +/- 108.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=219000, episode_reward=-26.20 +/- 140.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=220000, episode_reward=-40.80 +/- 240.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=221000, episode_reward=116.20 +/- 114.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=222000, episode_reward=75.80 +/- 197.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=223000, episode_reward=161.40 +/- 148.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=224000, episode_reward=104.20 +/- 156.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=225000, episode_reward=23.00 +/- 180.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=226000, episode_reward=-129.80 +/- 206.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=227000, episode_reward=5.80 +/- 113.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=228000, episode_reward=163.00 +/- 105.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=229000, episode_reward=61.80 +/- 125.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=230000, episode_reward=50.80 +/- 159.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=231000, episode_reward=-3.60 +/- 130.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=232000, episode_reward=69.60 +/- 148.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=233000, episode_reward=-23.60 +/- 170.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=234000, episode_reward=82.60 +/- 138.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=235000, episode_reward=56.20 +/- 153.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=236000, episode_reward=121.80 +/- 81.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=237000, episode_reward=-79.80 +/- 174.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=238000, episode_reward=78.00 +/- 130.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=239000, episode_reward=-134.60 +/- 81.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=240000, episode_reward=123.80 +/- 152.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=241000, episode_reward=67.40 +/- 143.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=242000, episode_reward=60.60 +/- 132.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=243000, episode_reward=105.80 +/- 148.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=244000, episode_reward=-74.20 +/- 114.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=245000, episode_reward=92.20 +/- 143.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=246000, episode_reward=67.80 +/- 160.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=247000, episode_reward=-24.60 +/- 152.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=248000, episode_reward=39.00 +/- 209.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=249000, episode_reward=61.40 +/- 181.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=250000, episode_reward=110.00 +/- 105.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=251000, episode_reward=15.00 +/- 223.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=252000, episode_reward=84.60 +/- 166.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=253000, episode_reward=-28.40 +/- 172.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=254000, episode_reward=95.80 +/- 124.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=255000, episode_reward=32.80 +/- 113.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=256000, episode_reward=133.20 +/- 124.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=257000, episode_reward=-82.80 +/- 165.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=258000, episode_reward=186.00 +/- 18.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=259000, episode_reward=30.80 +/- 174.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=260000, episode_reward=43.40 +/- 187.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=261000, episode_reward=29.80 +/- 155.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=262000, episode_reward=142.60 +/- 114.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=263000, episode_reward=54.20 +/- 154.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=264000, episode_reward=50.40 +/- 181.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=265000, episode_reward=-83.40 +/- 145.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=266000, episode_reward=97.60 +/- 151.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=267000, episode_reward=118.00 +/- 104.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=268000, episode_reward=114.40 +/- 107.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=269000, episode_reward=-13.20 +/- 155.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=270000, episode_reward=-120.40 +/- 70.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=271000, episode_reward=91.00 +/- 166.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=272000, episode_reward=-50.40 +/- 172.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=273000, episode_reward=64.60 +/- 108.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=274000, episode_reward=18.60 +/- 164.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=275000, episode_reward=213.00 +/- 55.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=276000, episode_reward=-6.00 +/- 129.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=277000, episode_reward=127.40 +/- 120.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=278000, episode_reward=62.40 +/- 125.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=279000, episode_reward=80.80 +/- 121.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=280000, episode_reward=144.80 +/- 32.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=281000, episode_reward=232.80 +/- 127.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=282000, episode_reward=91.40 +/- 126.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=283000, episode_reward=91.60 +/- 116.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=284000, episode_reward=-17.00 +/- 137.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=285000, episode_reward=121.20 +/- 143.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=286000, episode_reward=61.20 +/- 209.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=287000, episode_reward=123.00 +/- 163.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=288000, episode_reward=47.40 +/- 146.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=289000, episode_reward=183.40 +/- 34.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=290000, episode_reward=73.40 +/- 199.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=291000, episode_reward=154.00 +/- 127.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=292000, episode_reward=2.40 +/- 172.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=293000, episode_reward=90.00 +/- 85.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=294000, episode_reward=169.20 +/- 116.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=295000, episode_reward=124.20 +/- 169.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=296000, episode_reward=58.40 +/- 147.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=297000, episode_reward=99.00 +/- 124.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=298000, episode_reward=95.80 +/- 83.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=299000, episode_reward=-7.60 +/- 136.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=300000, episode_reward=188.80 +/- 56.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=301000, episode_reward=8.40 +/- 154.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=302000, episode_reward=-7.60 +/- 171.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=303000, episode_reward=33.40 +/- 109.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=304000, episode_reward=140.60 +/- 149.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=305000, episode_reward=91.20 +/- 156.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=306000, episode_reward=174.80 +/- 38.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=307000, episode_reward=126.80 +/- 141.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=308000, episode_reward=103.20 +/- 186.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=309000, episode_reward=-28.40 +/- 201.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=310000, episode_reward=109.20 +/- 82.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=311000, episode_reward=142.40 +/- 114.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=312000, episode_reward=-11.40 +/- 172.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=313000, episode_reward=52.20 +/- 173.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=314000, episode_reward=117.20 +/- 48.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=315000, episode_reward=83.60 +/- 130.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=316000, episode_reward=81.00 +/- 144.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=317000, episode_reward=101.00 +/- 116.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=318000, episode_reward=221.80 +/- 217.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=319000, episode_reward=125.00 +/- 199.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=320000, episode_reward=112.00 +/- 91.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=321000, episode_reward=66.20 +/- 146.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=322000, episode_reward=19.40 +/- 206.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=323000, episode_reward=191.40 +/- 67.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=324000, episode_reward=8.00 +/- 192.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=325000, episode_reward=121.40 +/- 145.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=326000, episode_reward=139.80 +/- 112.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=327000, episode_reward=69.20 +/- 142.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=328000, episode_reward=119.80 +/- 204.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=329000, episode_reward=107.00 +/- 158.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=330000, episode_reward=121.80 +/- 179.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=331000, episode_reward=44.20 +/- 145.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=332000, episode_reward=80.40 +/- 145.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=333000, episode_reward=29.20 +/- 182.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=334000, episode_reward=154.40 +/- 38.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=335000, episode_reward=-39.20 +/- 127.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=336000, episode_reward=89.60 +/- 163.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=337000, episode_reward=142.00 +/- 105.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=338000, episode_reward=175.40 +/- 44.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=339000, episode_reward=128.60 +/- 171.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=340000, episode_reward=94.00 +/- 120.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=341000, episode_reward=142.40 +/- 92.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=342000, episode_reward=98.60 +/- 110.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=343000, episode_reward=20.60 +/- 161.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=344000, episode_reward=-79.40 +/- 146.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=345000, episode_reward=-35.80 +/- 146.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=346000, episode_reward=13.60 +/- 155.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=347000, episode_reward=122.20 +/- 128.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=348000, episode_reward=140.40 +/- 126.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=349000, episode_reward=126.40 +/- 126.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=350000, episode_reward=65.80 +/- 171.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=351000, episode_reward=176.20 +/- 60.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=352000, episode_reward=-105.60 +/- 33.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=353000, episode_reward=53.40 +/- 132.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=354000, episode_reward=151.80 +/- 154.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=355000, episode_reward=26.60 +/- 188.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=356000, episode_reward=124.60 +/- 128.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=357000, episode_reward=6.00 +/- 125.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=358000, episode_reward=-42.80 +/- 177.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=359000, episode_reward=-86.60 +/- 161.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=360000, episode_reward=13.60 +/- 245.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=361000, episode_reward=122.00 +/- 166.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=362000, episode_reward=71.20 +/- 194.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=363000, episode_reward=-81.00 +/- 137.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=364000, episode_reward=-118.40 +/- 53.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=365000, episode_reward=77.60 +/- 173.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=366000, episode_reward=119.00 +/- 179.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=367000, episode_reward=28.40 +/- 143.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=368000, episode_reward=-46.80 +/- 184.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=369000, episode_reward=43.80 +/- 150.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=370000, episode_reward=-53.60 +/- 228.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=371000, episode_reward=105.60 +/- 171.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=372000, episode_reward=118.20 +/- 166.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=373000, episode_reward=-10.20 +/- 129.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=374000, episode_reward=27.00 +/- 181.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=375000, episode_reward=-35.80 +/- 186.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=376000, episode_reward=-114.20 +/- 153.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=377000, episode_reward=166.40 +/- 55.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=378000, episode_reward=93.40 +/- 160.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=379000, episode_reward=1.80 +/- 197.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=380000, episode_reward=12.60 +/- 178.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=381000, episode_reward=79.60 +/- 157.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=382000, episode_reward=126.80 +/- 176.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=383000, episode_reward=87.80 +/- 126.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=384000, episode_reward=102.00 +/- 148.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=385000, episode_reward=0.80 +/- 161.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=386000, episode_reward=43.40 +/- 166.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=387000, episode_reward=41.60 +/- 111.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=388000, episode_reward=127.00 +/- 128.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=389000, episode_reward=16.40 +/- 136.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=390000, episode_reward=107.40 +/- 123.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=391000, episode_reward=54.40 +/- 122.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=392000, episode_reward=-25.60 +/- 189.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=393000, episode_reward=20.80 +/- 148.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=394000, episode_reward=-36.20 +/- 169.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=395000, episode_reward=36.00 +/- 182.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=396000, episode_reward=80.80 +/- 171.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=397000, episode_reward=151.00 +/- 155.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=398000, episode_reward=38.80 +/- 221.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=399000, episode_reward=94.00 +/- 143.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=400000, episode_reward=124.20 +/- 158.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=401000, episode_reward=-12.80 +/- 96.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=402000, episode_reward=61.40 +/- 119.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=403000, episode_reward=-58.40 +/- 165.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=404000, episode_reward=235.00 +/- 88.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=405000, episode_reward=185.60 +/- 44.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=406000, episode_reward=26.20 +/- 156.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=407000, episode_reward=237.80 +/- 79.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=408000, episode_reward=-105.60 +/- 108.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=409000, episode_reward=38.40 +/- 151.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=410000, episode_reward=-79.20 +/- 135.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=411000, episode_reward=-55.00 +/- 118.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=412000, episode_reward=93.80 +/- 187.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=413000, episode_reward=133.80 +/- 159.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=414000, episode_reward=108.00 +/- 164.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=415000, episode_reward=125.20 +/- 144.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=416000, episode_reward=20.00 +/- 171.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=417000, episode_reward=47.60 +/- 155.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=418000, episode_reward=134.40 +/- 116.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=419000, episode_reward=62.20 +/- 127.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=420000, episode_reward=-54.00 +/- 164.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=421000, episode_reward=1.60 +/- 167.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=422000, episode_reward=58.60 +/- 149.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=423000, episode_reward=-39.40 +/- 164.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=424000, episode_reward=62.60 +/- 156.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=425000, episode_reward=11.80 +/- 161.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=426000, episode_reward=138.00 +/- 81.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=427000, episode_reward=111.60 +/- 187.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=428000, episode_reward=61.40 +/- 144.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=429000, episode_reward=82.80 +/- 157.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=430000, episode_reward=116.60 +/- 101.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=431000, episode_reward=121.60 +/- 61.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=432000, episode_reward=32.40 +/- 169.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=433000, episode_reward=-24.80 +/- 123.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=434000, episode_reward=38.00 +/- 149.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=435000, episode_reward=-49.40 +/- 156.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=436000, episode_reward=46.20 +/- 188.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=437000, episode_reward=15.00 +/- 153.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=438000, episode_reward=16.80 +/- 139.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=439000, episode_reward=-51.60 +/- 148.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=440000, episode_reward=52.20 +/- 150.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=441000, episode_reward=175.80 +/- 44.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=442000, episode_reward=30.60 +/- 141.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=443000, episode_reward=10.80 +/- 164.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=444000, episode_reward=-17.80 +/- 193.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=445000, episode_reward=53.00 +/- 102.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=446000, episode_reward=159.60 +/- 68.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=447000, episode_reward=76.60 +/- 140.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=448000, episode_reward=140.80 +/- 140.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=449000, episode_reward=82.80 +/- 130.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=450000, episode_reward=32.20 +/- 201.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=451000, episode_reward=86.00 +/- 135.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=452000, episode_reward=-50.20 +/- 109.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=453000, episode_reward=98.80 +/- 95.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=454000, episode_reward=-3.40 +/- 219.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=455000, episode_reward=67.40 +/- 144.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=456000, episode_reward=21.00 +/- 170.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=457000, episode_reward=122.60 +/- 144.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=458000, episode_reward=41.40 +/- 142.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=459000, episode_reward=88.80 +/- 186.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=460000, episode_reward=-3.40 +/- 133.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=461000, episode_reward=104.80 +/- 119.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=462000, episode_reward=206.20 +/- 58.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=463000, episode_reward=-88.60 +/- 182.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=464000, episode_reward=112.00 +/- 110.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=465000, episode_reward=46.00 +/- 176.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=466000, episode_reward=1.20 +/- 163.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=467000, episode_reward=31.20 +/- 172.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=468000, episode_reward=156.20 +/- 259.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=469000, episode_reward=67.00 +/- 126.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=470000, episode_reward=55.00 +/- 122.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=471000, episode_reward=82.40 +/- 158.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=472000, episode_reward=189.80 +/- 140.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=473000, episode_reward=116.00 +/- 160.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=474000, episode_reward=-13.80 +/- 152.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=475000, episode_reward=120.60 +/- 112.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=476000, episode_reward=141.80 +/- 81.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=477000, episode_reward=87.40 +/- 151.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=478000, episode_reward=75.40 +/- 123.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=479000, episode_reward=30.40 +/- 158.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=480000, episode_reward=115.20 +/- 137.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=481000, episode_reward=-16.00 +/- 149.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=482000, episode_reward=62.00 +/- 81.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=483000, episode_reward=95.00 +/- 132.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=484000, episode_reward=0.20 +/- 169.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=485000, episode_reward=-56.20 +/- 196.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=486000, episode_reward=155.60 +/- 52.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=487000, episode_reward=31.80 +/- 152.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=488000, episode_reward=95.40 +/- 101.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=489000, episode_reward=74.20 +/- 135.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=490000, episode_reward=171.20 +/- 48.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=491000, episode_reward=103.60 +/- 128.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=492000, episode_reward=44.40 +/- 178.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=493000, episode_reward=-83.80 +/- 102.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=494000, episode_reward=16.40 +/- 119.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=495000, episode_reward=125.00 +/- 158.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=496000, episode_reward=67.80 +/- 154.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=497000, episode_reward=-11.00 +/- 138.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=498000, episode_reward=36.80 +/- 165.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=499000, episode_reward=135.20 +/- 121.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=500000, episode_reward=162.00 +/- 101.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Gamma = 0.9, Learning Rate = 3e-05, Activation Function = Tanh, Neurons = 32, Process noise = 0.1, Measurement noise = 0.0\n",
      "Eval num_timesteps=1000, episode_reward=-445.60 +/- 48.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=-605.60 +/- 93.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=3000, episode_reward=-583.60 +/- 55.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=4000, episode_reward=-567.00 +/- 107.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=5000, episode_reward=-642.20 +/- 94.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=6000, episode_reward=-633.40 +/- 58.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=7000, episode_reward=-592.20 +/- 52.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=8000, episode_reward=-560.80 +/- 87.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=9000, episode_reward=-560.80 +/- 99.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=10000, episode_reward=-594.40 +/- 118.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=11000, episode_reward=-538.20 +/- 52.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=12000, episode_reward=-589.00 +/- 60.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=13000, episode_reward=-578.80 +/- 113.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=14000, episode_reward=-556.60 +/- 93.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=15000, episode_reward=-604.60 +/- 52.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=16000, episode_reward=-607.20 +/- 72.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=17000, episode_reward=-497.20 +/- 86.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=18000, episode_reward=-600.40 +/- 56.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=19000, episode_reward=-621.80 +/- 74.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=20000, episode_reward=-627.40 +/- 87.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=21000, episode_reward=-604.60 +/- 80.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=22000, episode_reward=-565.60 +/- 50.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=23000, episode_reward=-521.60 +/- 24.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=24000, episode_reward=-593.80 +/- 67.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=25000, episode_reward=-561.20 +/- 60.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=26000, episode_reward=-613.00 +/- 59.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=27000, episode_reward=-582.20 +/- 83.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=28000, episode_reward=-576.60 +/- 87.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=29000, episode_reward=-536.20 +/- 111.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=30000, episode_reward=-605.40 +/- 69.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=31000, episode_reward=-576.40 +/- 79.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=32000, episode_reward=-562.40 +/- 52.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=33000, episode_reward=-515.40 +/- 84.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=34000, episode_reward=-637.00 +/- 88.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=35000, episode_reward=-528.80 +/- 38.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=36000, episode_reward=-639.80 +/- 132.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=37000, episode_reward=-638.20 +/- 156.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=38000, episode_reward=-654.40 +/- 73.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=39000, episode_reward=-597.60 +/- 46.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=40000, episode_reward=-576.40 +/- 124.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=41000, episode_reward=-542.20 +/- 80.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=42000, episode_reward=-580.00 +/- 55.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=43000, episode_reward=-559.20 +/- 117.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=44000, episode_reward=-621.20 +/- 125.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=45000, episode_reward=-594.60 +/- 66.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=46000, episode_reward=-568.40 +/- 62.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=47000, episode_reward=-531.20 +/- 63.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=48000, episode_reward=-565.40 +/- 100.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=49000, episode_reward=-549.20 +/- 125.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=50000, episode_reward=-581.60 +/- 83.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=51000, episode_reward=166.40 +/- 145.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=52000, episode_reward=453.40 +/- 103.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=53000, episode_reward=-240.20 +/- 352.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=54000, episode_reward=-703.20 +/- 1059.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=55000, episode_reward=-56.40 +/- 231.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=56000, episode_reward=118.80 +/- 104.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=57000, episode_reward=-235.80 +/- 364.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=58000, episode_reward=-87.40 +/- 370.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=59000, episode_reward=-198.40 +/- 583.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=60000, episode_reward=-141.20 +/- 548.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=61000, episode_reward=-33.00 +/- 284.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=62000, episode_reward=107.80 +/- 205.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=63000, episode_reward=-182.40 +/- 663.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=64000, episode_reward=-102.60 +/- 419.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=65000, episode_reward=-264.00 +/- 468.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=66000, episode_reward=-31.60 +/- 401.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=67000, episode_reward=89.80 +/- 168.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=68000, episode_reward=-191.00 +/- 579.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=69000, episode_reward=24.20 +/- 222.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=70000, episode_reward=61.00 +/- 309.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=71000, episode_reward=273.00 +/- 261.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=72000, episode_reward=-20.40 +/- 521.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=73000, episode_reward=19.00 +/- 329.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=74000, episode_reward=163.80 +/- 132.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=75000, episode_reward=-261.60 +/- 515.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=76000, episode_reward=18.80 +/- 353.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=77000, episode_reward=-921.60 +/- 1459.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=78000, episode_reward=-21.60 +/- 592.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=79000, episode_reward=-144.60 +/- 244.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=80000, episode_reward=-517.00 +/- 749.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=81000, episode_reward=-41.80 +/- 493.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=82000, episode_reward=119.20 +/- 122.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=83000, episode_reward=40.40 +/- 248.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=84000, episode_reward=-156.60 +/- 250.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=85000, episode_reward=-93.60 +/- 279.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=86000, episode_reward=72.40 +/- 208.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=87000, episode_reward=-34.80 +/- 399.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=88000, episode_reward=-114.60 +/- 236.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=89000, episode_reward=-464.40 +/- 578.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=90000, episode_reward=-73.00 +/- 746.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=91000, episode_reward=-466.00 +/- 899.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=92000, episode_reward=-195.20 +/- 653.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=93000, episode_reward=-164.60 +/- 768.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=94000, episode_reward=-84.40 +/- 410.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=95000, episode_reward=-309.60 +/- 412.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=96000, episode_reward=-219.80 +/- 365.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=97000, episode_reward=-113.40 +/- 522.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=98000, episode_reward=-8.40 +/- 183.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=99000, episode_reward=-145.20 +/- 371.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=100000, episode_reward=-39.40 +/- 583.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=101000, episode_reward=89.00 +/- 296.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=102000, episode_reward=-357.80 +/- 442.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=103000, episode_reward=132.60 +/- 261.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=104000, episode_reward=104.20 +/- 395.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=105000, episode_reward=-0.60 +/- 412.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=106000, episode_reward=-243.60 +/- 439.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=107000, episode_reward=242.00 +/- 232.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=108000, episode_reward=219.00 +/- 159.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=109000, episode_reward=-41.20 +/- 205.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=110000, episode_reward=-22.20 +/- 371.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=111000, episode_reward=-166.60 +/- 287.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=112000, episode_reward=230.20 +/- 392.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=113000, episode_reward=71.60 +/- 344.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=114000, episode_reward=112.60 +/- 329.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=115000, episode_reward=28.80 +/- 194.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=116000, episode_reward=-586.60 +/- 692.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=117000, episode_reward=133.60 +/- 150.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=118000, episode_reward=341.80 +/- 337.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=119000, episode_reward=-303.00 +/- 374.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=120000, episode_reward=-54.60 +/- 431.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=121000, episode_reward=20.20 +/- 420.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=122000, episode_reward=-978.40 +/- 775.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=123000, episode_reward=-98.40 +/- 366.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=124000, episode_reward=-183.20 +/- 337.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=125000, episode_reward=-85.20 +/- 331.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=126000, episode_reward=53.40 +/- 419.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=127000, episode_reward=407.60 +/- 224.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=128000, episode_reward=100.80 +/- 466.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=129000, episode_reward=-538.40 +/- 380.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=130000, episode_reward=-292.00 +/- 493.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=131000, episode_reward=-200.20 +/- 521.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=132000, episode_reward=-108.80 +/- 465.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=133000, episode_reward=-516.20 +/- 640.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=134000, episode_reward=-149.00 +/- 301.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=135000, episode_reward=-141.20 +/- 716.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=136000, episode_reward=-158.60 +/- 321.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=137000, episode_reward=-44.00 +/- 454.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=138000, episode_reward=153.00 +/- 232.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=139000, episode_reward=-252.40 +/- 607.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=140000, episode_reward=121.00 +/- 205.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=141000, episode_reward=-217.40 +/- 341.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=142000, episode_reward=65.80 +/- 293.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=143000, episode_reward=-353.40 +/- 567.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=144000, episode_reward=-100.00 +/- 264.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=145000, episode_reward=58.20 +/- 240.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=146000, episode_reward=-89.40 +/- 329.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=147000, episode_reward=120.80 +/- 188.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=148000, episode_reward=-106.20 +/- 518.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=149000, episode_reward=-97.40 +/- 387.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=150000, episode_reward=-50.80 +/- 274.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=151000, episode_reward=-175.40 +/- 346.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=152000, episode_reward=54.80 +/- 251.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=153000, episode_reward=277.80 +/- 132.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=154000, episode_reward=56.80 +/- 262.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=155000, episode_reward=100.60 +/- 147.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=156000, episode_reward=-269.40 +/- 567.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=157000, episode_reward=90.40 +/- 454.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=158000, episode_reward=-76.20 +/- 353.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=159000, episode_reward=28.40 +/- 202.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=160000, episode_reward=75.00 +/- 134.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=161000, episode_reward=-342.80 +/- 263.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=162000, episode_reward=-20.60 +/- 369.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=163000, episode_reward=-58.40 +/- 446.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=164000, episode_reward=90.80 +/- 471.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=165000, episode_reward=19.00 +/- 168.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=166000, episode_reward=-252.60 +/- 528.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=167000, episode_reward=-45.80 +/- 389.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=168000, episode_reward=-213.00 +/- 344.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=169000, episode_reward=-149.40 +/- 391.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=170000, episode_reward=-53.80 +/- 428.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=171000, episode_reward=143.80 +/- 145.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=172000, episode_reward=76.20 +/- 283.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=173000, episode_reward=45.80 +/- 120.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=174000, episode_reward=10.00 +/- 304.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=175000, episode_reward=-303.60 +/- 537.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=176000, episode_reward=-256.00 +/- 328.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=177000, episode_reward=-17.20 +/- 318.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=178000, episode_reward=-230.80 +/- 499.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=179000, episode_reward=71.00 +/- 405.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=180000, episode_reward=-191.80 +/- 393.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=181000, episode_reward=240.00 +/- 237.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=182000, episode_reward=73.00 +/- 346.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=183000, episode_reward=82.60 +/- 65.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=184000, episode_reward=240.40 +/- 115.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=185000, episode_reward=85.60 +/- 163.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=186000, episode_reward=168.40 +/- 216.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=187000, episode_reward=273.20 +/- 399.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=188000, episode_reward=332.40 +/- 239.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=189000, episode_reward=66.20 +/- 355.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=190000, episode_reward=127.20 +/- 274.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=191000, episode_reward=-93.20 +/- 197.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=192000, episode_reward=207.00 +/- 474.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=193000, episode_reward=-131.40 +/- 253.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=194000, episode_reward=24.00 +/- 87.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=195000, episode_reward=53.60 +/- 352.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=196000, episode_reward=28.00 +/- 195.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=197000, episode_reward=4.80 +/- 252.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=198000, episode_reward=-444.00 +/- 281.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=199000, episode_reward=198.60 +/- 167.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=200000, episode_reward=-12.20 +/- 391.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=201000, episode_reward=215.20 +/- 199.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=202000, episode_reward=-140.80 +/- 237.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=203000, episode_reward=79.60 +/- 259.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=204000, episode_reward=113.20 +/- 269.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=205000, episode_reward=-384.00 +/- 373.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=206000, episode_reward=123.40 +/- 108.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=207000, episode_reward=251.80 +/- 348.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=208000, episode_reward=-217.60 +/- 275.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=209000, episode_reward=-75.80 +/- 183.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=210000, episode_reward=51.00 +/- 229.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=211000, episode_reward=-265.20 +/- 123.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=212000, episode_reward=42.20 +/- 439.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=213000, episode_reward=34.80 +/- 403.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=214000, episode_reward=-46.20 +/- 486.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=215000, episode_reward=56.20 +/- 400.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=216000, episode_reward=-112.40 +/- 318.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=217000, episode_reward=82.60 +/- 457.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=218000, episode_reward=215.00 +/- 175.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=219000, episode_reward=132.60 +/- 280.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=220000, episode_reward=162.20 +/- 476.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=221000, episode_reward=-205.20 +/- 293.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=222000, episode_reward=308.80 +/- 238.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=223000, episode_reward=49.20 +/- 215.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=224000, episode_reward=-18.00 +/- 436.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=225000, episode_reward=-69.00 +/- 279.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=226000, episode_reward=66.80 +/- 313.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=227000, episode_reward=-138.20 +/- 288.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=228000, episode_reward=181.60 +/- 209.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=229000, episode_reward=152.80 +/- 217.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=230000, episode_reward=214.60 +/- 409.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=231000, episode_reward=26.00 +/- 412.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=232000, episode_reward=170.40 +/- 258.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=233000, episode_reward=90.60 +/- 208.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=234000, episode_reward=277.00 +/- 98.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=235000, episode_reward=-13.60 +/- 280.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=236000, episode_reward=-39.00 +/- 295.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=237000, episode_reward=104.60 +/- 251.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=238000, episode_reward=13.80 +/- 203.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=239000, episode_reward=158.60 +/- 290.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=240000, episode_reward=50.80 +/- 308.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=241000, episode_reward=31.20 +/- 246.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=242000, episode_reward=129.20 +/- 128.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=243000, episode_reward=-236.40 +/- 416.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=244000, episode_reward=103.00 +/- 161.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=245000, episode_reward=0.60 +/- 283.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=246000, episode_reward=168.60 +/- 344.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=247000, episode_reward=48.40 +/- 135.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=248000, episode_reward=113.60 +/- 251.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=249000, episode_reward=-107.80 +/- 268.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=250000, episode_reward=174.20 +/- 99.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=251000, episode_reward=231.00 +/- 142.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=252000, episode_reward=150.60 +/- 256.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=253000, episode_reward=264.40 +/- 155.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=254000, episode_reward=67.60 +/- 360.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=255000, episode_reward=261.80 +/- 85.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=256000, episode_reward=-90.20 +/- 288.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=257000, episode_reward=141.20 +/- 301.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=258000, episode_reward=70.20 +/- 210.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=259000, episode_reward=-28.40 +/- 375.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=260000, episode_reward=130.80 +/- 257.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=261000, episode_reward=-160.60 +/- 320.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=262000, episode_reward=76.20 +/- 173.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=263000, episode_reward=102.60 +/- 331.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=264000, episode_reward=167.80 +/- 387.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=265000, episode_reward=-35.80 +/- 402.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=266000, episode_reward=220.80 +/- 232.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=267000, episode_reward=86.80 +/- 209.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=268000, episode_reward=181.00 +/- 151.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=269000, episode_reward=321.60 +/- 104.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=270000, episode_reward=204.00 +/- 144.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=271000, episode_reward=101.40 +/- 278.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=272000, episode_reward=87.00 +/- 215.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=273000, episode_reward=322.40 +/- 324.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=274000, episode_reward=-60.60 +/- 223.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=275000, episode_reward=4.80 +/- 112.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=276000, episode_reward=-28.60 +/- 174.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=277000, episode_reward=335.20 +/- 309.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=278000, episode_reward=-37.40 +/- 186.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=279000, episode_reward=144.80 +/- 236.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=280000, episode_reward=143.20 +/- 195.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=281000, episode_reward=24.60 +/- 103.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=282000, episode_reward=61.00 +/- 295.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=283000, episode_reward=104.40 +/- 256.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=284000, episode_reward=144.60 +/- 360.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=285000, episode_reward=156.40 +/- 180.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=286000, episode_reward=-80.00 +/- 321.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=287000, episode_reward=154.20 +/- 235.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=288000, episode_reward=307.40 +/- 106.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=289000, episode_reward=402.40 +/- 166.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=290000, episode_reward=257.40 +/- 289.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=291000, episode_reward=353.80 +/- 95.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=292000, episode_reward=264.60 +/- 228.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=293000, episode_reward=300.40 +/- 199.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=294000, episode_reward=164.80 +/- 123.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=295000, episode_reward=73.00 +/- 218.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=296000, episode_reward=148.20 +/- 288.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=297000, episode_reward=105.40 +/- 307.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=298000, episode_reward=234.00 +/- 415.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=299000, episode_reward=-158.60 +/- 192.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=300000, episode_reward=35.40 +/- 136.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=301000, episode_reward=308.40 +/- 106.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=302000, episode_reward=135.80 +/- 263.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=303000, episode_reward=95.60 +/- 216.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=304000, episode_reward=111.40 +/- 221.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=305000, episode_reward=70.40 +/- 158.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=306000, episode_reward=152.40 +/- 167.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=307000, episode_reward=-12.80 +/- 73.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=308000, episode_reward=115.00 +/- 143.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=309000, episode_reward=187.40 +/- 245.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=310000, episode_reward=1.40 +/- 328.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=311000, episode_reward=-136.20 +/- 266.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=312000, episode_reward=128.80 +/- 162.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=313000, episode_reward=-73.80 +/- 260.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=314000, episode_reward=72.20 +/- 231.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=315000, episode_reward=151.40 +/- 178.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=316000, episode_reward=-50.00 +/- 213.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=317000, episode_reward=156.40 +/- 108.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=318000, episode_reward=-35.60 +/- 283.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=319000, episode_reward=238.20 +/- 212.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=320000, episode_reward=249.40 +/- 237.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=321000, episode_reward=97.60 +/- 331.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=322000, episode_reward=64.80 +/- 297.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=323000, episode_reward=253.40 +/- 151.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=324000, episode_reward=55.20 +/- 239.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=325000, episode_reward=-36.60 +/- 265.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=326000, episode_reward=186.40 +/- 186.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=327000, episode_reward=151.80 +/- 182.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=328000, episode_reward=87.60 +/- 134.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=329000, episode_reward=186.60 +/- 146.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=330000, episode_reward=25.80 +/- 257.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=331000, episode_reward=55.00 +/- 117.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=332000, episode_reward=152.80 +/- 163.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=333000, episode_reward=28.60 +/- 203.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=334000, episode_reward=154.20 +/- 97.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=335000, episode_reward=357.60 +/- 177.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=336000, episode_reward=61.40 +/- 280.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=337000, episode_reward=127.20 +/- 149.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=338000, episode_reward=102.20 +/- 113.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=339000, episode_reward=20.80 +/- 174.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=340000, episode_reward=154.00 +/- 195.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=341000, episode_reward=225.80 +/- 189.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=342000, episode_reward=35.40 +/- 114.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=343000, episode_reward=136.20 +/- 164.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=344000, episode_reward=38.80 +/- 213.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=345000, episode_reward=30.80 +/- 274.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=346000, episode_reward=234.00 +/- 187.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=347000, episode_reward=108.80 +/- 217.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=348000, episode_reward=150.60 +/- 114.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=349000, episode_reward=82.20 +/- 176.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=350000, episode_reward=-24.00 +/- 188.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=351000, episode_reward=180.60 +/- 172.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=352000, episode_reward=136.40 +/- 224.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=353000, episode_reward=216.80 +/- 152.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=354000, episode_reward=250.40 +/- 263.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=355000, episode_reward=29.20 +/- 200.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=356000, episode_reward=217.80 +/- 192.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=357000, episode_reward=239.80 +/- 154.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=358000, episode_reward=219.40 +/- 298.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=359000, episode_reward=70.60 +/- 165.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=360000, episode_reward=341.00 +/- 135.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=361000, episode_reward=206.20 +/- 219.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=362000, episode_reward=186.60 +/- 175.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=363000, episode_reward=143.40 +/- 171.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=364000, episode_reward=124.00 +/- 287.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=365000, episode_reward=-74.60 +/- 259.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=366000, episode_reward=88.80 +/- 212.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=367000, episode_reward=461.20 +/- 143.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=368000, episode_reward=47.20 +/- 115.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=369000, episode_reward=47.20 +/- 247.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=370000, episode_reward=205.60 +/- 69.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=371000, episode_reward=-57.20 +/- 150.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=372000, episode_reward=21.20 +/- 195.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=373000, episode_reward=232.20 +/- 368.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=374000, episode_reward=13.40 +/- 258.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=375000, episode_reward=136.00 +/- 190.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=376000, episode_reward=328.00 +/- 152.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=377000, episode_reward=192.80 +/- 291.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=378000, episode_reward=63.20 +/- 407.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=379000, episode_reward=101.00 +/- 227.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=380000, episode_reward=19.60 +/- 336.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=381000, episode_reward=395.20 +/- 147.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=382000, episode_reward=-45.20 +/- 141.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=383000, episode_reward=101.40 +/- 256.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=384000, episode_reward=36.80 +/- 253.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=385000, episode_reward=4.20 +/- 245.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=386000, episode_reward=22.60 +/- 193.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=387000, episode_reward=187.80 +/- 138.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=388000, episode_reward=43.60 +/- 96.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=389000, episode_reward=141.80 +/- 210.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=390000, episode_reward=63.80 +/- 179.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=391000, episode_reward=95.60 +/- 257.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=392000, episode_reward=170.20 +/- 291.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=393000, episode_reward=95.60 +/- 176.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=394000, episode_reward=38.40 +/- 182.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=395000, episode_reward=-108.40 +/- 263.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=396000, episode_reward=234.00 +/- 184.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=397000, episode_reward=95.60 +/- 249.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=398000, episode_reward=-110.40 +/- 285.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=399000, episode_reward=6.00 +/- 170.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=400000, episode_reward=26.80 +/- 316.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=401000, episode_reward=394.60 +/- 336.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=402000, episode_reward=-53.80 +/- 272.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=403000, episode_reward=219.00 +/- 229.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=404000, episode_reward=82.20 +/- 219.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=405000, episode_reward=59.00 +/- 284.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=406000, episode_reward=92.00 +/- 94.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=407000, episode_reward=150.20 +/- 235.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=408000, episode_reward=-102.00 +/- 140.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=409000, episode_reward=163.60 +/- 227.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=410000, episode_reward=226.80 +/- 209.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=411000, episode_reward=210.20 +/- 157.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=412000, episode_reward=222.20 +/- 214.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=413000, episode_reward=150.40 +/- 391.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=414000, episode_reward=81.20 +/- 47.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=415000, episode_reward=327.80 +/- 143.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=416000, episode_reward=87.40 +/- 148.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=417000, episode_reward=-31.40 +/- 118.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=418000, episode_reward=175.60 +/- 104.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=419000, episode_reward=9.20 +/- 175.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=420000, episode_reward=-111.00 +/- 161.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=421000, episode_reward=243.60 +/- 205.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=422000, episode_reward=217.60 +/- 124.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=423000, episode_reward=251.20 +/- 296.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=424000, episode_reward=89.00 +/- 199.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=425000, episode_reward=302.60 +/- 209.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=426000, episode_reward=236.40 +/- 240.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=427000, episode_reward=-32.40 +/- 251.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=428000, episode_reward=200.80 +/- 199.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=429000, episode_reward=163.00 +/- 190.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=430000, episode_reward=226.20 +/- 67.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=431000, episode_reward=133.60 +/- 99.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=432000, episode_reward=265.60 +/- 56.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=433000, episode_reward=184.80 +/- 169.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=434000, episode_reward=285.00 +/- 129.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=435000, episode_reward=213.00 +/- 220.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=436000, episode_reward=215.00 +/- 64.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=437000, episode_reward=53.80 +/- 270.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=438000, episode_reward=73.60 +/- 194.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=439000, episode_reward=139.60 +/- 237.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=440000, episode_reward=94.40 +/- 259.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=441000, episode_reward=146.80 +/- 229.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=442000, episode_reward=249.40 +/- 121.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=443000, episode_reward=152.40 +/- 108.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=444000, episode_reward=191.80 +/- 246.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=445000, episode_reward=127.20 +/- 80.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=446000, episode_reward=197.20 +/- 246.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=447000, episode_reward=151.60 +/- 224.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=448000, episode_reward=72.80 +/- 277.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=449000, episode_reward=121.00 +/- 198.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=450000, episode_reward=151.80 +/- 266.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=451000, episode_reward=271.80 +/- 108.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=452000, episode_reward=124.20 +/- 250.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=453000, episode_reward=239.20 +/- 262.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=454000, episode_reward=203.00 +/- 251.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=455000, episode_reward=77.60 +/- 118.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=456000, episode_reward=62.00 +/- 170.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=457000, episode_reward=379.00 +/- 170.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=458000, episode_reward=367.00 +/- 284.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=459000, episode_reward=183.60 +/- 209.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=460000, episode_reward=279.40 +/- 232.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=461000, episode_reward=19.80 +/- 208.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=462000, episode_reward=320.00 +/- 235.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=463000, episode_reward=253.40 +/- 151.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=464000, episode_reward=303.20 +/- 178.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=465000, episode_reward=80.40 +/- 121.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=466000, episode_reward=117.80 +/- 158.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=467000, episode_reward=187.80 +/- 218.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=468000, episode_reward=145.60 +/- 268.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=469000, episode_reward=101.40 +/- 190.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=470000, episode_reward=72.00 +/- 298.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=471000, episode_reward=-44.00 +/- 192.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=472000, episode_reward=389.00 +/- 197.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=473000, episode_reward=208.00 +/- 259.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=474000, episode_reward=189.60 +/- 317.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=475000, episode_reward=190.60 +/- 281.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=476000, episode_reward=254.60 +/- 198.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=477000, episode_reward=76.80 +/- 202.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=478000, episode_reward=273.20 +/- 171.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=479000, episode_reward=-46.40 +/- 204.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=480000, episode_reward=68.20 +/- 115.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=481000, episode_reward=410.40 +/- 94.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=482000, episode_reward=303.80 +/- 216.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=483000, episode_reward=239.20 +/- 73.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=484000, episode_reward=228.20 +/- 202.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=485000, episode_reward=111.80 +/- 290.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=486000, episode_reward=391.00 +/- 194.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=487000, episode_reward=-39.80 +/- 209.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=488000, episode_reward=111.40 +/- 264.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=489000, episode_reward=347.00 +/- 284.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=490000, episode_reward=125.60 +/- 241.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=491000, episode_reward=72.60 +/- 214.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=492000, episode_reward=214.60 +/- 197.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=493000, episode_reward=279.20 +/- 248.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=494000, episode_reward=249.80 +/- 205.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=495000, episode_reward=76.80 +/- 259.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=496000, episode_reward=53.40 +/- 92.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=497000, episode_reward=125.80 +/- 189.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=498000, episode_reward=158.80 +/- 260.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=499000, episode_reward=117.40 +/- 165.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=500000, episode_reward=233.20 +/- 259.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Gamma = 0.9, Learning Rate = 3e-05, Activation Function = Tanh, Neurons = 64, Process noise = 0.1, Measurement noise = 0.0\n",
      "Eval num_timesteps=1000, episode_reward=-622.00 +/- 207.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=-532.60 +/- 87.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3000, episode_reward=-807.60 +/- 91.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=4000, episode_reward=-615.20 +/- 167.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=5000, episode_reward=-674.00 +/- 124.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=6000, episode_reward=-650.40 +/- 82.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=7000, episode_reward=-730.40 +/- 131.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=8000, episode_reward=-588.00 +/- 129.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=9000, episode_reward=-575.80 +/- 76.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=10000, episode_reward=-620.20 +/- 165.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=11000, episode_reward=-742.80 +/- 218.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=12000, episode_reward=-612.80 +/- 57.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=13000, episode_reward=-520.60 +/- 219.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=14000, episode_reward=-635.40 +/- 161.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=15000, episode_reward=-599.60 +/- 117.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=16000, episode_reward=-464.60 +/- 225.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=17000, episode_reward=-537.40 +/- 110.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=18000, episode_reward=-524.60 +/- 116.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=19000, episode_reward=-644.00 +/- 120.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=20000, episode_reward=-531.60 +/- 142.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=21000, episode_reward=-557.40 +/- 209.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=22000, episode_reward=-566.60 +/- 84.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=23000, episode_reward=-624.60 +/- 255.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=24000, episode_reward=-648.60 +/- 340.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=25000, episode_reward=-580.00 +/- 246.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=26000, episode_reward=-627.20 +/- 96.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=27000, episode_reward=-642.40 +/- 196.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=28000, episode_reward=-578.00 +/- 195.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=29000, episode_reward=-550.40 +/- 211.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=30000, episode_reward=-483.80 +/- 221.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=31000, episode_reward=-639.80 +/- 272.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=32000, episode_reward=-712.00 +/- 222.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=33000, episode_reward=-609.80 +/- 233.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=34000, episode_reward=-605.40 +/- 125.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=35000, episode_reward=-520.40 +/- 115.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=36000, episode_reward=-619.00 +/- 168.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=37000, episode_reward=-719.20 +/- 139.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=38000, episode_reward=-647.40 +/- 100.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=39000, episode_reward=-613.40 +/- 81.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=40000, episode_reward=-626.20 +/- 109.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=41000, episode_reward=-593.00 +/- 202.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=42000, episode_reward=-591.20 +/- 262.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=43000, episode_reward=-784.80 +/- 255.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=44000, episode_reward=-553.40 +/- 133.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=45000, episode_reward=-681.00 +/- 94.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=46000, episode_reward=-656.20 +/- 221.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=47000, episode_reward=-635.20 +/- 282.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=48000, episode_reward=-509.20 +/- 229.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=49000, episode_reward=-579.00 +/- 54.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=50000, episode_reward=-737.20 +/- 255.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=51000, episode_reward=-5590.20 +/- 2221.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=52000, episode_reward=-6698.00 +/- 2234.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=53000, episode_reward=-202.40 +/- 278.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=54000, episode_reward=108.00 +/- 208.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=55000, episode_reward=148.00 +/- 198.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=56000, episode_reward=-401.20 +/- 772.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=57000, episode_reward=55.80 +/- 311.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=58000, episode_reward=-227.20 +/- 797.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=59000, episode_reward=65.40 +/- 215.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=60000, episode_reward=175.20 +/- 240.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=61000, episode_reward=-22.00 +/- 256.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=62000, episode_reward=-39.00 +/- 263.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=63000, episode_reward=-671.80 +/- 1193.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=64000, episode_reward=95.80 +/- 466.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=65000, episode_reward=72.00 +/- 147.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=66000, episode_reward=-742.80 +/- 1250.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=67000, episode_reward=283.00 +/- 176.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=68000, episode_reward=-58.00 +/- 221.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=69000, episode_reward=-365.60 +/- 711.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=70000, episode_reward=112.00 +/- 177.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=71000, episode_reward=-103.00 +/- 498.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=72000, episode_reward=107.80 +/- 331.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=73000, episode_reward=-91.00 +/- 538.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=74000, episode_reward=161.60 +/- 275.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=75000, episode_reward=-274.80 +/- 480.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=76000, episode_reward=404.40 +/- 89.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=77000, episode_reward=-273.80 +/- 390.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=78000, episode_reward=119.80 +/- 468.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=79000, episode_reward=-185.60 +/- 314.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=80000, episode_reward=95.20 +/- 146.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=81000, episode_reward=147.00 +/- 230.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=82000, episode_reward=-13.20 +/- 299.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=83000, episode_reward=11.00 +/- 320.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=84000, episode_reward=-5.20 +/- 320.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=85000, episode_reward=337.80 +/- 227.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=86000, episode_reward=114.20 +/- 323.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=87000, episode_reward=-149.00 +/- 351.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=88000, episode_reward=-27.20 +/- 278.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=89000, episode_reward=115.20 +/- 242.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=90000, episode_reward=-29.60 +/- 194.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=91000, episode_reward=-27.40 +/- 296.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=92000, episode_reward=17.80 +/- 257.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=93000, episode_reward=138.80 +/- 150.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=94000, episode_reward=-201.60 +/- 272.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=95000, episode_reward=112.80 +/- 343.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=96000, episode_reward=73.00 +/- 243.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=97000, episode_reward=-310.00 +/- 325.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=98000, episode_reward=-66.60 +/- 119.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=99000, episode_reward=-151.60 +/- 173.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=100000, episode_reward=304.60 +/- 242.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=101000, episode_reward=-436.80 +/- 468.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=102000, episode_reward=110.60 +/- 220.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=103000, episode_reward=-318.60 +/- 460.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=104000, episode_reward=-70.60 +/- 482.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=105000, episode_reward=-293.00 +/- 261.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=106000, episode_reward=136.80 +/- 211.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=107000, episode_reward=97.40 +/- 221.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=108000, episode_reward=30.80 +/- 156.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=109000, episode_reward=283.20 +/- 202.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=110000, episode_reward=314.00 +/- 116.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=111000, episode_reward=39.20 +/- 407.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=112000, episode_reward=64.40 +/- 248.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=113000, episode_reward=-147.00 +/- 345.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=114000, episode_reward=179.60 +/- 147.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=115000, episode_reward=254.20 +/- 116.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=116000, episode_reward=-19.40 +/- 389.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=117000, episode_reward=-90.80 +/- 606.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=118000, episode_reward=151.20 +/- 287.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=119000, episode_reward=-157.40 +/- 419.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=120000, episode_reward=129.60 +/- 203.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=121000, episode_reward=101.80 +/- 143.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=122000, episode_reward=215.40 +/- 95.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=123000, episode_reward=-213.60 +/- 338.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=124000, episode_reward=58.80 +/- 309.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=125000, episode_reward=-428.80 +/- 367.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=126000, episode_reward=204.40 +/- 306.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=127000, episode_reward=-99.20 +/- 249.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=128000, episode_reward=24.40 +/- 295.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=129000, episode_reward=254.40 +/- 240.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=130000, episode_reward=-113.60 +/- 233.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=131000, episode_reward=7.60 +/- 67.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=132000, episode_reward=-183.00 +/- 586.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=133000, episode_reward=92.20 +/- 180.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=134000, episode_reward=127.60 +/- 213.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=135000, episode_reward=114.60 +/- 411.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=136000, episode_reward=229.40 +/- 138.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=137000, episode_reward=72.60 +/- 297.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=138000, episode_reward=-35.20 +/- 91.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=139000, episode_reward=253.00 +/- 180.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=140000, episode_reward=102.00 +/- 295.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=141000, episode_reward=-77.40 +/- 294.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=142000, episode_reward=21.80 +/- 371.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=143000, episode_reward=402.20 +/- 258.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=144000, episode_reward=177.80 +/- 199.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=145000, episode_reward=66.00 +/- 274.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=146000, episode_reward=19.60 +/- 270.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=147000, episode_reward=155.40 +/- 117.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=148000, episode_reward=94.60 +/- 228.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=149000, episode_reward=-233.80 +/- 129.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=150000, episode_reward=273.00 +/- 338.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=151000, episode_reward=114.20 +/- 397.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=152000, episode_reward=-6.80 +/- 153.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=153000, episode_reward=197.20 +/- 235.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=154000, episode_reward=-47.40 +/- 267.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=155000, episode_reward=113.60 +/- 217.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=156000, episode_reward=294.40 +/- 307.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=157000, episode_reward=-129.60 +/- 140.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=158000, episode_reward=117.00 +/- 185.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=159000, episode_reward=377.20 +/- 179.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=160000, episode_reward=61.60 +/- 102.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=161000, episode_reward=206.40 +/- 173.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=162000, episode_reward=124.80 +/- 187.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=163000, episode_reward=248.20 +/- 299.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=164000, episode_reward=138.20 +/- 238.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=165000, episode_reward=153.60 +/- 167.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=166000, episode_reward=166.80 +/- 285.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=167000, episode_reward=-96.00 +/- 313.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=168000, episode_reward=180.20 +/- 298.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=169000, episode_reward=92.20 +/- 256.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=170000, episode_reward=-217.40 +/- 481.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=171000, episode_reward=193.80 +/- 204.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=172000, episode_reward=-169.00 +/- 353.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=173000, episode_reward=109.60 +/- 200.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=174000, episode_reward=-83.20 +/- 155.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=175000, episode_reward=48.40 +/- 229.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=176000, episode_reward=272.40 +/- 173.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=177000, episode_reward=138.60 +/- 250.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=178000, episode_reward=166.80 +/- 234.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=179000, episode_reward=8.00 +/- 123.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=180000, episode_reward=-35.80 +/- 148.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=181000, episode_reward=42.00 +/- 230.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=182000, episode_reward=85.80 +/- 242.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=183000, episode_reward=142.80 +/- 297.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=184000, episode_reward=280.40 +/- 256.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=185000, episode_reward=55.80 +/- 210.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=186000, episode_reward=104.60 +/- 244.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=187000, episode_reward=302.80 +/- 223.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=188000, episode_reward=218.80 +/- 121.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=189000, episode_reward=114.60 +/- 177.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=190000, episode_reward=158.80 +/- 85.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=191000, episode_reward=342.80 +/- 238.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=192000, episode_reward=49.80 +/- 231.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=193000, episode_reward=361.60 +/- 204.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=194000, episode_reward=67.40 +/- 250.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=195000, episode_reward=102.00 +/- 77.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=196000, episode_reward=273.40 +/- 237.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=197000, episode_reward=161.80 +/- 143.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=198000, episode_reward=234.60 +/- 304.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=199000, episode_reward=411.60 +/- 147.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=200000, episode_reward=39.80 +/- 251.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=201000, episode_reward=77.20 +/- 195.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=202000, episode_reward=175.00 +/- 219.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=203000, episode_reward=150.00 +/- 179.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=204000, episode_reward=271.00 +/- 242.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=205000, episode_reward=219.80 +/- 312.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=206000, episode_reward=61.20 +/- 336.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=207000, episode_reward=-113.00 +/- 183.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=208000, episode_reward=172.00 +/- 194.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=209000, episode_reward=155.20 +/- 231.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=210000, episode_reward=118.20 +/- 299.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=211000, episode_reward=190.40 +/- 97.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=212000, episode_reward=218.80 +/- 184.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=213000, episode_reward=262.60 +/- 201.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=214000, episode_reward=196.40 +/- 191.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=215000, episode_reward=303.40 +/- 176.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=216000, episode_reward=175.60 +/- 116.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=217000, episode_reward=-2.80 +/- 196.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=218000, episode_reward=214.20 +/- 170.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=219000, episode_reward=267.00 +/- 286.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=220000, episode_reward=96.00 +/- 281.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=221000, episode_reward=274.00 +/- 114.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=222000, episode_reward=182.80 +/- 53.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=223000, episode_reward=136.20 +/- 70.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=224000, episode_reward=143.80 +/- 156.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=225000, episode_reward=305.00 +/- 289.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=226000, episode_reward=38.60 +/- 121.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=227000, episode_reward=69.60 +/- 132.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=228000, episode_reward=117.60 +/- 228.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=229000, episode_reward=240.40 +/- 243.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=230000, episode_reward=180.80 +/- 202.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=231000, episode_reward=150.60 +/- 256.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=232000, episode_reward=220.80 +/- 135.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=233000, episode_reward=180.40 +/- 226.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=234000, episode_reward=-11.20 +/- 190.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=235000, episode_reward=261.00 +/- 129.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=236000, episode_reward=353.60 +/- 234.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=237000, episode_reward=279.60 +/- 211.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=238000, episode_reward=419.20 +/- 149.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=239000, episode_reward=241.60 +/- 247.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=240000, episode_reward=235.20 +/- 206.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=241000, episode_reward=135.20 +/- 182.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=242000, episode_reward=357.60 +/- 177.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=243000, episode_reward=144.40 +/- 150.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=244000, episode_reward=408.40 +/- 229.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=245000, episode_reward=337.80 +/- 115.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=246000, episode_reward=176.40 +/- 198.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=247000, episode_reward=272.20 +/- 271.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=248000, episode_reward=271.80 +/- 189.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=249000, episode_reward=223.00 +/- 215.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=250000, episode_reward=269.00 +/- 156.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=251000, episode_reward=198.00 +/- 207.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=252000, episode_reward=304.00 +/- 168.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=253000, episode_reward=185.20 +/- 165.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=254000, episode_reward=286.00 +/- 188.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=255000, episode_reward=262.20 +/- 234.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=256000, episode_reward=459.80 +/- 239.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=257000, episode_reward=309.60 +/- 116.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=258000, episode_reward=-64.60 +/- 248.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=259000, episode_reward=337.00 +/- 353.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=260000, episode_reward=232.40 +/- 259.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=261000, episode_reward=224.40 +/- 48.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=262000, episode_reward=134.20 +/- 220.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=263000, episode_reward=225.40 +/- 190.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=264000, episode_reward=236.40 +/- 146.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=265000, episode_reward=64.60 +/- 123.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=266000, episode_reward=135.80 +/- 276.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=267000, episode_reward=26.00 +/- 262.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=268000, episode_reward=339.80 +/- 200.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=269000, episode_reward=153.60 +/- 272.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=270000, episode_reward=172.80 +/- 274.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=271000, episode_reward=227.80 +/- 97.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=272000, episode_reward=270.80 +/- 105.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=273000, episode_reward=112.40 +/- 373.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=274000, episode_reward=231.60 +/- 197.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=275000, episode_reward=333.20 +/- 91.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=276000, episode_reward=162.80 +/- 88.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=277000, episode_reward=157.40 +/- 212.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=278000, episode_reward=191.40 +/- 208.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=279000, episode_reward=374.60 +/- 110.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=280000, episode_reward=47.20 +/- 232.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=281000, episode_reward=2.80 +/- 177.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=282000, episode_reward=-65.20 +/- 273.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=283000, episode_reward=60.60 +/- 274.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=284000, episode_reward=331.00 +/- 210.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=285000, episode_reward=245.80 +/- 212.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=286000, episode_reward=355.60 +/- 234.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=287000, episode_reward=-166.60 +/- 354.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=288000, episode_reward=142.80 +/- 49.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=289000, episode_reward=141.80 +/- 43.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=290000, episode_reward=208.80 +/- 249.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=291000, episode_reward=258.80 +/- 165.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=292000, episode_reward=106.60 +/- 127.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=293000, episode_reward=86.80 +/- 286.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=294000, episode_reward=181.60 +/- 58.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=295000, episode_reward=86.40 +/- 195.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=296000, episode_reward=317.60 +/- 172.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=297000, episode_reward=202.80 +/- 249.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=298000, episode_reward=232.20 +/- 103.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=299000, episode_reward=80.00 +/- 238.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=300000, episode_reward=144.40 +/- 107.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=301000, episode_reward=-99.00 +/- 124.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=302000, episode_reward=133.20 +/- 221.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=303000, episode_reward=102.80 +/- 186.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=304000, episode_reward=155.20 +/- 188.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=305000, episode_reward=381.20 +/- 117.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=306000, episode_reward=231.80 +/- 210.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=307000, episode_reward=107.60 +/- 226.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=308000, episode_reward=340.40 +/- 124.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=309000, episode_reward=39.40 +/- 228.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=310000, episode_reward=203.00 +/- 308.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=311000, episode_reward=175.00 +/- 183.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=312000, episode_reward=353.20 +/- 119.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=313000, episode_reward=-61.00 +/- 151.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=314000, episode_reward=214.40 +/- 72.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=315000, episode_reward=250.60 +/- 189.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=316000, episode_reward=102.40 +/- 190.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=317000, episode_reward=277.80 +/- 60.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=318000, episode_reward=361.80 +/- 84.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=319000, episode_reward=126.40 +/- 263.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=320000, episode_reward=227.40 +/- 221.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=321000, episode_reward=343.00 +/- 154.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=322000, episode_reward=185.60 +/- 136.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=323000, episode_reward=291.40 +/- 204.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=324000, episode_reward=362.00 +/- 148.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=325000, episode_reward=175.60 +/- 208.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=326000, episode_reward=139.60 +/- 234.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=327000, episode_reward=163.00 +/- 13.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=328000, episode_reward=330.40 +/- 85.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=329000, episode_reward=227.40 +/- 111.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=330000, episode_reward=182.40 +/- 240.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=331000, episode_reward=147.20 +/- 143.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=332000, episode_reward=114.20 +/- 230.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=333000, episode_reward=207.80 +/- 223.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=334000, episode_reward=138.40 +/- 131.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=335000, episode_reward=323.60 +/- 157.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=336000, episode_reward=25.60 +/- 296.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=337000, episode_reward=223.80 +/- 165.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=338000, episode_reward=113.20 +/- 155.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=339000, episode_reward=337.20 +/- 179.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=340000, episode_reward=426.80 +/- 201.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=341000, episode_reward=406.00 +/- 211.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=342000, episode_reward=272.20 +/- 183.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=343000, episode_reward=218.20 +/- 208.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=344000, episode_reward=254.00 +/- 144.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=345000, episode_reward=270.40 +/- 152.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=346000, episode_reward=285.20 +/- 223.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=347000, episode_reward=101.60 +/- 173.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=348000, episode_reward=166.60 +/- 182.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=349000, episode_reward=135.80 +/- 244.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=350000, episode_reward=-22.80 +/- 109.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=351000, episode_reward=74.20 +/- 196.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=352000, episode_reward=157.20 +/- 62.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=353000, episode_reward=225.80 +/- 247.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=354000, episode_reward=128.60 +/- 244.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=355000, episode_reward=236.40 +/- 236.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=356000, episode_reward=32.80 +/- 178.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=357000, episode_reward=115.80 +/- 242.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=358000, episode_reward=216.60 +/- 147.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=359000, episode_reward=102.60 +/- 168.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=360000, episode_reward=172.40 +/- 139.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=361000, episode_reward=362.20 +/- 141.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=362000, episode_reward=224.20 +/- 92.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=363000, episode_reward=158.60 +/- 338.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=364000, episode_reward=368.80 +/- 141.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=365000, episode_reward=237.60 +/- 217.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=366000, episode_reward=217.00 +/- 153.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=367000, episode_reward=331.80 +/- 154.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=368000, episode_reward=241.40 +/- 130.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=369000, episode_reward=83.40 +/- 299.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=370000, episode_reward=220.40 +/- 196.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=371000, episode_reward=207.20 +/- 164.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=372000, episode_reward=177.80 +/- 269.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=373000, episode_reward=261.80 +/- 150.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=374000, episode_reward=187.00 +/- 272.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=375000, episode_reward=64.20 +/- 258.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=376000, episode_reward=159.80 +/- 196.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=377000, episode_reward=55.20 +/- 174.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=378000, episode_reward=184.00 +/- 157.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=379000, episode_reward=339.00 +/- 204.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=380000, episode_reward=208.60 +/- 111.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=381000, episode_reward=178.40 +/- 279.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=382000, episode_reward=297.20 +/- 282.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=383000, episode_reward=396.40 +/- 246.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=384000, episode_reward=250.80 +/- 144.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=385000, episode_reward=138.00 +/- 237.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=386000, episode_reward=255.60 +/- 177.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=387000, episode_reward=344.60 +/- 213.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=388000, episode_reward=237.40 +/- 309.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=389000, episode_reward=-51.00 +/- 292.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=390000, episode_reward=217.40 +/- 195.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=391000, episode_reward=-20.00 +/- 436.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=392000, episode_reward=186.80 +/- 358.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=393000, episode_reward=263.00 +/- 141.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=394000, episode_reward=328.60 +/- 323.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=395000, episode_reward=319.80 +/- 272.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=396000, episode_reward=443.00 +/- 145.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=397000, episode_reward=457.40 +/- 319.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=398000, episode_reward=190.60 +/- 243.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=399000, episode_reward=285.40 +/- 224.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=400000, episode_reward=333.80 +/- 150.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=401000, episode_reward=193.80 +/- 243.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=402000, episode_reward=432.20 +/- 216.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=403000, episode_reward=306.60 +/- 305.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=404000, episode_reward=387.00 +/- 315.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=405000, episode_reward=277.40 +/- 424.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=406000, episode_reward=290.80 +/- 163.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=407000, episode_reward=557.80 +/- 179.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=408000, episode_reward=306.20 +/- 279.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=409000, episode_reward=670.60 +/- 171.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=410000, episode_reward=506.60 +/- 352.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=411000, episode_reward=518.20 +/- 235.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=412000, episode_reward=478.80 +/- 292.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=413000, episode_reward=279.40 +/- 162.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=414000, episode_reward=557.80 +/- 273.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=415000, episode_reward=218.20 +/- 372.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=416000, episode_reward=364.00 +/- 194.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=417000, episode_reward=616.80 +/- 363.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=418000, episode_reward=453.00 +/- 320.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=419000, episode_reward=647.20 +/- 178.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=420000, episode_reward=535.80 +/- 367.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=421000, episode_reward=550.00 +/- 294.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=422000, episode_reward=499.80 +/- 259.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=423000, episode_reward=354.60 +/- 348.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=424000, episode_reward=677.00 +/- 493.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=425000, episode_reward=248.00 +/- 157.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=426000, episode_reward=526.20 +/- 246.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=427000, episode_reward=586.00 +/- 310.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=428000, episode_reward=695.00 +/- 330.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=429000, episode_reward=624.80 +/- 273.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=430000, episode_reward=559.20 +/- 292.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=431000, episode_reward=655.80 +/- 222.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=432000, episode_reward=536.40 +/- 272.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=433000, episode_reward=681.80 +/- 263.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=434000, episode_reward=445.00 +/- 178.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=435000, episode_reward=706.20 +/- 293.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=436000, episode_reward=555.80 +/- 357.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=437000, episode_reward=735.40 +/- 239.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=438000, episode_reward=651.60 +/- 366.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=439000, episode_reward=633.00 +/- 457.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=440000, episode_reward=511.60 +/- 215.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=441000, episode_reward=757.20 +/- 307.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=442000, episode_reward=531.20 +/- 307.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=443000, episode_reward=767.20 +/- 140.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=444000, episode_reward=531.20 +/- 91.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=445000, episode_reward=544.00 +/- 200.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=446000, episode_reward=628.00 +/- 348.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=447000, episode_reward=678.40 +/- 284.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=448000, episode_reward=599.40 +/- 367.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=449000, episode_reward=789.40 +/- 82.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=450000, episode_reward=789.60 +/- 156.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=451000, episode_reward=306.20 +/- 161.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=452000, episode_reward=847.40 +/- 222.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=453000, episode_reward=787.80 +/- 169.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=454000, episode_reward=604.60 +/- 494.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=455000, episode_reward=565.60 +/- 231.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=456000, episode_reward=938.00 +/- 213.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=457000, episode_reward=938.20 +/- 348.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=458000, episode_reward=675.40 +/- 315.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=459000, episode_reward=652.60 +/- 213.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=460000, episode_reward=627.20 +/- 458.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=461000, episode_reward=449.60 +/- 379.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=462000, episode_reward=521.60 +/- 188.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=463000, episode_reward=564.60 +/- 224.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=464000, episode_reward=876.60 +/- 243.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=465000, episode_reward=738.00 +/- 333.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=466000, episode_reward=693.80 +/- 310.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=467000, episode_reward=731.80 +/- 247.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=468000, episode_reward=581.80 +/- 346.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=469000, episode_reward=743.60 +/- 212.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=470000, episode_reward=-541.60 +/- 2892.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=471000, episode_reward=571.80 +/- 361.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=472000, episode_reward=754.00 +/- 238.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=473000, episode_reward=877.80 +/- 347.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=474000, episode_reward=535.00 +/- 311.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=475000, episode_reward=778.40 +/- 363.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=476000, episode_reward=781.40 +/- 219.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=477000, episode_reward=644.80 +/- 356.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=478000, episode_reward=714.00 +/- 304.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=479000, episode_reward=664.20 +/- 356.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=480000, episode_reward=779.00 +/- 247.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=481000, episode_reward=728.00 +/- 317.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=482000, episode_reward=954.20 +/- 374.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=483000, episode_reward=500.60 +/- 247.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=484000, episode_reward=843.40 +/- 346.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=485000, episode_reward=592.80 +/- 324.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=486000, episode_reward=860.40 +/- 466.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=487000, episode_reward=561.00 +/- 556.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=488000, episode_reward=569.80 +/- 453.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=489000, episode_reward=807.40 +/- 514.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=490000, episode_reward=945.40 +/- 457.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=491000, episode_reward=668.20 +/- 313.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=492000, episode_reward=808.60 +/- 202.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=493000, episode_reward=1070.00 +/- 181.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=494000, episode_reward=743.60 +/- 175.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=495000, episode_reward=953.40 +/- 263.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=496000, episode_reward=627.40 +/- 90.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=497000, episode_reward=813.20 +/- 567.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=498000, episode_reward=835.20 +/- 280.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=499000, episode_reward=589.60 +/- 350.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=500000, episode_reward=570.60 +/- 169.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Gamma = 0.9, Learning Rate = 3e-05, Activation Function = ReLU, Neurons = 8, Process noise = 0.1, Measurement noise = 0.0\n",
      "Eval num_timesteps=1000, episode_reward=-3511.00 +/- 69.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=-3565.00 +/- 41.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=3000, episode_reward=-3474.00 +/- 104.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4000, episode_reward=-3501.00 +/- 82.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=5000, episode_reward=-3529.00 +/- 47.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=6000, episode_reward=-3472.00 +/- 32.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=7000, episode_reward=-3455.00 +/- 70.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=8000, episode_reward=-3489.00 +/- 52.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=9000, episode_reward=-3468.00 +/- 54.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=10000, episode_reward=-3531.00 +/- 175.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=11000, episode_reward=-3485.00 +/- 79.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=12000, episode_reward=-3516.00 +/- 91.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=13000, episode_reward=-3559.00 +/- 115.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=14000, episode_reward=-3492.00 +/- 83.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=15000, episode_reward=-3564.00 +/- 87.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=16000, episode_reward=-3469.00 +/- 78.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=17000, episode_reward=-3527.00 +/- 87.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=18000, episode_reward=-3414.00 +/- 136.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=19000, episode_reward=-3522.00 +/- 101.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=20000, episode_reward=-3494.00 +/- 10.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=21000, episode_reward=-3467.00 +/- 82.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=22000, episode_reward=-3505.00 +/- 86.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=23000, episode_reward=-3481.00 +/- 46.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=24000, episode_reward=-3517.00 +/- 106.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=25000, episode_reward=-3516.00 +/- 80.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=26000, episode_reward=-3508.00 +/- 55.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=27000, episode_reward=-3470.00 +/- 103.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=28000, episode_reward=-3485.00 +/- 112.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=29000, episode_reward=-3474.00 +/- 71.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=30000, episode_reward=-3472.00 +/- 51.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=31000, episode_reward=-3491.00 +/- 38.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=32000, episode_reward=-3616.00 +/- 48.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=33000, episode_reward=-3557.00 +/- 67.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=34000, episode_reward=-3481.00 +/- 108.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=35000, episode_reward=-3489.00 +/- 40.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=36000, episode_reward=-3483.00 +/- 48.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=37000, episode_reward=-3483.00 +/- 52.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=38000, episode_reward=-3473.00 +/- 85.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=39000, episode_reward=-3532.00 +/- 64.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=40000, episode_reward=-3496.00 +/- 64.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=41000, episode_reward=-3543.00 +/- 76.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=42000, episode_reward=-3582.00 +/- 66.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=43000, episode_reward=-3452.00 +/- 94.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=44000, episode_reward=-3455.00 +/- 66.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=45000, episode_reward=-3483.00 +/- 89.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=46000, episode_reward=-3474.00 +/- 58.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=47000, episode_reward=-3484.00 +/- 40.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=48000, episode_reward=-3535.00 +/- 117.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=49000, episode_reward=-3451.00 +/- 78.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=50000, episode_reward=-3411.00 +/- 61.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=51000, episode_reward=-3495.00 +/- 126.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=52000, episode_reward=-3494.00 +/- 89.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=53000, episode_reward=-3517.00 +/- 94.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=54000, episode_reward=-3492.00 +/- 84.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=55000, episode_reward=-1733.60 +/- 186.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=56000, episode_reward=-821.40 +/- 97.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=57000, episode_reward=-162.80 +/- 117.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=58000, episode_reward=290.40 +/- 131.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=59000, episode_reward=321.60 +/- 79.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=60000, episode_reward=94.40 +/- 232.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=61000, episode_reward=182.80 +/- 100.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=62000, episode_reward=160.60 +/- 130.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=63000, episode_reward=119.40 +/- 121.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=64000, episode_reward=136.40 +/- 168.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=65000, episode_reward=245.20 +/- 64.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=66000, episode_reward=79.60 +/- 185.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=67000, episode_reward=228.60 +/- 208.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=68000, episode_reward=281.80 +/- 110.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=69000, episode_reward=143.20 +/- 168.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=70000, episode_reward=316.40 +/- 189.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=71000, episode_reward=210.40 +/- 142.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=72000, episode_reward=160.20 +/- 131.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=73000, episode_reward=334.00 +/- 75.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=74000, episode_reward=431.40 +/- 162.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=75000, episode_reward=176.60 +/- 188.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=76000, episode_reward=162.60 +/- 139.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=77000, episode_reward=307.80 +/- 254.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=78000, episode_reward=333.80 +/- 151.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=79000, episode_reward=275.40 +/- 182.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=80000, episode_reward=313.60 +/- 218.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=81000, episode_reward=445.20 +/- 290.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=82000, episode_reward=393.40 +/- 159.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=83000, episode_reward=395.80 +/- 90.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=84000, episode_reward=354.00 +/- 259.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=85000, episode_reward=400.80 +/- 148.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=86000, episode_reward=286.80 +/- 131.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=87000, episode_reward=381.40 +/- 233.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=88000, episode_reward=416.00 +/- 165.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=89000, episode_reward=288.80 +/- 384.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=90000, episode_reward=342.80 +/- 203.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=91000, episode_reward=357.80 +/- 124.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=92000, episode_reward=343.00 +/- 123.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=93000, episode_reward=323.80 +/- 158.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=94000, episode_reward=299.60 +/- 180.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=95000, episode_reward=340.80 +/- 144.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=96000, episode_reward=501.80 +/- 81.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=97000, episode_reward=415.60 +/- 187.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=98000, episode_reward=250.00 +/- 156.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=99000, episode_reward=351.00 +/- 175.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=100000, episode_reward=554.80 +/- 178.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=101000, episode_reward=284.00 +/- 213.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=102000, episode_reward=456.60 +/- 202.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=103000, episode_reward=430.00 +/- 125.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=104000, episode_reward=261.60 +/- 140.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=105000, episode_reward=427.60 +/- 259.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=106000, episode_reward=122.40 +/- 272.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=107000, episode_reward=515.20 +/- 203.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=108000, episode_reward=187.60 +/- 171.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=109000, episode_reward=495.00 +/- 199.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=110000, episode_reward=244.00 +/- 219.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=111000, episode_reward=373.00 +/- 267.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=112000, episode_reward=465.00 +/- 283.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=113000, episode_reward=424.00 +/- 163.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=114000, episode_reward=286.40 +/- 183.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=115000, episode_reward=398.80 +/- 228.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=116000, episode_reward=213.80 +/- 223.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=117000, episode_reward=382.60 +/- 104.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=118000, episode_reward=342.00 +/- 97.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=119000, episode_reward=294.00 +/- 110.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=120000, episode_reward=194.80 +/- 217.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=121000, episode_reward=200.00 +/- 190.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=122000, episode_reward=363.00 +/- 139.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=123000, episode_reward=122.40 +/- 186.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=124000, episode_reward=290.20 +/- 152.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=125000, episode_reward=366.80 +/- 101.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=126000, episode_reward=338.20 +/- 106.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=127000, episode_reward=384.00 +/- 229.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=128000, episode_reward=436.80 +/- 226.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=129000, episode_reward=367.00 +/- 153.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=130000, episode_reward=535.60 +/- 93.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=131000, episode_reward=224.00 +/- 282.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=132000, episode_reward=351.00 +/- 107.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=133000, episode_reward=385.00 +/- 225.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=134000, episode_reward=186.60 +/- 156.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=135000, episode_reward=395.00 +/- 232.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=136000, episode_reward=321.60 +/- 135.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=137000, episode_reward=237.60 +/- 285.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=138000, episode_reward=399.20 +/- 200.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=139000, episode_reward=275.40 +/- 137.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=140000, episode_reward=314.80 +/- 270.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=141000, episode_reward=146.00 +/- 86.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=142000, episode_reward=285.60 +/- 182.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=143000, episode_reward=187.40 +/- 149.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=144000, episode_reward=-17.00 +/- 173.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=145000, episode_reward=56.00 +/- 261.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=146000, episode_reward=-936.40 +/- 260.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=147000, episode_reward=-4803.40 +/- 475.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=148000, episode_reward=-7014.80 +/- 1795.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=149000, episode_reward=-6423.00 +/- 2531.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=150000, episode_reward=-5709.00 +/- 644.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=151000, episode_reward=-6553.60 +/- 1858.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=152000, episode_reward=-4931.00 +/- 1502.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=153000, episode_reward=-1352.60 +/- 365.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=154000, episode_reward=-701.60 +/- 330.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=155000, episode_reward=-273.80 +/- 298.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=156000, episode_reward=41.60 +/- 351.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=157000, episode_reward=-91.20 +/- 190.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=158000, episode_reward=28.80 +/- 320.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=159000, episode_reward=23.80 +/- 179.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=160000, episode_reward=165.60 +/- 216.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=161000, episode_reward=244.40 +/- 84.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=162000, episode_reward=-26.40 +/- 216.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=163000, episode_reward=242.20 +/- 193.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=164000, episode_reward=220.80 +/- 299.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=165000, episode_reward=76.40 +/- 209.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=166000, episode_reward=184.20 +/- 303.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=167000, episode_reward=60.80 +/- 149.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=168000, episode_reward=-54.80 +/- 284.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=169000, episode_reward=-398.80 +/- 117.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=170000, episode_reward=-834.00 +/- 251.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=171000, episode_reward=-697.40 +/- 350.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=172000, episode_reward=-244.60 +/- 197.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=173000, episode_reward=-360.20 +/- 178.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=174000, episode_reward=-662.00 +/- 282.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=175000, episode_reward=-1369.60 +/- 321.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=176000, episode_reward=-2541.40 +/- 484.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=177000, episode_reward=-3715.60 +/- 1123.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=178000, episode_reward=-5992.00 +/- 1742.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=179000, episode_reward=-6517.20 +/- 1534.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=180000, episode_reward=-6969.40 +/- 2739.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=181000, episode_reward=-7308.20 +/- 1452.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=182000, episode_reward=-2984.20 +/- 467.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=183000, episode_reward=-2032.40 +/- 408.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=184000, episode_reward=-1641.80 +/- 195.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=185000, episode_reward=-1304.40 +/- 329.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=186000, episode_reward=-1224.20 +/- 582.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=187000, episode_reward=-867.60 +/- 245.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=188000, episode_reward=-1087.40 +/- 290.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=189000, episode_reward=-795.60 +/- 184.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=190000, episode_reward=-1111.60 +/- 399.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=191000, episode_reward=-1097.40 +/- 386.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=192000, episode_reward=-831.40 +/- 250.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=193000, episode_reward=-968.60 +/- 480.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=194000, episode_reward=-1054.60 +/- 191.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=195000, episode_reward=-959.60 +/- 91.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=196000, episode_reward=-1351.60 +/- 247.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=197000, episode_reward=-1584.20 +/- 368.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=198000, episode_reward=-1658.00 +/- 294.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=199000, episode_reward=-2156.80 +/- 296.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=200000, episode_reward=-2890.60 +/- 686.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=201000, episode_reward=-2478.40 +/- 626.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=202000, episode_reward=-1862.20 +/- 437.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=203000, episode_reward=-1920.80 +/- 354.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=204000, episode_reward=-1606.60 +/- 435.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=205000, episode_reward=-1784.40 +/- 194.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=206000, episode_reward=-2080.80 +/- 243.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=207000, episode_reward=-3024.80 +/- 470.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=208000, episode_reward=-3508.00 +/- 971.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=209000, episode_reward=-3925.20 +/- 602.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=210000, episode_reward=-5684.40 +/- 858.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=211000, episode_reward=-3951.80 +/- 440.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=212000, episode_reward=-3103.00 +/- 448.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=213000, episode_reward=-2265.80 +/- 434.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=214000, episode_reward=-2318.20 +/- 631.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=215000, episode_reward=-1995.20 +/- 333.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=216000, episode_reward=-2099.20 +/- 403.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=217000, episode_reward=-1980.60 +/- 391.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=218000, episode_reward=-2006.00 +/- 335.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=219000, episode_reward=-2068.00 +/- 573.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=220000, episode_reward=-1869.20 +/- 592.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=221000, episode_reward=-2021.20 +/- 409.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=222000, episode_reward=-1466.60 +/- 406.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=223000, episode_reward=-1517.60 +/- 476.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=224000, episode_reward=-1339.80 +/- 233.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=225000, episode_reward=-1159.60 +/- 299.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=226000, episode_reward=-1853.40 +/- 413.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=227000, episode_reward=-1723.80 +/- 206.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=228000, episode_reward=-1651.00 +/- 226.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=229000, episode_reward=-1714.20 +/- 323.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=230000, episode_reward=-2023.40 +/- 287.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=231000, episode_reward=-1896.00 +/- 218.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=232000, episode_reward=-1931.20 +/- 340.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=233000, episode_reward=-1275.80 +/- 398.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=234000, episode_reward=-1646.40 +/- 427.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=235000, episode_reward=-1038.00 +/- 470.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=236000, episode_reward=-1434.40 +/- 335.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=237000, episode_reward=-1533.80 +/- 192.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=238000, episode_reward=-1399.60 +/- 197.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=239000, episode_reward=-1578.40 +/- 467.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=240000, episode_reward=-1445.40 +/- 420.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=241000, episode_reward=-1652.00 +/- 493.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=242000, episode_reward=-1286.40 +/- 482.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=243000, episode_reward=-955.00 +/- 549.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=244000, episode_reward=-1003.40 +/- 448.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=245000, episode_reward=-1188.00 +/- 253.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=246000, episode_reward=-1221.60 +/- 221.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=247000, episode_reward=-1266.80 +/- 422.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=248000, episode_reward=-1385.00 +/- 343.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=249000, episode_reward=-1406.60 +/- 245.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=250000, episode_reward=-1184.00 +/- 119.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=251000, episode_reward=-973.40 +/- 271.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=252000, episode_reward=-1012.60 +/- 359.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=253000, episode_reward=-924.20 +/- 290.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=254000, episode_reward=-757.80 +/- 289.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=255000, episode_reward=-634.60 +/- 297.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=256000, episode_reward=-668.00 +/- 244.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=257000, episode_reward=-837.60 +/- 357.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=258000, episode_reward=-629.80 +/- 367.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=259000, episode_reward=-726.80 +/- 193.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=260000, episode_reward=-765.00 +/- 292.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=261000, episode_reward=-483.00 +/- 263.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=262000, episode_reward=-512.80 +/- 162.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=263000, episode_reward=-402.60 +/- 88.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=264000, episode_reward=-407.60 +/- 242.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=265000, episode_reward=-565.20 +/- 173.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=266000, episode_reward=-231.20 +/- 272.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=267000, episode_reward=-267.20 +/- 110.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=268000, episode_reward=-190.80 +/- 278.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=269000, episode_reward=114.60 +/- 111.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=270000, episode_reward=-303.00 +/- 285.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=271000, episode_reward=-96.20 +/- 25.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=272000, episode_reward=-101.80 +/- 121.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=273000, episode_reward=27.80 +/- 218.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=274000, episode_reward=-19.20 +/- 81.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=275000, episode_reward=51.80 +/- 279.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=276000, episode_reward=45.80 +/- 291.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=277000, episode_reward=-32.60 +/- 74.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=278000, episode_reward=156.00 +/- 164.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=279000, episode_reward=168.80 +/- 253.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=280000, episode_reward=112.40 +/- 131.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=281000, episode_reward=227.60 +/- 194.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=282000, episode_reward=249.00 +/- 130.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=283000, episode_reward=256.40 +/- 180.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=284000, episode_reward=458.00 +/- 212.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=285000, episode_reward=370.80 +/- 102.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=286000, episode_reward=375.20 +/- 211.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=287000, episode_reward=394.00 +/- 182.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=288000, episode_reward=271.20 +/- 236.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=289000, episode_reward=136.40 +/- 149.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=290000, episode_reward=52.20 +/- 227.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=291000, episode_reward=173.00 +/- 61.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=292000, episode_reward=8.80 +/- 195.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=293000, episode_reward=38.60 +/- 115.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=294000, episode_reward=-104.20 +/- 118.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=295000, episode_reward=92.40 +/- 115.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=296000, episode_reward=54.00 +/- 186.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=297000, episode_reward=-37.20 +/- 163.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=298000, episode_reward=95.80 +/- 145.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=299000, episode_reward=119.80 +/- 156.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=300000, episode_reward=67.20 +/- 136.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=301000, episode_reward=96.60 +/- 138.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=302000, episode_reward=72.20 +/- 177.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=303000, episode_reward=100.00 +/- 113.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=304000, episode_reward=44.00 +/- 117.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=305000, episode_reward=63.00 +/- 195.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=306000, episode_reward=-28.80 +/- 163.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=307000, episode_reward=133.80 +/- 152.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=308000, episode_reward=57.20 +/- 165.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=309000, episode_reward=108.20 +/- 104.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=310000, episode_reward=133.80 +/- 133.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=311000, episode_reward=6.40 +/- 112.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=312000, episode_reward=-55.60 +/- 126.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=313000, episode_reward=-58.40 +/- 80.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=314000, episode_reward=16.40 +/- 163.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=315000, episode_reward=117.00 +/- 190.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=316000, episode_reward=14.20 +/- 150.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=317000, episode_reward=38.20 +/- 132.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=318000, episode_reward=57.80 +/- 168.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=319000, episode_reward=146.20 +/- 52.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=320000, episode_reward=-1.80 +/- 146.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=321000, episode_reward=182.20 +/- 55.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=322000, episode_reward=144.00 +/- 158.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=323000, episode_reward=-68.40 +/- 155.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=324000, episode_reward=-96.20 +/- 170.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=325000, episode_reward=-7.00 +/- 111.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=326000, episode_reward=168.20 +/- 148.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=327000, episode_reward=160.00 +/- 66.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=328000, episode_reward=57.20 +/- 182.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=329000, episode_reward=117.20 +/- 134.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=330000, episode_reward=129.00 +/- 92.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=331000, episode_reward=111.60 +/- 151.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=332000, episode_reward=-67.00 +/- 180.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=333000, episode_reward=81.40 +/- 167.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=334000, episode_reward=98.20 +/- 82.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=335000, episode_reward=84.00 +/- 117.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=336000, episode_reward=8.00 +/- 138.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=337000, episode_reward=-23.60 +/- 133.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=338000, episode_reward=51.40 +/- 162.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=339000, episode_reward=8.80 +/- 126.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=340000, episode_reward=-55.60 +/- 147.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=341000, episode_reward=111.00 +/- 125.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=342000, episode_reward=155.80 +/- 56.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=343000, episode_reward=72.20 +/- 92.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=344000, episode_reward=52.60 +/- 155.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=345000, episode_reward=159.40 +/- 114.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=346000, episode_reward=78.20 +/- 169.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=347000, episode_reward=111.80 +/- 132.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=348000, episode_reward=41.40 +/- 163.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=349000, episode_reward=90.80 +/- 72.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=350000, episode_reward=47.60 +/- 110.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=351000, episode_reward=75.60 +/- 183.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=352000, episode_reward=-40.40 +/- 161.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=353000, episode_reward=76.80 +/- 132.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=354000, episode_reward=-25.20 +/- 120.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=355000, episode_reward=122.40 +/- 101.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=356000, episode_reward=71.80 +/- 108.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=357000, episode_reward=107.60 +/- 98.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=358000, episode_reward=114.20 +/- 170.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=359000, episode_reward=58.80 +/- 131.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=360000, episode_reward=180.20 +/- 26.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=361000, episode_reward=53.40 +/- 145.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=362000, episode_reward=98.60 +/- 126.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=363000, episode_reward=102.60 +/- 133.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=364000, episode_reward=44.00 +/- 181.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=365000, episode_reward=114.00 +/- 117.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=366000, episode_reward=53.80 +/- 130.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=367000, episode_reward=-16.20 +/- 124.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=368000, episode_reward=-39.60 +/- 135.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=369000, episode_reward=57.00 +/- 163.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=370000, episode_reward=21.40 +/- 147.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=371000, episode_reward=145.40 +/- 144.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=372000, episode_reward=74.80 +/- 77.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=373000, episode_reward=83.80 +/- 153.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=374000, episode_reward=115.60 +/- 80.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=375000, episode_reward=-79.60 +/- 120.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=376000, episode_reward=17.40 +/- 161.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=377000, episode_reward=-50.20 +/- 84.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=378000, episode_reward=27.60 +/- 164.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=379000, episode_reward=-70.80 +/- 137.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=380000, episode_reward=69.60 +/- 147.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=381000, episode_reward=-20.80 +/- 156.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=382000, episode_reward=-26.80 +/- 175.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=383000, episode_reward=-30.00 +/- 120.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=384000, episode_reward=57.60 +/- 152.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=385000, episode_reward=73.60 +/- 186.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=386000, episode_reward=83.80 +/- 293.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=387000, episode_reward=-92.80 +/- 125.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=388000, episode_reward=44.20 +/- 159.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=389000, episode_reward=170.60 +/- 104.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=390000, episode_reward=-55.40 +/- 163.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=391000, episode_reward=-2.00 +/- 160.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=392000, episode_reward=60.60 +/- 140.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=393000, episode_reward=106.40 +/- 109.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=394000, episode_reward=62.40 +/- 153.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=395000, episode_reward=163.40 +/- 51.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=396000, episode_reward=12.00 +/- 136.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=397000, episode_reward=0.80 +/- 131.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=398000, episode_reward=-46.80 +/- 199.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=399000, episode_reward=138.20 +/- 117.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=400000, episode_reward=27.40 +/- 145.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=401000, episode_reward=40.80 +/- 143.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=402000, episode_reward=65.20 +/- 120.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=403000, episode_reward=149.80 +/- 52.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=404000, episode_reward=-54.80 +/- 167.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=405000, episode_reward=-138.00 +/- 143.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=406000, episode_reward=25.60 +/- 195.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=407000, episode_reward=74.20 +/- 183.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=408000, episode_reward=7.60 +/- 168.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=409000, episode_reward=-101.40 +/- 130.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=410000, episode_reward=14.20 +/- 129.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=411000, episode_reward=98.80 +/- 117.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=412000, episode_reward=56.80 +/- 97.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=413000, episode_reward=113.20 +/- 127.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=414000, episode_reward=-25.40 +/- 170.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=415000, episode_reward=59.40 +/- 150.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=416000, episode_reward=-61.00 +/- 248.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=417000, episode_reward=-40.60 +/- 224.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=418000, episode_reward=15.20 +/- 165.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=419000, episode_reward=44.80 +/- 145.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=420000, episode_reward=24.20 +/- 211.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=421000, episode_reward=110.80 +/- 157.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=422000, episode_reward=49.80 +/- 162.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=423000, episode_reward=41.80 +/- 177.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=424000, episode_reward=167.80 +/- 94.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=425000, episode_reward=143.40 +/- 60.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=426000, episode_reward=-77.40 +/- 179.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=427000, episode_reward=-55.80 +/- 175.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=428000, episode_reward=118.80 +/- 42.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=429000, episode_reward=86.00 +/- 128.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=430000, episode_reward=-61.80 +/- 127.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=431000, episode_reward=40.20 +/- 127.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=432000, episode_reward=17.80 +/- 150.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=433000, episode_reward=69.40 +/- 159.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=434000, episode_reward=28.00 +/- 151.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=435000, episode_reward=99.00 +/- 130.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=436000, episode_reward=51.60 +/- 104.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=437000, episode_reward=116.80 +/- 122.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=438000, episode_reward=147.40 +/- 132.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=439000, episode_reward=40.00 +/- 148.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=440000, episode_reward=46.00 +/- 113.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=441000, episode_reward=10.80 +/- 154.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=442000, episode_reward=145.40 +/- 169.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=443000, episode_reward=62.60 +/- 122.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=444000, episode_reward=83.60 +/- 166.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=445000, episode_reward=42.40 +/- 162.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=446000, episode_reward=-35.60 +/- 163.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=447000, episode_reward=-13.80 +/- 162.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=448000, episode_reward=58.20 +/- 176.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=449000, episode_reward=0.80 +/- 216.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=450000, episode_reward=85.00 +/- 155.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=451000, episode_reward=-18.00 +/- 161.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=452000, episode_reward=53.20 +/- 123.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=453000, episode_reward=179.80 +/- 133.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=454000, episode_reward=-15.40 +/- 162.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=455000, episode_reward=-146.00 +/- 198.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=456000, episode_reward=51.60 +/- 114.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=457000, episode_reward=176.80 +/- 37.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=458000, episode_reward=130.60 +/- 164.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=459000, episode_reward=5.20 +/- 149.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=460000, episode_reward=201.20 +/- 213.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=461000, episode_reward=-95.20 +/- 122.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=462000, episode_reward=50.20 +/- 135.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=463000, episode_reward=132.80 +/- 143.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=464000, episode_reward=-52.20 +/- 180.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=465000, episode_reward=79.80 +/- 181.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=466000, episode_reward=128.20 +/- 147.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=467000, episode_reward=-24.00 +/- 159.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=468000, episode_reward=200.00 +/- 27.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=469000, episode_reward=-86.20 +/- 118.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=470000, episode_reward=72.40 +/- 161.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=471000, episode_reward=52.80 +/- 184.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=472000, episode_reward=-57.60 +/- 91.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=473000, episode_reward=-132.60 +/- 169.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=474000, episode_reward=-53.60 +/- 112.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=475000, episode_reward=93.40 +/- 187.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=476000, episode_reward=126.60 +/- 89.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=477000, episode_reward=26.60 +/- 158.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=478000, episode_reward=89.00 +/- 129.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=479000, episode_reward=150.00 +/- 126.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=480000, episode_reward=138.60 +/- 73.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=481000, episode_reward=67.40 +/- 171.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=482000, episode_reward=179.00 +/- 185.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=483000, episode_reward=68.80 +/- 150.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=484000, episode_reward=103.60 +/- 134.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=485000, episode_reward=156.00 +/- 171.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=486000, episode_reward=-26.80 +/- 126.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=487000, episode_reward=52.20 +/- 180.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=488000, episode_reward=43.20 +/- 145.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=489000, episode_reward=-56.20 +/- 200.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=490000, episode_reward=-103.40 +/- 111.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=491000, episode_reward=111.60 +/- 162.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=492000, episode_reward=108.80 +/- 115.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=493000, episode_reward=-63.40 +/- 168.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=494000, episode_reward=62.60 +/- 180.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=495000, episode_reward=100.60 +/- 123.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=496000, episode_reward=34.60 +/- 166.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=497000, episode_reward=78.40 +/- 147.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=498000, episode_reward=-42.20 +/- 181.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=499000, episode_reward=67.00 +/- 146.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=500000, episode_reward=55.00 +/- 169.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Gamma = 0.9, Learning Rate = 3e-05, Activation Function = ReLU, Neurons = 32, Process noise = 0.1, Measurement noise = 0.0\n",
      "Eval num_timesteps=1000, episode_reward=226.40 +/- 125.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=252.80 +/- 75.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3000, episode_reward=333.40 +/- 186.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4000, episode_reward=299.40 +/- 165.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=5000, episode_reward=228.60 +/- 253.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=6000, episode_reward=-27.60 +/- 237.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=7000, episode_reward=335.40 +/- 160.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=8000, episode_reward=323.80 +/- 118.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=9000, episode_reward=347.80 +/- 176.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=10000, episode_reward=337.40 +/- 100.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=11000, episode_reward=180.40 +/- 141.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=12000, episode_reward=431.80 +/- 155.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=13000, episode_reward=320.40 +/- 199.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=14000, episode_reward=335.20 +/- 138.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=15000, episode_reward=373.00 +/- 163.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=16000, episode_reward=257.00 +/- 135.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=17000, episode_reward=281.40 +/- 197.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=18000, episode_reward=90.80 +/- 243.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=19000, episode_reward=185.60 +/- 21.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=20000, episode_reward=273.40 +/- 150.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=21000, episode_reward=355.40 +/- 108.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=22000, episode_reward=336.00 +/- 159.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=23000, episode_reward=271.00 +/- 133.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=24000, episode_reward=191.80 +/- 49.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=25000, episode_reward=300.60 +/- 181.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=26000, episode_reward=316.20 +/- 137.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=27000, episode_reward=358.80 +/- 139.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=28000, episode_reward=296.60 +/- 142.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=29000, episode_reward=266.40 +/- 219.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=30000, episode_reward=272.60 +/- 258.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=31000, episode_reward=250.20 +/- 124.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=32000, episode_reward=318.20 +/- 212.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=33000, episode_reward=219.60 +/- 91.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=34000, episode_reward=338.40 +/- 108.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=35000, episode_reward=256.20 +/- 130.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=36000, episode_reward=231.60 +/- 142.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=37000, episode_reward=439.60 +/- 86.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=38000, episode_reward=389.80 +/- 218.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=39000, episode_reward=279.80 +/- 163.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=40000, episode_reward=227.00 +/- 121.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=41000, episode_reward=187.00 +/- 168.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=42000, episode_reward=299.60 +/- 183.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=43000, episode_reward=272.80 +/- 159.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=44000, episode_reward=248.00 +/- 132.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=45000, episode_reward=340.60 +/- 163.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=46000, episode_reward=366.40 +/- 216.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=47000, episode_reward=253.20 +/- 177.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=48000, episode_reward=229.20 +/- 113.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=49000, episode_reward=347.20 +/- 183.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=50000, episode_reward=328.60 +/- 96.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=51000, episode_reward=387.60 +/- 142.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=52000, episode_reward=494.60 +/- 240.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=53000, episode_reward=40.40 +/- 134.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=54000, episode_reward=133.40 +/- 110.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=55000, episode_reward=155.40 +/- 27.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=56000, episode_reward=42.20 +/- 129.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=57000, episode_reward=-42.80 +/- 186.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=58000, episode_reward=-24.20 +/- 249.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=59000, episode_reward=131.00 +/- 110.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=60000, episode_reward=1.80 +/- 173.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=61000, episode_reward=-61.60 +/- 128.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=62000, episode_reward=28.20 +/- 210.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=63000, episode_reward=-8.60 +/- 146.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=64000, episode_reward=110.00 +/- 149.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=65000, episode_reward=97.40 +/- 142.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=66000, episode_reward=111.40 +/- 162.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=67000, episode_reward=86.20 +/- 150.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=68000, episode_reward=126.00 +/- 126.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=69000, episode_reward=110.80 +/- 164.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=70000, episode_reward=11.80 +/- 118.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=71000, episode_reward=176.40 +/- 183.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=72000, episode_reward=94.00 +/- 109.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=73000, episode_reward=29.80 +/- 202.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=74000, episode_reward=90.40 +/- 123.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=75000, episode_reward=-36.20 +/- 109.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=76000, episode_reward=25.20 +/- 188.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=77000, episode_reward=-15.20 +/- 129.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=78000, episode_reward=27.60 +/- 149.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=79000, episode_reward=13.40 +/- 125.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=80000, episode_reward=-138.20 +/- 134.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=81000, episode_reward=110.60 +/- 185.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=82000, episode_reward=-13.40 +/- 200.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=83000, episode_reward=17.80 +/- 167.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=84000, episode_reward=49.80 +/- 110.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=85000, episode_reward=84.60 +/- 160.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=86000, episode_reward=-41.20 +/- 151.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=87000, episode_reward=-2.40 +/- 100.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=88000, episode_reward=62.80 +/- 185.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=89000, episode_reward=-14.60 +/- 235.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=90000, episode_reward=86.00 +/- 159.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=91000, episode_reward=71.40 +/- 144.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=92000, episode_reward=3.20 +/- 114.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=93000, episode_reward=150.40 +/- 110.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=94000, episode_reward=21.60 +/- 173.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=95000, episode_reward=63.20 +/- 151.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=96000, episode_reward=107.80 +/- 154.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=97000, episode_reward=71.40 +/- 153.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=98000, episode_reward=137.40 +/- 160.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=99000, episode_reward=417.40 +/- 150.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=100000, episode_reward=-227.20 +/- 428.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=101000, episode_reward=-9.00 +/- 156.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=102000, episode_reward=-42.20 +/- 172.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=103000, episode_reward=-20.00 +/- 188.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=104000, episode_reward=-3330.80 +/- 2723.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=105000, episode_reward=-3786.60 +/- 2363.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=106000, episode_reward=-1588.60 +/- 1973.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=107000, episode_reward=-322.40 +/- 244.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=108000, episode_reward=144.60 +/- 161.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=109000, episode_reward=-1.20 +/- 427.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=110000, episode_reward=156.20 +/- 299.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=111000, episode_reward=-103.00 +/- 457.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=112000, episode_reward=-817.00 +/- 1927.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=113000, episode_reward=-865.80 +/- 759.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=114000, episode_reward=-66.80 +/- 470.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=115000, episode_reward=-323.60 +/- 350.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=116000, episode_reward=-225.20 +/- 548.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=117000, episode_reward=22.40 +/- 150.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=118000, episode_reward=-581.60 +/- 647.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=119000, episode_reward=-241.00 +/- 478.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=120000, episode_reward=-196.20 +/- 500.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=121000, episode_reward=-81.60 +/- 350.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=122000, episode_reward=-352.60 +/- 486.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=123000, episode_reward=-47.20 +/- 169.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=124000, episode_reward=-126.40 +/- 273.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=125000, episode_reward=-194.00 +/- 282.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=126000, episode_reward=-491.40 +/- 952.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=127000, episode_reward=-317.20 +/- 770.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=128000, episode_reward=94.80 +/- 386.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=129000, episode_reward=-145.20 +/- 375.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=130000, episode_reward=84.80 +/- 187.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=131000, episode_reward=-243.40 +/- 410.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=132000, episode_reward=151.60 +/- 154.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=133000, episode_reward=56.60 +/- 254.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=134000, episode_reward=8.20 +/- 77.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=135000, episode_reward=39.80 +/- 130.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=136000, episode_reward=-59.60 +/- 129.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=137000, episode_reward=-168.60 +/- 208.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=138000, episode_reward=34.20 +/- 144.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=139000, episode_reward=26.00 +/- 413.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=140000, episode_reward=-136.80 +/- 158.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=141000, episode_reward=30.80 +/- 358.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=142000, episode_reward=363.20 +/- 215.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=143000, episode_reward=312.80 +/- 72.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=144000, episode_reward=99.40 +/- 134.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=145000, episode_reward=259.20 +/- 174.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=146000, episode_reward=94.00 +/- 370.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=147000, episode_reward=-11.20 +/- 266.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=148000, episode_reward=119.00 +/- 113.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=149000, episode_reward=271.60 +/- 152.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=150000, episode_reward=195.60 +/- 355.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=151000, episode_reward=231.60 +/- 165.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=152000, episode_reward=181.20 +/- 64.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=153000, episode_reward=182.60 +/- 251.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=154000, episode_reward=222.80 +/- 111.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=155000, episode_reward=123.80 +/- 197.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=156000, episode_reward=140.80 +/- 200.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=157000, episode_reward=112.60 +/- 204.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=158000, episode_reward=284.20 +/- 47.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=159000, episode_reward=182.00 +/- 102.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=160000, episode_reward=257.60 +/- 254.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=161000, episode_reward=90.80 +/- 106.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=162000, episode_reward=330.60 +/- 203.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=163000, episode_reward=187.20 +/- 190.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=164000, episode_reward=197.60 +/- 151.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=165000, episode_reward=243.00 +/- 137.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=166000, episode_reward=296.60 +/- 163.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=167000, episode_reward=75.20 +/- 184.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=168000, episode_reward=67.60 +/- 116.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=169000, episode_reward=110.00 +/- 204.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=170000, episode_reward=297.80 +/- 199.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=171000, episode_reward=228.00 +/- 184.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=172000, episode_reward=297.40 +/- 166.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=173000, episode_reward=36.00 +/- 108.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=174000, episode_reward=244.00 +/- 220.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=175000, episode_reward=206.40 +/- 100.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=176000, episode_reward=289.80 +/- 172.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=177000, episode_reward=239.40 +/- 359.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=178000, episode_reward=257.20 +/- 125.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=179000, episode_reward=144.40 +/- 104.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=180000, episode_reward=341.20 +/- 173.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=181000, episode_reward=377.20 +/- 216.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=182000, episode_reward=240.40 +/- 124.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=183000, episode_reward=336.60 +/- 152.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=184000, episode_reward=386.00 +/- 215.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=185000, episode_reward=322.60 +/- 169.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=186000, episode_reward=341.80 +/- 211.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=187000, episode_reward=169.20 +/- 120.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=188000, episode_reward=437.40 +/- 40.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=189000, episode_reward=295.40 +/- 162.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=190000, episode_reward=406.00 +/- 169.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=191000, episode_reward=208.80 +/- 108.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=192000, episode_reward=262.80 +/- 145.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=193000, episode_reward=374.40 +/- 136.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=194000, episode_reward=252.80 +/- 282.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=195000, episode_reward=410.20 +/- 272.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=196000, episode_reward=160.60 +/- 136.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=197000, episode_reward=173.40 +/- 255.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=198000, episode_reward=313.60 +/- 92.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=199000, episode_reward=288.80 +/- 294.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=200000, episode_reward=190.20 +/- 259.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=201000, episode_reward=274.40 +/- 186.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=202000, episode_reward=188.80 +/- 221.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=203000, episode_reward=377.60 +/- 202.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=204000, episode_reward=23.20 +/- 90.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=205000, episode_reward=262.20 +/- 244.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=206000, episode_reward=73.40 +/- 279.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=207000, episode_reward=457.00 +/- 168.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=208000, episode_reward=176.00 +/- 120.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=209000, episode_reward=302.00 +/- 26.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=210000, episode_reward=114.60 +/- 251.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=211000, episode_reward=317.60 +/- 79.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=212000, episode_reward=310.40 +/- 172.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=213000, episode_reward=250.40 +/- 143.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=214000, episode_reward=262.60 +/- 148.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=215000, episode_reward=207.80 +/- 98.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=216000, episode_reward=375.20 +/- 143.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=217000, episode_reward=135.00 +/- 256.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=218000, episode_reward=234.20 +/- 175.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=219000, episode_reward=317.60 +/- 190.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=220000, episode_reward=162.20 +/- 163.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=221000, episode_reward=275.20 +/- 151.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=222000, episode_reward=133.60 +/- 219.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=223000, episode_reward=264.00 +/- 207.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=224000, episode_reward=394.20 +/- 182.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=225000, episode_reward=159.80 +/- 216.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=226000, episode_reward=273.20 +/- 133.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=227000, episode_reward=358.60 +/- 206.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=228000, episode_reward=276.20 +/- 122.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=229000, episode_reward=455.00 +/- 87.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=230000, episode_reward=333.20 +/- 160.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=231000, episode_reward=261.60 +/- 193.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=232000, episode_reward=363.00 +/- 160.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=233000, episode_reward=167.80 +/- 157.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=234000, episode_reward=265.40 +/- 170.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=235000, episode_reward=434.20 +/- 166.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=236000, episode_reward=330.80 +/- 121.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=237000, episode_reward=418.60 +/- 215.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=238000, episode_reward=357.60 +/- 113.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=239000, episode_reward=420.60 +/- 160.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=240000, episode_reward=342.20 +/- 149.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=241000, episode_reward=174.20 +/- 67.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=242000, episode_reward=399.00 +/- 113.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=243000, episode_reward=146.20 +/- 100.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=244000, episode_reward=282.40 +/- 247.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=245000, episode_reward=164.80 +/- 103.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=246000, episode_reward=198.20 +/- 173.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=247000, episode_reward=379.80 +/- 197.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=248000, episode_reward=307.80 +/- 142.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=249000, episode_reward=187.00 +/- 129.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=250000, episode_reward=329.40 +/- 68.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=251000, episode_reward=263.40 +/- 358.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=252000, episode_reward=166.20 +/- 203.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=253000, episode_reward=278.00 +/- 337.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=254000, episode_reward=370.40 +/- 117.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=255000, episode_reward=229.00 +/- 184.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=256000, episode_reward=227.00 +/- 124.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=257000, episode_reward=224.00 +/- 230.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=258000, episode_reward=148.40 +/- 171.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=259000, episode_reward=376.40 +/- 170.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=260000, episode_reward=263.60 +/- 147.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=261000, episode_reward=227.80 +/- 157.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=262000, episode_reward=223.80 +/- 263.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=263000, episode_reward=141.20 +/- 346.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=264000, episode_reward=260.60 +/- 203.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=265000, episode_reward=188.00 +/- 119.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=266000, episode_reward=316.00 +/- 92.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=267000, episode_reward=179.20 +/- 230.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=268000, episode_reward=329.20 +/- 192.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=269000, episode_reward=227.60 +/- 77.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=270000, episode_reward=306.00 +/- 296.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=271000, episode_reward=310.00 +/- 204.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=272000, episode_reward=269.00 +/- 226.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=273000, episode_reward=374.80 +/- 197.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=274000, episode_reward=282.00 +/- 213.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=275000, episode_reward=202.40 +/- 144.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=276000, episode_reward=259.80 +/- 206.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=277000, episode_reward=301.80 +/- 144.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=278000, episode_reward=334.20 +/- 137.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=279000, episode_reward=320.60 +/- 197.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=280000, episode_reward=315.60 +/- 158.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=281000, episode_reward=188.20 +/- 224.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=282000, episode_reward=169.40 +/- 128.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=283000, episode_reward=175.40 +/- 90.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=284000, episode_reward=153.40 +/- 168.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=285000, episode_reward=167.40 +/- 253.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=286000, episode_reward=416.20 +/- 208.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=287000, episode_reward=329.40 +/- 187.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=288000, episode_reward=276.80 +/- 199.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=289000, episode_reward=41.00 +/- 308.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=290000, episode_reward=347.80 +/- 195.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=291000, episode_reward=60.80 +/- 227.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=292000, episode_reward=288.20 +/- 159.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=293000, episode_reward=151.40 +/- 215.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=294000, episode_reward=214.80 +/- 225.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=295000, episode_reward=205.60 +/- 141.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=296000, episode_reward=331.40 +/- 141.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=297000, episode_reward=390.20 +/- 160.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=298000, episode_reward=125.60 +/- 140.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=299000, episode_reward=324.00 +/- 176.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=300000, episode_reward=349.00 +/- 149.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=301000, episode_reward=151.20 +/- 178.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=302000, episode_reward=271.20 +/- 96.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=303000, episode_reward=328.00 +/- 167.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=304000, episode_reward=231.80 +/- 298.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=305000, episode_reward=345.20 +/- 187.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=306000, episode_reward=239.60 +/- 131.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=307000, episode_reward=139.80 +/- 156.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=308000, episode_reward=138.80 +/- 170.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=309000, episode_reward=276.60 +/- 145.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=310000, episode_reward=376.00 +/- 112.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=311000, episode_reward=226.00 +/- 170.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=312000, episode_reward=182.40 +/- 54.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=313000, episode_reward=191.00 +/- 203.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=314000, episode_reward=224.60 +/- 165.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=315000, episode_reward=115.00 +/- 174.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=316000, episode_reward=388.00 +/- 101.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=317000, episode_reward=309.40 +/- 152.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=318000, episode_reward=207.60 +/- 120.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=319000, episode_reward=279.80 +/- 295.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=320000, episode_reward=123.40 +/- 155.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=321000, episode_reward=363.80 +/- 159.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=322000, episode_reward=123.60 +/- 271.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=323000, episode_reward=116.60 +/- 154.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=324000, episode_reward=250.00 +/- 163.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=325000, episode_reward=214.20 +/- 175.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=326000, episode_reward=257.20 +/- 195.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=327000, episode_reward=268.40 +/- 154.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=328000, episode_reward=269.60 +/- 89.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=329000, episode_reward=56.60 +/- 147.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=330000, episode_reward=254.40 +/- 168.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=331000, episode_reward=80.60 +/- 162.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=332000, episode_reward=365.80 +/- 79.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=333000, episode_reward=354.20 +/- 135.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=334000, episode_reward=139.60 +/- 256.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=335000, episode_reward=248.00 +/- 182.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=336000, episode_reward=270.20 +/- 144.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=337000, episode_reward=373.40 +/- 75.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=338000, episode_reward=472.80 +/- 49.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=339000, episode_reward=296.40 +/- 101.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=340000, episode_reward=226.80 +/- 205.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=341000, episode_reward=278.80 +/- 169.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=342000, episode_reward=277.80 +/- 168.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=343000, episode_reward=293.40 +/- 157.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=344000, episode_reward=229.20 +/- 191.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=345000, episode_reward=258.00 +/- 136.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=346000, episode_reward=372.80 +/- 127.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=347000, episode_reward=273.20 +/- 187.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=348000, episode_reward=265.20 +/- 291.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=349000, episode_reward=193.80 +/- 182.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=350000, episode_reward=270.80 +/- 221.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=351000, episode_reward=100.80 +/- 214.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=352000, episode_reward=363.40 +/- 260.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=353000, episode_reward=132.80 +/- 93.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=354000, episode_reward=295.60 +/- 218.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=355000, episode_reward=308.00 +/- 195.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=356000, episode_reward=287.60 +/- 127.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=357000, episode_reward=183.00 +/- 164.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=358000, episode_reward=-3.20 +/- 218.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=359000, episode_reward=180.60 +/- 99.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=360000, episode_reward=236.20 +/- 108.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=361000, episode_reward=172.60 +/- 111.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=362000, episode_reward=336.20 +/- 201.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=363000, episode_reward=303.80 +/- 143.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=364000, episode_reward=229.60 +/- 126.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=365000, episode_reward=208.00 +/- 172.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=366000, episode_reward=299.60 +/- 286.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=367000, episode_reward=260.60 +/- 243.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=368000, episode_reward=346.60 +/- 92.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=369000, episode_reward=290.20 +/- 218.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=370000, episode_reward=315.80 +/- 118.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=371000, episode_reward=284.00 +/- 89.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=372000, episode_reward=327.20 +/- 147.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=373000, episode_reward=171.00 +/- 207.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=374000, episode_reward=239.60 +/- 143.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=375000, episode_reward=189.20 +/- 204.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=376000, episode_reward=355.40 +/- 252.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=377000, episode_reward=167.40 +/- 195.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=378000, episode_reward=101.60 +/- 93.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=379000, episode_reward=221.00 +/- 158.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=380000, episode_reward=71.40 +/- 297.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=381000, episode_reward=234.20 +/- 237.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=382000, episode_reward=349.60 +/- 157.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=383000, episode_reward=172.00 +/- 195.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=384000, episode_reward=98.80 +/- 90.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=385000, episode_reward=275.40 +/- 95.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=386000, episode_reward=317.00 +/- 188.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=387000, episode_reward=288.20 +/- 125.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=388000, episode_reward=278.00 +/- 100.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=389000, episode_reward=335.00 +/- 270.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=390000, episode_reward=370.80 +/- 151.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=391000, episode_reward=235.40 +/- 239.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=392000, episode_reward=157.80 +/- 208.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=393000, episode_reward=375.40 +/- 268.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=394000, episode_reward=254.00 +/- 313.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=395000, episode_reward=196.60 +/- 87.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=396000, episode_reward=115.60 +/- 232.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=397000, episode_reward=121.00 +/- 26.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=398000, episode_reward=56.40 +/- 266.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=399000, episode_reward=321.80 +/- 152.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=400000, episode_reward=350.20 +/- 115.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=401000, episode_reward=301.20 +/- 208.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=402000, episode_reward=318.80 +/- 259.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=403000, episode_reward=337.60 +/- 129.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=404000, episode_reward=443.00 +/- 143.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=405000, episode_reward=227.00 +/- 230.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=406000, episode_reward=271.40 +/- 195.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=407000, episode_reward=252.00 +/- 130.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=408000, episode_reward=114.40 +/- 170.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=409000, episode_reward=150.40 +/- 253.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=410000, episode_reward=203.80 +/- 162.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=411000, episode_reward=209.20 +/- 172.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=412000, episode_reward=325.40 +/- 178.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=413000, episode_reward=368.40 +/- 172.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=414000, episode_reward=291.80 +/- 176.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=415000, episode_reward=189.20 +/- 173.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=416000, episode_reward=165.60 +/- 257.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=417000, episode_reward=253.00 +/- 197.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=418000, episode_reward=219.00 +/- 243.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=419000, episode_reward=222.80 +/- 305.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=420000, episode_reward=355.60 +/- 119.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=421000, episode_reward=275.40 +/- 250.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=422000, episode_reward=210.40 +/- 206.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=423000, episode_reward=115.80 +/- 185.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=424000, episode_reward=324.20 +/- 182.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=425000, episode_reward=327.40 +/- 101.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=426000, episode_reward=295.80 +/- 133.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=427000, episode_reward=315.20 +/- 187.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=428000, episode_reward=370.80 +/- 263.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=429000, episode_reward=209.40 +/- 195.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=430000, episode_reward=305.40 +/- 160.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=431000, episode_reward=243.20 +/- 217.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=432000, episode_reward=201.00 +/- 185.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=433000, episode_reward=239.60 +/- 208.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=434000, episode_reward=99.20 +/- 135.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=435000, episode_reward=319.60 +/- 210.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=436000, episode_reward=362.20 +/- 168.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=437000, episode_reward=272.40 +/- 82.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=438000, episode_reward=221.60 +/- 218.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=439000, episode_reward=222.80 +/- 94.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=440000, episode_reward=249.80 +/- 224.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=441000, episode_reward=280.60 +/- 200.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=442000, episode_reward=94.20 +/- 204.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=443000, episode_reward=324.00 +/- 161.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=444000, episode_reward=94.60 +/- 118.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=445000, episode_reward=355.20 +/- 118.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=446000, episode_reward=180.00 +/- 93.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=447000, episode_reward=154.00 +/- 183.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=448000, episode_reward=156.20 +/- 312.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=449000, episode_reward=197.40 +/- 222.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=450000, episode_reward=167.20 +/- 107.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=451000, episode_reward=165.80 +/- 165.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=452000, episode_reward=365.80 +/- 279.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=453000, episode_reward=252.20 +/- 62.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=454000, episode_reward=329.20 +/- 139.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=455000, episode_reward=227.60 +/- 198.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=456000, episode_reward=92.40 +/- 111.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=457000, episode_reward=309.40 +/- 190.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=458000, episode_reward=227.00 +/- 227.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=459000, episode_reward=337.80 +/- 156.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=460000, episode_reward=298.60 +/- 176.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=461000, episode_reward=240.80 +/- 213.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=462000, episode_reward=155.20 +/- 163.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=463000, episode_reward=325.60 +/- 151.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=464000, episode_reward=274.60 +/- 108.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=465000, episode_reward=292.00 +/- 117.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=466000, episode_reward=229.40 +/- 183.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=467000, episode_reward=204.80 +/- 173.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=468000, episode_reward=279.60 +/- 176.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=469000, episode_reward=84.00 +/- 264.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=470000, episode_reward=24.40 +/- 88.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=471000, episode_reward=196.60 +/- 276.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=472000, episode_reward=153.00 +/- 183.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=473000, episode_reward=415.40 +/- 255.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=474000, episode_reward=282.20 +/- 90.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=475000, episode_reward=423.40 +/- 66.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=476000, episode_reward=195.40 +/- 148.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=477000, episode_reward=263.40 +/- 216.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=478000, episode_reward=100.80 +/- 132.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=479000, episode_reward=207.80 +/- 142.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=480000, episode_reward=259.80 +/- 185.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=481000, episode_reward=257.60 +/- 309.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=482000, episode_reward=177.20 +/- 246.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=483000, episode_reward=254.20 +/- 136.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=484000, episode_reward=247.20 +/- 211.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=485000, episode_reward=238.60 +/- 49.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=486000, episode_reward=294.80 +/- 158.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=487000, episode_reward=390.40 +/- 138.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=488000, episode_reward=279.60 +/- 259.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=489000, episode_reward=331.20 +/- 100.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=490000, episode_reward=158.40 +/- 279.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=491000, episode_reward=195.40 +/- 162.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=492000, episode_reward=278.40 +/- 182.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=493000, episode_reward=192.20 +/- 78.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=494000, episode_reward=194.40 +/- 166.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=495000, episode_reward=305.20 +/- 90.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=496000, episode_reward=171.60 +/- 212.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=497000, episode_reward=209.80 +/- 198.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=498000, episode_reward=295.80 +/- 87.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=499000, episode_reward=139.40 +/- 223.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=500000, episode_reward=235.60 +/- 199.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Gamma = 0.9, Learning Rate = 3e-05, Activation Function = ReLU, Neurons = 64, Process noise = 0.1, Measurement noise = 0.0\n",
      "Eval num_timesteps=1000, episode_reward=-6675.00 +/- 25.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=-6717.00 +/- 49.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=3000, episode_reward=-6711.40 +/- 80.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=4000, episode_reward=-6598.80 +/- 51.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=5000, episode_reward=-6657.80 +/- 105.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=6000, episode_reward=-6692.80 +/- 91.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=7000, episode_reward=-6691.60 +/- 73.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=8000, episode_reward=-6679.40 +/- 49.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=9000, episode_reward=-6648.00 +/- 90.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=10000, episode_reward=-6676.20 +/- 80.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=11000, episode_reward=-6686.40 +/- 76.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=12000, episode_reward=-6679.80 +/- 79.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=13000, episode_reward=-6622.40 +/- 82.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=14000, episode_reward=-6677.00 +/- 64.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=15000, episode_reward=-6621.60 +/- 61.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=16000, episode_reward=-6674.40 +/- 90.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=17000, episode_reward=-6640.60 +/- 72.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=18000, episode_reward=-6717.00 +/- 22.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=19000, episode_reward=-6619.40 +/- 84.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=20000, episode_reward=-6608.00 +/- 32.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=21000, episode_reward=-6691.80 +/- 65.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=22000, episode_reward=-6690.20 +/- 65.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=23000, episode_reward=-6629.00 +/- 73.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=24000, episode_reward=-6660.60 +/- 15.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=25000, episode_reward=-6713.60 +/- 97.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=26000, episode_reward=-6678.20 +/- 57.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=27000, episode_reward=-6754.60 +/- 19.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=28000, episode_reward=-6662.60 +/- 84.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=29000, episode_reward=-6678.60 +/- 149.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=30000, episode_reward=-6627.40 +/- 80.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=31000, episode_reward=-6681.60 +/- 84.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=32000, episode_reward=-6595.00 +/- 56.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=33000, episode_reward=-6659.60 +/- 54.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=34000, episode_reward=-6685.00 +/- 99.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=35000, episode_reward=-6674.00 +/- 86.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=36000, episode_reward=-6674.80 +/- 83.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=37000, episode_reward=-6642.20 +/- 85.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=38000, episode_reward=-6639.60 +/- 26.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=39000, episode_reward=-6692.00 +/- 43.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=40000, episode_reward=-6662.40 +/- 64.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=41000, episode_reward=-6675.60 +/- 77.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=42000, episode_reward=-6691.20 +/- 25.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=43000, episode_reward=-6714.00 +/- 42.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=44000, episode_reward=-6651.60 +/- 57.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=45000, episode_reward=-6658.40 +/- 61.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=46000, episode_reward=-6680.40 +/- 103.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=47000, episode_reward=-6669.20 +/- 60.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=48000, episode_reward=-6685.40 +/- 67.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=49000, episode_reward=-6660.00 +/- 88.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=50000, episode_reward=-6630.40 +/- 73.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=51000, episode_reward=-1390.00 +/- 585.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=52000, episode_reward=-3152.00 +/- 299.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=53000, episode_reward=-1508.40 +/- 327.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=54000, episode_reward=363.60 +/- 248.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=55000, episode_reward=333.20 +/- 343.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=56000, episode_reward=452.60 +/- 248.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=57000, episode_reward=348.20 +/- 177.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=58000, episode_reward=442.00 +/- 232.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=59000, episode_reward=384.60 +/- 190.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=60000, episode_reward=247.60 +/- 253.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=61000, episode_reward=340.80 +/- 158.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=62000, episode_reward=-709.20 +/- 854.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=63000, episode_reward=-179.80 +/- 511.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=64000, episode_reward=176.40 +/- 239.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=65000, episode_reward=-194.40 +/- 298.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=66000, episode_reward=-292.20 +/- 277.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=67000, episode_reward=-468.20 +/- 559.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=68000, episode_reward=187.60 +/- 67.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=69000, episode_reward=46.20 +/- 274.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=70000, episode_reward=-604.40 +/- 629.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=71000, episode_reward=150.40 +/- 260.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=72000, episode_reward=6.20 +/- 356.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=73000, episode_reward=-173.20 +/- 288.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=74000, episode_reward=102.20 +/- 606.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=75000, episode_reward=161.40 +/- 348.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=76000, episode_reward=78.20 +/- 402.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=77000, episode_reward=-87.20 +/- 373.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=78000, episode_reward=-35.20 +/- 225.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=79000, episode_reward=123.00 +/- 229.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=80000, episode_reward=215.20 +/- 123.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=81000, episode_reward=99.60 +/- 266.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=82000, episode_reward=474.40 +/- 156.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=83000, episode_reward=-20.00 +/- 236.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=84000, episode_reward=283.00 +/- 297.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=85000, episode_reward=268.60 +/- 192.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=86000, episode_reward=134.80 +/- 138.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=87000, episode_reward=2.20 +/- 281.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=88000, episode_reward=139.60 +/- 247.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=89000, episode_reward=107.40 +/- 202.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=90000, episode_reward=267.80 +/- 118.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=91000, episode_reward=214.20 +/- 319.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=92000, episode_reward=136.20 +/- 128.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=93000, episode_reward=178.60 +/- 170.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=94000, episode_reward=143.40 +/- 158.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=95000, episode_reward=87.20 +/- 155.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=96000, episode_reward=198.40 +/- 206.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=97000, episode_reward=415.60 +/- 241.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=98000, episode_reward=197.00 +/- 175.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=99000, episode_reward=212.80 +/- 98.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=100000, episode_reward=266.80 +/- 139.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=101000, episode_reward=273.80 +/- 175.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=102000, episode_reward=56.60 +/- 220.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=103000, episode_reward=233.40 +/- 176.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=104000, episode_reward=127.40 +/- 184.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=105000, episode_reward=147.80 +/- 186.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=106000, episode_reward=163.60 +/- 83.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=107000, episode_reward=138.20 +/- 242.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=108000, episode_reward=171.00 +/- 230.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=109000, episode_reward=480.00 +/- 77.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=110000, episode_reward=352.80 +/- 95.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=111000, episode_reward=141.00 +/- 222.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=112000, episode_reward=148.60 +/- 176.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=113000, episode_reward=191.20 +/- 274.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=114000, episode_reward=119.60 +/- 75.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=115000, episode_reward=334.80 +/- 163.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=116000, episode_reward=236.20 +/- 124.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=117000, episode_reward=343.40 +/- 115.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=118000, episode_reward=255.20 +/- 177.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=119000, episode_reward=169.40 +/- 212.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=120000, episode_reward=414.40 +/- 159.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=121000, episode_reward=169.80 +/- 186.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=122000, episode_reward=165.60 +/- 114.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=123000, episode_reward=322.80 +/- 193.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=124000, episode_reward=119.40 +/- 83.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=125000, episode_reward=188.20 +/- 143.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=126000, episode_reward=218.40 +/- 107.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=127000, episode_reward=165.80 +/- 133.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=128000, episode_reward=390.20 +/- 142.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=129000, episode_reward=444.80 +/- 92.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=130000, episode_reward=364.40 +/- 231.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=131000, episode_reward=196.60 +/- 164.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=132000, episode_reward=284.40 +/- 134.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=133000, episode_reward=100.60 +/- 87.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=134000, episode_reward=321.40 +/- 186.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=135000, episode_reward=332.80 +/- 249.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=136000, episode_reward=414.20 +/- 290.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=137000, episode_reward=224.20 +/- 154.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=138000, episode_reward=280.40 +/- 150.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=139000, episode_reward=319.20 +/- 168.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=140000, episode_reward=98.00 +/- 273.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=141000, episode_reward=259.00 +/- 195.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=142000, episode_reward=145.00 +/- 53.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=143000, episode_reward=287.20 +/- 141.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=144000, episode_reward=199.60 +/- 130.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=145000, episode_reward=301.80 +/- 41.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=146000, episode_reward=307.00 +/- 344.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=147000, episode_reward=221.40 +/- 127.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=148000, episode_reward=132.40 +/- 115.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=149000, episode_reward=171.80 +/- 97.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=150000, episode_reward=209.20 +/- 129.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=151000, episode_reward=270.60 +/- 207.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=152000, episode_reward=364.40 +/- 199.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=153000, episode_reward=366.00 +/- 224.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=154000, episode_reward=258.00 +/- 234.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=155000, episode_reward=327.40 +/- 45.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=156000, episode_reward=387.00 +/- 31.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=157000, episode_reward=142.80 +/- 176.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=158000, episode_reward=306.00 +/- 121.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=159000, episode_reward=310.20 +/- 186.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=160000, episode_reward=427.60 +/- 220.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=161000, episode_reward=309.20 +/- 128.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=162000, episode_reward=226.20 +/- 146.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=163000, episode_reward=330.80 +/- 232.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=164000, episode_reward=230.80 +/- 124.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=165000, episode_reward=140.20 +/- 76.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=166000, episode_reward=243.00 +/- 100.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=167000, episode_reward=322.60 +/- 177.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=168000, episode_reward=152.20 +/- 273.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=169000, episode_reward=284.40 +/- 145.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=170000, episode_reward=425.60 +/- 149.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=171000, episode_reward=251.40 +/- 96.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=172000, episode_reward=389.00 +/- 85.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=173000, episode_reward=160.40 +/- 150.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=174000, episode_reward=204.40 +/- 187.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=175000, episode_reward=148.40 +/- 96.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=176000, episode_reward=-54.20 +/- 294.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=177000, episode_reward=182.00 +/- 133.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=178000, episode_reward=196.80 +/- 204.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=179000, episode_reward=65.40 +/- 249.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=180000, episode_reward=449.60 +/- 52.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=181000, episode_reward=173.60 +/- 164.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=182000, episode_reward=332.20 +/- 155.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=183000, episode_reward=162.60 +/- 194.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=184000, episode_reward=288.80 +/- 275.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=185000, episode_reward=118.60 +/- 58.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=186000, episode_reward=203.20 +/- 246.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=187000, episode_reward=221.00 +/- 209.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=188000, episode_reward=163.40 +/- 271.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=189000, episode_reward=168.40 +/- 116.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=190000, episode_reward=186.80 +/- 302.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=191000, episode_reward=226.20 +/- 144.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=192000, episode_reward=205.20 +/- 160.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=193000, episode_reward=90.80 +/- 55.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=194000, episode_reward=103.60 +/- 108.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=195000, episode_reward=388.00 +/- 101.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=196000, episode_reward=330.40 +/- 111.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=197000, episode_reward=440.40 +/- 52.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=198000, episode_reward=285.60 +/- 171.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=199000, episode_reward=331.00 +/- 163.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=200000, episode_reward=322.00 +/- 171.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=201000, episode_reward=344.00 +/- 129.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=202000, episode_reward=341.60 +/- 141.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=203000, episode_reward=177.80 +/- 110.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=204000, episode_reward=372.40 +/- 143.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=205000, episode_reward=303.60 +/- 222.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=206000, episode_reward=411.40 +/- 135.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=207000, episode_reward=216.00 +/- 152.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=208000, episode_reward=172.60 +/- 231.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=209000, episode_reward=326.80 +/- 109.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=210000, episode_reward=134.20 +/- 148.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=211000, episode_reward=489.60 +/- 173.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=212000, episode_reward=241.40 +/- 192.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=213000, episode_reward=454.40 +/- 148.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=214000, episode_reward=336.80 +/- 189.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=215000, episode_reward=282.20 +/- 244.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=216000, episode_reward=302.40 +/- 167.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=217000, episode_reward=174.20 +/- 337.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=218000, episode_reward=385.00 +/- 149.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=219000, episode_reward=329.20 +/- 197.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=220000, episode_reward=175.20 +/- 155.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=221000, episode_reward=161.60 +/- 96.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=222000, episode_reward=118.80 +/- 115.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=223000, episode_reward=310.00 +/- 175.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=224000, episode_reward=282.60 +/- 223.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=225000, episode_reward=375.60 +/- 95.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=226000, episode_reward=374.60 +/- 129.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=227000, episode_reward=165.20 +/- 231.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=228000, episode_reward=275.40 +/- 158.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=229000, episode_reward=341.20 +/- 142.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=230000, episode_reward=267.20 +/- 211.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=231000, episode_reward=384.60 +/- 204.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=232000, episode_reward=89.60 +/- 122.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=233000, episode_reward=220.60 +/- 297.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=234000, episode_reward=234.00 +/- 155.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=235000, episode_reward=235.20 +/- 405.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=236000, episode_reward=183.40 +/- 148.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=237000, episode_reward=175.60 +/- 177.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=238000, episode_reward=358.00 +/- 148.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=239000, episode_reward=302.20 +/- 113.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=240000, episode_reward=209.20 +/- 161.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=241000, episode_reward=258.20 +/- 237.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=242000, episode_reward=263.20 +/- 205.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=243000, episode_reward=395.80 +/- 139.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=244000, episode_reward=262.40 +/- 118.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=245000, episode_reward=282.20 +/- 137.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=246000, episode_reward=220.00 +/- 210.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=247000, episode_reward=274.60 +/- 66.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=248000, episode_reward=317.40 +/- 197.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=249000, episode_reward=249.40 +/- 214.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=250000, episode_reward=111.40 +/- 230.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=251000, episode_reward=282.60 +/- 162.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=252000, episode_reward=186.80 +/- 195.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=253000, episode_reward=386.80 +/- 209.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=254000, episode_reward=300.40 +/- 173.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=255000, episode_reward=252.40 +/- 161.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=256000, episode_reward=194.00 +/- 169.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=257000, episode_reward=289.40 +/- 130.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=258000, episode_reward=204.60 +/- 148.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=259000, episode_reward=187.20 +/- 270.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=260000, episode_reward=304.60 +/- 268.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=261000, episode_reward=175.20 +/- 183.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=262000, episode_reward=245.00 +/- 126.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=263000, episode_reward=296.60 +/- 93.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=264000, episode_reward=242.60 +/- 190.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=265000, episode_reward=396.20 +/- 136.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=266000, episode_reward=378.60 +/- 108.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=267000, episode_reward=224.20 +/- 163.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=268000, episode_reward=193.80 +/- 106.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=269000, episode_reward=417.60 +/- 57.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=270000, episode_reward=253.80 +/- 113.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=271000, episode_reward=215.00 +/- 200.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=272000, episode_reward=418.20 +/- 88.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=273000, episode_reward=273.00 +/- 170.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=274000, episode_reward=406.00 +/- 253.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=275000, episode_reward=265.00 +/- 265.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=276000, episode_reward=238.60 +/- 42.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=277000, episode_reward=315.60 +/- 149.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=278000, episode_reward=285.00 +/- 102.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=279000, episode_reward=301.40 +/- 175.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=280000, episode_reward=465.00 +/- 161.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=281000, episode_reward=189.60 +/- 194.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=282000, episode_reward=141.80 +/- 335.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=283000, episode_reward=395.80 +/- 83.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=284000, episode_reward=468.00 +/- 178.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=285000, episode_reward=412.60 +/- 125.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=286000, episode_reward=422.80 +/- 220.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=287000, episode_reward=248.80 +/- 226.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=288000, episode_reward=356.40 +/- 214.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=289000, episode_reward=365.60 +/- 257.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=290000, episode_reward=139.20 +/- 308.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=291000, episode_reward=235.80 +/- 82.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=292000, episode_reward=234.20 +/- 195.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=293000, episode_reward=107.20 +/- 79.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=294000, episode_reward=313.60 +/- 156.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=295000, episode_reward=364.60 +/- 117.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=296000, episode_reward=331.80 +/- 182.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=297000, episode_reward=183.40 +/- 251.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=298000, episode_reward=224.60 +/- 114.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=299000, episode_reward=550.00 +/- 189.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=300000, episode_reward=151.40 +/- 137.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=301000, episode_reward=368.20 +/- 235.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=302000, episode_reward=326.60 +/- 103.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=303000, episode_reward=331.80 +/- 102.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=304000, episode_reward=181.60 +/- 94.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=305000, episode_reward=273.20 +/- 254.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=306000, episode_reward=268.40 +/- 245.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=307000, episode_reward=309.20 +/- 164.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=308000, episode_reward=238.00 +/- 197.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=309000, episode_reward=414.60 +/- 170.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=310000, episode_reward=117.20 +/- 273.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=311000, episode_reward=135.60 +/- 186.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=312000, episode_reward=223.80 +/- 339.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=313000, episode_reward=247.40 +/- 160.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=314000, episode_reward=138.20 +/- 210.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=315000, episode_reward=211.80 +/- 197.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=316000, episode_reward=320.40 +/- 111.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=317000, episode_reward=313.20 +/- 208.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=318000, episode_reward=369.80 +/- 202.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=319000, episode_reward=162.60 +/- 205.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=320000, episode_reward=210.60 +/- 205.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=321000, episode_reward=312.20 +/- 96.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=322000, episode_reward=293.60 +/- 139.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=323000, episode_reward=408.40 +/- 117.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=324000, episode_reward=348.20 +/- 155.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=325000, episode_reward=212.60 +/- 241.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=326000, episode_reward=339.20 +/- 235.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=327000, episode_reward=207.20 +/- 158.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=328000, episode_reward=329.60 +/- 143.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=329000, episode_reward=134.00 +/- 119.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=330000, episode_reward=344.20 +/- 118.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=331000, episode_reward=376.80 +/- 251.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=332000, episode_reward=208.80 +/- 62.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=333000, episode_reward=305.20 +/- 190.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=334000, episode_reward=191.60 +/- 219.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=335000, episode_reward=286.80 +/- 95.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=336000, episode_reward=377.20 +/- 244.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=337000, episode_reward=234.40 +/- 95.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=338000, episode_reward=296.00 +/- 138.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=339000, episode_reward=118.80 +/- 136.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=340000, episode_reward=209.00 +/- 217.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=341000, episode_reward=377.80 +/- 161.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=342000, episode_reward=457.00 +/- 116.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=343000, episode_reward=434.60 +/- 278.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=344000, episode_reward=304.80 +/- 156.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=345000, episode_reward=204.60 +/- 177.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=346000, episode_reward=408.20 +/- 68.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=347000, episode_reward=449.80 +/- 104.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=348000, episode_reward=349.00 +/- 160.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=349000, episode_reward=304.60 +/- 231.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=350000, episode_reward=316.00 +/- 178.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=351000, episode_reward=436.60 +/- 97.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=352000, episode_reward=249.80 +/- 160.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=353000, episode_reward=200.40 +/- 139.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=354000, episode_reward=187.00 +/- 220.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=355000, episode_reward=298.00 +/- 112.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=356000, episode_reward=326.40 +/- 129.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=357000, episode_reward=332.20 +/- 244.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=358000, episode_reward=342.40 +/- 86.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=359000, episode_reward=360.20 +/- 183.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=360000, episode_reward=391.20 +/- 144.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=361000, episode_reward=182.00 +/- 165.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=362000, episode_reward=274.20 +/- 258.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=363000, episode_reward=219.60 +/- 126.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=364000, episode_reward=333.20 +/- 119.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=365000, episode_reward=348.60 +/- 112.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=366000, episode_reward=234.80 +/- 178.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=367000, episode_reward=198.20 +/- 100.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=368000, episode_reward=391.00 +/- 106.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=369000, episode_reward=362.20 +/- 197.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=370000, episode_reward=172.00 +/- 97.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=371000, episode_reward=276.40 +/- 75.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=372000, episode_reward=258.80 +/- 251.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=373000, episode_reward=281.80 +/- 183.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=374000, episode_reward=263.60 +/- 158.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=375000, episode_reward=210.20 +/- 148.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=376000, episode_reward=295.00 +/- 148.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=377000, episode_reward=310.80 +/- 252.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=378000, episode_reward=311.40 +/- 217.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=379000, episode_reward=331.00 +/- 256.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=380000, episode_reward=435.60 +/- 108.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=381000, episode_reward=289.60 +/- 103.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=382000, episode_reward=293.80 +/- 133.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=383000, episode_reward=155.20 +/- 87.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=384000, episode_reward=284.20 +/- 132.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=385000, episode_reward=433.00 +/- 169.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=386000, episode_reward=249.00 +/- 211.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=387000, episode_reward=500.00 +/- 148.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=388000, episode_reward=445.20 +/- 93.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=389000, episode_reward=218.60 +/- 169.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=390000, episode_reward=222.80 +/- 102.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=391000, episode_reward=334.00 +/- 70.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=392000, episode_reward=279.40 +/- 192.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=393000, episode_reward=322.60 +/- 109.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=394000, episode_reward=231.60 +/- 314.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=395000, episode_reward=384.80 +/- 162.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=396000, episode_reward=277.80 +/- 192.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=397000, episode_reward=439.20 +/- 176.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=398000, episode_reward=266.40 +/- 297.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=399000, episode_reward=485.80 +/- 206.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=400000, episode_reward=242.60 +/- 253.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=401000, episode_reward=353.00 +/- 241.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=402000, episode_reward=477.20 +/- 51.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=403000, episode_reward=422.00 +/- 253.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=404000, episode_reward=427.40 +/- 252.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=405000, episode_reward=314.00 +/- 221.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=406000, episode_reward=172.20 +/- 146.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=407000, episode_reward=438.60 +/- 117.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=408000, episode_reward=459.00 +/- 217.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=409000, episode_reward=402.60 +/- 111.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=410000, episode_reward=256.40 +/- 98.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=411000, episode_reward=366.40 +/- 209.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=412000, episode_reward=303.20 +/- 88.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=413000, episode_reward=527.00 +/- 140.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=414000, episode_reward=382.80 +/- 127.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=415000, episode_reward=360.40 +/- 148.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=416000, episode_reward=383.20 +/- 262.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=417000, episode_reward=250.60 +/- 131.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=418000, episode_reward=393.40 +/- 79.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=419000, episode_reward=193.00 +/- 173.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=420000, episode_reward=280.20 +/- 150.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=421000, episode_reward=194.60 +/- 122.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=422000, episode_reward=313.60 +/- 107.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=423000, episode_reward=245.60 +/- 149.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=424000, episode_reward=357.80 +/- 77.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=425000, episode_reward=479.20 +/- 194.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=426000, episode_reward=487.80 +/- 102.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=427000, episode_reward=427.00 +/- 139.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=428000, episode_reward=346.40 +/- 141.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=429000, episode_reward=327.80 +/- 215.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=430000, episode_reward=146.20 +/- 295.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=431000, episode_reward=482.40 +/- 107.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=432000, episode_reward=418.00 +/- 151.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=433000, episode_reward=231.20 +/- 124.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=434000, episode_reward=439.00 +/- 171.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=435000, episode_reward=339.20 +/- 138.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=436000, episode_reward=285.80 +/- 92.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=437000, episode_reward=361.40 +/- 149.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=438000, episode_reward=415.80 +/- 179.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=439000, episode_reward=348.60 +/- 171.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=440000, episode_reward=347.00 +/- 162.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=441000, episode_reward=348.20 +/- 111.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=442000, episode_reward=430.00 +/- 248.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=443000, episode_reward=304.60 +/- 225.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=444000, episode_reward=107.80 +/- 140.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=445000, episode_reward=256.20 +/- 234.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=446000, episode_reward=350.00 +/- 116.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=447000, episode_reward=348.40 +/- 109.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=448000, episode_reward=207.20 +/- 199.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=449000, episode_reward=441.80 +/- 167.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=450000, episode_reward=320.80 +/- 234.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=451000, episode_reward=309.00 +/- 186.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=452000, episode_reward=361.60 +/- 314.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=453000, episode_reward=244.80 +/- 232.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=454000, episode_reward=308.20 +/- 216.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=455000, episode_reward=385.60 +/- 253.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=456000, episode_reward=266.60 +/- 120.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=457000, episode_reward=404.60 +/- 113.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=458000, episode_reward=277.40 +/- 135.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=459000, episode_reward=345.00 +/- 128.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=460000, episode_reward=465.40 +/- 52.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=461000, episode_reward=291.20 +/- 84.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=462000, episode_reward=270.80 +/- 80.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=463000, episode_reward=332.80 +/- 142.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=464000, episode_reward=252.80 +/- 144.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=465000, episode_reward=335.60 +/- 134.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=466000, episode_reward=253.40 +/- 173.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=467000, episode_reward=270.80 +/- 292.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=468000, episode_reward=362.80 +/- 66.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=469000, episode_reward=276.80 +/- 183.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=470000, episode_reward=209.20 +/- 253.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=471000, episode_reward=265.20 +/- 109.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=472000, episode_reward=393.80 +/- 236.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=473000, episode_reward=241.60 +/- 222.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=474000, episode_reward=395.20 +/- 160.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=475000, episode_reward=180.80 +/- 278.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=476000, episode_reward=405.00 +/- 239.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=477000, episode_reward=181.80 +/- 188.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=478000, episode_reward=521.20 +/- 121.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=479000, episode_reward=495.40 +/- 126.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=480000, episode_reward=263.40 +/- 105.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=481000, episode_reward=288.60 +/- 216.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=482000, episode_reward=268.20 +/- 143.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=483000, episode_reward=433.00 +/- 112.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=484000, episode_reward=304.40 +/- 154.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=485000, episode_reward=401.60 +/- 227.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=486000, episode_reward=104.60 +/- 201.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=487000, episode_reward=320.80 +/- 30.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=488000, episode_reward=200.00 +/- 85.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=489000, episode_reward=225.80 +/- 132.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=490000, episode_reward=316.60 +/- 144.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=491000, episode_reward=350.60 +/- 185.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=492000, episode_reward=451.20 +/- 122.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=493000, episode_reward=194.20 +/- 196.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=494000, episode_reward=226.40 +/- 139.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=495000, episode_reward=275.60 +/- 143.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=496000, episode_reward=363.20 +/- 294.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=497000, episode_reward=183.00 +/- 174.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=498000, episode_reward=311.80 +/- 82.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=499000, episode_reward=230.20 +/- 197.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=500000, episode_reward=225.80 +/- 115.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Gamma = 0.99, Learning Rate = 0.003, Activation Function = Tanh, Neurons = 8, Process noise = 0.1, Measurement noise = 0.0\n",
      "Eval num_timesteps=1000, episode_reward=-615.00 +/- 69.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=-493.60 +/- 165.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3000, episode_reward=-598.20 +/- 51.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=4000, episode_reward=-611.40 +/- 63.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=5000, episode_reward=-604.20 +/- 22.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=6000, episode_reward=-551.20 +/- 69.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=7000, episode_reward=-625.60 +/- 96.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=8000, episode_reward=-522.00 +/- 95.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=9000, episode_reward=-616.60 +/- 39.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=10000, episode_reward=-712.60 +/- 81.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=11000, episode_reward=-637.20 +/- 123.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=12000, episode_reward=-618.80 +/- 25.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=13000, episode_reward=-585.20 +/- 86.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=14000, episode_reward=-573.80 +/- 85.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=15000, episode_reward=-564.20 +/- 67.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=16000, episode_reward=-538.40 +/- 51.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=17000, episode_reward=-569.00 +/- 41.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=18000, episode_reward=-614.60 +/- 111.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=19000, episode_reward=-582.00 +/- 50.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=20000, episode_reward=-643.80 +/- 68.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=21000, episode_reward=-649.00 +/- 50.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=22000, episode_reward=-628.40 +/- 58.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=23000, episode_reward=-635.60 +/- 88.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=24000, episode_reward=-602.20 +/- 32.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=25000, episode_reward=-552.80 +/- 125.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=26000, episode_reward=-578.20 +/- 85.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=27000, episode_reward=-580.00 +/- 55.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=28000, episode_reward=-574.40 +/- 70.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=29000, episode_reward=-624.40 +/- 63.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=30000, episode_reward=-587.20 +/- 91.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=31000, episode_reward=-587.40 +/- 125.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=32000, episode_reward=-643.60 +/- 58.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=33000, episode_reward=-532.20 +/- 76.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=34000, episode_reward=-655.20 +/- 160.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=35000, episode_reward=-563.00 +/- 59.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=36000, episode_reward=-566.00 +/- 38.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=37000, episode_reward=-587.60 +/- 38.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=38000, episode_reward=-614.80 +/- 42.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=39000, episode_reward=-648.40 +/- 112.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=40000, episode_reward=-646.60 +/- 52.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=41000, episode_reward=-640.00 +/- 84.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=42000, episode_reward=-653.80 +/- 63.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=43000, episode_reward=-605.40 +/- 51.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=44000, episode_reward=-598.00 +/- 111.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=45000, episode_reward=-646.80 +/- 33.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=46000, episode_reward=-557.00 +/- 61.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=47000, episode_reward=-542.80 +/- 42.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=48000, episode_reward=-619.60 +/- 76.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=49000, episode_reward=-596.20 +/- 66.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=50000, episode_reward=-576.20 +/- 93.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=51000, episode_reward=72.20 +/- 468.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=52000, episode_reward=-203.80 +/- 251.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=53000, episode_reward=-2302.80 +/- 221.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=54000, episode_reward=-1136.40 +/- 159.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=55000, episode_reward=-488.40 +/- 157.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=56000, episode_reward=-399.40 +/- 163.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=57000, episode_reward=-610.40 +/- 558.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=58000, episode_reward=-219.80 +/- 471.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=59000, episode_reward=-214.60 +/- 386.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=60000, episode_reward=-1014.60 +/- 225.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=61000, episode_reward=-493.00 +/- 266.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=62000, episode_reward=-1036.80 +/- 209.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=63000, episode_reward=-860.20 +/- 202.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=64000, episode_reward=-626.60 +/- 312.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=65000, episode_reward=-230.40 +/- 43.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=66000, episode_reward=-164.00 +/- 129.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=67000, episode_reward=-63.60 +/- 82.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=68000, episode_reward=-121.20 +/- 254.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=69000, episode_reward=-91.20 +/- 171.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=70000, episode_reward=31.40 +/- 143.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=71000, episode_reward=-137.20 +/- 470.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=72000, episode_reward=-126.40 +/- 155.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=73000, episode_reward=-23.60 +/- 136.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=74000, episode_reward=185.80 +/- 230.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=75000, episode_reward=17.40 +/- 415.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=76000, episode_reward=168.80 +/- 141.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=77000, episode_reward=221.40 +/- 109.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=78000, episode_reward=82.20 +/- 244.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=79000, episode_reward=172.60 +/- 159.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=80000, episode_reward=-71.20 +/- 394.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=81000, episode_reward=-42.40 +/- 182.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=82000, episode_reward=-205.40 +/- 190.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=83000, episode_reward=-129.00 +/- 238.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=84000, episode_reward=160.60 +/- 117.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=85000, episode_reward=97.00 +/- 240.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=86000, episode_reward=132.80 +/- 162.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=87000, episode_reward=467.40 +/- 507.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=88000, episode_reward=337.20 +/- 578.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=89000, episode_reward=368.40 +/- 399.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=90000, episode_reward=366.40 +/- 350.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=91000, episode_reward=14.60 +/- 288.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=92000, episode_reward=-90.20 +/- 408.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=93000, episode_reward=-551.20 +/- 315.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=94000, episode_reward=147.40 +/- 289.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=95000, episode_reward=-68.80 +/- 412.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=96000, episode_reward=-11.40 +/- 348.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=97000, episode_reward=603.00 +/- 264.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=98000, episode_reward=228.40 +/- 289.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=99000, episode_reward=170.40 +/- 669.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=100000, episode_reward=715.80 +/- 382.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=101000, episode_reward=234.40 +/- 220.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=102000, episode_reward=-189.40 +/- 307.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=103000, episode_reward=549.00 +/- 277.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=104000, episode_reward=182.80 +/- 299.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=105000, episode_reward=542.00 +/- 230.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=106000, episode_reward=157.40 +/- 414.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=107000, episode_reward=658.60 +/- 312.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=108000, episode_reward=693.60 +/- 250.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=109000, episode_reward=668.80 +/- 343.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=110000, episode_reward=340.00 +/- 310.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=111000, episode_reward=-172.60 +/- 422.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=112000, episode_reward=202.80 +/- 261.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=113000, episode_reward=144.20 +/- 85.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=114000, episode_reward=-486.60 +/- 2559.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=115000, episode_reward=-541.40 +/- 1710.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=116000, episode_reward=767.00 +/- 414.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=117000, episode_reward=599.60 +/- 419.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=118000, episode_reward=493.60 +/- 249.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=119000, episode_reward=725.60 +/- 103.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=120000, episode_reward=955.00 +/- 291.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=121000, episode_reward=879.60 +/- 152.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=122000, episode_reward=540.40 +/- 299.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=123000, episode_reward=545.60 +/- 335.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=124000, episode_reward=803.20 +/- 263.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=125000, episode_reward=1117.80 +/- 390.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=126000, episode_reward=811.00 +/- 248.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=127000, episode_reward=-2629.40 +/- 6740.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=128000, episode_reward=741.80 +/- 321.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=129000, episode_reward=1051.20 +/- 511.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=130000, episode_reward=863.60 +/- 432.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=131000, episode_reward=-1311.80 +/- 3697.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=132000, episode_reward=-104.00 +/- 1603.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=133000, episode_reward=553.00 +/- 315.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=134000, episode_reward=860.00 +/- 318.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=135000, episode_reward=857.20 +/- 227.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=136000, episode_reward=463.20 +/- 331.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=137000, episode_reward=411.80 +/- 179.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=138000, episode_reward=655.40 +/- 366.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=139000, episode_reward=885.20 +/- 371.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=140000, episode_reward=790.20 +/- 477.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=141000, episode_reward=667.60 +/- 238.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=142000, episode_reward=753.80 +/- 204.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=143000, episode_reward=898.40 +/- 362.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=144000, episode_reward=-2769.60 +/- 7192.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=145000, episode_reward=1116.60 +/- 100.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=146000, episode_reward=842.80 +/- 225.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=147000, episode_reward=818.20 +/- 167.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=148000, episode_reward=-1570.20 +/- 4160.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=149000, episode_reward=941.40 +/- 236.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=150000, episode_reward=592.20 +/- 343.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=151000, episode_reward=1102.60 +/- 369.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=152000, episode_reward=640.60 +/- 442.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=153000, episode_reward=608.00 +/- 377.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=154000, episode_reward=744.80 +/- 218.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=155000, episode_reward=974.40 +/- 447.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=156000, episode_reward=804.20 +/- 224.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=157000, episode_reward=1122.60 +/- 483.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=158000, episode_reward=721.20 +/- 389.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=159000, episode_reward=193.40 +/- 1420.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=160000, episode_reward=851.00 +/- 170.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=161000, episode_reward=-1377.60 +/- 3865.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=162000, episode_reward=542.80 +/- 149.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=163000, episode_reward=686.80 +/- 382.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=164000, episode_reward=1034.20 +/- 265.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=165000, episode_reward=622.20 +/- 365.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=166000, episode_reward=-397.80 +/- 2517.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=167000, episode_reward=877.20 +/- 453.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=168000, episode_reward=917.20 +/- 329.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=169000, episode_reward=-841.60 +/- 2809.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=170000, episode_reward=774.00 +/- 388.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=171000, episode_reward=764.80 +/- 369.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=172000, episode_reward=758.00 +/- 261.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=173000, episode_reward=620.40 +/- 223.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=174000, episode_reward=912.40 +/- 75.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=175000, episode_reward=755.40 +/- 241.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=176000, episode_reward=645.40 +/- 207.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=177000, episode_reward=745.60 +/- 116.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=178000, episode_reward=310.40 +/- 629.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=179000, episode_reward=851.00 +/- 227.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=180000, episode_reward=709.20 +/- 394.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=181000, episode_reward=733.00 +/- 294.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=182000, episode_reward=331.60 +/- 254.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=183000, episode_reward=769.80 +/- 275.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=184000, episode_reward=355.80 +/- 431.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=185000, episode_reward=771.60 +/- 351.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=186000, episode_reward=566.40 +/- 457.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=187000, episode_reward=1047.80 +/- 410.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=188000, episode_reward=826.20 +/- 290.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=189000, episode_reward=832.80 +/- 237.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=190000, episode_reward=860.80 +/- 337.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=191000, episode_reward=476.80 +/- 301.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=192000, episode_reward=736.80 +/- 257.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=193000, episode_reward=78.80 +/- 1774.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=194000, episode_reward=815.40 +/- 212.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=195000, episode_reward=654.80 +/- 260.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=196000, episode_reward=-2998.00 +/- 7202.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=197000, episode_reward=871.60 +/- 407.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=198000, episode_reward=823.60 +/- 514.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=199000, episode_reward=501.20 +/- 321.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=200000, episode_reward=709.00 +/- 350.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=201000, episode_reward=641.40 +/- 259.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=202000, episode_reward=622.60 +/- 343.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=203000, episode_reward=-118.60 +/- 1187.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=204000, episode_reward=282.00 +/- 1215.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=205000, episode_reward=693.80 +/- 473.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=206000, episode_reward=504.20 +/- 216.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=207000, episode_reward=794.40 +/- 325.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=208000, episode_reward=764.80 +/- 331.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=209000, episode_reward=880.00 +/- 344.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=210000, episode_reward=-1035.60 +/- 4218.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=211000, episode_reward=740.60 +/- 314.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=212000, episode_reward=524.80 +/- 221.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=213000, episode_reward=657.60 +/- 283.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=214000, episode_reward=-1136.20 +/- 1704.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=215000, episode_reward=1057.60 +/- 274.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=216000, episode_reward=928.20 +/- 375.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=217000, episode_reward=820.40 +/- 366.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=218000, episode_reward=858.00 +/- 138.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=219000, episode_reward=969.20 +/- 323.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=220000, episode_reward=866.20 +/- 200.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=221000, episode_reward=249.20 +/- 1056.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=222000, episode_reward=859.20 +/- 151.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=223000, episode_reward=821.40 +/- 445.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=224000, episode_reward=844.80 +/- 438.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=225000, episode_reward=730.00 +/- 140.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=226000, episode_reward=796.40 +/- 78.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=227000, episode_reward=840.80 +/- 357.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=228000, episode_reward=839.60 +/- 366.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=229000, episode_reward=882.40 +/- 273.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=230000, episode_reward=417.80 +/- 356.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=231000, episode_reward=386.40 +/- 464.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=232000, episode_reward=385.80 +/- 618.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=233000, episode_reward=699.20 +/- 515.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=234000, episode_reward=382.40 +/- 113.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=235000, episode_reward=780.20 +/- 390.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=236000, episode_reward=912.80 +/- 336.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=237000, episode_reward=490.80 +/- 504.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=238000, episode_reward=-5151.80 +/- 7810.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=239000, episode_reward=616.40 +/- 270.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=240000, episode_reward=638.40 +/- 349.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=241000, episode_reward=419.80 +/- 451.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=242000, episode_reward=242.60 +/- 364.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=243000, episode_reward=-282.40 +/- 1329.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=244000, episode_reward=-1056.60 +/- 2994.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=245000, episode_reward=571.20 +/- 245.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=246000, episode_reward=645.40 +/- 365.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=247000, episode_reward=656.00 +/- 525.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=248000, episode_reward=667.20 +/- 307.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=249000, episode_reward=-4938.60 +/- 7015.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=250000, episode_reward=787.00 +/- 254.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=251000, episode_reward=376.20 +/- 239.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=252000, episode_reward=-106.00 +/- 1121.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=253000, episode_reward=-1648.20 +/- 4426.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=254000, episode_reward=563.80 +/- 246.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=255000, episode_reward=-1573.40 +/- 2015.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=256000, episode_reward=963.60 +/- 250.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=257000, episode_reward=908.60 +/- 293.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=258000, episode_reward=470.20 +/- 420.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=259000, episode_reward=752.00 +/- 253.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=260000, episode_reward=682.80 +/- 610.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=261000, episode_reward=35.80 +/- 278.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=262000, episode_reward=428.80 +/- 106.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=263000, episode_reward=512.00 +/- 620.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=264000, episode_reward=566.00 +/- 118.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=265000, episode_reward=613.20 +/- 322.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=266000, episode_reward=789.80 +/- 497.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=267000, episode_reward=750.60 +/- 199.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=268000, episode_reward=447.00 +/- 238.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=269000, episode_reward=-1693.40 +/- 5204.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=270000, episode_reward=-1455.60 +/- 4345.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=271000, episode_reward=221.80 +/- 262.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=272000, episode_reward=659.60 +/- 373.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=273000, episode_reward=348.00 +/- 507.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=274000, episode_reward=-1527.00 +/- 4315.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=275000, episode_reward=701.20 +/- 239.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=276000, episode_reward=688.60 +/- 327.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=277000, episode_reward=133.20 +/- 312.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=278000, episode_reward=273.40 +/- 573.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=279000, episode_reward=329.00 +/- 239.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=280000, episode_reward=857.00 +/- 344.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=281000, episode_reward=-354.60 +/- 1351.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=282000, episode_reward=467.20 +/- 243.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=283000, episode_reward=548.20 +/- 408.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=284000, episode_reward=-91.00 +/- 1234.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=285000, episode_reward=712.60 +/- 273.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=286000, episode_reward=632.60 +/- 274.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=287000, episode_reward=562.20 +/- 480.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=288000, episode_reward=808.60 +/- 419.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=289000, episode_reward=822.20 +/- 542.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=290000, episode_reward=515.20 +/- 417.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=291000, episode_reward=160.20 +/- 235.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=292000, episode_reward=-82.80 +/- 334.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=293000, episode_reward=679.40 +/- 306.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=294000, episode_reward=931.60 +/- 396.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=295000, episode_reward=732.00 +/- 545.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=296000, episode_reward=534.80 +/- 431.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=297000, episode_reward=559.40 +/- 290.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=298000, episode_reward=-2534.80 +/- 6723.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=299000, episode_reward=673.00 +/- 230.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=300000, episode_reward=716.00 +/- 449.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=301000, episode_reward=-3750.00 +/- 6111.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=302000, episode_reward=354.20 +/- 449.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=303000, episode_reward=612.60 +/- 312.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=304000, episode_reward=656.60 +/- 274.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=305000, episode_reward=503.60 +/- 189.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=306000, episode_reward=-583.60 +/- 2473.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=307000, episode_reward=852.60 +/- 337.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=308000, episode_reward=634.80 +/- 165.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=309000, episode_reward=938.60 +/- 139.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=310000, episode_reward=497.80 +/- 176.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=311000, episode_reward=168.80 +/- 353.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=312000, episode_reward=574.40 +/- 356.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=313000, episode_reward=575.20 +/- 350.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=314000, episode_reward=348.80 +/- 238.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=315000, episode_reward=663.60 +/- 531.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=316000, episode_reward=880.60 +/- 302.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=317000, episode_reward=492.40 +/- 185.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=318000, episode_reward=537.60 +/- 343.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=319000, episode_reward=445.60 +/- 255.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=320000, episode_reward=435.80 +/- 199.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=321000, episode_reward=571.40 +/- 319.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=322000, episode_reward=36.00 +/- 309.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=323000, episode_reward=113.80 +/- 372.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=324000, episode_reward=-408.20 +/- 1089.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=325000, episode_reward=515.60 +/- 145.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=326000, episode_reward=148.00 +/- 355.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=327000, episode_reward=617.00 +/- 232.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=328000, episode_reward=511.20 +/- 315.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=329000, episode_reward=304.20 +/- 479.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=330000, episode_reward=646.20 +/- 221.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=331000, episode_reward=172.80 +/- 403.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=332000, episode_reward=259.20 +/- 429.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=333000, episode_reward=384.00 +/- 193.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=334000, episode_reward=408.80 +/- 327.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=335000, episode_reward=581.60 +/- 405.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=336000, episode_reward=183.20 +/- 292.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=337000, episode_reward=853.60 +/- 261.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=338000, episode_reward=421.80 +/- 371.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=339000, episode_reward=564.80 +/- 434.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=340000, episode_reward=574.00 +/- 322.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=341000, episode_reward=491.80 +/- 189.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=342000, episode_reward=724.60 +/- 429.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=343000, episode_reward=593.00 +/- 162.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=344000, episode_reward=672.60 +/- 211.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=345000, episode_reward=699.80 +/- 269.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=346000, episode_reward=850.00 +/- 179.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=347000, episode_reward=709.00 +/- 379.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=348000, episode_reward=749.80 +/- 326.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=349000, episode_reward=628.40 +/- 171.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=350000, episode_reward=816.80 +/- 172.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=351000, episode_reward=634.80 +/- 379.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=352000, episode_reward=953.20 +/- 409.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=353000, episode_reward=632.40 +/- 328.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=354000, episode_reward=416.40 +/- 389.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=355000, episode_reward=576.00 +/- 546.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=356000, episode_reward=667.80 +/- 229.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=357000, episode_reward=-2099.80 +/- 5400.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=358000, episode_reward=-2422.80 +/- 6251.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=359000, episode_reward=-1310.00 +/- 3957.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=360000, episode_reward=988.40 +/- 151.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=361000, episode_reward=525.80 +/- 311.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=362000, episode_reward=339.80 +/- 122.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=363000, episode_reward=830.80 +/- 540.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=364000, episode_reward=221.40 +/- 282.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=365000, episode_reward=173.20 +/- 499.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=366000, episode_reward=-404.40 +/- 2217.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=367000, episode_reward=482.80 +/- 275.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=368000, episode_reward=-1755.40 +/- 5236.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=369000, episode_reward=920.80 +/- 268.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=370000, episode_reward=814.40 +/- 245.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=371000, episode_reward=116.00 +/- 467.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=372000, episode_reward=348.20 +/- 207.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=373000, episode_reward=569.40 +/- 343.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=374000, episode_reward=398.40 +/- 98.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=375000, episode_reward=572.60 +/- 255.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=376000, episode_reward=486.00 +/- 192.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=377000, episode_reward=699.20 +/- 210.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=378000, episode_reward=336.60 +/- 199.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=379000, episode_reward=828.20 +/- 282.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=380000, episode_reward=564.40 +/- 305.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=381000, episode_reward=180.00 +/- 276.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=382000, episode_reward=-2546.40 +/- 5599.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=383000, episode_reward=320.20 +/- 225.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=384000, episode_reward=441.80 +/- 173.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=385000, episode_reward=539.80 +/- 233.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=386000, episode_reward=408.20 +/- 303.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=387000, episode_reward=711.00 +/- 341.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=388000, episode_reward=126.40 +/- 420.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=389000, episode_reward=438.60 +/- 313.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=390000, episode_reward=-269.00 +/- 1867.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=391000, episode_reward=3.00 +/- 456.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=392000, episode_reward=46.60 +/- 282.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=393000, episode_reward=443.80 +/- 221.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=394000, episode_reward=218.60 +/- 980.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=395000, episode_reward=567.20 +/- 291.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=396000, episode_reward=637.00 +/- 321.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=397000, episode_reward=618.60 +/- 285.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=398000, episode_reward=656.60 +/- 132.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=399000, episode_reward=-2384.40 +/- 6283.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=400000, episode_reward=454.60 +/- 137.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=401000, episode_reward=157.40 +/- 802.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=402000, episode_reward=545.20 +/- 214.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=403000, episode_reward=798.00 +/- 290.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=404000, episode_reward=785.00 +/- 283.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=405000, episode_reward=837.00 +/- 373.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=406000, episode_reward=480.40 +/- 143.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=407000, episode_reward=560.80 +/- 237.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=408000, episode_reward=857.40 +/- 126.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=409000, episode_reward=632.00 +/- 297.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=410000, episode_reward=768.20 +/- 281.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=411000, episode_reward=610.00 +/- 37.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=412000, episode_reward=519.20 +/- 207.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=413000, episode_reward=257.00 +/- 267.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=414000, episode_reward=772.00 +/- 279.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=415000, episode_reward=585.00 +/- 478.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=416000, episode_reward=845.40 +/- 273.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=417000, episode_reward=595.60 +/- 262.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=418000, episode_reward=-1892.60 +/- 5176.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=419000, episode_reward=-994.00 +/- 3175.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=420000, episode_reward=624.20 +/- 207.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=421000, episode_reward=388.80 +/- 341.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=422000, episode_reward=-1511.20 +/- 3707.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=423000, episode_reward=253.00 +/- 171.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=424000, episode_reward=575.00 +/- 181.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=425000, episode_reward=474.80 +/- 295.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=426000, episode_reward=-2469.80 +/- 3903.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=427000, episode_reward=514.80 +/- 117.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=428000, episode_reward=931.80 +/- 189.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=429000, episode_reward=699.80 +/- 240.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=430000, episode_reward=876.00 +/- 377.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=431000, episode_reward=410.00 +/- 403.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=432000, episode_reward=131.20 +/- 164.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=433000, episode_reward=383.60 +/- 613.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=434000, episode_reward=519.00 +/- 347.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=435000, episode_reward=807.20 +/- 432.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=436000, episode_reward=589.20 +/- 447.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=437000, episode_reward=284.00 +/- 474.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=438000, episode_reward=846.60 +/- 274.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=439000, episode_reward=779.20 +/- 150.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=440000, episode_reward=638.60 +/- 440.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=441000, episode_reward=-563.60 +/- 727.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=442000, episode_reward=462.60 +/- 375.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=443000, episode_reward=114.60 +/- 304.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=444000, episode_reward=354.60 +/- 345.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=445000, episode_reward=385.00 +/- 317.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=446000, episode_reward=540.40 +/- 326.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=447000, episode_reward=440.60 +/- 379.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=448000, episode_reward=685.60 +/- 251.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=449000, episode_reward=628.00 +/- 182.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=450000, episode_reward=446.60 +/- 220.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=451000, episode_reward=329.20 +/- 330.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=452000, episode_reward=400.20 +/- 369.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=453000, episode_reward=216.00 +/- 451.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=454000, episode_reward=390.40 +/- 238.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=455000, episode_reward=399.80 +/- 174.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=456000, episode_reward=673.20 +/- 558.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=457000, episode_reward=130.00 +/- 306.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=458000, episode_reward=465.00 +/- 324.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=459000, episode_reward=441.60 +/- 414.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=460000, episode_reward=416.00 +/- 259.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=461000, episode_reward=439.60 +/- 385.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=462000, episode_reward=598.20 +/- 467.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=463000, episode_reward=513.80 +/- 395.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=464000, episode_reward=339.40 +/- 147.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=465000, episode_reward=612.20 +/- 284.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=466000, episode_reward=538.00 +/- 327.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=467000, episode_reward=640.80 +/- 327.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=468000, episode_reward=761.00 +/- 321.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=469000, episode_reward=482.00 +/- 83.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=470000, episode_reward=858.00 +/- 366.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=471000, episode_reward=867.00 +/- 432.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=472000, episode_reward=198.60 +/- 395.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=473000, episode_reward=691.80 +/- 281.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=474000, episode_reward=-624.00 +/- 2410.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=475000, episode_reward=-1397.40 +/- 4058.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=476000, episode_reward=635.20 +/- 146.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=477000, episode_reward=550.40 +/- 253.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=478000, episode_reward=-1766.60 +/- 4652.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=479000, episode_reward=595.80 +/- 205.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=480000, episode_reward=497.40 +/- 112.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=481000, episode_reward=266.80 +/- 404.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=482000, episode_reward=155.20 +/- 413.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=483000, episode_reward=537.20 +/- 204.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=484000, episode_reward=690.00 +/- 221.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=485000, episode_reward=686.80 +/- 297.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=486000, episode_reward=915.80 +/- 116.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=487000, episode_reward=548.40 +/- 347.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=488000, episode_reward=621.00 +/- 424.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=489000, episode_reward=807.00 +/- 233.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=490000, episode_reward=936.20 +/- 268.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=491000, episode_reward=193.80 +/- 533.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=492000, episode_reward=301.80 +/- 322.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=493000, episode_reward=644.40 +/- 305.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=494000, episode_reward=557.60 +/- 441.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=495000, episode_reward=576.20 +/- 243.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=496000, episode_reward=863.60 +/- 553.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=497000, episode_reward=784.60 +/- 320.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=498000, episode_reward=450.20 +/- 230.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=499000, episode_reward=979.80 +/- 230.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=500000, episode_reward=897.80 +/- 188.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Gamma = 0.99, Learning Rate = 0.003, Activation Function = Tanh, Neurons = 32, Process noise = 0.1, Measurement noise = 0.0\n",
      "Eval num_timesteps=1000, episode_reward=-5866.80 +/- 534.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=-5331.60 +/- 2097.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3000, episode_reward=-6648.60 +/- 952.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=4000, episode_reward=-6642.20 +/- 1039.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=5000, episode_reward=-4579.80 +/- 1115.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=6000, episode_reward=-7581.40 +/- 1387.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=7000, episode_reward=-7160.40 +/- 1016.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=8000, episode_reward=-5806.80 +/- 965.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=9000, episode_reward=-5721.40 +/- 2028.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=10000, episode_reward=-4975.20 +/- 1401.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=11000, episode_reward=-5588.20 +/- 1882.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=12000, episode_reward=-6456.40 +/- 2056.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=13000, episode_reward=-6295.60 +/- 1315.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=14000, episode_reward=-6042.60 +/- 803.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=15000, episode_reward=-5897.20 +/- 1390.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=16000, episode_reward=-6583.80 +/- 2265.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=17000, episode_reward=-6564.80 +/- 2200.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=18000, episode_reward=-4889.00 +/- 1305.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=19000, episode_reward=-5382.40 +/- 1734.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=20000, episode_reward=-7194.20 +/- 3050.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=21000, episode_reward=-6320.60 +/- 1882.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=22000, episode_reward=-6693.40 +/- 752.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=23000, episode_reward=-5690.80 +/- 1784.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=24000, episode_reward=-6915.80 +/- 2145.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=25000, episode_reward=-6185.80 +/- 1977.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=26000, episode_reward=-4028.60 +/- 2549.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=27000, episode_reward=-6382.60 +/- 1390.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=28000, episode_reward=-4614.60 +/- 1146.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=29000, episode_reward=-5436.40 +/- 1139.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=30000, episode_reward=-6356.60 +/- 2592.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=31000, episode_reward=-5704.20 +/- 1985.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=32000, episode_reward=-3929.20 +/- 1355.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=33000, episode_reward=-6411.20 +/- 1439.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=34000, episode_reward=-6551.00 +/- 991.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=35000, episode_reward=-6961.60 +/- 1567.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=36000, episode_reward=-4805.80 +/- 1865.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=37000, episode_reward=-6999.40 +/- 1217.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=38000, episode_reward=-6478.20 +/- 2245.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=39000, episode_reward=-6481.20 +/- 1370.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=40000, episode_reward=-5858.60 +/- 722.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=41000, episode_reward=-5315.00 +/- 1622.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=42000, episode_reward=-5512.40 +/- 1524.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=43000, episode_reward=-6912.20 +/- 407.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=44000, episode_reward=-6079.60 +/- 1401.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=45000, episode_reward=-6259.00 +/- 1563.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=46000, episode_reward=-7125.80 +/- 705.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=47000, episode_reward=-6090.40 +/- 2100.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=48000, episode_reward=-6223.60 +/- 1387.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=49000, episode_reward=-5840.80 +/- 1982.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=50000, episode_reward=-5492.80 +/- 1358.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=51000, episode_reward=-5805.00 +/- 2296.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=52000, episode_reward=26.80 +/- 262.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=53000, episode_reward=209.40 +/- 333.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=54000, episode_reward=-181.40 +/- 305.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=55000, episode_reward=-99.40 +/- 171.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=56000, episode_reward=48.00 +/- 319.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=57000, episode_reward=10.60 +/- 320.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=58000, episode_reward=-68.20 +/- 171.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=59000, episode_reward=-104.80 +/- 145.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=60000, episode_reward=79.80 +/- 209.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=61000, episode_reward=-4849.80 +/- 1299.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=62000, episode_reward=-4627.20 +/- 1489.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=63000, episode_reward=-4820.20 +/- 1445.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=64000, episode_reward=-540.60 +/- 257.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=65000, episode_reward=-4114.80 +/- 2582.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=66000, episode_reward=-3185.80 +/- 2897.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=67000, episode_reward=-4691.80 +/- 3840.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=68000, episode_reward=19.00 +/- 237.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=69000, episode_reward=-3587.60 +/- 347.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=70000, episode_reward=-204.80 +/- 143.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=71000, episode_reward=-140.00 +/- 274.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=72000, episode_reward=-329.60 +/- 381.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=73000, episode_reward=260.80 +/- 253.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=74000, episode_reward=89.80 +/- 241.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=75000, episode_reward=-82.40 +/- 123.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=76000, episode_reward=191.60 +/- 267.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=77000, episode_reward=250.00 +/- 129.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=78000, episode_reward=312.20 +/- 127.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=79000, episode_reward=258.20 +/- 262.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=80000, episode_reward=139.00 +/- 132.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=81000, episode_reward=-51.00 +/- 331.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=82000, episode_reward=391.80 +/- 160.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=83000, episode_reward=294.00 +/- 174.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=84000, episode_reward=198.00 +/- 191.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=85000, episode_reward=298.60 +/- 237.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=86000, episode_reward=327.80 +/- 110.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=87000, episode_reward=390.60 +/- 229.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=88000, episode_reward=419.20 +/- 136.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=89000, episode_reward=193.40 +/- 166.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=90000, episode_reward=244.20 +/- 122.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=91000, episode_reward=256.60 +/- 231.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=92000, episode_reward=410.60 +/- 169.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=93000, episode_reward=296.00 +/- 106.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=94000, episode_reward=235.60 +/- 239.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=95000, episode_reward=382.20 +/- 190.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=96000, episode_reward=270.40 +/- 166.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=97000, episode_reward=520.60 +/- 237.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=98000, episode_reward=494.80 +/- 239.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=99000, episode_reward=590.40 +/- 181.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=100000, episode_reward=738.40 +/- 199.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=101000, episode_reward=523.80 +/- 266.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=102000, episode_reward=817.20 +/- 363.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=103000, episode_reward=994.40 +/- 187.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=104000, episode_reward=740.80 +/- 230.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=105000, episode_reward=640.00 +/- 361.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=106000, episode_reward=879.40 +/- 84.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=107000, episode_reward=996.40 +/- 380.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=108000, episode_reward=521.20 +/- 247.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=109000, episode_reward=932.40 +/- 184.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=110000, episode_reward=1056.20 +/- 329.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=111000, episode_reward=-1919.20 +/- 5466.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=112000, episode_reward=823.60 +/- 224.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=113000, episode_reward=799.40 +/- 128.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=114000, episode_reward=833.20 +/- 268.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=115000, episode_reward=484.00 +/- 296.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=116000, episode_reward=759.60 +/- 423.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=117000, episode_reward=919.40 +/- 337.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=118000, episode_reward=616.40 +/- 169.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=119000, episode_reward=708.80 +/- 364.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=120000, episode_reward=767.80 +/- 274.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=121000, episode_reward=721.80 +/- 358.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=122000, episode_reward=1028.00 +/- 251.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=123000, episode_reward=972.80 +/- 239.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=124000, episode_reward=1014.00 +/- 403.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=125000, episode_reward=1073.00 +/- 204.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=126000, episode_reward=710.60 +/- 207.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=127000, episode_reward=981.20 +/- 150.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=128000, episode_reward=980.20 +/- 139.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=129000, episode_reward=663.80 +/- 223.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=130000, episode_reward=954.80 +/- 234.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=131000, episode_reward=-331.60 +/- 2865.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=132000, episode_reward=986.20 +/- 418.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=133000, episode_reward=972.80 +/- 291.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=134000, episode_reward=926.40 +/- 273.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=135000, episode_reward=1409.40 +/- 236.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=136000, episode_reward=907.60 +/- 331.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=137000, episode_reward=531.40 +/- 100.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=138000, episode_reward=1013.40 +/- 53.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=139000, episode_reward=1006.80 +/- 171.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=140000, episode_reward=747.40 +/- 264.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=141000, episode_reward=706.80 +/- 157.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=142000, episode_reward=816.60 +/- 116.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=143000, episode_reward=919.40 +/- 348.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=144000, episode_reward=546.60 +/- 202.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=145000, episode_reward=926.40 +/- 395.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=146000, episode_reward=773.20 +/- 82.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=147000, episode_reward=943.40 +/- 233.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=148000, episode_reward=1036.20 +/- 220.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=149000, episode_reward=971.00 +/- 219.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=150000, episode_reward=685.40 +/- 143.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=151000, episode_reward=1056.40 +/- 178.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=152000, episode_reward=1007.20 +/- 152.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=153000, episode_reward=-326.20 +/- 1948.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=154000, episode_reward=643.60 +/- 294.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=155000, episode_reward=929.00 +/- 219.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=156000, episode_reward=702.20 +/- 121.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=157000, episode_reward=688.80 +/- 212.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=158000, episode_reward=-1738.40 +/- 5007.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=159000, episode_reward=599.20 +/- 30.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=160000, episode_reward=446.60 +/- 289.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=161000, episode_reward=980.00 +/- 181.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=162000, episode_reward=620.60 +/- 191.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=163000, episode_reward=801.40 +/- 483.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=164000, episode_reward=833.80 +/- 157.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=165000, episode_reward=723.20 +/- 186.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=166000, episode_reward=755.00 +/- 206.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=167000, episode_reward=1080.40 +/- 285.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=168000, episode_reward=683.80 +/- 479.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=169000, episode_reward=820.20 +/- 325.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=170000, episode_reward=817.20 +/- 202.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=171000, episode_reward=-884.00 +/- 3467.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=172000, episode_reward=871.60 +/- 280.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=173000, episode_reward=984.20 +/- 103.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=174000, episode_reward=1148.40 +/- 138.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=175000, episode_reward=808.60 +/- 341.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=176000, episode_reward=927.60 +/- 151.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=177000, episode_reward=1106.40 +/- 276.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=178000, episode_reward=765.40 +/- 318.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=179000, episode_reward=957.20 +/- 179.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=180000, episode_reward=1084.00 +/- 151.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=181000, episode_reward=591.80 +/- 257.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=182000, episode_reward=1117.40 +/- 280.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=183000, episode_reward=1011.00 +/- 386.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=184000, episode_reward=895.00 +/- 253.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=185000, episode_reward=761.80 +/- 414.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=186000, episode_reward=1038.40 +/- 340.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=187000, episode_reward=1042.20 +/- 315.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=188000, episode_reward=791.20 +/- 159.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=189000, episode_reward=870.60 +/- 235.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=190000, episode_reward=1069.00 +/- 89.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=191000, episode_reward=989.40 +/- 229.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=192000, episode_reward=1279.20 +/- 55.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=193000, episode_reward=992.00 +/- 366.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=194000, episode_reward=1166.60 +/- 218.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=195000, episode_reward=1034.00 +/- 350.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=196000, episode_reward=808.40 +/- 312.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=197000, episode_reward=1025.00 +/- 283.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=198000, episode_reward=988.80 +/- 221.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=199000, episode_reward=1184.20 +/- 203.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=200000, episode_reward=961.00 +/- 195.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=201000, episode_reward=504.80 +/- 158.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=202000, episode_reward=942.40 +/- 343.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=203000, episode_reward=1126.60 +/- 216.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=204000, episode_reward=692.60 +/- 366.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=205000, episode_reward=1010.40 +/- 243.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=206000, episode_reward=1146.40 +/- 246.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=207000, episode_reward=1177.80 +/- 160.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=208000, episode_reward=1251.80 +/- 307.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=209000, episode_reward=1296.60 +/- 171.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=210000, episode_reward=1058.00 +/- 298.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=211000, episode_reward=1015.00 +/- 454.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=212000, episode_reward=-3462.20 +/- 6277.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=213000, episode_reward=1264.40 +/- 81.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=214000, episode_reward=1152.20 +/- 152.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=215000, episode_reward=1342.00 +/- 207.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=216000, episode_reward=1259.60 +/- 131.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=217000, episode_reward=1352.00 +/- 300.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=218000, episode_reward=1170.40 +/- 167.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=219000, episode_reward=1299.00 +/- 281.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=220000, episode_reward=1260.80 +/- 293.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=221000, episode_reward=1094.80 +/- 132.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=222000, episode_reward=1327.80 +/- 229.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=223000, episode_reward=-339.40 +/- 3029.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=224000, episode_reward=1187.00 +/- 285.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=225000, episode_reward=911.20 +/- 174.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=226000, episode_reward=1156.00 +/- 193.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=227000, episode_reward=906.80 +/- 128.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=228000, episode_reward=962.00 +/- 321.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=229000, episode_reward=910.00 +/- 139.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=230000, episode_reward=1113.00 +/- 313.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=231000, episode_reward=736.60 +/- 214.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=232000, episode_reward=444.40 +/- 163.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=233000, episode_reward=863.20 +/- 261.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=234000, episode_reward=746.20 +/- 254.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=235000, episode_reward=844.20 +/- 413.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=236000, episode_reward=887.80 +/- 260.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=237000, episode_reward=858.20 +/- 228.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=238000, episode_reward=801.60 +/- 177.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=239000, episode_reward=662.20 +/- 321.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=240000, episode_reward=858.00 +/- 129.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=241000, episode_reward=908.40 +/- 253.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=242000, episode_reward=671.40 +/- 119.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=243000, episode_reward=850.80 +/- 387.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=244000, episode_reward=577.00 +/- 139.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=245000, episode_reward=810.80 +/- 319.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=246000, episode_reward=898.60 +/- 336.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=247000, episode_reward=985.40 +/- 246.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=248000, episode_reward=811.40 +/- 248.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=249000, episode_reward=614.80 +/- 289.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=250000, episode_reward=533.40 +/- 241.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=251000, episode_reward=780.40 +/- 343.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=252000, episode_reward=-1964.40 +/- 5196.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=253000, episode_reward=848.00 +/- 261.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=254000, episode_reward=624.60 +/- 138.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=255000, episode_reward=530.60 +/- 287.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=256000, episode_reward=723.80 +/- 336.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=257000, episode_reward=701.40 +/- 262.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=258000, episode_reward=940.40 +/- 182.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=259000, episode_reward=913.00 +/- 80.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=260000, episode_reward=633.20 +/- 203.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=261000, episode_reward=659.60 +/- 291.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=262000, episode_reward=1018.40 +/- 297.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=263000, episode_reward=767.20 +/- 86.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=264000, episode_reward=951.80 +/- 178.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=265000, episode_reward=897.20 +/- 241.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=266000, episode_reward=755.80 +/- 237.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=267000, episode_reward=945.20 +/- 193.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=268000, episode_reward=805.60 +/- 369.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=269000, episode_reward=726.00 +/- 201.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=270000, episode_reward=914.40 +/- 174.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=271000, episode_reward=970.80 +/- 243.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=272000, episode_reward=827.80 +/- 255.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=273000, episode_reward=914.60 +/- 377.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=274000, episode_reward=745.40 +/- 320.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=275000, episode_reward=937.40 +/- 238.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=276000, episode_reward=694.40 +/- 154.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=277000, episode_reward=753.60 +/- 262.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=278000, episode_reward=601.00 +/- 303.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=279000, episode_reward=705.20 +/- 276.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=280000, episode_reward=723.20 +/- 355.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=281000, episode_reward=926.60 +/- 700.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=282000, episode_reward=1297.20 +/- 398.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=283000, episode_reward=804.20 +/- 142.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=284000, episode_reward=860.40 +/- 151.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=285000, episode_reward=1391.40 +/- 227.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=286000, episode_reward=980.60 +/- 374.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=287000, episode_reward=910.00 +/- 342.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=288000, episode_reward=1221.40 +/- 502.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=289000, episode_reward=551.60 +/- 350.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=290000, episode_reward=745.00 +/- 369.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=291000, episode_reward=877.40 +/- 317.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=292000, episode_reward=946.80 +/- 222.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=293000, episode_reward=1239.20 +/- 334.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=294000, episode_reward=1141.20 +/- 277.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=295000, episode_reward=783.40 +/- 94.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=296000, episode_reward=1026.00 +/- 102.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=297000, episode_reward=733.80 +/- 325.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=298000, episode_reward=924.40 +/- 313.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=299000, episode_reward=1184.80 +/- 454.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=300000, episode_reward=1105.00 +/- 323.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=301000, episode_reward=1029.80 +/- 272.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=302000, episode_reward=1394.80 +/- 299.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=303000, episode_reward=1052.60 +/- 267.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=304000, episode_reward=1194.40 +/- 252.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=305000, episode_reward=1328.40 +/- 185.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=306000, episode_reward=1095.60 +/- 91.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=307000, episode_reward=1027.00 +/- 274.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=308000, episode_reward=1370.00 +/- 319.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=309000, episode_reward=993.00 +/- 70.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=310000, episode_reward=1411.60 +/- 335.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=311000, episode_reward=1140.60 +/- 549.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=312000, episode_reward=1331.80 +/- 198.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=313000, episode_reward=1282.80 +/- 222.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=314000, episode_reward=1119.40 +/- 312.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=315000, episode_reward=1159.80 +/- 179.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=316000, episode_reward=1188.80 +/- 89.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=317000, episode_reward=1313.00 +/- 304.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=318000, episode_reward=1184.20 +/- 202.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=319000, episode_reward=1169.40 +/- 305.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=320000, episode_reward=1419.80 +/- 349.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=321000, episode_reward=1299.00 +/- 221.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=322000, episode_reward=1272.20 +/- 398.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=323000, episode_reward=1046.40 +/- 249.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=324000, episode_reward=1173.80 +/- 374.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=325000, episode_reward=1179.20 +/- 281.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=326000, episode_reward=1208.60 +/- 258.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=327000, episode_reward=1182.20 +/- 347.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=328000, episode_reward=991.20 +/- 346.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=329000, episode_reward=970.80 +/- 216.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=330000, episode_reward=1098.20 +/- 252.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=331000, episode_reward=1254.00 +/- 185.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=332000, episode_reward=-276.40 +/- 2984.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=333000, episode_reward=1352.20 +/- 115.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=334000, episode_reward=1259.80 +/- 155.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=335000, episode_reward=1163.60 +/- 286.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=336000, episode_reward=1229.20 +/- 268.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=337000, episode_reward=1237.00 +/- 114.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=338000, episode_reward=1279.40 +/- 354.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=339000, episode_reward=1276.40 +/- 390.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=340000, episode_reward=1190.00 +/- 427.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=341000, episode_reward=862.40 +/- 174.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=342000, episode_reward=1315.00 +/- 240.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=343000, episode_reward=1405.20 +/- 285.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=344000, episode_reward=1174.00 +/- 123.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=345000, episode_reward=1163.00 +/- 468.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=346000, episode_reward=1651.00 +/- 141.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=347000, episode_reward=1232.40 +/- 365.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=348000, episode_reward=1393.40 +/- 309.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=349000, episode_reward=1385.80 +/- 480.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=350000, episode_reward=973.20 +/- 313.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=351000, episode_reward=1003.40 +/- 477.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=352000, episode_reward=1206.60 +/- 354.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=353000, episode_reward=1337.80 +/- 158.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=354000, episode_reward=1361.60 +/- 223.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=355000, episode_reward=1266.20 +/- 159.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=356000, episode_reward=1313.80 +/- 201.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=357000, episode_reward=1294.00 +/- 405.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=358000, episode_reward=1448.60 +/- 182.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=359000, episode_reward=1088.80 +/- 413.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=360000, episode_reward=906.20 +/- 141.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=361000, episode_reward=1235.00 +/- 315.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=362000, episode_reward=927.60 +/- 221.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=363000, episode_reward=1330.00 +/- 213.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=364000, episode_reward=1182.20 +/- 148.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=365000, episode_reward=1227.40 +/- 365.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=366000, episode_reward=1345.20 +/- 311.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=367000, episode_reward=969.20 +/- 363.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=368000, episode_reward=909.80 +/- 363.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=369000, episode_reward=1047.80 +/- 331.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=370000, episode_reward=1253.00 +/- 357.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=371000, episode_reward=974.60 +/- 228.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=372000, episode_reward=1318.00 +/- 260.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=373000, episode_reward=1082.20 +/- 158.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=374000, episode_reward=1406.60 +/- 215.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=375000, episode_reward=1360.40 +/- 248.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=376000, episode_reward=1131.60 +/- 308.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=377000, episode_reward=1234.40 +/- 428.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=378000, episode_reward=992.00 +/- 201.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=379000, episode_reward=1347.20 +/- 401.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=380000, episode_reward=1361.20 +/- 302.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=381000, episode_reward=1022.60 +/- 288.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=382000, episode_reward=1154.60 +/- 90.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=383000, episode_reward=1162.60 +/- 326.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=384000, episode_reward=1073.60 +/- 300.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=385000, episode_reward=1173.80 +/- 252.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=386000, episode_reward=1373.20 +/- 136.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=387000, episode_reward=936.40 +/- 210.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=388000, episode_reward=1408.60 +/- 219.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=389000, episode_reward=1776.80 +/- 226.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=390000, episode_reward=1338.60 +/- 179.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=391000, episode_reward=1037.60 +/- 251.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=392000, episode_reward=1228.60 +/- 189.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=393000, episode_reward=1473.80 +/- 286.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=394000, episode_reward=1102.40 +/- 271.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=395000, episode_reward=1460.20 +/- 229.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=396000, episode_reward=1342.80 +/- 282.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=397000, episode_reward=1583.80 +/- 198.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=398000, episode_reward=1524.60 +/- 285.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=399000, episode_reward=1255.60 +/- 203.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=400000, episode_reward=1759.80 +/- 205.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=401000, episode_reward=1013.20 +/- 343.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=402000, episode_reward=1308.00 +/- 175.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=403000, episode_reward=1166.40 +/- 142.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=404000, episode_reward=1385.00 +/- 322.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=405000, episode_reward=1554.00 +/- 195.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=406000, episode_reward=1515.00 +/- 194.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=407000, episode_reward=1356.60 +/- 559.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=408000, episode_reward=1225.20 +/- 196.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=409000, episode_reward=1512.80 +/- 81.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=410000, episode_reward=1443.80 +/- 254.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=411000, episode_reward=1202.40 +/- 387.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=412000, episode_reward=1086.60 +/- 256.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=413000, episode_reward=1398.40 +/- 233.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=414000, episode_reward=1353.60 +/- 81.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=415000, episode_reward=1336.40 +/- 81.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=416000, episode_reward=945.40 +/- 202.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=417000, episode_reward=1766.40 +/- 360.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=418000, episode_reward=1617.20 +/- 256.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=419000, episode_reward=1346.80 +/- 201.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=420000, episode_reward=1382.20 +/- 244.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=421000, episode_reward=1405.60 +/- 129.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=422000, episode_reward=1284.20 +/- 62.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=423000, episode_reward=1437.60 +/- 158.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=424000, episode_reward=1185.40 +/- 193.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=425000, episode_reward=1574.60 +/- 65.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=426000, episode_reward=1707.40 +/- 268.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=427000, episode_reward=1625.80 +/- 256.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=428000, episode_reward=1470.80 +/- 297.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=429000, episode_reward=1081.80 +/- 290.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=430000, episode_reward=1459.80 +/- 180.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=431000, episode_reward=1099.20 +/- 317.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=432000, episode_reward=1457.20 +/- 258.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=433000, episode_reward=1226.00 +/- 270.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=434000, episode_reward=1471.00 +/- 271.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=435000, episode_reward=1367.40 +/- 419.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=436000, episode_reward=1430.60 +/- 370.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=437000, episode_reward=1484.20 +/- 197.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=438000, episode_reward=1270.00 +/- 253.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=439000, episode_reward=1524.60 +/- 306.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=440000, episode_reward=1380.20 +/- 303.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=441000, episode_reward=1233.80 +/- 109.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=442000, episode_reward=1126.20 +/- 330.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=443000, episode_reward=1561.60 +/- 450.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=444000, episode_reward=1559.00 +/- 269.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=445000, episode_reward=1315.20 +/- 205.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=446000, episode_reward=1372.00 +/- 265.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=447000, episode_reward=1111.20 +/- 75.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=448000, episode_reward=1290.00 +/- 191.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=449000, episode_reward=1661.40 +/- 157.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=450000, episode_reward=1526.20 +/- 149.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=451000, episode_reward=1043.80 +/- 145.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=452000, episode_reward=1515.40 +/- 177.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=453000, episode_reward=1255.80 +/- 179.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=454000, episode_reward=1505.80 +/- 151.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=455000, episode_reward=1311.60 +/- 439.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=456000, episode_reward=1385.00 +/- 439.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=457000, episode_reward=1490.40 +/- 163.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=458000, episode_reward=1316.60 +/- 475.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=459000, episode_reward=1691.60 +/- 168.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=460000, episode_reward=1442.80 +/- 216.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=461000, episode_reward=1199.40 +/- 192.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=462000, episode_reward=1100.20 +/- 231.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=463000, episode_reward=1210.60 +/- 213.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=464000, episode_reward=1490.60 +/- 172.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=465000, episode_reward=837.00 +/- 190.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=466000, episode_reward=1159.00 +/- 276.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=467000, episode_reward=1450.40 +/- 172.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=468000, episode_reward=1438.60 +/- 109.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=469000, episode_reward=1219.00 +/- 315.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=470000, episode_reward=1603.60 +/- 89.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=471000, episode_reward=1132.00 +/- 152.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=472000, episode_reward=992.20 +/- 148.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=473000, episode_reward=1038.40 +/- 290.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=474000, episode_reward=1202.20 +/- 210.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=475000, episode_reward=813.00 +/- 283.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=476000, episode_reward=932.00 +/- 127.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=477000, episode_reward=1363.40 +/- 141.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=478000, episode_reward=1148.40 +/- 166.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=479000, episode_reward=1073.60 +/- 175.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=480000, episode_reward=1120.20 +/- 307.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=481000, episode_reward=869.80 +/- 233.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=482000, episode_reward=996.40 +/- 522.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=483000, episode_reward=1358.60 +/- 152.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=484000, episode_reward=817.60 +/- 389.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=485000, episode_reward=1160.00 +/- 274.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=486000, episode_reward=966.40 +/- 170.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=487000, episode_reward=1139.80 +/- 444.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=488000, episode_reward=864.20 +/- 235.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=489000, episode_reward=464.40 +/- 159.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=490000, episode_reward=1454.80 +/- 209.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=491000, episode_reward=1007.20 +/- 399.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=492000, episode_reward=802.40 +/- 94.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=493000, episode_reward=740.20 +/- 178.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=494000, episode_reward=1117.00 +/- 80.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=495000, episode_reward=1063.20 +/- 326.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=496000, episode_reward=1126.00 +/- 440.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=497000, episode_reward=889.60 +/- 207.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=498000, episode_reward=1123.40 +/- 284.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=499000, episode_reward=1192.40 +/- 135.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=500000, episode_reward=1481.60 +/- 172.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Gamma = 0.99, Learning Rate = 0.003, Activation Function = Tanh, Neurons = 64, Process noise = 0.1, Measurement noise = 0.0\n",
      "Eval num_timesteps=1000, episode_reward=-1176.20 +/- 723.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=-1365.00 +/- 481.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=3000, episode_reward=-1490.40 +/- 464.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=4000, episode_reward=-1697.40 +/- 549.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=5000, episode_reward=-777.80 +/- 886.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=6000, episode_reward=-682.80 +/- 764.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=7000, episode_reward=-1742.60 +/- 683.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=8000, episode_reward=-1345.60 +/- 552.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=9000, episode_reward=-1350.00 +/- 633.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=10000, episode_reward=-1516.80 +/- 911.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=11000, episode_reward=-1079.60 +/- 806.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=12000, episode_reward=-1766.80 +/- 404.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=13000, episode_reward=-1395.60 +/- 798.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=14000, episode_reward=-1127.00 +/- 807.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=15000, episode_reward=-1694.60 +/- 484.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=16000, episode_reward=-1652.00 +/- 403.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=17000, episode_reward=-1122.40 +/- 880.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=18000, episode_reward=-1262.80 +/- 839.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=19000, episode_reward=-679.00 +/- 812.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=20000, episode_reward=-1366.00 +/- 683.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=21000, episode_reward=-1570.00 +/- 897.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=22000, episode_reward=-1776.00 +/- 441.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=23000, episode_reward=-1395.60 +/- 848.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=24000, episode_reward=-1015.00 +/- 655.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=25000, episode_reward=-1583.40 +/- 516.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=26000, episode_reward=-1304.40 +/- 319.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=27000, episode_reward=-967.40 +/- 990.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=28000, episode_reward=-1872.00 +/- 215.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=29000, episode_reward=-1289.40 +/- 956.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=30000, episode_reward=-1109.40 +/- 930.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=31000, episode_reward=-1224.80 +/- 753.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=32000, episode_reward=-1361.80 +/- 447.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=33000, episode_reward=-1096.00 +/- 765.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=34000, episode_reward=-1631.20 +/- 772.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=35000, episode_reward=-1086.20 +/- 874.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=36000, episode_reward=-1214.00 +/- 904.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=37000, episode_reward=-263.20 +/- 798.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=38000, episode_reward=-1311.00 +/- 786.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=39000, episode_reward=-1663.20 +/- 384.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=40000, episode_reward=-1265.40 +/- 569.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=41000, episode_reward=-1595.80 +/- 466.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=42000, episode_reward=-981.00 +/- 1012.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=43000, episode_reward=-1047.00 +/- 982.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=44000, episode_reward=-1644.00 +/- 284.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=45000, episode_reward=-726.00 +/- 568.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=46000, episode_reward=-1421.40 +/- 810.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=47000, episode_reward=-654.80 +/- 884.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=48000, episode_reward=-1086.60 +/- 1153.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=49000, episode_reward=-764.60 +/- 887.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=50000, episode_reward=-876.00 +/- 856.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=51000, episode_reward=-5308.00 +/- 1092.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=52000, episode_reward=252.40 +/- 242.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=53000, episode_reward=105.00 +/- 271.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=54000, episode_reward=-10.80 +/- 153.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=55000, episode_reward=-395.20 +/- 402.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=56000, episode_reward=-207.20 +/- 225.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=57000, episode_reward=-10.60 +/- 195.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=58000, episode_reward=-198.40 +/- 147.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=59000, episode_reward=-89.00 +/- 250.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=60000, episode_reward=-69.20 +/- 293.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=61000, episode_reward=-79.80 +/- 262.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=62000, episode_reward=-214.00 +/- 93.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=63000, episode_reward=90.60 +/- 207.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=64000, episode_reward=179.80 +/- 234.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=65000, episode_reward=-156.40 +/- 459.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=66000, episode_reward=213.60 +/- 139.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=67000, episode_reward=0.40 +/- 267.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=68000, episode_reward=61.00 +/- 133.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=69000, episode_reward=247.00 +/- 178.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=70000, episode_reward=337.20 +/- 191.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=71000, episode_reward=273.00 +/- 96.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=72000, episode_reward=64.60 +/- 216.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=73000, episode_reward=171.40 +/- 135.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=74000, episode_reward=320.20 +/- 272.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=75000, episode_reward=245.60 +/- 133.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=76000, episode_reward=869.40 +/- 142.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=77000, episode_reward=-1333.00 +/- 4525.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=78000, episode_reward=741.00 +/- 334.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=79000, episode_reward=369.40 +/- 231.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=80000, episode_reward=637.20 +/- 412.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=81000, episode_reward=657.60 +/- 330.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=82000, episode_reward=510.80 +/- 282.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=83000, episode_reward=783.20 +/- 78.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=84000, episode_reward=1007.00 +/- 194.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=85000, episode_reward=884.00 +/- 400.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=86000, episode_reward=1032.60 +/- 244.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=87000, episode_reward=-1331.20 +/- 3696.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=88000, episode_reward=630.60 +/- 216.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=89000, episode_reward=787.60 +/- 236.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=90000, episode_reward=797.00 +/- 209.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=91000, episode_reward=827.40 +/- 261.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=92000, episode_reward=821.00 +/- 276.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=93000, episode_reward=885.20 +/- 269.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=94000, episode_reward=800.60 +/- 354.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=95000, episode_reward=965.00 +/- 340.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=96000, episode_reward=702.80 +/- 222.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=97000, episode_reward=663.80 +/- 292.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=98000, episode_reward=511.00 +/- 244.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=99000, episode_reward=951.40 +/- 154.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=100000, episode_reward=608.40 +/- 339.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=101000, episode_reward=916.60 +/- 288.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=102000, episode_reward=726.80 +/- 237.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=103000, episode_reward=740.20 +/- 278.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=104000, episode_reward=654.60 +/- 304.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=105000, episode_reward=1085.20 +/- 240.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=106000, episode_reward=749.60 +/- 438.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=107000, episode_reward=754.40 +/- 227.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=108000, episode_reward=599.80 +/- 148.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=109000, episode_reward=-2255.40 +/- 5962.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=110000, episode_reward=-482.20 +/- 2697.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=111000, episode_reward=718.20 +/- 202.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=112000, episode_reward=-1135.80 +/- 4118.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=113000, episode_reward=992.80 +/- 212.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=114000, episode_reward=768.40 +/- 391.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=115000, episode_reward=860.80 +/- 347.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=116000, episode_reward=736.40 +/- 177.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=117000, episode_reward=744.80 +/- 497.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=118000, episode_reward=955.40 +/- 392.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=119000, episode_reward=777.20 +/- 356.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=120000, episode_reward=716.60 +/- 508.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=121000, episode_reward=855.40 +/- 237.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=122000, episode_reward=1195.20 +/- 353.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=123000, episode_reward=859.60 +/- 139.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=124000, episode_reward=970.40 +/- 273.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=125000, episode_reward=875.80 +/- 194.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=126000, episode_reward=809.80 +/- 234.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=127000, episode_reward=1057.40 +/- 309.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=128000, episode_reward=958.60 +/- 172.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=129000, episode_reward=918.80 +/- 266.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=130000, episode_reward=993.20 +/- 257.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=131000, episode_reward=845.20 +/- 413.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=132000, episode_reward=1057.20 +/- 172.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=133000, episode_reward=1217.20 +/- 351.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=134000, episode_reward=1212.00 +/- 229.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=135000, episode_reward=1316.60 +/- 366.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=136000, episode_reward=1140.60 +/- 185.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=137000, episode_reward=1204.20 +/- 286.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=138000, episode_reward=998.20 +/- 290.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=139000, episode_reward=1043.60 +/- 314.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=140000, episode_reward=1040.80 +/- 207.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=141000, episode_reward=1406.00 +/- 424.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=142000, episode_reward=1150.40 +/- 117.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=143000, episode_reward=885.60 +/- 158.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=144000, episode_reward=1126.80 +/- 265.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=145000, episode_reward=1030.20 +/- 320.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=146000, episode_reward=1306.00 +/- 218.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=147000, episode_reward=1284.20 +/- 235.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=148000, episode_reward=1061.80 +/- 314.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=149000, episode_reward=972.80 +/- 159.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=150000, episode_reward=1044.80 +/- 297.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=151000, episode_reward=893.60 +/- 351.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=152000, episode_reward=886.00 +/- 290.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=153000, episode_reward=1326.20 +/- 264.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=154000, episode_reward=1111.80 +/- 189.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=155000, episode_reward=740.00 +/- 425.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=156000, episode_reward=1343.40 +/- 87.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=157000, episode_reward=1081.60 +/- 300.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=158000, episode_reward=1144.00 +/- 335.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=159000, episode_reward=1116.20 +/- 280.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=160000, episode_reward=1010.00 +/- 140.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=161000, episode_reward=1082.40 +/- 53.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=162000, episode_reward=1022.00 +/- 449.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=163000, episode_reward=989.80 +/- 393.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=164000, episode_reward=775.00 +/- 365.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=165000, episode_reward=1277.40 +/- 190.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=166000, episode_reward=1085.60 +/- 299.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=167000, episode_reward=1295.40 +/- 312.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=168000, episode_reward=1020.20 +/- 122.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=169000, episode_reward=1098.00 +/- 479.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=170000, episode_reward=482.00 +/- 236.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=171000, episode_reward=863.00 +/- 434.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=172000, episode_reward=1456.80 +/- 244.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=173000, episode_reward=1267.40 +/- 223.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=174000, episode_reward=1091.00 +/- 205.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=175000, episode_reward=1128.40 +/- 288.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=176000, episode_reward=1237.00 +/- 122.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=177000, episode_reward=941.80 +/- 196.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=178000, episode_reward=1219.20 +/- 234.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=179000, episode_reward=1191.00 +/- 261.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=180000, episode_reward=1256.60 +/- 39.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=181000, episode_reward=980.60 +/- 160.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=182000, episode_reward=1360.20 +/- 209.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=183000, episode_reward=1070.80 +/- 348.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=184000, episode_reward=1123.60 +/- 127.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=185000, episode_reward=1105.80 +/- 301.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=186000, episode_reward=979.60 +/- 356.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=187000, episode_reward=1233.00 +/- 261.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=188000, episode_reward=1064.60 +/- 285.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=189000, episode_reward=1462.20 +/- 197.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=190000, episode_reward=1566.80 +/- 267.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=191000, episode_reward=1152.80 +/- 276.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=192000, episode_reward=1319.60 +/- 427.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=193000, episode_reward=1356.80 +/- 203.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=194000, episode_reward=-1470.00 +/- 5317.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=195000, episode_reward=1318.20 +/- 245.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=196000, episode_reward=1479.60 +/- 265.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=197000, episode_reward=1331.80 +/- 286.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=198000, episode_reward=1357.20 +/- 164.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=199000, episode_reward=1300.00 +/- 248.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=200000, episode_reward=1605.20 +/- 97.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=201000, episode_reward=1545.00 +/- 270.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=202000, episode_reward=1224.40 +/- 191.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=203000, episode_reward=1283.80 +/- 98.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=204000, episode_reward=1212.00 +/- 127.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=205000, episode_reward=1509.80 +/- 280.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=206000, episode_reward=1088.40 +/- 262.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=207000, episode_reward=1060.60 +/- 275.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=208000, episode_reward=1155.40 +/- 400.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=209000, episode_reward=775.00 +/- 293.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=210000, episode_reward=1290.40 +/- 426.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=211000, episode_reward=1167.80 +/- 272.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=212000, episode_reward=1391.20 +/- 194.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=213000, episode_reward=1404.20 +/- 192.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=214000, episode_reward=1348.60 +/- 285.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=215000, episode_reward=1504.60 +/- 158.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=216000, episode_reward=1304.40 +/- 218.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=217000, episode_reward=1293.00 +/- 264.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=218000, episode_reward=1375.80 +/- 170.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=219000, episode_reward=1380.00 +/- 230.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=220000, episode_reward=1139.60 +/- 223.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=221000, episode_reward=1237.60 +/- 182.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=222000, episode_reward=1255.60 +/- 294.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=223000, episode_reward=1099.20 +/- 308.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=224000, episode_reward=1249.40 +/- 201.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=225000, episode_reward=1362.40 +/- 77.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=226000, episode_reward=1443.00 +/- 217.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=227000, episode_reward=1449.60 +/- 312.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=228000, episode_reward=1415.80 +/- 156.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=229000, episode_reward=1238.20 +/- 289.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=230000, episode_reward=1320.80 +/- 260.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=231000, episode_reward=995.20 +/- 360.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=232000, episode_reward=1166.60 +/- 174.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=233000, episode_reward=1533.40 +/- 182.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=234000, episode_reward=1491.80 +/- 142.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=235000, episode_reward=1501.20 +/- 207.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=236000, episode_reward=1492.20 +/- 232.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=237000, episode_reward=1500.20 +/- 280.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=238000, episode_reward=1445.40 +/- 240.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=239000, episode_reward=1124.60 +/- 264.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=240000, episode_reward=1123.80 +/- 274.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=241000, episode_reward=1294.20 +/- 290.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=242000, episode_reward=1335.00 +/- 337.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=243000, episode_reward=1251.80 +/- 253.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=244000, episode_reward=1147.00 +/- 341.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=245000, episode_reward=1507.60 +/- 243.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=246000, episode_reward=1454.40 +/- 142.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=247000, episode_reward=1295.40 +/- 261.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=248000, episode_reward=1306.80 +/- 268.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=249000, episode_reward=1480.60 +/- 267.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=250000, episode_reward=1346.80 +/- 263.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=251000, episode_reward=1207.80 +/- 355.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=252000, episode_reward=1503.60 +/- 246.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=253000, episode_reward=1246.20 +/- 316.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=254000, episode_reward=1067.40 +/- 217.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=255000, episode_reward=1048.00 +/- 211.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=256000, episode_reward=944.40 +/- 227.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=257000, episode_reward=1406.20 +/- 278.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=258000, episode_reward=1056.60 +/- 241.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=259000, episode_reward=1402.80 +/- 164.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=260000, episode_reward=1356.80 +/- 192.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=261000, episode_reward=1177.20 +/- 361.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=262000, episode_reward=1016.40 +/- 291.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=263000, episode_reward=1326.40 +/- 240.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=264000, episode_reward=1260.40 +/- 281.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=265000, episode_reward=877.00 +/- 87.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=266000, episode_reward=1145.40 +/- 144.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=267000, episode_reward=1153.60 +/- 446.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=268000, episode_reward=1211.60 +/- 130.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=269000, episode_reward=337.60 +/- 1338.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=270000, episode_reward=1287.40 +/- 195.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=271000, episode_reward=1315.60 +/- 134.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=272000, episode_reward=1340.40 +/- 196.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=273000, episode_reward=1617.00 +/- 226.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=274000, episode_reward=1278.40 +/- 179.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=275000, episode_reward=1464.20 +/- 298.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=276000, episode_reward=1215.40 +/- 151.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=277000, episode_reward=1015.60 +/- 239.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=278000, episode_reward=1373.00 +/- 277.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=279000, episode_reward=1379.00 +/- 231.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=280000, episode_reward=1374.20 +/- 359.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=281000, episode_reward=1291.20 +/- 256.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=282000, episode_reward=1268.80 +/- 118.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=283000, episode_reward=1283.20 +/- 107.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=284000, episode_reward=1192.80 +/- 270.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=285000, episode_reward=1204.40 +/- 126.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=286000, episode_reward=1180.60 +/- 216.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=287000, episode_reward=1272.20 +/- 322.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=288000, episode_reward=1117.20 +/- 345.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=289000, episode_reward=1323.60 +/- 266.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=290000, episode_reward=1319.40 +/- 231.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=291000, episode_reward=1436.20 +/- 164.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=292000, episode_reward=1288.80 +/- 420.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=293000, episode_reward=1415.80 +/- 176.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=294000, episode_reward=1045.00 +/- 283.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=295000, episode_reward=1117.60 +/- 147.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=296000, episode_reward=1469.20 +/- 172.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=297000, episode_reward=1450.60 +/- 262.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=298000, episode_reward=1667.60 +/- 218.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=299000, episode_reward=1309.40 +/- 342.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=300000, episode_reward=1455.20 +/- 241.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=301000, episode_reward=963.20 +/- 353.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=302000, episode_reward=1092.60 +/- 282.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=303000, episode_reward=1515.60 +/- 269.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=304000, episode_reward=1245.00 +/- 230.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=305000, episode_reward=981.20 +/- 141.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=306000, episode_reward=1474.80 +/- 295.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=307000, episode_reward=810.20 +/- 374.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=308000, episode_reward=1038.20 +/- 328.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=309000, episode_reward=909.40 +/- 291.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=310000, episode_reward=1106.00 +/- 305.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=311000, episode_reward=1208.40 +/- 294.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=312000, episode_reward=1072.40 +/- 256.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=313000, episode_reward=1281.20 +/- 325.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=314000, episode_reward=1156.80 +/- 222.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=315000, episode_reward=1253.80 +/- 178.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=316000, episode_reward=-509.20 +/- 3479.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=317000, episode_reward=1010.00 +/- 75.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=318000, episode_reward=551.40 +/- 1734.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=319000, episode_reward=1229.20 +/- 321.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=320000, episode_reward=950.80 +/- 305.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=321000, episode_reward=1151.60 +/- 187.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=322000, episode_reward=933.60 +/- 245.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=323000, episode_reward=1284.40 +/- 216.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=324000, episode_reward=1461.00 +/- 235.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=325000, episode_reward=1282.40 +/- 414.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=326000, episode_reward=1063.60 +/- 415.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=327000, episode_reward=1190.40 +/- 178.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=328000, episode_reward=846.00 +/- 418.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=329000, episode_reward=1110.80 +/- 374.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=330000, episode_reward=962.40 +/- 258.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=331000, episode_reward=1200.40 +/- 384.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=332000, episode_reward=1561.20 +/- 259.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=333000, episode_reward=1341.80 +/- 279.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=334000, episode_reward=1216.80 +/- 302.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=335000, episode_reward=1416.00 +/- 278.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=336000, episode_reward=1221.40 +/- 158.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=337000, episode_reward=1359.40 +/- 373.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=338000, episode_reward=1255.00 +/- 356.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=339000, episode_reward=1217.00 +/- 248.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=340000, episode_reward=1334.00 +/- 498.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=341000, episode_reward=944.20 +/- 194.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=342000, episode_reward=1223.40 +/- 233.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=343000, episode_reward=1014.80 +/- 210.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=344000, episode_reward=1238.80 +/- 230.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=345000, episode_reward=1025.00 +/- 404.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=346000, episode_reward=1011.00 +/- 317.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=347000, episode_reward=1274.20 +/- 326.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=348000, episode_reward=1139.00 +/- 217.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=349000, episode_reward=1040.40 +/- 272.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=350000, episode_reward=1044.20 +/- 180.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=351000, episode_reward=722.80 +/- 353.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=352000, episode_reward=1154.20 +/- 392.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=353000, episode_reward=1353.80 +/- 329.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=354000, episode_reward=1024.20 +/- 211.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=355000, episode_reward=1140.00 +/- 204.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=356000, episode_reward=1078.20 +/- 238.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=357000, episode_reward=1195.60 +/- 418.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=358000, episode_reward=975.40 +/- 396.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=359000, episode_reward=1070.60 +/- 352.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=360000, episode_reward=986.60 +/- 237.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=361000, episode_reward=927.80 +/- 274.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=362000, episode_reward=1148.00 +/- 123.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=363000, episode_reward=1167.40 +/- 215.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=364000, episode_reward=686.20 +/- 1072.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=365000, episode_reward=1169.80 +/- 290.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=366000, episode_reward=1078.60 +/- 190.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=367000, episode_reward=1026.60 +/- 99.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=368000, episode_reward=1226.40 +/- 308.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=369000, episode_reward=950.20 +/- 357.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=370000, episode_reward=1213.80 +/- 279.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=371000, episode_reward=1371.60 +/- 308.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=372000, episode_reward=1018.80 +/- 211.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=373000, episode_reward=1119.60 +/- 268.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=374000, episode_reward=951.00 +/- 211.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=375000, episode_reward=1200.00 +/- 200.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=376000, episode_reward=682.20 +/- 185.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=377000, episode_reward=1131.00 +/- 190.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=378000, episode_reward=1121.60 +/- 254.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=379000, episode_reward=1006.80 +/- 339.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=380000, episode_reward=1071.40 +/- 281.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=381000, episode_reward=1446.80 +/- 120.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=382000, episode_reward=1380.40 +/- 331.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=383000, episode_reward=1126.40 +/- 142.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=384000, episode_reward=857.80 +/- 275.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=385000, episode_reward=1074.40 +/- 208.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=386000, episode_reward=1405.40 +/- 238.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=387000, episode_reward=1091.20 +/- 231.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=388000, episode_reward=1099.40 +/- 273.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=389000, episode_reward=1087.40 +/- 143.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=390000, episode_reward=1169.00 +/- 291.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=391000, episode_reward=1366.20 +/- 143.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=392000, episode_reward=954.00 +/- 206.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=393000, episode_reward=1220.00 +/- 198.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=394000, episode_reward=1146.40 +/- 243.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=395000, episode_reward=1159.60 +/- 253.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=396000, episode_reward=1241.60 +/- 255.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=397000, episode_reward=1502.40 +/- 324.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=398000, episode_reward=1319.20 +/- 224.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=399000, episode_reward=1087.00 +/- 250.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=400000, episode_reward=1050.60 +/- 324.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=401000, episode_reward=980.40 +/- 380.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=402000, episode_reward=917.00 +/- 305.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=403000, episode_reward=1218.60 +/- 353.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=404000, episode_reward=1250.60 +/- 318.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=405000, episode_reward=1392.20 +/- 382.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=406000, episode_reward=1320.40 +/- 162.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=407000, episode_reward=1166.00 +/- 102.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=408000, episode_reward=1009.00 +/- 70.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=409000, episode_reward=1247.20 +/- 273.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=410000, episode_reward=1317.00 +/- 142.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=411000, episode_reward=1528.60 +/- 93.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=412000, episode_reward=1183.80 +/- 127.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=413000, episode_reward=1344.20 +/- 407.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=414000, episode_reward=980.40 +/- 293.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=415000, episode_reward=1204.20 +/- 274.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=416000, episode_reward=-1142.80 +/- 5071.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=417000, episode_reward=1265.20 +/- 235.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=418000, episode_reward=1112.40 +/- 358.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=419000, episode_reward=1211.80 +/- 309.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=420000, episode_reward=902.40 +/- 192.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=421000, episode_reward=1385.00 +/- 122.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=422000, episode_reward=1358.80 +/- 347.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=423000, episode_reward=1084.20 +/- 136.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=424000, episode_reward=1102.00 +/- 312.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=425000, episode_reward=1420.40 +/- 273.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=426000, episode_reward=998.20 +/- 250.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=427000, episode_reward=1375.80 +/- 259.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=428000, episode_reward=1307.80 +/- 174.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=429000, episode_reward=1194.80 +/- 269.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=430000, episode_reward=1380.40 +/- 171.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=431000, episode_reward=1243.20 +/- 211.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=432000, episode_reward=1514.60 +/- 333.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=433000, episode_reward=1340.80 +/- 411.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=434000, episode_reward=1499.40 +/- 132.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=435000, episode_reward=1205.20 +/- 293.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=436000, episode_reward=1322.80 +/- 110.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=437000, episode_reward=943.80 +/- 189.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=438000, episode_reward=1304.40 +/- 213.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=439000, episode_reward=1177.40 +/- 184.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=440000, episode_reward=1535.20 +/- 95.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=441000, episode_reward=1359.80 +/- 171.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=442000, episode_reward=1342.80 +/- 227.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=443000, episode_reward=1034.80 +/- 121.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=444000, episode_reward=1074.80 +/- 248.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=445000, episode_reward=1332.00 +/- 313.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=446000, episode_reward=1306.80 +/- 230.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=447000, episode_reward=1048.00 +/- 371.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=448000, episode_reward=1281.00 +/- 181.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=449000, episode_reward=1377.80 +/- 173.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=450000, episode_reward=1312.00 +/- 166.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=451000, episode_reward=1071.00 +/- 210.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=452000, episode_reward=1027.20 +/- 232.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=453000, episode_reward=1239.60 +/- 291.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=454000, episode_reward=1060.80 +/- 224.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=455000, episode_reward=975.40 +/- 454.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=456000, episode_reward=1182.00 +/- 170.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=457000, episode_reward=1187.00 +/- 450.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=458000, episode_reward=1491.00 +/- 283.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=459000, episode_reward=979.80 +/- 238.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=460000, episode_reward=1236.60 +/- 303.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=461000, episode_reward=1679.40 +/- 142.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=462000, episode_reward=1323.20 +/- 273.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=463000, episode_reward=833.00 +/- 223.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=464000, episode_reward=1170.40 +/- 110.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=465000, episode_reward=1101.00 +/- 167.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=466000, episode_reward=1088.60 +/- 369.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=467000, episode_reward=1288.80 +/- 206.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=468000, episode_reward=1297.60 +/- 225.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=469000, episode_reward=1010.80 +/- 270.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=470000, episode_reward=1414.40 +/- 371.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=471000, episode_reward=1272.80 +/- 376.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=472000, episode_reward=-1541.40 +/- 5323.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=473000, episode_reward=1504.40 +/- 347.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=474000, episode_reward=1564.00 +/- 336.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=475000, episode_reward=1042.20 +/- 348.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=476000, episode_reward=1524.80 +/- 222.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=477000, episode_reward=1314.40 +/- 242.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=478000, episode_reward=927.60 +/- 155.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=479000, episode_reward=1277.60 +/- 153.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=480000, episode_reward=834.20 +/- 348.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=481000, episode_reward=1106.40 +/- 195.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=482000, episode_reward=865.00 +/- 199.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=483000, episode_reward=977.60 +/- 247.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=484000, episode_reward=1488.60 +/- 151.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=485000, episode_reward=897.60 +/- 270.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=486000, episode_reward=1234.20 +/- 254.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=487000, episode_reward=1310.40 +/- 159.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=488000, episode_reward=1177.60 +/- 423.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=489000, episode_reward=1007.00 +/- 324.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=490000, episode_reward=1518.20 +/- 334.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=491000, episode_reward=1080.40 +/- 166.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=492000, episode_reward=1543.00 +/- 298.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=493000, episode_reward=1419.20 +/- 288.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=494000, episode_reward=1379.00 +/- 237.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=495000, episode_reward=1512.00 +/- 327.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=496000, episode_reward=1421.80 +/- 186.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=497000, episode_reward=148.20 +/- 2606.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=498000, episode_reward=1271.00 +/- 401.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=499000, episode_reward=1258.20 +/- 422.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=500000, episode_reward=1625.60 +/- 284.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Gamma = 0.99, Learning Rate = 0.003, Activation Function = ReLU, Neurons = 8, Process noise = 0.1, Measurement noise = 0.0\n",
      "Eval num_timesteps=1000, episode_reward=-560.00 +/- 89.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=-582.80 +/- 62.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=3000, episode_reward=-556.80 +/- 52.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4000, episode_reward=-602.00 +/- 75.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=5000, episode_reward=-581.60 +/- 40.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=6000, episode_reward=-632.40 +/- 147.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=7000, episode_reward=-657.20 +/- 80.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=8000, episode_reward=-603.20 +/- 108.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=9000, episode_reward=-613.40 +/- 50.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=10000, episode_reward=-569.20 +/- 40.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=11000, episode_reward=-641.00 +/- 22.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=12000, episode_reward=-588.20 +/- 77.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=13000, episode_reward=-581.40 +/- 22.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=14000, episode_reward=-583.80 +/- 78.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=15000, episode_reward=-660.20 +/- 170.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=16000, episode_reward=-566.60 +/- 46.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=17000, episode_reward=-624.00 +/- 39.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=18000, episode_reward=-590.80 +/- 35.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=19000, episode_reward=-554.40 +/- 80.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=20000, episode_reward=-612.60 +/- 62.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=21000, episode_reward=-578.00 +/- 51.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=22000, episode_reward=-562.80 +/- 80.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=23000, episode_reward=-660.60 +/- 96.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=24000, episode_reward=-564.40 +/- 71.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=25000, episode_reward=-707.60 +/- 84.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=26000, episode_reward=-585.40 +/- 44.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=27000, episode_reward=-540.60 +/- 99.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=28000, episode_reward=-610.20 +/- 88.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=29000, episode_reward=-508.80 +/- 33.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=30000, episode_reward=-554.80 +/- 114.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=31000, episode_reward=-547.20 +/- 56.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=32000, episode_reward=-595.40 +/- 40.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=33000, episode_reward=-507.00 +/- 44.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=34000, episode_reward=-557.60 +/- 35.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=35000, episode_reward=-647.00 +/- 134.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=36000, episode_reward=-569.20 +/- 89.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=37000, episode_reward=-685.20 +/- 21.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=38000, episode_reward=-536.20 +/- 92.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=39000, episode_reward=-660.00 +/- 60.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=40000, episode_reward=-598.00 +/- 27.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=41000, episode_reward=-540.80 +/- 63.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=42000, episode_reward=-500.20 +/- 105.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=43000, episode_reward=-585.20 +/- 84.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=44000, episode_reward=-616.00 +/- 82.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=45000, episode_reward=-597.00 +/- 102.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=46000, episode_reward=-569.80 +/- 38.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=47000, episode_reward=-557.80 +/- 97.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=48000, episode_reward=-532.40 +/- 86.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=49000, episode_reward=-672.00 +/- 108.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=50000, episode_reward=-623.40 +/- 55.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=51000, episode_reward=-8208.00 +/- 1338.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=52000, episode_reward=-445.60 +/- 172.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=53000, episode_reward=-86.60 +/- 179.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=54000, episode_reward=176.80 +/- 255.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=55000, episode_reward=341.20 +/- 95.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=56000, episode_reward=371.20 +/- 197.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=57000, episode_reward=373.20 +/- 139.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=58000, episode_reward=263.60 +/- 182.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=59000, episode_reward=218.00 +/- 67.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=60000, episode_reward=98.40 +/- 144.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=61000, episode_reward=215.20 +/- 226.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=62000, episode_reward=48.60 +/- 245.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=63000, episode_reward=406.80 +/- 243.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=64000, episode_reward=125.80 +/- 159.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=65000, episode_reward=299.00 +/- 214.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=66000, episode_reward=110.20 +/- 161.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=67000, episode_reward=49.20 +/- 236.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=68000, episode_reward=268.20 +/- 190.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=69000, episode_reward=518.60 +/- 115.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=70000, episode_reward=89.80 +/- 150.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=71000, episode_reward=122.60 +/- 136.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=72000, episode_reward=409.80 +/- 213.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=73000, episode_reward=619.60 +/- 75.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=74000, episode_reward=165.00 +/- 198.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=75000, episode_reward=336.20 +/- 131.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=76000, episode_reward=208.40 +/- 281.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=77000, episode_reward=505.00 +/- 188.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=78000, episode_reward=325.40 +/- 198.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=79000, episode_reward=305.40 +/- 190.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=80000, episode_reward=413.20 +/- 278.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=81000, episode_reward=309.40 +/- 240.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=82000, episode_reward=36.80 +/- 68.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=83000, episode_reward=408.80 +/- 145.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=84000, episode_reward=324.00 +/- 127.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=85000, episode_reward=400.80 +/- 164.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=86000, episode_reward=297.60 +/- 168.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=87000, episode_reward=378.60 +/- 132.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=88000, episode_reward=344.00 +/- 239.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=89000, episode_reward=313.00 +/- 189.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=90000, episode_reward=401.40 +/- 294.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=91000, episode_reward=438.40 +/- 163.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=92000, episode_reward=478.60 +/- 208.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=93000, episode_reward=172.00 +/- 260.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=94000, episode_reward=379.40 +/- 101.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=95000, episode_reward=348.60 +/- 180.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=96000, episode_reward=371.00 +/- 148.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=97000, episode_reward=282.40 +/- 278.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=98000, episode_reward=349.20 +/- 232.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=99000, episode_reward=407.40 +/- 157.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=100000, episode_reward=244.00 +/- 299.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=101000, episode_reward=357.80 +/- 245.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=102000, episode_reward=391.40 +/- 282.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=103000, episode_reward=262.80 +/- 271.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=104000, episode_reward=480.80 +/- 120.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=105000, episode_reward=395.00 +/- 35.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=106000, episode_reward=209.20 +/- 204.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=107000, episode_reward=271.80 +/- 134.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=108000, episode_reward=400.80 +/- 222.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=109000, episode_reward=326.60 +/- 73.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=110000, episode_reward=316.40 +/- 250.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=111000, episode_reward=358.20 +/- 210.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=112000, episode_reward=443.60 +/- 129.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=113000, episode_reward=558.20 +/- 164.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=114000, episode_reward=469.60 +/- 173.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=115000, episode_reward=370.00 +/- 91.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=116000, episode_reward=251.60 +/- 131.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=117000, episode_reward=340.20 +/- 114.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=118000, episode_reward=325.00 +/- 188.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=119000, episode_reward=220.80 +/- 245.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=120000, episode_reward=299.20 +/- 275.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=121000, episode_reward=262.20 +/- 216.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=122000, episode_reward=319.00 +/- 203.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=123000, episode_reward=166.40 +/- 133.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=124000, episode_reward=356.20 +/- 215.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=125000, episode_reward=426.40 +/- 151.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=126000, episode_reward=338.20 +/- 177.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=127000, episode_reward=406.00 +/- 183.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=128000, episode_reward=330.40 +/- 134.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=129000, episode_reward=249.60 +/- 100.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=130000, episode_reward=335.20 +/- 148.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=131000, episode_reward=366.80 +/- 241.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=132000, episode_reward=543.60 +/- 232.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=133000, episode_reward=270.20 +/- 128.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=134000, episode_reward=437.40 +/- 415.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=135000, episode_reward=302.20 +/- 146.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=136000, episode_reward=352.60 +/- 53.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=137000, episode_reward=125.20 +/- 177.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=138000, episode_reward=343.60 +/- 182.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=139000, episode_reward=359.80 +/- 151.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=140000, episode_reward=381.60 +/- 99.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=141000, episode_reward=175.40 +/- 240.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=142000, episode_reward=343.20 +/- 137.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=143000, episode_reward=346.60 +/- 146.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=144000, episode_reward=378.40 +/- 74.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=145000, episode_reward=438.60 +/- 224.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=146000, episode_reward=481.60 +/- 209.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=147000, episode_reward=305.40 +/- 107.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=148000, episode_reward=418.00 +/- 175.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=149000, episode_reward=302.40 +/- 98.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=150000, episode_reward=510.80 +/- 189.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=151000, episode_reward=338.20 +/- 255.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=152000, episode_reward=142.40 +/- 253.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=153000, episode_reward=278.00 +/- 171.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=154000, episode_reward=156.40 +/- 295.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=155000, episode_reward=400.80 +/- 124.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=156000, episode_reward=322.00 +/- 125.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=157000, episode_reward=219.00 +/- 171.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=158000, episode_reward=259.60 +/- 245.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=159000, episode_reward=212.40 +/- 148.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=160000, episode_reward=376.40 +/- 64.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=161000, episode_reward=325.20 +/- 173.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=162000, episode_reward=445.00 +/- 150.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=163000, episode_reward=314.60 +/- 232.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=164000, episode_reward=250.20 +/- 106.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=165000, episode_reward=149.60 +/- 251.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=166000, episode_reward=216.20 +/- 174.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=167000, episode_reward=259.40 +/- 138.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=168000, episode_reward=343.20 +/- 167.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=169000, episode_reward=345.80 +/- 225.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=170000, episode_reward=244.00 +/- 102.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=171000, episode_reward=328.60 +/- 132.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=172000, episode_reward=393.40 +/- 208.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=173000, episode_reward=342.60 +/- 208.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=174000, episode_reward=302.60 +/- 215.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=175000, episode_reward=332.00 +/- 265.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=176000, episode_reward=277.00 +/- 157.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=177000, episode_reward=277.60 +/- 206.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=178000, episode_reward=145.80 +/- 188.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=179000, episode_reward=442.40 +/- 157.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=180000, episode_reward=647.40 +/- 146.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=181000, episode_reward=371.80 +/- 145.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=182000, episode_reward=343.40 +/- 163.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=183000, episode_reward=353.00 +/- 119.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=184000, episode_reward=269.40 +/- 116.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=185000, episode_reward=392.80 +/- 86.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=186000, episode_reward=384.20 +/- 211.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=187000, episode_reward=99.60 +/- 153.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=188000, episode_reward=472.00 +/- 87.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=189000, episode_reward=348.80 +/- 282.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=190000, episode_reward=325.40 +/- 188.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=191000, episode_reward=284.80 +/- 290.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=192000, episode_reward=544.80 +/- 177.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=193000, episode_reward=315.40 +/- 227.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=194000, episode_reward=528.80 +/- 86.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=195000, episode_reward=428.80 +/- 101.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=196000, episode_reward=323.60 +/- 263.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=197000, episode_reward=272.00 +/- 236.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=198000, episode_reward=306.40 +/- 242.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=199000, episode_reward=168.60 +/- 362.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=200000, episode_reward=165.00 +/- 281.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=201000, episode_reward=322.80 +/- 144.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=202000, episode_reward=276.20 +/- 106.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=203000, episode_reward=245.00 +/- 124.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=204000, episode_reward=302.40 +/- 179.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=205000, episode_reward=322.00 +/- 156.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=206000, episode_reward=331.20 +/- 137.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=207000, episode_reward=270.40 +/- 190.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=208000, episode_reward=326.40 +/- 240.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=209000, episode_reward=450.80 +/- 148.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=210000, episode_reward=258.00 +/- 297.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=211000, episode_reward=554.00 +/- 181.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=212000, episode_reward=317.60 +/- 145.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=213000, episode_reward=343.00 +/- 267.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=214000, episode_reward=449.40 +/- 272.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=215000, episode_reward=10.80 +/- 135.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=216000, episode_reward=452.40 +/- 198.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=217000, episode_reward=457.00 +/- 78.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=218000, episode_reward=410.20 +/- 255.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=219000, episode_reward=553.80 +/- 56.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=220000, episode_reward=282.20 +/- 134.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=221000, episode_reward=464.40 +/- 153.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=222000, episode_reward=488.20 +/- 286.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=223000, episode_reward=236.20 +/- 124.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=224000, episode_reward=-240.40 +/- 208.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=225000, episode_reward=234.20 +/- 237.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=226000, episode_reward=391.20 +/- 230.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=227000, episode_reward=326.80 +/- 168.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=228000, episode_reward=469.20 +/- 202.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=229000, episode_reward=472.60 +/- 76.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=230000, episode_reward=324.80 +/- 116.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=231000, episode_reward=391.40 +/- 354.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=232000, episode_reward=196.80 +/- 98.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=233000, episode_reward=325.80 +/- 189.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=234000, episode_reward=371.60 +/- 76.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=235000, episode_reward=42.40 +/- 196.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=236000, episode_reward=379.00 +/- 161.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=237000, episode_reward=255.60 +/- 241.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=238000, episode_reward=283.40 +/- 183.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=239000, episode_reward=-538.00 +/- 761.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=240000, episode_reward=303.00 +/- 204.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=241000, episode_reward=128.20 +/- 135.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=242000, episode_reward=193.20 +/- 159.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=243000, episode_reward=568.80 +/- 143.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=244000, episode_reward=-12.00 +/- 176.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=245000, episode_reward=520.60 +/- 209.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=246000, episode_reward=435.20 +/- 172.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=247000, episode_reward=298.00 +/- 199.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=248000, episode_reward=272.40 +/- 87.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=249000, episode_reward=544.80 +/- 147.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=250000, episode_reward=282.40 +/- 223.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=251000, episode_reward=410.60 +/- 121.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=252000, episode_reward=306.20 +/- 137.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=253000, episode_reward=508.00 +/- 88.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=254000, episode_reward=300.20 +/- 223.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=255000, episode_reward=296.00 +/- 265.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=256000, episode_reward=177.40 +/- 261.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=257000, episode_reward=336.60 +/- 186.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=258000, episode_reward=356.20 +/- 197.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=259000, episode_reward=398.40 +/- 114.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=260000, episode_reward=506.60 +/- 245.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=261000, episode_reward=390.00 +/- 60.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=262000, episode_reward=225.00 +/- 244.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=263000, episode_reward=623.20 +/- 129.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=264000, episode_reward=440.40 +/- 151.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=265000, episode_reward=412.20 +/- 159.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=266000, episode_reward=452.40 +/- 333.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=267000, episode_reward=535.00 +/- 86.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=268000, episode_reward=486.40 +/- 251.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=269000, episode_reward=135.00 +/- 159.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=270000, episode_reward=415.80 +/- 242.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=271000, episode_reward=310.40 +/- 86.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=272000, episode_reward=354.00 +/- 122.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=273000, episode_reward=251.80 +/- 176.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=274000, episode_reward=214.40 +/- 59.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=275000, episode_reward=400.00 +/- 99.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=276000, episode_reward=301.00 +/- 224.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=277000, episode_reward=297.60 +/- 207.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=278000, episode_reward=326.20 +/- 151.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=279000, episode_reward=364.40 +/- 187.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=280000, episode_reward=443.20 +/- 82.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=281000, episode_reward=365.80 +/- 123.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=282000, episode_reward=199.20 +/- 206.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=283000, episode_reward=337.40 +/- 105.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=284000, episode_reward=219.60 +/- 122.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=285000, episode_reward=226.00 +/- 205.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=286000, episode_reward=441.60 +/- 125.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=287000, episode_reward=438.20 +/- 192.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=288000, episode_reward=316.40 +/- 159.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=289000, episode_reward=393.00 +/- 106.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=290000, episode_reward=215.20 +/- 148.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=291000, episode_reward=294.20 +/- 63.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=292000, episode_reward=276.80 +/- 201.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=293000, episode_reward=380.20 +/- 227.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=294000, episode_reward=643.40 +/- 144.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=295000, episode_reward=411.20 +/- 116.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=296000, episode_reward=455.00 +/- 147.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=297000, episode_reward=284.20 +/- 289.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=298000, episode_reward=23.00 +/- 137.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=299000, episode_reward=39.80 +/- 120.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=300000, episode_reward=450.80 +/- 247.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=301000, episode_reward=587.40 +/- 150.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=302000, episode_reward=460.80 +/- 139.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=303000, episode_reward=448.00 +/- 207.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=304000, episode_reward=331.60 +/- 181.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=305000, episode_reward=526.80 +/- 146.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=306000, episode_reward=894.20 +/- 168.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=307000, episode_reward=537.20 +/- 119.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=308000, episode_reward=607.00 +/- 159.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=309000, episode_reward=530.00 +/- 298.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=310000, episode_reward=435.20 +/- 127.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=311000, episode_reward=509.00 +/- 210.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=312000, episode_reward=773.40 +/- 361.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=313000, episode_reward=829.20 +/- 352.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=314000, episode_reward=675.20 +/- 139.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=315000, episode_reward=529.40 +/- 250.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=316000, episode_reward=449.20 +/- 358.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=317000, episode_reward=735.80 +/- 175.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=318000, episode_reward=627.40 +/- 291.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=319000, episode_reward=597.80 +/- 259.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=320000, episode_reward=462.60 +/- 187.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=321000, episode_reward=365.40 +/- 232.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=322000, episode_reward=849.00 +/- 183.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=323000, episode_reward=599.20 +/- 211.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=324000, episode_reward=575.00 +/- 162.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=325000, episode_reward=607.00 +/- 161.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=326000, episode_reward=635.20 +/- 263.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=327000, episode_reward=844.20 +/- 165.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=328000, episode_reward=987.00 +/- 225.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=329000, episode_reward=925.40 +/- 173.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=330000, episode_reward=895.40 +/- 289.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=331000, episode_reward=904.40 +/- 319.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=332000, episode_reward=1109.80 +/- 266.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=333000, episode_reward=803.20 +/- 458.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=334000, episode_reward=824.80 +/- 149.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=335000, episode_reward=856.40 +/- 183.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=336000, episode_reward=760.80 +/- 340.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=337000, episode_reward=1021.40 +/- 194.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=338000, episode_reward=1225.80 +/- 262.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=339000, episode_reward=945.80 +/- 202.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=340000, episode_reward=1229.40 +/- 227.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=341000, episode_reward=965.20 +/- 203.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=342000, episode_reward=1060.20 +/- 279.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=343000, episode_reward=1175.20 +/- 156.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=344000, episode_reward=998.80 +/- 316.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=345000, episode_reward=788.00 +/- 179.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=346000, episode_reward=1054.60 +/- 269.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=347000, episode_reward=1034.60 +/- 282.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=348000, episode_reward=869.00 +/- 134.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=349000, episode_reward=1117.60 +/- 178.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=350000, episode_reward=1181.40 +/- 224.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=351000, episode_reward=1381.60 +/- 224.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=352000, episode_reward=1212.60 +/- 271.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=353000, episode_reward=1223.40 +/- 82.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=354000, episode_reward=1368.40 +/- 173.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=355000, episode_reward=1112.20 +/- 175.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=356000, episode_reward=1392.00 +/- 299.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=357000, episode_reward=1367.40 +/- 293.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=358000, episode_reward=958.40 +/- 133.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=359000, episode_reward=1197.80 +/- 302.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=360000, episode_reward=1201.60 +/- 163.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=361000, episode_reward=1178.60 +/- 132.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=362000, episode_reward=1763.80 +/- 133.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=363000, episode_reward=1215.00 +/- 413.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=364000, episode_reward=1212.80 +/- 311.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=365000, episode_reward=1665.20 +/- 317.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=366000, episode_reward=1105.60 +/- 264.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=367000, episode_reward=1628.60 +/- 83.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=368000, episode_reward=1140.60 +/- 190.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=369000, episode_reward=1810.40 +/- 228.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=370000, episode_reward=1591.60 +/- 258.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=371000, episode_reward=1584.20 +/- 183.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=372000, episode_reward=919.60 +/- 252.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=373000, episode_reward=1661.20 +/- 153.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=374000, episode_reward=1381.80 +/- 194.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=375000, episode_reward=1513.60 +/- 148.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=376000, episode_reward=1298.20 +/- 221.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=377000, episode_reward=1340.60 +/- 247.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=378000, episode_reward=1304.20 +/- 271.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=379000, episode_reward=1217.80 +/- 269.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=380000, episode_reward=1340.20 +/- 63.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=381000, episode_reward=1147.20 +/- 189.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=382000, episode_reward=1009.40 +/- 201.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=383000, episode_reward=1434.00 +/- 151.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=384000, episode_reward=1350.80 +/- 320.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=385000, episode_reward=1312.40 +/- 224.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=386000, episode_reward=1369.60 +/- 128.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=387000, episode_reward=986.80 +/- 270.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=388000, episode_reward=1442.80 +/- 83.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=389000, episode_reward=1296.20 +/- 330.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=390000, episode_reward=1227.20 +/- 335.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=391000, episode_reward=911.20 +/- 103.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=392000, episode_reward=1165.60 +/- 298.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=393000, episode_reward=1687.80 +/- 256.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=394000, episode_reward=714.60 +/- 349.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=395000, episode_reward=1660.80 +/- 75.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=396000, episode_reward=1204.00 +/- 174.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=397000, episode_reward=1050.40 +/- 248.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=398000, episode_reward=1466.00 +/- 256.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=399000, episode_reward=1439.40 +/- 316.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=400000, episode_reward=1186.40 +/- 151.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=401000, episode_reward=1444.60 +/- 339.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=402000, episode_reward=1479.20 +/- 342.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=403000, episode_reward=1711.40 +/- 366.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=404000, episode_reward=1424.60 +/- 243.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=405000, episode_reward=1249.40 +/- 199.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=406000, episode_reward=1546.80 +/- 224.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=407000, episode_reward=1311.80 +/- 223.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=408000, episode_reward=1244.00 +/- 335.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=409000, episode_reward=1452.60 +/- 181.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=410000, episode_reward=1609.60 +/- 130.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=411000, episode_reward=1421.80 +/- 258.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=412000, episode_reward=1722.00 +/- 78.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=413000, episode_reward=1197.00 +/- 57.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=414000, episode_reward=1295.80 +/- 132.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=415000, episode_reward=1680.40 +/- 147.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=416000, episode_reward=1225.00 +/- 287.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=417000, episode_reward=1253.80 +/- 168.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=418000, episode_reward=1331.40 +/- 407.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=419000, episode_reward=1635.80 +/- 182.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=420000, episode_reward=1568.80 +/- 210.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=421000, episode_reward=1373.60 +/- 297.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=422000, episode_reward=1391.60 +/- 266.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=423000, episode_reward=954.60 +/- 247.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=424000, episode_reward=1575.20 +/- 247.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=425000, episode_reward=1345.80 +/- 237.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=426000, episode_reward=1363.80 +/- 225.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=427000, episode_reward=1366.60 +/- 77.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=428000, episode_reward=1427.60 +/- 146.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=429000, episode_reward=1318.40 +/- 179.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=430000, episode_reward=1680.80 +/- 245.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=431000, episode_reward=1547.80 +/- 163.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=432000, episode_reward=1612.00 +/- 269.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=433000, episode_reward=969.40 +/- 162.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=434000, episode_reward=1368.00 +/- 176.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=435000, episode_reward=1435.80 +/- 335.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=436000, episode_reward=1428.20 +/- 196.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=437000, episode_reward=1140.60 +/- 140.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=438000, episode_reward=1508.60 +/- 397.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=439000, episode_reward=1492.60 +/- 209.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=440000, episode_reward=1357.60 +/- 174.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=441000, episode_reward=1520.60 +/- 109.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=442000, episode_reward=1447.40 +/- 333.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=443000, episode_reward=1419.60 +/- 283.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=444000, episode_reward=1389.20 +/- 153.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=445000, episode_reward=1516.40 +/- 230.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=446000, episode_reward=1536.00 +/- 119.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=447000, episode_reward=1671.20 +/- 257.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=448000, episode_reward=1461.80 +/- 240.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=449000, episode_reward=1486.00 +/- 139.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=450000, episode_reward=1429.80 +/- 313.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=451000, episode_reward=1115.80 +/- 239.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=452000, episode_reward=1168.00 +/- 146.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=453000, episode_reward=1412.60 +/- 192.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=454000, episode_reward=1389.80 +/- 185.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=455000, episode_reward=1353.40 +/- 355.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=456000, episode_reward=845.80 +/- 157.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=457000, episode_reward=1647.80 +/- 270.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=458000, episode_reward=1597.00 +/- 226.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=459000, episode_reward=1604.40 +/- 373.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=460000, episode_reward=1248.60 +/- 99.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=461000, episode_reward=1252.80 +/- 401.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=462000, episode_reward=1781.80 +/- 197.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=463000, episode_reward=1573.40 +/- 75.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=464000, episode_reward=1211.20 +/- 264.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=465000, episode_reward=1191.20 +/- 312.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=466000, episode_reward=1655.40 +/- 217.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=467000, episode_reward=1261.80 +/- 218.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=468000, episode_reward=1696.60 +/- 259.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=469000, episode_reward=1565.60 +/- 272.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=470000, episode_reward=1669.40 +/- 201.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=471000, episode_reward=1547.40 +/- 204.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=472000, episode_reward=1261.80 +/- 206.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=473000, episode_reward=1168.00 +/- 413.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=474000, episode_reward=1136.20 +/- 372.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=475000, episode_reward=1216.40 +/- 110.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=476000, episode_reward=1304.80 +/- 137.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=477000, episode_reward=1557.00 +/- 210.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=478000, episode_reward=1131.60 +/- 181.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=479000, episode_reward=1245.20 +/- 222.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=480000, episode_reward=1310.80 +/- 166.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=481000, episode_reward=1550.40 +/- 176.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=482000, episode_reward=1558.60 +/- 196.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=483000, episode_reward=1108.40 +/- 48.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=484000, episode_reward=1366.20 +/- 410.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=485000, episode_reward=1623.20 +/- 190.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=486000, episode_reward=1593.60 +/- 167.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=487000, episode_reward=1460.40 +/- 299.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=488000, episode_reward=1569.20 +/- 347.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=489000, episode_reward=1499.00 +/- 170.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=490000, episode_reward=1650.40 +/- 170.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=491000, episode_reward=865.40 +/- 197.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=492000, episode_reward=1505.20 +/- 186.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=493000, episode_reward=1330.60 +/- 340.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=494000, episode_reward=1105.20 +/- 125.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=495000, episode_reward=1179.60 +/- 363.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=496000, episode_reward=1205.20 +/- 267.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=497000, episode_reward=1220.60 +/- 343.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=498000, episode_reward=1437.00 +/- 206.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=499000, episode_reward=1065.60 +/- 412.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=500000, episode_reward=1418.80 +/- 207.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Gamma = 0.99, Learning Rate = 0.003, Activation Function = ReLU, Neurons = 32, Process noise = 0.1, Measurement noise = 0.0\n",
      "Eval num_timesteps=1000, episode_reward=-3465.00 +/- 123.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=-3489.00 +/- 84.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=3000, episode_reward=-3498.00 +/- 52.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=4000, episode_reward=-3449.00 +/- 62.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=5000, episode_reward=-3505.00 +/- 77.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=6000, episode_reward=-3446.00 +/- 65.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=7000, episode_reward=-3448.00 +/- 66.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=8000, episode_reward=-3492.00 +/- 63.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=9000, episode_reward=-3490.00 +/- 44.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=10000, episode_reward=-3461.00 +/- 74.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=11000, episode_reward=-3472.00 +/- 110.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=12000, episode_reward=-3489.00 +/- 38.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=13000, episode_reward=-3415.00 +/- 20.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=14000, episode_reward=-3536.00 +/- 65.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=15000, episode_reward=-3519.00 +/- 81.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=16000, episode_reward=-3490.00 +/- 29.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=17000, episode_reward=-3499.00 +/- 30.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=18000, episode_reward=-3526.00 +/- 93.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=19000, episode_reward=-3449.00 +/- 125.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=20000, episode_reward=-3455.00 +/- 113.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=21000, episode_reward=-3577.00 +/- 90.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=22000, episode_reward=-3477.00 +/- 55.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=23000, episode_reward=-3514.00 +/- 17.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=24000, episode_reward=-3451.00 +/- 38.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=25000, episode_reward=-3531.00 +/- 60.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=26000, episode_reward=-3481.00 +/- 135.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=27000, episode_reward=-3519.00 +/- 79.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=28000, episode_reward=-3521.00 +/- 70.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=29000, episode_reward=-3449.00 +/- 58.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=30000, episode_reward=-3461.00 +/- 132.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=31000, episode_reward=-3478.00 +/- 61.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=32000, episode_reward=-3480.00 +/- 121.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=33000, episode_reward=-3473.00 +/- 97.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=34000, episode_reward=-3465.00 +/- 56.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=35000, episode_reward=-3511.00 +/- 53.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=36000, episode_reward=-3532.00 +/- 103.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=37000, episode_reward=-3455.00 +/- 59.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=38000, episode_reward=-3494.00 +/- 33.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=39000, episode_reward=-3514.00 +/- 110.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=40000, episode_reward=-3509.00 +/- 57.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=41000, episode_reward=-3500.00 +/- 74.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=42000, episode_reward=-3440.00 +/- 83.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=43000, episode_reward=-3473.00 +/- 96.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=44000, episode_reward=-3498.00 +/- 38.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=45000, episode_reward=-3527.00 +/- 68.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=46000, episode_reward=-3482.00 +/- 37.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=47000, episode_reward=-3371.00 +/- 57.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=48000, episode_reward=-3525.00 +/- 26.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=49000, episode_reward=-3485.00 +/- 101.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=50000, episode_reward=-3428.00 +/- 52.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=51000, episode_reward=369.20 +/- 78.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=52000, episode_reward=399.00 +/- 259.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=53000, episode_reward=205.40 +/- 164.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=54000, episode_reward=463.00 +/- 115.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=55000, episode_reward=244.60 +/- 170.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=56000, episode_reward=364.00 +/- 159.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=57000, episode_reward=337.40 +/- 177.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=58000, episode_reward=324.40 +/- 320.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=59000, episode_reward=260.60 +/- 227.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=60000, episode_reward=231.00 +/- 249.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=61000, episode_reward=531.80 +/- 90.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=62000, episode_reward=239.40 +/- 241.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=63000, episode_reward=416.80 +/- 96.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=64000, episode_reward=281.80 +/- 142.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=65000, episode_reward=335.40 +/- 181.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=66000, episode_reward=389.20 +/- 156.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=67000, episode_reward=407.40 +/- 162.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=68000, episode_reward=220.60 +/- 111.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=69000, episode_reward=291.00 +/- 205.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=70000, episode_reward=300.20 +/- 257.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=71000, episode_reward=442.60 +/- 146.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=72000, episode_reward=280.60 +/- 61.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=73000, episode_reward=236.80 +/- 109.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=74000, episode_reward=408.80 +/- 168.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=75000, episode_reward=189.40 +/- 149.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=76000, episode_reward=373.20 +/- 114.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=77000, episode_reward=240.40 +/- 49.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=78000, episode_reward=400.40 +/- 143.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=79000, episode_reward=421.40 +/- 160.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=80000, episode_reward=432.60 +/- 75.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=81000, episode_reward=378.20 +/- 128.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=82000, episode_reward=278.20 +/- 134.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=83000, episode_reward=450.60 +/- 216.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=84000, episode_reward=313.20 +/- 182.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=85000, episode_reward=347.00 +/- 228.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=86000, episode_reward=355.40 +/- 143.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=87000, episode_reward=453.80 +/- 135.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=88000, episode_reward=359.60 +/- 93.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=89000, episode_reward=344.00 +/- 250.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=90000, episode_reward=450.00 +/- 77.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=91000, episode_reward=448.20 +/- 122.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=92000, episode_reward=287.20 +/- 146.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=93000, episode_reward=385.80 +/- 145.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=94000, episode_reward=398.20 +/- 214.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=95000, episode_reward=241.00 +/- 140.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=96000, episode_reward=208.00 +/- 165.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=97000, episode_reward=375.40 +/- 314.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=98000, episode_reward=262.20 +/- 198.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=99000, episode_reward=491.80 +/- 87.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=100000, episode_reward=234.60 +/- 145.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=101000, episode_reward=266.20 +/- 135.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=102000, episode_reward=314.40 +/- 149.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=103000, episode_reward=379.80 +/- 197.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=104000, episode_reward=317.60 +/- 55.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=105000, episode_reward=336.00 +/- 134.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=106000, episode_reward=364.00 +/- 200.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=107000, episode_reward=215.00 +/- 20.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=108000, episode_reward=281.00 +/- 141.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=109000, episode_reward=338.60 +/- 129.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=110000, episode_reward=439.00 +/- 50.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=111000, episode_reward=409.40 +/- 156.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=112000, episode_reward=428.80 +/- 97.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=113000, episode_reward=244.80 +/- 116.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=114000, episode_reward=579.00 +/- 269.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=115000, episode_reward=477.80 +/- 115.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=116000, episode_reward=342.60 +/- 211.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=117000, episode_reward=391.00 +/- 168.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=118000, episode_reward=420.40 +/- 193.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=119000, episode_reward=182.00 +/- 234.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=120000, episode_reward=467.20 +/- 219.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=121000, episode_reward=332.40 +/- 203.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=122000, episode_reward=316.00 +/- 127.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=123000, episode_reward=367.00 +/- 143.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=124000, episode_reward=251.20 +/- 218.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=125000, episode_reward=308.80 +/- 185.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=126000, episode_reward=324.80 +/- 110.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=127000, episode_reward=224.80 +/- 120.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=128000, episode_reward=406.20 +/- 206.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=129000, episode_reward=268.20 +/- 156.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=130000, episode_reward=415.20 +/- 128.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=131000, episode_reward=439.20 +/- 138.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=132000, episode_reward=382.40 +/- 62.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=133000, episode_reward=498.40 +/- 162.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=134000, episode_reward=361.60 +/- 225.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=135000, episode_reward=530.40 +/- 103.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=136000, episode_reward=433.40 +/- 179.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=137000, episode_reward=524.60 +/- 214.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=138000, episode_reward=523.20 +/- 110.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=139000, episode_reward=309.40 +/- 215.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=140000, episode_reward=376.00 +/- 160.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=141000, episode_reward=315.80 +/- 218.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=142000, episode_reward=409.40 +/- 221.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=143000, episode_reward=351.40 +/- 182.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=144000, episode_reward=301.20 +/- 182.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=145000, episode_reward=468.80 +/- 24.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=146000, episode_reward=355.40 +/- 227.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=147000, episode_reward=399.20 +/- 125.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=148000, episode_reward=389.80 +/- 182.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=149000, episode_reward=290.00 +/- 304.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=150000, episode_reward=425.40 +/- 224.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=151000, episode_reward=341.60 +/- 62.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=152000, episode_reward=469.80 +/- 93.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=153000, episode_reward=418.00 +/- 152.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=154000, episode_reward=442.60 +/- 147.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=155000, episode_reward=357.00 +/- 170.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=156000, episode_reward=220.60 +/- 251.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=157000, episode_reward=371.80 +/- 28.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=158000, episode_reward=400.80 +/- 78.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=159000, episode_reward=533.80 +/- 136.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=160000, episode_reward=317.40 +/- 166.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=161000, episode_reward=328.80 +/- 262.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=162000, episode_reward=496.40 +/- 76.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=163000, episode_reward=420.40 +/- 132.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=164000, episode_reward=360.40 +/- 238.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=165000, episode_reward=378.80 +/- 89.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=166000, episode_reward=353.80 +/- 115.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=167000, episode_reward=389.80 +/- 220.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=168000, episode_reward=181.20 +/- 117.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=169000, episode_reward=436.40 +/- 169.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=170000, episode_reward=365.00 +/- 179.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=171000, episode_reward=244.80 +/- 189.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=172000, episode_reward=299.00 +/- 201.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=173000, episode_reward=267.00 +/- 167.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=174000, episode_reward=414.00 +/- 138.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=175000, episode_reward=461.00 +/- 140.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=176000, episode_reward=299.00 +/- 157.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=177000, episode_reward=461.60 +/- 169.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=178000, episode_reward=288.60 +/- 153.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=179000, episode_reward=356.40 +/- 287.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=180000, episode_reward=334.80 +/- 324.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=181000, episode_reward=556.20 +/- 222.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=182000, episode_reward=449.80 +/- 162.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=183000, episode_reward=322.80 +/- 65.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=184000, episode_reward=503.80 +/- 205.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=185000, episode_reward=472.20 +/- 187.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=186000, episode_reward=447.40 +/- 238.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=187000, episode_reward=686.00 +/- 254.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=188000, episode_reward=544.20 +/- 197.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=189000, episode_reward=528.20 +/- 108.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=190000, episode_reward=511.60 +/- 251.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=191000, episode_reward=634.60 +/- 291.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=192000, episode_reward=392.20 +/- 159.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=193000, episode_reward=585.00 +/- 170.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=194000, episode_reward=526.00 +/- 198.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=195000, episode_reward=736.80 +/- 185.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=196000, episode_reward=603.00 +/- 50.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=197000, episode_reward=684.20 +/- 240.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=198000, episode_reward=678.20 +/- 164.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=199000, episode_reward=813.40 +/- 275.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=200000, episode_reward=434.60 +/- 163.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=201000, episode_reward=681.40 +/- 96.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=202000, episode_reward=351.20 +/- 40.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=203000, episode_reward=773.60 +/- 245.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=204000, episode_reward=877.80 +/- 165.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=205000, episode_reward=758.20 +/- 153.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=206000, episode_reward=1093.20 +/- 344.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=207000, episode_reward=980.00 +/- 291.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=208000, episode_reward=604.60 +/- 253.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=209000, episode_reward=556.00 +/- 237.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=210000, episode_reward=604.60 +/- 208.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=211000, episode_reward=1048.00 +/- 224.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=212000, episode_reward=649.00 +/- 113.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=213000, episode_reward=482.20 +/- 275.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=214000, episode_reward=629.40 +/- 191.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=215000, episode_reward=707.40 +/- 217.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=216000, episode_reward=417.60 +/- 281.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=217000, episode_reward=441.60 +/- 232.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=218000, episode_reward=755.00 +/- 273.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=219000, episode_reward=700.60 +/- 245.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=220000, episode_reward=549.40 +/- 33.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=221000, episode_reward=661.60 +/- 250.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=222000, episode_reward=673.00 +/- 203.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=223000, episode_reward=504.80 +/- 283.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=224000, episode_reward=917.20 +/- 303.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=225000, episode_reward=916.20 +/- 198.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=226000, episode_reward=802.40 +/- 282.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=227000, episode_reward=935.60 +/- 290.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=228000, episode_reward=739.40 +/- 141.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=229000, episode_reward=850.60 +/- 431.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=230000, episode_reward=648.20 +/- 314.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=231000, episode_reward=811.60 +/- 217.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=232000, episode_reward=620.40 +/- 166.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=233000, episode_reward=646.60 +/- 227.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=234000, episode_reward=1079.40 +/- 285.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=235000, episode_reward=585.00 +/- 240.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=236000, episode_reward=962.60 +/- 264.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=237000, episode_reward=1180.00 +/- 280.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=238000, episode_reward=686.60 +/- 85.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=239000, episode_reward=1037.40 +/- 142.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=240000, episode_reward=927.60 +/- 94.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=241000, episode_reward=1251.00 +/- 404.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=242000, episode_reward=1406.80 +/- 249.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=243000, episode_reward=1296.60 +/- 254.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=244000, episode_reward=1257.00 +/- 337.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=245000, episode_reward=1257.40 +/- 315.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=246000, episode_reward=931.40 +/- 280.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=247000, episode_reward=1461.00 +/- 177.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=248000, episode_reward=1151.60 +/- 457.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=249000, episode_reward=964.60 +/- 165.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=250000, episode_reward=1424.00 +/- 264.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=251000, episode_reward=1425.80 +/- 182.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=252000, episode_reward=1546.80 +/- 233.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=253000, episode_reward=1343.80 +/- 212.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=254000, episode_reward=882.80 +/- 331.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=255000, episode_reward=1417.40 +/- 328.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=256000, episode_reward=1574.20 +/- 252.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=257000, episode_reward=1242.40 +/- 279.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=258000, episode_reward=1203.00 +/- 213.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=259000, episode_reward=1133.20 +/- 153.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=260000, episode_reward=1110.60 +/- 353.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=261000, episode_reward=1426.20 +/- 235.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=262000, episode_reward=1101.00 +/- 260.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=263000, episode_reward=1426.20 +/- 437.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=264000, episode_reward=1516.20 +/- 92.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=265000, episode_reward=1720.40 +/- 101.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=266000, episode_reward=1712.80 +/- 163.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=267000, episode_reward=1366.20 +/- 136.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=268000, episode_reward=1554.00 +/- 310.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=269000, episode_reward=1340.20 +/- 247.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=270000, episode_reward=1350.60 +/- 140.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=271000, episode_reward=1503.20 +/- 366.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=272000, episode_reward=1195.80 +/- 350.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=273000, episode_reward=1177.20 +/- 264.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=274000, episode_reward=1446.00 +/- 299.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=275000, episode_reward=1444.20 +/- 298.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=276000, episode_reward=1704.80 +/- 198.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=277000, episode_reward=1200.60 +/- 209.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=278000, episode_reward=1225.20 +/- 165.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=279000, episode_reward=1366.40 +/- 135.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=280000, episode_reward=1025.20 +/- 317.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=281000, episode_reward=1494.60 +/- 283.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=282000, episode_reward=1030.20 +/- 423.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=283000, episode_reward=1399.80 +/- 301.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=284000, episode_reward=1016.40 +/- 359.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=285000, episode_reward=834.60 +/- 239.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=286000, episode_reward=1292.00 +/- 360.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=287000, episode_reward=1171.20 +/- 204.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=288000, episode_reward=1060.60 +/- 127.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=289000, episode_reward=1096.60 +/- 452.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=290000, episode_reward=1177.60 +/- 291.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=291000, episode_reward=1262.60 +/- 268.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=292000, episode_reward=1568.80 +/- 110.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=293000, episode_reward=1700.00 +/- 136.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=294000, episode_reward=1312.00 +/- 186.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=295000, episode_reward=1230.60 +/- 312.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=296000, episode_reward=1693.20 +/- 209.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=297000, episode_reward=1083.60 +/- 342.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=298000, episode_reward=1391.00 +/- 124.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=299000, episode_reward=1180.20 +/- 275.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=300000, episode_reward=1583.40 +/- 156.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=301000, episode_reward=1091.00 +/- 271.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=302000, episode_reward=1246.20 +/- 256.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=303000, episode_reward=1450.00 +/- 293.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=304000, episode_reward=1413.80 +/- 239.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=305000, episode_reward=1443.40 +/- 234.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=306000, episode_reward=1571.00 +/- 219.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=307000, episode_reward=1361.60 +/- 286.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=308000, episode_reward=1696.00 +/- 192.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=309000, episode_reward=1422.20 +/- 234.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=310000, episode_reward=1545.60 +/- 210.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=311000, episode_reward=1401.80 +/- 229.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=312000, episode_reward=1085.80 +/- 194.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=313000, episode_reward=1372.40 +/- 289.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=314000, episode_reward=874.80 +/- 355.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=315000, episode_reward=1579.20 +/- 220.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=316000, episode_reward=1516.80 +/- 137.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=317000, episode_reward=1177.00 +/- 460.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=318000, episode_reward=1472.60 +/- 251.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=319000, episode_reward=1186.00 +/- 289.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=320000, episode_reward=1158.20 +/- 352.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=321000, episode_reward=1499.20 +/- 110.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=322000, episode_reward=1455.80 +/- 138.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=323000, episode_reward=1101.60 +/- 190.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=324000, episode_reward=1342.40 +/- 300.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=325000, episode_reward=1335.60 +/- 284.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=326000, episode_reward=1320.80 +/- 370.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=327000, episode_reward=818.60 +/- 226.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=328000, episode_reward=915.80 +/- 247.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=329000, episode_reward=954.60 +/- 234.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=330000, episode_reward=1507.80 +/- 265.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=331000, episode_reward=1360.80 +/- 337.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=332000, episode_reward=1351.00 +/- 175.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=333000, episode_reward=1750.60 +/- 226.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=334000, episode_reward=1146.40 +/- 306.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=335000, episode_reward=1444.40 +/- 193.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=336000, episode_reward=1304.60 +/- 339.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=337000, episode_reward=1520.80 +/- 78.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=338000, episode_reward=1462.00 +/- 151.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=339000, episode_reward=1560.60 +/- 349.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=340000, episode_reward=1521.60 +/- 249.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=341000, episode_reward=1420.80 +/- 344.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=342000, episode_reward=1399.00 +/- 268.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=343000, episode_reward=1586.20 +/- 193.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=344000, episode_reward=1110.00 +/- 317.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=345000, episode_reward=1137.60 +/- 348.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=346000, episode_reward=1420.80 +/- 300.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=347000, episode_reward=1196.60 +/- 98.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=348000, episode_reward=1122.40 +/- 218.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=349000, episode_reward=1386.20 +/- 237.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=350000, episode_reward=1349.00 +/- 488.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=351000, episode_reward=1627.60 +/- 161.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=352000, episode_reward=1412.40 +/- 277.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=353000, episode_reward=1460.00 +/- 183.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=354000, episode_reward=732.40 +/- 251.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=355000, episode_reward=1530.40 +/- 221.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=356000, episode_reward=1557.40 +/- 277.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=357000, episode_reward=1532.40 +/- 139.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=358000, episode_reward=737.00 +/- 374.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=359000, episode_reward=1253.40 +/- 402.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=360000, episode_reward=1216.40 +/- 236.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=361000, episode_reward=1377.80 +/- 200.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=362000, episode_reward=862.80 +/- 196.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=363000, episode_reward=1096.60 +/- 255.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=364000, episode_reward=660.20 +/- 301.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=365000, episode_reward=1628.60 +/- 91.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=366000, episode_reward=1440.00 +/- 158.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=367000, episode_reward=652.20 +/- 336.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=368000, episode_reward=919.60 +/- 304.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=369000, episode_reward=1054.40 +/- 376.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=370000, episode_reward=642.80 +/- 456.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=371000, episode_reward=1282.80 +/- 210.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=372000, episode_reward=1304.80 +/- 111.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=373000, episode_reward=1349.20 +/- 237.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=374000, episode_reward=1363.00 +/- 157.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=375000, episode_reward=1478.20 +/- 578.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=376000, episode_reward=1157.00 +/- 280.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=377000, episode_reward=1459.80 +/- 122.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=378000, episode_reward=1208.80 +/- 230.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=379000, episode_reward=1356.60 +/- 211.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=380000, episode_reward=917.40 +/- 198.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=381000, episode_reward=1511.60 +/- 262.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=382000, episode_reward=1561.80 +/- 172.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=383000, episode_reward=1057.60 +/- 300.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=384000, episode_reward=1275.60 +/- 266.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=385000, episode_reward=1299.00 +/- 191.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=386000, episode_reward=909.20 +/- 232.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=387000, episode_reward=1269.20 +/- 299.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=388000, episode_reward=880.00 +/- 263.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=389000, episode_reward=1170.60 +/- 146.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=390000, episode_reward=829.60 +/- 211.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=391000, episode_reward=1374.20 +/- 402.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=392000, episode_reward=852.20 +/- 295.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=393000, episode_reward=904.40 +/- 221.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=394000, episode_reward=1144.00 +/- 279.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=395000, episode_reward=758.40 +/- 147.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=396000, episode_reward=1118.80 +/- 402.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=397000, episode_reward=1502.40 +/- 220.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=398000, episode_reward=1409.60 +/- 205.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=399000, episode_reward=942.80 +/- 505.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=400000, episode_reward=930.60 +/- 153.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=401000, episode_reward=1154.00 +/- 275.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=402000, episode_reward=1330.40 +/- 248.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=403000, episode_reward=1090.40 +/- 299.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=404000, episode_reward=1293.80 +/- 328.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=405000, episode_reward=1141.20 +/- 194.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=406000, episode_reward=817.60 +/- 250.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=407000, episode_reward=1308.00 +/- 154.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=408000, episode_reward=1425.60 +/- 228.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=409000, episode_reward=1801.20 +/- 414.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=410000, episode_reward=1807.00 +/- 193.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=411000, episode_reward=847.60 +/- 438.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=412000, episode_reward=1210.40 +/- 138.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=413000, episode_reward=1140.40 +/- 342.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=414000, episode_reward=1414.20 +/- 195.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=415000, episode_reward=1058.20 +/- 124.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=416000, episode_reward=1235.20 +/- 241.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=417000, episode_reward=1303.60 +/- 91.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=418000, episode_reward=1318.60 +/- 415.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=419000, episode_reward=1250.00 +/- 175.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=420000, episode_reward=1445.40 +/- 147.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=421000, episode_reward=1619.20 +/- 541.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=422000, episode_reward=1097.00 +/- 251.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=423000, episode_reward=1322.20 +/- 232.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=424000, episode_reward=1119.00 +/- 425.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=425000, episode_reward=1103.80 +/- 332.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=426000, episode_reward=1417.60 +/- 327.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=427000, episode_reward=495.20 +/- 215.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=428000, episode_reward=718.80 +/- 378.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=429000, episode_reward=1243.20 +/- 219.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=430000, episode_reward=1245.60 +/- 425.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=431000, episode_reward=889.80 +/- 33.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=432000, episode_reward=660.40 +/- 220.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=433000, episode_reward=1521.20 +/- 218.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=434000, episode_reward=1491.60 +/- 404.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=435000, episode_reward=1515.00 +/- 230.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=436000, episode_reward=1055.00 +/- 266.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=437000, episode_reward=1151.00 +/- 160.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=438000, episode_reward=1233.40 +/- 271.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=439000, episode_reward=1087.60 +/- 427.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=440000, episode_reward=1608.80 +/- 268.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=441000, episode_reward=1029.40 +/- 339.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=442000, episode_reward=1597.00 +/- 226.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=443000, episode_reward=1576.60 +/- 220.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=444000, episode_reward=1382.00 +/- 171.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=445000, episode_reward=1264.40 +/- 410.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=446000, episode_reward=1131.40 +/- 118.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=447000, episode_reward=984.00 +/- 183.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=448000, episode_reward=1029.20 +/- 292.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=449000, episode_reward=1500.20 +/- 267.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=450000, episode_reward=1719.00 +/- 160.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=451000, episode_reward=1062.00 +/- 400.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=452000, episode_reward=1134.00 +/- 222.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=453000, episode_reward=910.40 +/- 372.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=454000, episode_reward=1278.00 +/- 288.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=455000, episode_reward=1629.40 +/- 187.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=456000, episode_reward=1162.60 +/- 185.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=457000, episode_reward=1192.40 +/- 187.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=458000, episode_reward=1108.20 +/- 206.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=459000, episode_reward=1190.00 +/- 391.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=460000, episode_reward=1371.00 +/- 289.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=461000, episode_reward=1135.20 +/- 102.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=462000, episode_reward=1320.20 +/- 360.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=463000, episode_reward=1143.60 +/- 343.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=464000, episode_reward=1439.60 +/- 365.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=465000, episode_reward=977.20 +/- 142.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=466000, episode_reward=1498.40 +/- 250.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=467000, episode_reward=1190.60 +/- 314.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=468000, episode_reward=1503.00 +/- 176.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=469000, episode_reward=189.60 +/- 192.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=470000, episode_reward=1085.80 +/- 227.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=471000, episode_reward=1057.40 +/- 179.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=472000, episode_reward=1225.00 +/- 133.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=473000, episode_reward=1430.00 +/- 249.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=474000, episode_reward=1149.20 +/- 457.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=475000, episode_reward=1212.20 +/- 373.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=476000, episode_reward=1559.40 +/- 233.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=477000, episode_reward=1659.60 +/- 78.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=478000, episode_reward=1242.20 +/- 233.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=479000, episode_reward=1486.20 +/- 230.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=480000, episode_reward=1550.20 +/- 248.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=481000, episode_reward=1431.20 +/- 127.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=482000, episode_reward=1620.40 +/- 374.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=483000, episode_reward=1498.20 +/- 173.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=484000, episode_reward=1600.00 +/- 184.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=485000, episode_reward=856.00 +/- 289.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=486000, episode_reward=1707.20 +/- 267.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=487000, episode_reward=958.00 +/- 392.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=488000, episode_reward=1175.00 +/- 254.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=489000, episode_reward=1124.40 +/- 460.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=490000, episode_reward=1626.60 +/- 207.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=491000, episode_reward=1616.20 +/- 88.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=492000, episode_reward=1052.00 +/- 189.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=493000, episode_reward=1326.40 +/- 341.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=494000, episode_reward=1206.00 +/- 290.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=495000, episode_reward=1287.60 +/- 318.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=496000, episode_reward=1046.40 +/- 373.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=497000, episode_reward=1447.80 +/- 258.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=498000, episode_reward=1596.20 +/- 248.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=499000, episode_reward=989.00 +/- 177.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=500000, episode_reward=696.80 +/- 104.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Gamma = 0.99, Learning Rate = 0.003, Activation Function = ReLU, Neurons = 64, Process noise = 0.1, Measurement noise = 0.0\n",
      "Eval num_timesteps=1000, episode_reward=-1451.00 +/- 48.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=-1502.00 +/- 38.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=3000, episode_reward=-1496.00 +/- 64.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=4000, episode_reward=-1493.00 +/- 41.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=5000, episode_reward=-1448.00 +/- 71.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=6000, episode_reward=-1481.00 +/- 70.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=7000, episode_reward=-1468.00 +/- 56.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=8000, episode_reward=-1548.00 +/- 58.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=9000, episode_reward=-1543.00 +/- 53.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=10000, episode_reward=-1491.00 +/- 27.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=11000, episode_reward=-1529.00 +/- 64.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=12000, episode_reward=-1540.00 +/- 51.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=13000, episode_reward=-1467.00 +/- 129.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=14000, episode_reward=-1503.00 +/- 35.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=15000, episode_reward=-1523.00 +/- 46.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=16000, episode_reward=-1531.00 +/- 37.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=17000, episode_reward=-1551.00 +/- 48.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=18000, episode_reward=-1452.00 +/- 38.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=19000, episode_reward=-1471.00 +/- 91.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=20000, episode_reward=-1546.00 +/- 71.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=21000, episode_reward=-1514.00 +/- 88.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=22000, episode_reward=-1541.00 +/- 38.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=23000, episode_reward=-1509.00 +/- 49.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=24000, episode_reward=-1535.00 +/- 96.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=25000, episode_reward=-1481.00 +/- 64.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=26000, episode_reward=-1444.00 +/- 108.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=27000, episode_reward=-1506.00 +/- 148.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=28000, episode_reward=-1427.00 +/- 83.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=29000, episode_reward=-1466.00 +/- 55.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=30000, episode_reward=-1482.00 +/- 42.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=31000, episode_reward=-1469.00 +/- 27.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=32000, episode_reward=-1520.00 +/- 59.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=33000, episode_reward=-1445.00 +/- 55.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=34000, episode_reward=-1479.00 +/- 77.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=35000, episode_reward=-1549.00 +/- 80.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=36000, episode_reward=-1498.00 +/- 54.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=37000, episode_reward=-1446.00 +/- 101.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=38000, episode_reward=-1456.00 +/- 62.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=39000, episode_reward=-1531.00 +/- 99.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=40000, episode_reward=-1454.00 +/- 63.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=41000, episode_reward=-1465.00 +/- 72.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=42000, episode_reward=-1542.00 +/- 103.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=43000, episode_reward=-1458.00 +/- 78.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=44000, episode_reward=-1489.00 +/- 57.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=45000, episode_reward=-1574.00 +/- 62.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=46000, episode_reward=-1544.00 +/- 73.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=47000, episode_reward=-1484.00 +/- 70.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=48000, episode_reward=-1514.00 +/- 77.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=49000, episode_reward=-1547.00 +/- 94.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=50000, episode_reward=-1464.00 +/- 46.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=51000, episode_reward=421.20 +/- 260.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=52000, episode_reward=405.40 +/- 81.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=53000, episode_reward=-88.40 +/- 140.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=54000, episode_reward=-3.00 +/- 128.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=55000, episode_reward=-130.40 +/- 92.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=56000, episode_reward=224.80 +/- 148.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=57000, episode_reward=287.20 +/- 196.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=58000, episode_reward=61.00 +/- 125.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=59000, episode_reward=164.00 +/- 238.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=60000, episode_reward=224.80 +/- 263.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=61000, episode_reward=290.00 +/- 142.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=62000, episode_reward=476.40 +/- 140.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=63000, episode_reward=331.20 +/- 213.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=64000, episode_reward=398.60 +/- 212.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=65000, episode_reward=366.20 +/- 277.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=66000, episode_reward=471.60 +/- 93.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=67000, episode_reward=365.00 +/- 116.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=68000, episode_reward=398.40 +/- 222.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=69000, episode_reward=257.80 +/- 207.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=70000, episode_reward=379.20 +/- 169.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=71000, episode_reward=139.80 +/- 254.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=72000, episode_reward=280.40 +/- 156.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=73000, episode_reward=320.60 +/- 217.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=74000, episode_reward=394.80 +/- 162.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=75000, episode_reward=557.00 +/- 143.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=76000, episode_reward=376.40 +/- 100.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=77000, episode_reward=466.20 +/- 302.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=78000, episode_reward=295.20 +/- 300.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=79000, episode_reward=195.60 +/- 173.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=80000, episode_reward=238.20 +/- 183.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=81000, episode_reward=278.00 +/- 151.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=82000, episode_reward=465.80 +/- 212.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=83000, episode_reward=473.40 +/- 244.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=84000, episode_reward=380.20 +/- 314.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=85000, episode_reward=285.20 +/- 178.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=86000, episode_reward=378.20 +/- 260.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=87000, episode_reward=408.00 +/- 204.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=88000, episode_reward=356.20 +/- 202.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=89000, episode_reward=272.00 +/- 128.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=90000, episode_reward=401.40 +/- 104.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=91000, episode_reward=110.80 +/- 254.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=92000, episode_reward=398.60 +/- 131.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=93000, episode_reward=256.00 +/- 70.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=94000, episode_reward=287.20 +/- 223.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=95000, episode_reward=374.60 +/- 131.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=96000, episode_reward=347.00 +/- 203.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=97000, episode_reward=261.20 +/- 160.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=98000, episode_reward=362.60 +/- 141.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=99000, episode_reward=423.20 +/- 255.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=100000, episode_reward=347.00 +/- 248.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=101000, episode_reward=465.00 +/- 142.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=102000, episode_reward=389.80 +/- 162.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=103000, episode_reward=334.60 +/- 112.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=104000, episode_reward=311.60 +/- 59.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=105000, episode_reward=315.60 +/- 250.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=106000, episode_reward=409.80 +/- 221.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=107000, episode_reward=418.40 +/- 206.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=108000, episode_reward=568.60 +/- 279.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=109000, episode_reward=516.00 +/- 207.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=110000, episode_reward=191.60 +/- 205.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=111000, episode_reward=580.80 +/- 98.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=112000, episode_reward=340.60 +/- 109.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=113000, episode_reward=475.00 +/- 212.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=114000, episode_reward=412.20 +/- 186.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=115000, episode_reward=352.20 +/- 228.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=116000, episode_reward=410.20 +/- 175.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=117000, episode_reward=238.20 +/- 89.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=118000, episode_reward=272.00 +/- 224.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=119000, episode_reward=203.40 +/- 171.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=120000, episode_reward=322.80 +/- 209.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=121000, episode_reward=651.80 +/- 114.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=122000, episode_reward=434.20 +/- 212.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=123000, episode_reward=625.80 +/- 358.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=124000, episode_reward=533.00 +/- 223.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=125000, episode_reward=424.40 +/- 289.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=126000, episode_reward=366.00 +/- 138.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=127000, episode_reward=677.40 +/- 322.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=128000, episode_reward=716.00 +/- 260.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=129000, episode_reward=841.60 +/- 108.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=130000, episode_reward=716.00 +/- 247.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=131000, episode_reward=646.20 +/- 192.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=132000, episode_reward=810.20 +/- 211.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=133000, episode_reward=820.60 +/- 341.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=134000, episode_reward=915.40 +/- 243.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=135000, episode_reward=1019.40 +/- 211.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=136000, episode_reward=887.60 +/- 295.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=137000, episode_reward=1005.00 +/- 141.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=138000, episode_reward=1021.60 +/- 150.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=139000, episode_reward=905.60 +/- 207.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=140000, episode_reward=1082.80 +/- 239.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=141000, episode_reward=1503.40 +/- 80.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=142000, episode_reward=795.00 +/- 404.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=143000, episode_reward=1095.00 +/- 473.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=144000, episode_reward=1086.60 +/- 296.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=145000, episode_reward=1315.20 +/- 354.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=146000, episode_reward=1403.20 +/- 130.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=147000, episode_reward=1114.40 +/- 205.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=148000, episode_reward=541.00 +/- 429.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=149000, episode_reward=1050.20 +/- 145.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=150000, episode_reward=1234.80 +/- 347.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=151000, episode_reward=1272.80 +/- 253.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=152000, episode_reward=1052.60 +/- 305.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=153000, episode_reward=1347.20 +/- 409.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=154000, episode_reward=828.80 +/- 275.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=155000, episode_reward=1590.40 +/- 274.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=156000, episode_reward=1175.40 +/- 385.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=157000, episode_reward=1158.60 +/- 294.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=158000, episode_reward=1072.80 +/- 217.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=159000, episode_reward=1640.60 +/- 45.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=160000, episode_reward=1730.20 +/- 225.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=161000, episode_reward=1535.00 +/- 87.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=162000, episode_reward=1157.00 +/- 281.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=163000, episode_reward=977.40 +/- 331.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=164000, episode_reward=1393.20 +/- 105.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=165000, episode_reward=1192.80 +/- 442.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=166000, episode_reward=760.60 +/- 547.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=167000, episode_reward=1008.20 +/- 367.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=168000, episode_reward=1317.60 +/- 390.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=169000, episode_reward=1422.00 +/- 281.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=170000, episode_reward=1392.20 +/- 259.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=171000, episode_reward=1335.80 +/- 215.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=172000, episode_reward=1016.00 +/- 334.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=173000, episode_reward=1004.20 +/- 311.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=174000, episode_reward=1249.80 +/- 315.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=175000, episode_reward=1029.00 +/- 169.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=176000, episode_reward=1180.20 +/- 157.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=177000, episode_reward=1212.20 +/- 320.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=178000, episode_reward=1353.60 +/- 99.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=179000, episode_reward=1270.40 +/- 184.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=180000, episode_reward=1404.40 +/- 207.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=181000, episode_reward=877.60 +/- 202.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=182000, episode_reward=1071.00 +/- 307.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=183000, episode_reward=1431.60 +/- 210.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=184000, episode_reward=1299.60 +/- 278.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=185000, episode_reward=1059.80 +/- 228.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=186000, episode_reward=997.20 +/- 243.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=187000, episode_reward=804.00 +/- 451.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=188000, episode_reward=1672.00 +/- 114.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=189000, episode_reward=1085.20 +/- 364.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=190000, episode_reward=1161.60 +/- 402.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=191000, episode_reward=1174.80 +/- 352.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=192000, episode_reward=1338.60 +/- 159.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=193000, episode_reward=1505.80 +/- 237.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=194000, episode_reward=1531.00 +/- 173.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=195000, episode_reward=1006.60 +/- 156.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=196000, episode_reward=860.00 +/- 301.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=197000, episode_reward=1027.60 +/- 105.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=198000, episode_reward=1309.00 +/- 203.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=199000, episode_reward=1604.60 +/- 329.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=200000, episode_reward=997.80 +/- 287.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=201000, episode_reward=1449.40 +/- 332.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=202000, episode_reward=1338.60 +/- 166.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=203000, episode_reward=1088.60 +/- 432.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=204000, episode_reward=1381.00 +/- 489.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=205000, episode_reward=1161.60 +/- 66.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=206000, episode_reward=1092.00 +/- 381.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=207000, episode_reward=1091.20 +/- 191.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=208000, episode_reward=1337.40 +/- 176.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=209000, episode_reward=1305.80 +/- 374.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=210000, episode_reward=1435.00 +/- 95.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=211000, episode_reward=959.40 +/- 195.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=212000, episode_reward=974.00 +/- 209.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=213000, episode_reward=919.80 +/- 184.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=214000, episode_reward=1161.20 +/- 522.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=215000, episode_reward=1214.20 +/- 188.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=216000, episode_reward=1738.20 +/- 122.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=217000, episode_reward=1421.00 +/- 128.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=218000, episode_reward=1426.80 +/- 224.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=219000, episode_reward=1168.80 +/- 266.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=220000, episode_reward=1161.40 +/- 141.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=221000, episode_reward=1270.60 +/- 321.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=222000, episode_reward=1221.20 +/- 493.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=223000, episode_reward=1296.40 +/- 422.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=224000, episode_reward=935.00 +/- 312.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=225000, episode_reward=1302.00 +/- 366.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=226000, episode_reward=1396.80 +/- 293.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=227000, episode_reward=969.40 +/- 136.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=228000, episode_reward=1079.00 +/- 423.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=229000, episode_reward=1210.20 +/- 140.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=230000, episode_reward=984.00 +/- 301.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=231000, episode_reward=1509.20 +/- 396.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=232000, episode_reward=1237.40 +/- 261.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=233000, episode_reward=1706.00 +/- 259.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=234000, episode_reward=1633.80 +/- 214.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=235000, episode_reward=1415.00 +/- 379.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=236000, episode_reward=1435.80 +/- 303.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=237000, episode_reward=1499.80 +/- 320.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=238000, episode_reward=843.80 +/- 132.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=239000, episode_reward=1075.60 +/- 110.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=240000, episode_reward=1201.40 +/- 391.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=241000, episode_reward=1673.20 +/- 239.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=242000, episode_reward=794.00 +/- 433.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=243000, episode_reward=1191.60 +/- 233.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=244000, episode_reward=1289.60 +/- 288.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=245000, episode_reward=1614.00 +/- 217.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=246000, episode_reward=1147.80 +/- 218.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=247000, episode_reward=1580.40 +/- 231.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=248000, episode_reward=1229.00 +/- 259.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=249000, episode_reward=1103.20 +/- 339.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=250000, episode_reward=1537.80 +/- 348.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=251000, episode_reward=926.20 +/- 251.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=252000, episode_reward=1215.80 +/- 220.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=253000, episode_reward=1203.00 +/- 185.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=254000, episode_reward=1276.60 +/- 233.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=255000, episode_reward=1399.60 +/- 212.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=256000, episode_reward=812.00 +/- 347.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=257000, episode_reward=1164.00 +/- 436.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=258000, episode_reward=1603.80 +/- 178.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=259000, episode_reward=1175.40 +/- 188.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=260000, episode_reward=1275.60 +/- 379.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=261000, episode_reward=1441.60 +/- 186.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=262000, episode_reward=1479.00 +/- 123.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=263000, episode_reward=1297.20 +/- 145.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=264000, episode_reward=1160.80 +/- 456.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=265000, episode_reward=1586.80 +/- 218.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=266000, episode_reward=1131.00 +/- 175.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=267000, episode_reward=1276.40 +/- 262.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=268000, episode_reward=1394.20 +/- 302.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=269000, episode_reward=1011.60 +/- 267.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=270000, episode_reward=1641.60 +/- 78.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=271000, episode_reward=1237.80 +/- 200.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=272000, episode_reward=1231.20 +/- 556.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=273000, episode_reward=1052.40 +/- 346.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=274000, episode_reward=841.40 +/- 158.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=275000, episode_reward=1352.80 +/- 260.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=276000, episode_reward=960.40 +/- 217.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=277000, episode_reward=1441.60 +/- 104.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=278000, episode_reward=1340.60 +/- 142.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=279000, episode_reward=1055.80 +/- 73.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=280000, episode_reward=1526.20 +/- 66.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=281000, episode_reward=1479.00 +/- 257.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=282000, episode_reward=999.80 +/- 331.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=283000, episode_reward=1242.60 +/- 267.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=284000, episode_reward=1377.60 +/- 276.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=285000, episode_reward=747.60 +/- 192.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=286000, episode_reward=1230.00 +/- 175.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=287000, episode_reward=1523.80 +/- 371.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=288000, episode_reward=994.20 +/- 165.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=289000, episode_reward=1121.20 +/- 250.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=290000, episode_reward=1082.80 +/- 312.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=291000, episode_reward=1402.40 +/- 181.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=292000, episode_reward=1437.40 +/- 250.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=293000, episode_reward=1288.00 +/- 234.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=294000, episode_reward=1206.80 +/- 240.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=295000, episode_reward=1317.80 +/- 180.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=296000, episode_reward=1249.40 +/- 529.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=297000, episode_reward=1403.00 +/- 336.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=298000, episode_reward=1496.60 +/- 143.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=299000, episode_reward=998.20 +/- 334.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=300000, episode_reward=1668.20 +/- 391.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=301000, episode_reward=918.80 +/- 336.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=302000, episode_reward=1339.60 +/- 233.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=303000, episode_reward=906.00 +/- 235.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=304000, episode_reward=1147.60 +/- 389.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=305000, episode_reward=809.20 +/- 258.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=306000, episode_reward=1593.00 +/- 272.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=307000, episode_reward=303.60 +/- 91.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=308000, episode_reward=1630.40 +/- 385.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=309000, episode_reward=1297.40 +/- 260.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=310000, episode_reward=1133.20 +/- 245.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=311000, episode_reward=1194.80 +/- 86.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=312000, episode_reward=1312.60 +/- 179.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=313000, episode_reward=1175.00 +/- 271.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=314000, episode_reward=1269.60 +/- 124.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=315000, episode_reward=1238.60 +/- 312.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=316000, episode_reward=1044.40 +/- 136.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=317000, episode_reward=900.00 +/- 471.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=318000, episode_reward=1177.40 +/- 349.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=319000, episode_reward=884.00 +/- 134.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=320000, episode_reward=1303.60 +/- 105.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=321000, episode_reward=1415.20 +/- 81.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=322000, episode_reward=1285.60 +/- 238.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=323000, episode_reward=885.00 +/- 184.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=324000, episode_reward=892.20 +/- 509.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=325000, episode_reward=1475.80 +/- 300.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=326000, episode_reward=1296.80 +/- 369.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=327000, episode_reward=1542.60 +/- 84.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=328000, episode_reward=1199.40 +/- 224.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=329000, episode_reward=1649.80 +/- 253.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=330000, episode_reward=996.60 +/- 218.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=331000, episode_reward=1405.00 +/- 121.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=332000, episode_reward=1460.40 +/- 158.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=333000, episode_reward=1555.20 +/- 213.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=334000, episode_reward=1510.20 +/- 340.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=335000, episode_reward=1696.60 +/- 183.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=336000, episode_reward=1254.20 +/- 428.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=337000, episode_reward=1529.80 +/- 249.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=338000, episode_reward=1533.40 +/- 460.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=339000, episode_reward=1150.40 +/- 142.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=340000, episode_reward=1058.60 +/- 470.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=341000, episode_reward=1336.00 +/- 203.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=342000, episode_reward=1641.00 +/- 346.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=343000, episode_reward=1351.20 +/- 385.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=344000, episode_reward=1045.20 +/- 478.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=345000, episode_reward=1592.20 +/- 197.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=346000, episode_reward=1332.60 +/- 467.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=347000, episode_reward=1612.80 +/- 234.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=348000, episode_reward=1494.00 +/- 278.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=349000, episode_reward=1583.40 +/- 198.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=350000, episode_reward=1210.40 +/- 415.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=351000, episode_reward=1758.40 +/- 70.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=352000, episode_reward=1615.40 +/- 134.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=353000, episode_reward=1461.80 +/- 196.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=354000, episode_reward=1609.60 +/- 181.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=355000, episode_reward=1301.60 +/- 254.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=356000, episode_reward=1534.60 +/- 297.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=357000, episode_reward=1430.40 +/- 222.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=358000, episode_reward=708.40 +/- 168.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=359000, episode_reward=1149.20 +/- 281.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=360000, episode_reward=1409.80 +/- 257.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=361000, episode_reward=1574.00 +/- 194.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=362000, episode_reward=1603.20 +/- 116.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=363000, episode_reward=916.60 +/- 343.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=364000, episode_reward=1450.80 +/- 223.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=365000, episode_reward=1191.00 +/- 273.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=366000, episode_reward=1248.80 +/- 179.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=367000, episode_reward=1437.00 +/- 187.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=368000, episode_reward=1077.60 +/- 325.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=369000, episode_reward=1349.00 +/- 231.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=370000, episode_reward=1172.40 +/- 181.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=371000, episode_reward=946.20 +/- 81.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=372000, episode_reward=1341.00 +/- 223.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=373000, episode_reward=1415.40 +/- 106.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=374000, episode_reward=624.00 +/- 273.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=375000, episode_reward=1213.60 +/- 265.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=376000, episode_reward=730.60 +/- 299.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=377000, episode_reward=1259.80 +/- 177.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=378000, episode_reward=1645.20 +/- 220.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=379000, episode_reward=629.20 +/- 326.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=380000, episode_reward=1295.40 +/- 476.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=381000, episode_reward=1649.40 +/- 316.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=382000, episode_reward=1236.40 +/- 336.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=383000, episode_reward=1245.80 +/- 533.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=384000, episode_reward=1064.40 +/- 263.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=385000, episode_reward=1425.60 +/- 57.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=386000, episode_reward=1167.00 +/- 460.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=387000, episode_reward=1237.00 +/- 335.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=388000, episode_reward=908.00 +/- 143.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=389000, episode_reward=1435.20 +/- 235.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=390000, episode_reward=952.60 +/- 381.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=391000, episode_reward=1259.40 +/- 290.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=392000, episode_reward=1512.20 +/- 104.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=393000, episode_reward=1230.20 +/- 223.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=394000, episode_reward=1635.40 +/- 129.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=395000, episode_reward=1715.20 +/- 168.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=396000, episode_reward=862.60 +/- 342.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=397000, episode_reward=1449.40 +/- 302.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=398000, episode_reward=1539.00 +/- 152.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=399000, episode_reward=1315.20 +/- 291.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=400000, episode_reward=1616.00 +/- 207.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=401000, episode_reward=1419.80 +/- 184.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=402000, episode_reward=1149.40 +/- 261.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=403000, episode_reward=912.20 +/- 289.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=404000, episode_reward=1154.40 +/- 402.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=405000, episode_reward=1192.80 +/- 345.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=406000, episode_reward=1005.60 +/- 203.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=407000, episode_reward=1546.40 +/- 115.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=408000, episode_reward=681.40 +/- 271.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=409000, episode_reward=1447.60 +/- 226.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=410000, episode_reward=816.40 +/- 330.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=411000, episode_reward=1400.20 +/- 230.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=412000, episode_reward=1808.20 +/- 199.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=413000, episode_reward=1353.80 +/- 207.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=414000, episode_reward=1476.40 +/- 379.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=415000, episode_reward=1359.80 +/- 285.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=416000, episode_reward=1649.40 +/- 277.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=417000, episode_reward=1275.40 +/- 335.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=418000, episode_reward=1556.80 +/- 373.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=419000, episode_reward=1505.20 +/- 195.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=420000, episode_reward=1185.00 +/- 332.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=421000, episode_reward=1279.00 +/- 243.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=422000, episode_reward=1763.00 +/- 96.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=423000, episode_reward=1421.60 +/- 218.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=424000, episode_reward=1127.60 +/- 242.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=425000, episode_reward=1775.00 +/- 209.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=426000, episode_reward=1610.60 +/- 364.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=427000, episode_reward=1284.00 +/- 314.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=428000, episode_reward=1535.60 +/- 264.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=429000, episode_reward=919.00 +/- 400.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=430000, episode_reward=1328.40 +/- 375.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=431000, episode_reward=1243.00 +/- 300.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=432000, episode_reward=1334.80 +/- 296.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=433000, episode_reward=1311.60 +/- 313.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=434000, episode_reward=1096.00 +/- 485.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=435000, episode_reward=1249.40 +/- 477.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=436000, episode_reward=983.80 +/- 376.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=437000, episode_reward=1085.40 +/- 199.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=438000, episode_reward=1544.40 +/- 182.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=439000, episode_reward=1194.80 +/- 333.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=440000, episode_reward=1712.40 +/- 340.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=441000, episode_reward=1049.40 +/- 470.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=442000, episode_reward=1330.00 +/- 328.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=443000, episode_reward=1609.20 +/- 308.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=444000, episode_reward=1310.00 +/- 215.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=445000, episode_reward=1216.60 +/- 263.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=446000, episode_reward=797.40 +/- 269.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=447000, episode_reward=1096.80 +/- 267.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=448000, episode_reward=551.60 +/- 188.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=449000, episode_reward=1488.20 +/- 266.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=450000, episode_reward=1324.00 +/- 259.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=451000, episode_reward=1096.40 +/- 371.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=452000, episode_reward=720.60 +/- 379.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=453000, episode_reward=1376.60 +/- 121.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=454000, episode_reward=1362.60 +/- 227.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=455000, episode_reward=1375.60 +/- 385.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=456000, episode_reward=1245.60 +/- 174.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=457000, episode_reward=737.00 +/- 224.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=458000, episode_reward=1023.00 +/- 319.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=459000, episode_reward=1360.20 +/- 180.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=460000, episode_reward=1257.80 +/- 252.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=461000, episode_reward=1461.40 +/- 302.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=462000, episode_reward=1457.40 +/- 248.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=463000, episode_reward=1456.60 +/- 215.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=464000, episode_reward=1635.00 +/- 228.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=465000, episode_reward=912.80 +/- 328.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=466000, episode_reward=820.00 +/- 279.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=467000, episode_reward=1420.20 +/- 194.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=468000, episode_reward=1363.80 +/- 209.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=469000, episode_reward=825.40 +/- 360.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=470000, episode_reward=648.20 +/- 270.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=471000, episode_reward=1392.80 +/- 309.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=472000, episode_reward=1229.60 +/- 410.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=473000, episode_reward=1376.80 +/- 261.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=474000, episode_reward=1012.80 +/- 232.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=475000, episode_reward=1241.80 +/- 298.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=476000, episode_reward=1449.00 +/- 132.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=477000, episode_reward=793.60 +/- 262.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=478000, episode_reward=1293.20 +/- 299.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=479000, episode_reward=1436.80 +/- 226.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=480000, episode_reward=1387.00 +/- 147.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=481000, episode_reward=1472.60 +/- 340.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=482000, episode_reward=1272.00 +/- 207.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=483000, episode_reward=1627.20 +/- 291.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=484000, episode_reward=1109.00 +/- 407.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=485000, episode_reward=1055.80 +/- 313.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=486000, episode_reward=1615.00 +/- 142.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=487000, episode_reward=1299.80 +/- 358.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=488000, episode_reward=1030.60 +/- 117.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=489000, episode_reward=943.80 +/- 271.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=490000, episode_reward=1057.80 +/- 231.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=491000, episode_reward=1323.60 +/- 432.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=492000, episode_reward=1421.20 +/- 241.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=493000, episode_reward=1103.60 +/- 381.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=494000, episode_reward=1077.00 +/- 211.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=495000, episode_reward=1048.00 +/- 653.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=496000, episode_reward=1473.00 +/- 267.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=497000, episode_reward=1496.20 +/- 289.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=498000, episode_reward=1077.00 +/- 190.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=499000, episode_reward=726.40 +/- 518.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=500000, episode_reward=1339.20 +/- 193.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Gamma = 0.99, Learning Rate = 0.0003, Activation Function = Tanh, Neurons = 8, Process noise = 0.1, Measurement noise = 0.0\n",
      "Eval num_timesteps=1000, episode_reward=-18975.00 +/- 70.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=-18995.00 +/- 26.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=3000, episode_reward=-19002.00 +/- 127.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=4000, episode_reward=-19068.00 +/- 130.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=5000, episode_reward=-19020.00 +/- 104.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=6000, episode_reward=-19042.00 +/- 91.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=7000, episode_reward=-19062.00 +/- 62.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=8000, episode_reward=-19014.00 +/- 93.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=9000, episode_reward=-19074.00 +/- 138.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=10000, episode_reward=-18976.00 +/- 49.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=11000, episode_reward=-19014.00 +/- 51.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=12000, episode_reward=-19024.00 +/- 92.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=13000, episode_reward=-19048.00 +/- 62.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=14000, episode_reward=-19062.00 +/- 93.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=15000, episode_reward=-19034.00 +/- 124.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=16000, episode_reward=-19022.00 +/- 87.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=17000, episode_reward=-19050.00 +/- 160.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=18000, episode_reward=-19006.00 +/- 67.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=19000, episode_reward=-19001.00 +/- 110.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=20000, episode_reward=-18985.00 +/- 54.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=21000, episode_reward=-19036.00 +/- 60.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=22000, episode_reward=-19056.00 +/- 63.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=23000, episode_reward=-18983.00 +/- 92.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=24000, episode_reward=-19026.00 +/- 95.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=25000, episode_reward=-18949.00 +/- 66.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=26000, episode_reward=-18985.00 +/- 96.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=27000, episode_reward=-19002.00 +/- 57.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=28000, episode_reward=-19078.00 +/- 95.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=29000, episode_reward=-19012.00 +/- 122.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=30000, episode_reward=-19006.00 +/- 79.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=31000, episode_reward=-19012.00 +/- 108.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=32000, episode_reward=-18977.00 +/- 48.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=33000, episode_reward=-18983.00 +/- 77.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=34000, episode_reward=-18957.00 +/- 60.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=35000, episode_reward=-19009.00 +/- 84.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=36000, episode_reward=-19109.00 +/- 69.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=37000, episode_reward=-19054.00 +/- 94.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=38000, episode_reward=-18987.00 +/- 41.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=39000, episode_reward=-18937.00 +/- 47.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-18991.00 +/- 79.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=41000, episode_reward=-19071.00 +/- 88.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=42000, episode_reward=-18998.00 +/- 90.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=43000, episode_reward=-18944.00 +/- 117.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=44000, episode_reward=-19044.00 +/- 96.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=45000, episode_reward=-18947.00 +/- 65.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=46000, episode_reward=-19006.00 +/- 49.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=47000, episode_reward=-19003.00 +/- 57.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=48000, episode_reward=-18956.00 +/- 86.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=49000, episode_reward=-18996.00 +/- 65.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=50000, episode_reward=-18986.00 +/- 88.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=51000, episode_reward=-54.60 +/- 512.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=52000, episode_reward=-111.80 +/- 411.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=53000, episode_reward=-1145.40 +/- 1551.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=54000, episode_reward=-270.60 +/- 614.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=55000, episode_reward=-34.80 +/- 316.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=56000, episode_reward=-624.00 +/- 756.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=57000, episode_reward=37.20 +/- 275.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=58000, episode_reward=-445.40 +/- 734.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=59000, episode_reward=183.40 +/- 382.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=60000, episode_reward=-158.80 +/- 386.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=61000, episode_reward=-325.00 +/- 626.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=62000, episode_reward=55.60 +/- 326.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=63000, episode_reward=-118.60 +/- 667.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=64000, episode_reward=45.00 +/- 283.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=65000, episode_reward=114.80 +/- 372.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=66000, episode_reward=-49.40 +/- 252.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=67000, episode_reward=26.40 +/- 191.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=68000, episode_reward=-346.20 +/- 1104.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=69000, episode_reward=-457.20 +/- 1300.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=70000, episode_reward=-158.40 +/- 253.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=71000, episode_reward=-208.60 +/- 337.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=72000, episode_reward=43.80 +/- 189.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=73000, episode_reward=-260.40 +/- 876.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=74000, episode_reward=128.60 +/- 223.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=75000, episode_reward=-505.80 +/- 493.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=76000, episode_reward=-340.20 +/- 238.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=77000, episode_reward=-95.40 +/- 141.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=78000, episode_reward=-212.20 +/- 304.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=79000, episode_reward=-43.80 +/- 413.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=80000, episode_reward=198.40 +/- 182.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=81000, episode_reward=236.20 +/- 207.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=82000, episode_reward=-491.80 +/- 334.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=83000, episode_reward=86.60 +/- 147.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=84000, episode_reward=-104.80 +/- 192.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=85000, episode_reward=8.20 +/- 146.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=86000, episode_reward=278.20 +/- 182.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=87000, episode_reward=49.00 +/- 278.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=88000, episode_reward=222.40 +/- 137.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=89000, episode_reward=172.20 +/- 239.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=90000, episode_reward=104.60 +/- 130.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=91000, episode_reward=15.00 +/- 302.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=92000, episode_reward=135.40 +/- 238.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=93000, episode_reward=-47.20 +/- 367.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=94000, episode_reward=91.60 +/- 240.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=95000, episode_reward=332.20 +/- 139.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=96000, episode_reward=13.80 +/- 199.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=97000, episode_reward=192.40 +/- 393.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=98000, episode_reward=122.00 +/- 144.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=99000, episode_reward=157.40 +/- 245.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=100000, episode_reward=87.60 +/- 456.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=101000, episode_reward=316.00 +/- 187.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=102000, episode_reward=44.40 +/- 135.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=103000, episode_reward=270.00 +/- 114.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=104000, episode_reward=198.00 +/- 211.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=105000, episode_reward=44.80 +/- 361.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=106000, episode_reward=54.00 +/- 236.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=107000, episode_reward=11.80 +/- 183.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=108000, episode_reward=148.20 +/- 254.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=109000, episode_reward=42.80 +/- 379.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=110000, episode_reward=148.80 +/- 325.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=111000, episode_reward=-88.40 +/- 233.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=112000, episode_reward=27.40 +/- 699.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=113000, episode_reward=-88.40 +/- 124.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=114000, episode_reward=216.20 +/- 160.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=115000, episode_reward=211.00 +/- 127.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=116000, episode_reward=-153.40 +/- 577.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=117000, episode_reward=37.40 +/- 111.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=118000, episode_reward=274.60 +/- 241.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=119000, episode_reward=314.00 +/- 184.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=120000, episode_reward=134.00 +/- 378.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=121000, episode_reward=251.80 +/- 85.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=122000, episode_reward=-76.80 +/- 151.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=123000, episode_reward=-11.20 +/- 278.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=124000, episode_reward=130.20 +/- 264.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=125000, episode_reward=8.80 +/- 211.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=126000, episode_reward=125.40 +/- 217.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=127000, episode_reward=110.40 +/- 343.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=128000, episode_reward=174.40 +/- 198.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=129000, episode_reward=29.20 +/- 271.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=130000, episode_reward=76.40 +/- 399.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=131000, episode_reward=1.60 +/- 325.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=132000, episode_reward=11.40 +/- 248.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=133000, episode_reward=109.60 +/- 123.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=134000, episode_reward=310.00 +/- 173.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=135000, episode_reward=328.80 +/- 218.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=136000, episode_reward=294.60 +/- 213.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=137000, episode_reward=192.20 +/- 291.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=138000, episode_reward=167.00 +/- 325.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=139000, episode_reward=29.60 +/- 236.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=140000, episode_reward=304.40 +/- 177.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=141000, episode_reward=394.00 +/- 327.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=142000, episode_reward=101.40 +/- 335.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=143000, episode_reward=209.80 +/- 228.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=144000, episode_reward=30.00 +/- 458.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=145000, episode_reward=243.80 +/- 144.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=146000, episode_reward=178.60 +/- 170.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=147000, episode_reward=247.20 +/- 290.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=148000, episode_reward=-32.40 +/- 428.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=149000, episode_reward=364.00 +/- 84.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=150000, episode_reward=252.80 +/- 280.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=151000, episode_reward=481.20 +/- 128.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=152000, episode_reward=333.80 +/- 222.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=153000, episode_reward=282.60 +/- 158.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=154000, episode_reward=511.20 +/- 206.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=155000, episode_reward=171.00 +/- 187.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=156000, episode_reward=412.80 +/- 255.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=157000, episode_reward=361.20 +/- 163.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=158000, episode_reward=327.60 +/- 159.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=159000, episode_reward=11.60 +/- 263.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=160000, episode_reward=232.40 +/- 110.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=161000, episode_reward=459.40 +/- 173.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=162000, episode_reward=505.40 +/- 178.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=163000, episode_reward=249.00 +/- 345.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=164000, episode_reward=235.40 +/- 342.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=165000, episode_reward=301.80 +/- 184.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=166000, episode_reward=196.60 +/- 287.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=167000, episode_reward=290.60 +/- 229.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=168000, episode_reward=335.20 +/- 229.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=169000, episode_reward=211.00 +/- 134.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=170000, episode_reward=298.20 +/- 434.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=171000, episode_reward=151.00 +/- 228.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=172000, episode_reward=394.00 +/- 256.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=173000, episode_reward=390.60 +/- 196.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=174000, episode_reward=427.40 +/- 170.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=175000, episode_reward=370.00 +/- 169.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=176000, episode_reward=283.20 +/- 107.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=177000, episode_reward=98.60 +/- 575.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=178000, episode_reward=515.00 +/- 231.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=179000, episode_reward=271.00 +/- 329.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=180000, episode_reward=335.80 +/- 137.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=181000, episode_reward=400.20 +/- 346.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=182000, episode_reward=409.40 +/- 192.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=183000, episode_reward=303.40 +/- 187.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=184000, episode_reward=386.00 +/- 214.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=185000, episode_reward=237.00 +/- 183.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=186000, episode_reward=151.80 +/- 272.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=187000, episode_reward=250.80 +/- 415.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=188000, episode_reward=398.00 +/- 286.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=189000, episode_reward=253.80 +/- 127.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=190000, episode_reward=454.20 +/- 244.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=191000, episode_reward=205.40 +/- 346.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=192000, episode_reward=282.40 +/- 257.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=193000, episode_reward=334.20 +/- 190.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=194000, episode_reward=374.40 +/- 236.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=195000, episode_reward=361.60 +/- 198.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=196000, episode_reward=341.00 +/- 214.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=197000, episode_reward=403.80 +/- 93.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=198000, episode_reward=522.60 +/- 115.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=199000, episode_reward=423.80 +/- 182.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=200000, episode_reward=384.80 +/- 290.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=201000, episode_reward=253.00 +/- 350.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=202000, episode_reward=190.00 +/- 183.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=203000, episode_reward=180.40 +/- 313.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=204000, episode_reward=256.40 +/- 422.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=205000, episode_reward=173.20 +/- 252.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=206000, episode_reward=469.60 +/- 263.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=207000, episode_reward=295.60 +/- 241.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=208000, episode_reward=438.00 +/- 201.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=209000, episode_reward=248.80 +/- 153.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=210000, episode_reward=388.60 +/- 306.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=211000, episode_reward=547.20 +/- 318.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=212000, episode_reward=309.60 +/- 341.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=213000, episode_reward=-46.60 +/- 352.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=214000, episode_reward=357.20 +/- 131.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=215000, episode_reward=400.60 +/- 206.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=216000, episode_reward=374.60 +/- 239.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=217000, episode_reward=382.20 +/- 159.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=218000, episode_reward=335.80 +/- 254.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=219000, episode_reward=459.00 +/- 180.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=220000, episode_reward=93.00 +/- 198.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=221000, episode_reward=175.00 +/- 334.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=222000, episode_reward=149.60 +/- 375.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=223000, episode_reward=133.40 +/- 166.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=224000, episode_reward=231.40 +/- 159.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=225000, episode_reward=177.60 +/- 209.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=226000, episode_reward=248.80 +/- 343.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=227000, episode_reward=180.00 +/- 233.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=228000, episode_reward=195.80 +/- 306.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=229000, episode_reward=226.20 +/- 418.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=230000, episode_reward=426.40 +/- 206.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=231000, episode_reward=436.80 +/- 326.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=232000, episode_reward=179.20 +/- 108.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=233000, episode_reward=368.20 +/- 111.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=234000, episode_reward=494.80 +/- 332.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=235000, episode_reward=457.20 +/- 131.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=236000, episode_reward=80.40 +/- 474.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=237000, episode_reward=347.40 +/- 314.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=238000, episode_reward=512.00 +/- 402.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=239000, episode_reward=394.40 +/- 154.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=240000, episode_reward=107.40 +/- 382.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=241000, episode_reward=387.60 +/- 295.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=242000, episode_reward=251.60 +/- 423.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=243000, episode_reward=412.40 +/- 205.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=244000, episode_reward=110.40 +/- 254.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=245000, episode_reward=159.80 +/- 199.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=246000, episode_reward=468.60 +/- 318.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=247000, episode_reward=450.00 +/- 296.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=248000, episode_reward=409.80 +/- 135.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=249000, episode_reward=580.40 +/- 332.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=250000, episode_reward=546.60 +/- 367.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=251000, episode_reward=361.40 +/- 202.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=252000, episode_reward=157.00 +/- 534.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=253000, episode_reward=456.00 +/- 318.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=254000, episode_reward=91.00 +/- 817.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=255000, episode_reward=533.20 +/- 227.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=256000, episode_reward=558.60 +/- 281.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=257000, episode_reward=478.20 +/- 549.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=258000, episode_reward=365.60 +/- 421.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=259000, episode_reward=695.80 +/- 207.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=260000, episode_reward=596.60 +/- 301.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=261000, episode_reward=761.60 +/- 172.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=262000, episode_reward=373.40 +/- 551.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=263000, episode_reward=653.80 +/- 337.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=264000, episode_reward=611.00 +/- 198.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=265000, episode_reward=684.00 +/- 387.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=266000, episode_reward=610.20 +/- 255.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=267000, episode_reward=493.00 +/- 354.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=268000, episode_reward=807.80 +/- 183.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=269000, episode_reward=645.80 +/- 168.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=270000, episode_reward=754.00 +/- 296.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=271000, episode_reward=581.60 +/- 395.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=272000, episode_reward=464.80 +/- 427.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=273000, episode_reward=681.40 +/- 246.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=274000, episode_reward=463.20 +/- 193.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=275000, episode_reward=559.40 +/- 210.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=276000, episode_reward=387.20 +/- 328.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=277000, episode_reward=493.80 +/- 55.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=278000, episode_reward=642.80 +/- 325.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=279000, episode_reward=872.00 +/- 223.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=280000, episode_reward=594.20 +/- 367.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=281000, episode_reward=610.20 +/- 372.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=282000, episode_reward=763.20 +/- 159.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=283000, episode_reward=852.60 +/- 549.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=284000, episode_reward=513.60 +/- 277.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=285000, episode_reward=684.20 +/- 136.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=286000, episode_reward=684.60 +/- 238.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=287000, episode_reward=500.00 +/- 557.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=288000, episode_reward=542.40 +/- 322.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=289000, episode_reward=809.40 +/- 351.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=290000, episode_reward=492.80 +/- 353.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=291000, episode_reward=668.20 +/- 210.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=292000, episode_reward=705.00 +/- 548.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=293000, episode_reward=807.40 +/- 117.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=294000, episode_reward=760.80 +/- 427.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=295000, episode_reward=932.20 +/- 129.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=296000, episode_reward=855.00 +/- 187.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=297000, episode_reward=740.40 +/- 214.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=298000, episode_reward=801.00 +/- 160.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=299000, episode_reward=783.60 +/- 214.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=300000, episode_reward=786.80 +/- 357.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=301000, episode_reward=1081.20 +/- 284.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=302000, episode_reward=379.40 +/- 342.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=303000, episode_reward=438.20 +/- 625.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=304000, episode_reward=655.00 +/- 361.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=305000, episode_reward=1007.80 +/- 316.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=306000, episode_reward=510.60 +/- 106.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=307000, episode_reward=636.20 +/- 402.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=308000, episode_reward=620.20 +/- 298.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=309000, episode_reward=732.20 +/- 391.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=310000, episode_reward=1224.20 +/- 302.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=311000, episode_reward=805.20 +/- 307.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=312000, episode_reward=1192.20 +/- 210.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=313000, episode_reward=743.40 +/- 292.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=314000, episode_reward=986.00 +/- 261.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=315000, episode_reward=863.40 +/- 255.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=316000, episode_reward=800.00 +/- 236.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=317000, episode_reward=533.20 +/- 716.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=318000, episode_reward=769.20 +/- 133.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=319000, episode_reward=947.20 +/- 163.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=320000, episode_reward=1052.00 +/- 214.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=321000, episode_reward=1044.80 +/- 210.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=322000, episode_reward=829.60 +/- 279.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=323000, episode_reward=940.60 +/- 163.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=324000, episode_reward=1104.20 +/- 318.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=325000, episode_reward=780.40 +/- 429.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=326000, episode_reward=509.20 +/- 399.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=327000, episode_reward=305.80 +/- 782.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=328000, episode_reward=1065.00 +/- 215.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=329000, episode_reward=698.00 +/- 234.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=330000, episode_reward=823.80 +/- 308.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=331000, episode_reward=651.00 +/- 210.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=332000, episode_reward=718.20 +/- 285.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=333000, episode_reward=792.40 +/- 458.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=334000, episode_reward=546.40 +/- 221.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=335000, episode_reward=887.40 +/- 199.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=336000, episode_reward=755.20 +/- 280.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=337000, episode_reward=755.40 +/- 249.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=338000, episode_reward=522.20 +/- 727.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=339000, episode_reward=1106.60 +/- 344.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=340000, episode_reward=900.20 +/- 366.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=341000, episode_reward=925.20 +/- 116.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=342000, episode_reward=-652.00 +/- 2686.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=343000, episode_reward=832.80 +/- 410.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=344000, episode_reward=1102.20 +/- 362.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=345000, episode_reward=842.80 +/- 216.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=346000, episode_reward=856.60 +/- 218.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=347000, episode_reward=1194.60 +/- 190.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=348000, episode_reward=854.60 +/- 137.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=349000, episode_reward=394.20 +/- 179.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=350000, episode_reward=811.20 +/- 332.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=351000, episode_reward=721.20 +/- 299.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=352000, episode_reward=1123.20 +/- 184.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=353000, episode_reward=661.60 +/- 92.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=354000, episode_reward=982.60 +/- 35.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=355000, episode_reward=933.60 +/- 250.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=356000, episode_reward=767.20 +/- 398.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=357000, episode_reward=968.20 +/- 137.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=358000, episode_reward=460.40 +/- 402.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=359000, episode_reward=1131.40 +/- 234.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=360000, episode_reward=963.00 +/- 291.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=361000, episode_reward=629.80 +/- 385.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=362000, episode_reward=729.80 +/- 153.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=363000, episode_reward=846.20 +/- 152.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=364000, episode_reward=1142.20 +/- 189.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=365000, episode_reward=766.00 +/- 277.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=366000, episode_reward=-1059.60 +/- 4018.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=367000, episode_reward=989.20 +/- 298.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=368000, episode_reward=911.60 +/- 142.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=369000, episode_reward=868.80 +/- 268.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=370000, episode_reward=987.80 +/- 339.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=371000, episode_reward=830.00 +/- 307.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=372000, episode_reward=587.20 +/- 408.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=373000, episode_reward=327.60 +/- 410.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=374000, episode_reward=792.00 +/- 278.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=375000, episode_reward=751.20 +/- 338.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=376000, episode_reward=980.00 +/- 240.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=377000, episode_reward=548.40 +/- 263.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=378000, episode_reward=965.20 +/- 148.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=379000, episode_reward=799.80 +/- 412.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=380000, episode_reward=778.40 +/- 666.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=381000, episode_reward=973.40 +/- 239.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=382000, episode_reward=737.60 +/- 124.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=383000, episode_reward=984.80 +/- 346.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=384000, episode_reward=613.00 +/- 114.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=385000, episode_reward=878.80 +/- 357.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=386000, episode_reward=828.20 +/- 168.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=387000, episode_reward=662.60 +/- 88.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=388000, episode_reward=1073.80 +/- 253.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=389000, episode_reward=731.40 +/- 378.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=390000, episode_reward=920.00 +/- 433.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=391000, episode_reward=871.00 +/- 282.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=392000, episode_reward=878.40 +/- 270.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=393000, episode_reward=1029.20 +/- 258.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=394000, episode_reward=808.40 +/- 254.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=395000, episode_reward=775.00 +/- 440.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=396000, episode_reward=755.80 +/- 384.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=397000, episode_reward=972.20 +/- 306.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=398000, episode_reward=815.00 +/- 326.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=399000, episode_reward=676.40 +/- 285.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=400000, episode_reward=700.40 +/- 321.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=401000, episode_reward=987.80 +/- 448.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=402000, episode_reward=655.20 +/- 616.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=403000, episode_reward=935.00 +/- 191.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=404000, episode_reward=-2272.00 +/- 6100.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=405000, episode_reward=714.60 +/- 414.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=406000, episode_reward=-1509.20 +/- 4634.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=407000, episode_reward=450.60 +/- 254.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=408000, episode_reward=839.80 +/- 171.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=409000, episode_reward=637.80 +/- 238.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=410000, episode_reward=948.60 +/- 187.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=411000, episode_reward=1014.20 +/- 193.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=412000, episode_reward=821.20 +/- 389.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=413000, episode_reward=706.40 +/- 566.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=414000, episode_reward=856.20 +/- 250.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=415000, episode_reward=893.20 +/- 252.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=416000, episode_reward=1115.00 +/- 258.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=417000, episode_reward=860.20 +/- 277.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=418000, episode_reward=537.00 +/- 299.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=419000, episode_reward=602.40 +/- 397.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=420000, episode_reward=786.40 +/- 381.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=421000, episode_reward=725.60 +/- 184.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=422000, episode_reward=909.00 +/- 170.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=423000, episode_reward=661.80 +/- 143.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=424000, episode_reward=1291.80 +/- 268.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=425000, episode_reward=1038.60 +/- 250.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=426000, episode_reward=794.40 +/- 306.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=427000, episode_reward=812.40 +/- 247.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=428000, episode_reward=1066.00 +/- 210.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=429000, episode_reward=704.00 +/- 269.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=430000, episode_reward=834.20 +/- 413.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=431000, episode_reward=825.00 +/- 112.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=432000, episode_reward=860.40 +/- 347.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=433000, episode_reward=1000.80 +/- 152.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=434000, episode_reward=735.00 +/- 458.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=435000, episode_reward=1095.80 +/- 282.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=436000, episode_reward=716.80 +/- 410.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=437000, episode_reward=721.20 +/- 206.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=438000, episode_reward=765.20 +/- 491.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=439000, episode_reward=995.40 +/- 181.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=440000, episode_reward=1090.20 +/- 304.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=441000, episode_reward=878.40 +/- 202.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=442000, episode_reward=796.20 +/- 286.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=443000, episode_reward=970.40 +/- 228.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=444000, episode_reward=822.60 +/- 170.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=445000, episode_reward=873.60 +/- 288.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=446000, episode_reward=821.80 +/- 210.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=447000, episode_reward=911.80 +/- 265.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=448000, episode_reward=799.00 +/- 147.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=449000, episode_reward=711.20 +/- 312.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=450000, episode_reward=902.20 +/- 346.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=451000, episode_reward=688.60 +/- 124.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=452000, episode_reward=951.20 +/- 221.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=453000, episode_reward=767.60 +/- 479.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=454000, episode_reward=770.20 +/- 228.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=455000, episode_reward=560.40 +/- 257.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=456000, episode_reward=794.80 +/- 234.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=457000, episode_reward=668.00 +/- 458.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=458000, episode_reward=897.20 +/- 124.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=459000, episode_reward=690.60 +/- 273.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=460000, episode_reward=591.40 +/- 406.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=461000, episode_reward=802.20 +/- 231.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=462000, episode_reward=818.20 +/- 553.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=463000, episode_reward=1104.80 +/- 209.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=464000, episode_reward=792.20 +/- 225.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=465000, episode_reward=1089.40 +/- 219.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=466000, episode_reward=807.00 +/- 116.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=467000, episode_reward=686.40 +/- 234.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=468000, episode_reward=827.80 +/- 539.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=469000, episode_reward=582.60 +/- 185.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=470000, episode_reward=702.80 +/- 369.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=471000, episode_reward=718.00 +/- 269.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=472000, episode_reward=557.40 +/- 475.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=473000, episode_reward=816.20 +/- 237.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=474000, episode_reward=681.20 +/- 390.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=475000, episode_reward=862.40 +/- 274.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=476000, episode_reward=749.60 +/- 220.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=477000, episode_reward=778.00 +/- 288.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=478000, episode_reward=837.00 +/- 249.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=479000, episode_reward=958.40 +/- 212.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=480000, episode_reward=983.80 +/- 140.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=481000, episode_reward=773.40 +/- 318.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=482000, episode_reward=551.60 +/- 291.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=483000, episode_reward=648.40 +/- 175.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=484000, episode_reward=548.60 +/- 341.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=485000, episode_reward=790.00 +/- 227.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=486000, episode_reward=612.80 +/- 173.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=487000, episode_reward=545.40 +/- 531.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=488000, episode_reward=605.20 +/- 310.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=489000, episode_reward=1009.00 +/- 216.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=490000, episode_reward=653.00 +/- 265.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=491000, episode_reward=566.00 +/- 478.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=492000, episode_reward=727.40 +/- 293.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=493000, episode_reward=755.20 +/- 384.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=494000, episode_reward=906.60 +/- 331.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=495000, episode_reward=1129.40 +/- 306.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=496000, episode_reward=758.40 +/- 374.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=497000, episode_reward=254.80 +/- 718.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=498000, episode_reward=-688.20 +/- 3099.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=499000, episode_reward=914.80 +/- 76.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=500000, episode_reward=693.00 +/- 156.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Gamma = 0.99, Learning Rate = 0.0003, Activation Function = Tanh, Neurons = 32, Process noise = 0.1, Measurement noise = 0.0\n",
      "Eval num_timesteps=1000, episode_reward=-1501.00 +/- 77.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=-1482.00 +/- 53.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3000, episode_reward=-1468.00 +/- 52.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4000, episode_reward=-1449.00 +/- 80.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=5000, episode_reward=-1495.00 +/- 75.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=6000, episode_reward=-1474.00 +/- 54.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=7000, episode_reward=-1495.00 +/- 52.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=8000, episode_reward=-1507.00 +/- 29.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=9000, episode_reward=-1499.00 +/- 53.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=10000, episode_reward=-1502.00 +/- 105.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=11000, episode_reward=-1456.00 +/- 31.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=12000, episode_reward=-1553.00 +/- 114.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=13000, episode_reward=-1475.00 +/- 83.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=14000, episode_reward=-1555.00 +/- 68.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=15000, episode_reward=-1435.00 +/- 55.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=16000, episode_reward=-1500.00 +/- 85.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=17000, episode_reward=-1524.00 +/- 41.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=18000, episode_reward=-1447.00 +/- 26.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=19000, episode_reward=-1496.00 +/- 71.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=20000, episode_reward=-1516.00 +/- 89.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=21000, episode_reward=-1438.00 +/- 98.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=22000, episode_reward=-1472.00 +/- 75.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=23000, episode_reward=-1548.00 +/- 46.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=24000, episode_reward=-1498.00 +/- 81.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=25000, episode_reward=-1456.00 +/- 45.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=26000, episode_reward=-1438.00 +/- 50.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=27000, episode_reward=-1517.00 +/- 23.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=28000, episode_reward=-1520.00 +/- 118.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=29000, episode_reward=-1457.00 +/- 117.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=30000, episode_reward=-1473.00 +/- 102.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=31000, episode_reward=-1486.00 +/- 43.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=32000, episode_reward=-1498.00 +/- 24.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=33000, episode_reward=-1420.00 +/- 49.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=34000, episode_reward=-1505.00 +/- 60.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=35000, episode_reward=-1539.00 +/- 83.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=36000, episode_reward=-1549.00 +/- 82.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=37000, episode_reward=-1489.00 +/- 57.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=38000, episode_reward=-1493.00 +/- 95.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=39000, episode_reward=-1467.00 +/- 63.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=40000, episode_reward=-1502.00 +/- 111.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=41000, episode_reward=-1474.00 +/- 85.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=42000, episode_reward=-1550.00 +/- 49.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=43000, episode_reward=-1503.00 +/- 48.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=44000, episode_reward=-1506.00 +/- 72.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=45000, episode_reward=-1491.00 +/- 68.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=46000, episode_reward=-1562.00 +/- 58.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=47000, episode_reward=-1489.00 +/- 20.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=48000, episode_reward=-1510.00 +/- 60.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=49000, episode_reward=-1523.00 +/- 93.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=50000, episode_reward=-1552.00 +/- 86.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=51000, episode_reward=-116.00 +/- 559.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=52000, episode_reward=-108.40 +/- 461.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=53000, episode_reward=-261.80 +/- 275.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=54000, episode_reward=82.80 +/- 155.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=55000, episode_reward=-7.40 +/- 309.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=56000, episode_reward=23.40 +/- 178.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=57000, episode_reward=47.60 +/- 132.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=58000, episode_reward=83.00 +/- 301.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=59000, episode_reward=183.20 +/- 149.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=60000, episode_reward=335.60 +/- 185.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=61000, episode_reward=-51.00 +/- 156.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=62000, episode_reward=-161.60 +/- 202.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=63000, episode_reward=11.60 +/- 216.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=64000, episode_reward=353.20 +/- 189.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=65000, episode_reward=200.40 +/- 275.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=66000, episode_reward=18.20 +/- 285.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=67000, episode_reward=-36.60 +/- 329.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=68000, episode_reward=248.20 +/- 102.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=69000, episode_reward=1.00 +/- 132.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=70000, episode_reward=-97.20 +/- 177.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=71000, episode_reward=158.00 +/- 224.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=72000, episode_reward=-25.80 +/- 336.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=73000, episode_reward=380.20 +/- 379.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=74000, episode_reward=33.60 +/- 277.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=75000, episode_reward=-49.40 +/- 161.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=76000, episode_reward=110.00 +/- 173.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=77000, episode_reward=-25.60 +/- 151.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=78000, episode_reward=258.00 +/- 245.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=79000, episode_reward=119.40 +/- 355.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=80000, episode_reward=52.00 +/- 142.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=81000, episode_reward=370.20 +/- 129.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=82000, episode_reward=8.20 +/- 239.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=83000, episode_reward=165.80 +/- 139.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=84000, episode_reward=179.80 +/- 238.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=85000, episode_reward=157.20 +/- 233.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=86000, episode_reward=130.20 +/- 229.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=87000, episode_reward=203.80 +/- 98.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=88000, episode_reward=276.00 +/- 126.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=89000, episode_reward=242.60 +/- 195.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=90000, episode_reward=127.80 +/- 102.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=91000, episode_reward=385.40 +/- 141.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=92000, episode_reward=35.20 +/- 274.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=93000, episode_reward=303.40 +/- 243.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=94000, episode_reward=172.40 +/- 246.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=95000, episode_reward=221.00 +/- 300.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=96000, episode_reward=115.60 +/- 492.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=97000, episode_reward=173.80 +/- 308.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=98000, episode_reward=304.00 +/- 313.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=99000, episode_reward=192.00 +/- 117.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=100000, episode_reward=263.00 +/- 188.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=101000, episode_reward=441.00 +/- 153.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=102000, episode_reward=153.40 +/- 223.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=103000, episode_reward=170.80 +/- 161.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=104000, episode_reward=272.00 +/- 365.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=105000, episode_reward=266.60 +/- 270.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=106000, episode_reward=541.40 +/- 172.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=107000, episode_reward=346.40 +/- 349.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=108000, episode_reward=435.00 +/- 115.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=109000, episode_reward=545.00 +/- 282.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=110000, episode_reward=326.00 +/- 211.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=111000, episode_reward=519.60 +/- 187.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=112000, episode_reward=647.40 +/- 316.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=113000, episode_reward=524.80 +/- 285.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=114000, episode_reward=658.80 +/- 143.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=115000, episode_reward=516.20 +/- 272.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=116000, episode_reward=738.00 +/- 333.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=117000, episode_reward=665.60 +/- 216.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=118000, episode_reward=593.00 +/- 339.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=119000, episode_reward=627.00 +/- 249.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=120000, episode_reward=748.40 +/- 339.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=121000, episode_reward=619.60 +/- 328.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=122000, episode_reward=798.20 +/- 211.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=123000, episode_reward=560.80 +/- 253.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=124000, episode_reward=655.60 +/- 159.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=125000, episode_reward=497.40 +/- 228.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=126000, episode_reward=974.60 +/- 90.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=127000, episode_reward=860.20 +/- 296.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=128000, episode_reward=602.40 +/- 116.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=129000, episode_reward=840.20 +/- 86.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=130000, episode_reward=688.60 +/- 307.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=131000, episode_reward=856.20 +/- 253.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=132000, episode_reward=894.80 +/- 214.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=133000, episode_reward=1115.40 +/- 243.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=134000, episode_reward=982.80 +/- 235.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=135000, episode_reward=913.40 +/- 285.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=136000, episode_reward=933.80 +/- 185.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=137000, episode_reward=1043.80 +/- 301.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=138000, episode_reward=922.20 +/- 109.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=139000, episode_reward=924.60 +/- 243.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=140000, episode_reward=1032.20 +/- 428.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=141000, episode_reward=951.60 +/- 358.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=142000, episode_reward=667.00 +/- 83.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=143000, episode_reward=818.80 +/- 230.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=144000, episode_reward=904.00 +/- 97.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=145000, episode_reward=804.40 +/- 355.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=146000, episode_reward=937.00 +/- 266.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=147000, episode_reward=950.20 +/- 245.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=148000, episode_reward=1046.00 +/- 252.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=149000, episode_reward=1080.20 +/- 157.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=150000, episode_reward=472.00 +/- 682.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=151000, episode_reward=992.60 +/- 181.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=152000, episode_reward=706.00 +/- 88.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=153000, episode_reward=1122.20 +/- 114.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=154000, episode_reward=1018.00 +/- 316.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=155000, episode_reward=1211.40 +/- 291.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=156000, episode_reward=939.80 +/- 222.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=157000, episode_reward=1069.00 +/- 124.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=158000, episode_reward=926.00 +/- 332.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=159000, episode_reward=999.20 +/- 174.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=160000, episode_reward=1026.80 +/- 83.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=161000, episode_reward=1036.60 +/- 170.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=162000, episode_reward=1016.20 +/- 325.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=163000, episode_reward=948.40 +/- 106.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=164000, episode_reward=963.20 +/- 290.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=165000, episode_reward=960.40 +/- 260.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=166000, episode_reward=865.00 +/- 326.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=167000, episode_reward=981.80 +/- 123.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=168000, episode_reward=793.60 +/- 301.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=169000, episode_reward=951.60 +/- 262.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=170000, episode_reward=1000.80 +/- 58.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=171000, episode_reward=1017.00 +/- 111.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=172000, episode_reward=1152.00 +/- 265.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=173000, episode_reward=727.20 +/- 288.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=174000, episode_reward=777.20 +/- 188.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=175000, episode_reward=689.00 +/- 248.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=176000, episode_reward=125.40 +/- 1236.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=177000, episode_reward=889.80 +/- 272.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=178000, episode_reward=1033.60 +/- 135.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=179000, episode_reward=763.60 +/- 210.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=180000, episode_reward=917.20 +/- 196.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=181000, episode_reward=682.40 +/- 107.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=182000, episode_reward=972.80 +/- 132.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=183000, episode_reward=843.00 +/- 296.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=184000, episode_reward=897.60 +/- 192.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=185000, episode_reward=1071.40 +/- 165.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=186000, episode_reward=1086.80 +/- 134.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=187000, episode_reward=754.40 +/- 457.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=188000, episode_reward=645.60 +/- 353.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=189000, episode_reward=917.40 +/- 338.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=190000, episode_reward=1211.80 +/- 301.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=191000, episode_reward=954.20 +/- 223.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=192000, episode_reward=-1203.60 +/- 3734.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=193000, episode_reward=907.20 +/- 237.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=194000, episode_reward=712.00 +/- 487.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=195000, episode_reward=747.40 +/- 198.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=196000, episode_reward=959.60 +/- 446.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=197000, episode_reward=952.40 +/- 243.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=198000, episode_reward=1051.60 +/- 303.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=199000, episode_reward=91.60 +/- 1499.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=200000, episode_reward=983.40 +/- 145.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=201000, episode_reward=679.80 +/- 349.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=202000, episode_reward=844.00 +/- 138.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=203000, episode_reward=-9.40 +/- 1592.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=204000, episode_reward=546.80 +/- 212.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=205000, episode_reward=875.00 +/- 181.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=206000, episode_reward=792.20 +/- 223.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=207000, episode_reward=1009.00 +/- 149.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=208000, episode_reward=872.00 +/- 354.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=209000, episode_reward=990.40 +/- 145.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=210000, episode_reward=768.20 +/- 341.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=211000, episode_reward=687.40 +/- 157.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=212000, episode_reward=442.60 +/- 177.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=213000, episode_reward=417.20 +/- 386.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=214000, episode_reward=497.00 +/- 356.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=215000, episode_reward=909.80 +/- 163.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=216000, episode_reward=941.20 +/- 92.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=217000, episode_reward=843.60 +/- 113.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=218000, episode_reward=840.20 +/- 250.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=219000, episode_reward=742.00 +/- 357.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=220000, episode_reward=998.80 +/- 278.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=221000, episode_reward=418.20 +/- 501.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=222000, episode_reward=672.80 +/- 231.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=223000, episode_reward=588.20 +/- 236.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=224000, episode_reward=506.60 +/- 394.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=225000, episode_reward=430.20 +/- 263.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=226000, episode_reward=824.60 +/- 185.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=227000, episode_reward=718.80 +/- 341.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=228000, episode_reward=1008.20 +/- 232.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=229000, episode_reward=775.40 +/- 243.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=230000, episode_reward=907.40 +/- 103.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=231000, episode_reward=750.80 +/- 195.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=232000, episode_reward=252.00 +/- 306.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=233000, episode_reward=777.20 +/- 261.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=234000, episode_reward=798.40 +/- 167.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=235000, episode_reward=771.20 +/- 315.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=236000, episode_reward=810.60 +/- 208.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=237000, episode_reward=681.40 +/- 381.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=238000, episode_reward=861.20 +/- 389.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=239000, episode_reward=946.60 +/- 301.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=240000, episode_reward=699.00 +/- 522.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=241000, episode_reward=801.20 +/- 523.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=242000, episode_reward=786.20 +/- 133.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=243000, episode_reward=596.00 +/- 252.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=244000, episode_reward=932.00 +/- 145.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=245000, episode_reward=789.00 +/- 268.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=246000, episode_reward=853.20 +/- 188.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=247000, episode_reward=883.40 +/- 583.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=248000, episode_reward=658.00 +/- 186.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=249000, episode_reward=548.20 +/- 399.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=250000, episode_reward=554.40 +/- 331.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=251000, episode_reward=530.20 +/- 332.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=252000, episode_reward=499.20 +/- 238.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=253000, episode_reward=660.20 +/- 270.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=254000, episode_reward=262.40 +/- 108.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=255000, episode_reward=638.20 +/- 238.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=256000, episode_reward=800.20 +/- 287.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=257000, episode_reward=811.40 +/- 234.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=258000, episode_reward=818.20 +/- 318.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=259000, episode_reward=802.40 +/- 248.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=260000, episode_reward=881.60 +/- 213.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=261000, episode_reward=586.80 +/- 506.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=262000, episode_reward=872.00 +/- 251.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=263000, episode_reward=621.00 +/- 249.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=264000, episode_reward=702.40 +/- 191.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=265000, episode_reward=922.40 +/- 142.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=266000, episode_reward=740.00 +/- 222.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=267000, episode_reward=856.60 +/- 363.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=268000, episode_reward=696.40 +/- 284.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=269000, episode_reward=898.00 +/- 145.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=270000, episode_reward=744.80 +/- 293.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=271000, episode_reward=876.00 +/- 203.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=272000, episode_reward=873.40 +/- 61.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=273000, episode_reward=662.40 +/- 141.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=274000, episode_reward=667.40 +/- 369.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=275000, episode_reward=777.40 +/- 172.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=276000, episode_reward=812.60 +/- 186.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=277000, episode_reward=813.00 +/- 198.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=278000, episode_reward=778.80 +/- 325.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=279000, episode_reward=891.60 +/- 365.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=280000, episode_reward=-2495.40 +/- 6625.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=281000, episode_reward=666.40 +/- 243.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=282000, episode_reward=-409.40 +/- 2242.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=283000, episode_reward=591.80 +/- 330.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=284000, episode_reward=-2004.20 +/- 5344.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=285000, episode_reward=505.20 +/- 194.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=286000, episode_reward=-1760.60 +/- 5298.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=287000, episode_reward=-2359.00 +/- 6504.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=288000, episode_reward=838.80 +/- 373.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=289000, episode_reward=-2278.40 +/- 6394.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=290000, episode_reward=840.20 +/- 86.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=291000, episode_reward=601.80 +/- 198.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=292000, episode_reward=400.00 +/- 432.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=293000, episode_reward=443.40 +/- 298.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=294000, episode_reward=-1725.00 +/- 5003.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=295000, episode_reward=-2600.60 +/- 6357.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=296000, episode_reward=802.80 +/- 199.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=297000, episode_reward=-972.20 +/- 3228.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=298000, episode_reward=750.60 +/- 192.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=299000, episode_reward=919.00 +/- 259.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=300000, episode_reward=796.80 +/- 421.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=301000, episode_reward=589.00 +/- 295.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=302000, episode_reward=859.60 +/- 428.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=303000, episode_reward=393.40 +/- 227.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=304000, episode_reward=595.80 +/- 220.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=305000, episode_reward=722.20 +/- 291.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=306000, episode_reward=675.80 +/- 260.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=307000, episode_reward=-1157.40 +/- 3465.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=308000, episode_reward=654.80 +/- 488.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=309000, episode_reward=874.40 +/- 271.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=310000, episode_reward=533.60 +/- 311.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=311000, episode_reward=627.00 +/- 182.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=312000, episode_reward=544.20 +/- 106.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=313000, episode_reward=668.00 +/- 90.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=314000, episode_reward=717.40 +/- 239.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=315000, episode_reward=431.20 +/- 160.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=316000, episode_reward=852.20 +/- 191.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=317000, episode_reward=829.40 +/- 380.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=318000, episode_reward=799.20 +/- 293.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=319000, episode_reward=748.60 +/- 195.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=320000, episode_reward=-385.60 +/- 2457.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=321000, episode_reward=-90.60 +/- 1436.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=322000, episode_reward=549.00 +/- 214.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=323000, episode_reward=704.60 +/- 433.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=324000, episode_reward=683.80 +/- 393.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=325000, episode_reward=645.80 +/- 251.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=326000, episode_reward=563.20 +/- 149.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=327000, episode_reward=800.00 +/- 163.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=328000, episode_reward=725.20 +/- 297.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=329000, episode_reward=893.00 +/- 224.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=330000, episode_reward=936.40 +/- 323.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=331000, episode_reward=531.00 +/- 326.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=332000, episode_reward=313.00 +/- 256.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=333000, episode_reward=790.00 +/- 472.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=334000, episode_reward=368.40 +/- 338.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=335000, episode_reward=785.60 +/- 269.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=336000, episode_reward=619.20 +/- 230.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=337000, episode_reward=632.40 +/- 388.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=338000, episode_reward=861.00 +/- 128.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=339000, episode_reward=838.40 +/- 97.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=340000, episode_reward=707.40 +/- 262.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=341000, episode_reward=721.00 +/- 255.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=342000, episode_reward=410.40 +/- 269.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=343000, episode_reward=651.80 +/- 121.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=344000, episode_reward=660.00 +/- 442.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=345000, episode_reward=887.40 +/- 218.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=346000, episode_reward=511.60 +/- 303.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=347000, episode_reward=777.80 +/- 274.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=348000, episode_reward=605.80 +/- 263.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=349000, episode_reward=633.40 +/- 420.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=350000, episode_reward=870.40 +/- 229.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=351000, episode_reward=365.80 +/- 317.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=352000, episode_reward=504.60 +/- 397.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=353000, episode_reward=656.00 +/- 278.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=354000, episode_reward=377.20 +/- 334.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=355000, episode_reward=494.60 +/- 224.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=356000, episode_reward=646.00 +/- 481.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=357000, episode_reward=586.40 +/- 366.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=358000, episode_reward=374.80 +/- 516.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=359000, episode_reward=849.20 +/- 86.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=360000, episode_reward=766.60 +/- 239.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=361000, episode_reward=600.00 +/- 365.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=362000, episode_reward=589.80 +/- 235.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=363000, episode_reward=523.80 +/- 233.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=364000, episode_reward=582.80 +/- 288.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=365000, episode_reward=617.00 +/- 250.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=366000, episode_reward=-318.60 +/- 1735.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=367000, episode_reward=578.00 +/- 261.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=368000, episode_reward=558.80 +/- 209.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=369000, episode_reward=604.80 +/- 329.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=370000, episode_reward=641.20 +/- 214.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=371000, episode_reward=1006.60 +/- 226.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=372000, episode_reward=776.00 +/- 304.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=373000, episode_reward=720.40 +/- 201.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=374000, episode_reward=994.00 +/- 173.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=375000, episode_reward=345.80 +/- 407.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=376000, episode_reward=773.80 +/- 322.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=377000, episode_reward=929.60 +/- 167.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=378000, episode_reward=158.00 +/- 428.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=379000, episode_reward=549.80 +/- 246.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=380000, episode_reward=833.80 +/- 258.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=381000, episode_reward=890.00 +/- 468.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=382000, episode_reward=503.00 +/- 432.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=383000, episode_reward=389.00 +/- 635.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=384000, episode_reward=423.20 +/- 115.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=385000, episode_reward=700.40 +/- 462.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=386000, episode_reward=657.60 +/- 247.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=387000, episode_reward=385.80 +/- 382.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=388000, episode_reward=674.80 +/- 349.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=389000, episode_reward=332.80 +/- 695.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=390000, episode_reward=683.80 +/- 234.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=391000, episode_reward=598.40 +/- 746.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=392000, episode_reward=613.60 +/- 393.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=393000, episode_reward=847.20 +/- 209.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=394000, episode_reward=619.40 +/- 230.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=395000, episode_reward=488.00 +/- 427.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=396000, episode_reward=609.00 +/- 267.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=397000, episode_reward=621.80 +/- 308.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=398000, episode_reward=672.40 +/- 339.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=399000, episode_reward=558.00 +/- 272.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=400000, episode_reward=672.00 +/- 453.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=401000, episode_reward=502.20 +/- 203.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=402000, episode_reward=447.80 +/- 224.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=403000, episode_reward=214.00 +/- 552.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=404000, episode_reward=471.40 +/- 229.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=405000, episode_reward=-175.80 +/- 886.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=406000, episode_reward=416.20 +/- 351.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=407000, episode_reward=810.00 +/- 223.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=408000, episode_reward=723.80 +/- 238.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=409000, episode_reward=594.40 +/- 370.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=410000, episode_reward=76.40 +/- 914.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=411000, episode_reward=392.80 +/- 520.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=412000, episode_reward=442.20 +/- 197.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=413000, episode_reward=618.00 +/- 235.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=414000, episode_reward=503.40 +/- 458.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=415000, episode_reward=475.60 +/- 432.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=416000, episode_reward=178.00 +/- 241.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=417000, episode_reward=534.40 +/- 518.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=418000, episode_reward=502.20 +/- 337.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=419000, episode_reward=664.00 +/- 371.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=420000, episode_reward=503.20 +/- 498.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=421000, episode_reward=336.60 +/- 667.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=422000, episode_reward=351.40 +/- 303.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=423000, episode_reward=81.80 +/- 464.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=424000, episode_reward=371.80 +/- 446.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=425000, episode_reward=247.20 +/- 705.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=426000, episode_reward=402.60 +/- 347.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=427000, episode_reward=344.40 +/- 494.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=428000, episode_reward=174.80 +/- 833.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=429000, episode_reward=531.00 +/- 244.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=430000, episode_reward=529.40 +/- 421.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=431000, episode_reward=471.00 +/- 348.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=432000, episode_reward=447.60 +/- 278.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=433000, episode_reward=597.00 +/- 216.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=434000, episode_reward=580.00 +/- 147.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=435000, episode_reward=685.40 +/- 245.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=436000, episode_reward=552.60 +/- 152.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=437000, episode_reward=889.20 +/- 178.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=438000, episode_reward=726.20 +/- 250.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=439000, episode_reward=839.80 +/- 277.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=440000, episode_reward=919.00 +/- 305.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=441000, episode_reward=690.40 +/- 172.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=442000, episode_reward=500.20 +/- 370.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=443000, episode_reward=870.20 +/- 360.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=444000, episode_reward=640.40 +/- 409.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=445000, episode_reward=591.00 +/- 206.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=446000, episode_reward=813.80 +/- 177.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=447000, episode_reward=519.60 +/- 415.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=448000, episode_reward=670.00 +/- 316.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=449000, episode_reward=444.20 +/- 302.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=450000, episode_reward=693.40 +/- 299.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=451000, episode_reward=538.80 +/- 426.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=452000, episode_reward=648.00 +/- 197.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=453000, episode_reward=-585.00 +/- 2572.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=454000, episode_reward=469.40 +/- 348.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=455000, episode_reward=626.00 +/- 327.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=456000, episode_reward=664.80 +/- 200.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=457000, episode_reward=752.20 +/- 259.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=458000, episode_reward=608.80 +/- 223.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=459000, episode_reward=848.40 +/- 217.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=460000, episode_reward=784.80 +/- 187.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=461000, episode_reward=768.20 +/- 433.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=462000, episode_reward=597.60 +/- 344.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=463000, episode_reward=463.20 +/- 342.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=464000, episode_reward=542.20 +/- 236.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=465000, episode_reward=432.40 +/- 379.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=466000, episode_reward=712.00 +/- 169.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=467000, episode_reward=-323.60 +/- 2093.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=468000, episode_reward=648.20 +/- 186.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=469000, episode_reward=263.80 +/- 895.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=470000, episode_reward=602.20 +/- 511.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=471000, episode_reward=780.40 +/- 306.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=472000, episode_reward=734.40 +/- 96.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=473000, episode_reward=433.80 +/- 122.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=474000, episode_reward=652.60 +/- 438.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=475000, episode_reward=597.00 +/- 202.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=476000, episode_reward=688.80 +/- 308.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=477000, episode_reward=521.20 +/- 299.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=478000, episode_reward=917.00 +/- 276.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=479000, episode_reward=467.60 +/- 305.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=480000, episode_reward=689.80 +/- 166.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=481000, episode_reward=397.40 +/- 119.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=482000, episode_reward=371.20 +/- 410.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=483000, episode_reward=512.80 +/- 260.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=484000, episode_reward=397.60 +/- 160.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=485000, episode_reward=531.00 +/- 211.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=486000, episode_reward=667.40 +/- 347.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=487000, episode_reward=434.00 +/- 224.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=488000, episode_reward=666.40 +/- 142.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=489000, episode_reward=574.80 +/- 242.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=490000, episode_reward=622.00 +/- 276.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=491000, episode_reward=323.80 +/- 192.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=492000, episode_reward=223.20 +/- 187.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=493000, episode_reward=562.60 +/- 299.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=494000, episode_reward=433.00 +/- 169.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=495000, episode_reward=489.20 +/- 227.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=496000, episode_reward=639.60 +/- 159.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=497000, episode_reward=684.80 +/- 83.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=498000, episode_reward=390.60 +/- 144.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=499000, episode_reward=964.20 +/- 198.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=500000, episode_reward=662.00 +/- 203.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Gamma = 0.99, Learning Rate = 0.0003, Activation Function = Tanh, Neurons = 64, Process noise = 0.1, Measurement noise = 0.0\n",
      "Eval num_timesteps=1000, episode_reward=-1718.60 +/- 945.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=-1782.20 +/- 754.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=3000, episode_reward=-1846.40 +/- 559.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=4000, episode_reward=-1418.60 +/- 590.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=5000, episode_reward=-2133.00 +/- 312.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=6000, episode_reward=-1787.40 +/- 574.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=7000, episode_reward=-1758.00 +/- 825.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=8000, episode_reward=-1939.80 +/- 402.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=9000, episode_reward=-2028.20 +/- 343.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=10000, episode_reward=-2046.80 +/- 462.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=11000, episode_reward=-2140.80 +/- 1020.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=12000, episode_reward=-2236.60 +/- 571.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=13000, episode_reward=-2316.20 +/- 532.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=14000, episode_reward=-2080.40 +/- 243.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=15000, episode_reward=-1894.20 +/- 430.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=16000, episode_reward=-2099.00 +/- 738.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=17000, episode_reward=-1396.80 +/- 335.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=18000, episode_reward=-1687.20 +/- 193.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=19000, episode_reward=-1783.80 +/- 820.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=20000, episode_reward=-1786.60 +/- 745.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=21000, episode_reward=-2225.40 +/- 777.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=22000, episode_reward=-1957.20 +/- 475.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=23000, episode_reward=-1872.40 +/- 333.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=24000, episode_reward=-2050.80 +/- 465.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=25000, episode_reward=-1587.00 +/- 294.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=26000, episode_reward=-2004.80 +/- 638.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=27000, episode_reward=-2103.60 +/- 580.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=28000, episode_reward=-2123.60 +/- 338.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=29000, episode_reward=-1753.20 +/- 290.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=30000, episode_reward=-1482.20 +/- 371.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=31000, episode_reward=-2105.60 +/- 533.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=32000, episode_reward=-1923.40 +/- 578.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=33000, episode_reward=-1921.40 +/- 522.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=34000, episode_reward=-1737.80 +/- 497.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=35000, episode_reward=-2075.40 +/- 490.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=36000, episode_reward=-2054.40 +/- 346.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=37000, episode_reward=-2402.20 +/- 649.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=38000, episode_reward=-2245.20 +/- 781.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=39000, episode_reward=-1683.80 +/- 415.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=40000, episode_reward=-2171.80 +/- 292.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=41000, episode_reward=-1515.40 +/- 471.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=42000, episode_reward=-2298.20 +/- 996.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=43000, episode_reward=-2385.80 +/- 805.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=44000, episode_reward=-2027.80 +/- 297.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=45000, episode_reward=-1934.60 +/- 179.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=46000, episode_reward=-1716.20 +/- 485.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=47000, episode_reward=-1953.20 +/- 544.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=48000, episode_reward=-2038.20 +/- 336.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=49000, episode_reward=-2121.60 +/- 433.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=50000, episode_reward=-2317.20 +/- 585.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=51000, episode_reward=-375.00 +/- 645.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=52000, episode_reward=170.40 +/- 337.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=53000, episode_reward=47.60 +/- 376.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=54000, episode_reward=216.40 +/- 217.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=55000, episode_reward=-3630.20 +/- 2487.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=56000, episode_reward=140.80 +/- 212.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=57000, episode_reward=88.00 +/- 358.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=58000, episode_reward=146.00 +/- 187.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=59000, episode_reward=-180.80 +/- 76.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=60000, episode_reward=47.00 +/- 154.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=61000, episode_reward=36.00 +/- 279.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=62000, episode_reward=-163.00 +/- 456.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=63000, episode_reward=181.00 +/- 298.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=64000, episode_reward=-64.40 +/- 380.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=65000, episode_reward=-88.40 +/- 172.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=66000, episode_reward=267.20 +/- 195.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=67000, episode_reward=250.60 +/- 293.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=68000, episode_reward=23.80 +/- 132.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=69000, episode_reward=-164.00 +/- 272.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=70000, episode_reward=283.20 +/- 115.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=71000, episode_reward=64.40 +/- 218.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=72000, episode_reward=212.80 +/- 187.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=73000, episode_reward=92.00 +/- 264.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=74000, episode_reward=99.80 +/- 59.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=75000, episode_reward=58.00 +/- 158.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=76000, episode_reward=59.60 +/- 97.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=77000, episode_reward=216.60 +/- 188.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=78000, episode_reward=-149.40 +/- 207.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=79000, episode_reward=247.80 +/- 131.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=80000, episode_reward=213.20 +/- 216.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=81000, episode_reward=184.60 +/- 302.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=82000, episode_reward=217.20 +/- 342.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=83000, episode_reward=108.40 +/- 142.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=84000, episode_reward=92.40 +/- 377.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=85000, episode_reward=220.80 +/- 183.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=86000, episode_reward=192.00 +/- 99.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=87000, episode_reward=240.20 +/- 130.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=88000, episode_reward=86.20 +/- 114.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=89000, episode_reward=212.60 +/- 168.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=90000, episode_reward=283.80 +/- 153.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=91000, episode_reward=337.40 +/- 247.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=92000, episode_reward=273.60 +/- 179.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=93000, episode_reward=243.00 +/- 240.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=94000, episode_reward=259.40 +/- 245.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=95000, episode_reward=357.80 +/- 150.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=96000, episode_reward=350.40 +/- 126.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=97000, episode_reward=320.00 +/- 149.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=98000, episode_reward=324.20 +/- 147.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=99000, episode_reward=156.80 +/- 273.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=100000, episode_reward=147.20 +/- 220.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=101000, episode_reward=198.80 +/- 196.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=102000, episode_reward=293.60 +/- 110.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=103000, episode_reward=150.00 +/- 140.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=104000, episode_reward=60.60 +/- 253.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=105000, episode_reward=262.00 +/- 229.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=106000, episode_reward=182.00 +/- 157.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=107000, episode_reward=331.60 +/- 121.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=108000, episode_reward=485.00 +/- 64.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=109000, episode_reward=376.80 +/- 87.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=110000, episode_reward=226.60 +/- 202.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=111000, episode_reward=285.20 +/- 147.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=112000, episode_reward=240.60 +/- 112.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=113000, episode_reward=238.00 +/- 168.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=114000, episode_reward=211.80 +/- 190.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=115000, episode_reward=213.60 +/- 249.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=116000, episode_reward=181.40 +/- 129.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=117000, episode_reward=593.40 +/- 227.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=118000, episode_reward=421.80 +/- 189.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=119000, episode_reward=609.60 +/- 80.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=120000, episode_reward=569.60 +/- 177.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=121000, episode_reward=539.80 +/- 443.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=122000, episode_reward=673.80 +/- 315.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=123000, episode_reward=600.40 +/- 241.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=124000, episode_reward=753.60 +/- 416.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=125000, episode_reward=929.00 +/- 85.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=126000, episode_reward=-2538.00 +/- 6801.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=127000, episode_reward=885.00 +/- 238.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=128000, episode_reward=661.00 +/- 307.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=129000, episode_reward=713.20 +/- 323.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=130000, episode_reward=1010.40 +/- 299.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=131000, episode_reward=1215.60 +/- 284.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=132000, episode_reward=1111.80 +/- 175.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=133000, episode_reward=-86.00 +/- 2484.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=134000, episode_reward=1073.00 +/- 278.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=135000, episode_reward=1118.00 +/- 393.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=136000, episode_reward=1053.40 +/- 129.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=137000, episode_reward=1062.00 +/- 246.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=138000, episode_reward=1095.60 +/- 111.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=139000, episode_reward=1161.20 +/- 169.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=140000, episode_reward=1212.20 +/- 100.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=141000, episode_reward=1183.00 +/- 271.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=142000, episode_reward=1005.80 +/- 261.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=143000, episode_reward=-2215.60 +/- 6492.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=144000, episode_reward=966.60 +/- 252.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=145000, episode_reward=759.40 +/- 261.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=146000, episode_reward=1077.20 +/- 89.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=147000, episode_reward=1167.60 +/- 138.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=148000, episode_reward=1070.20 +/- 196.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=149000, episode_reward=1197.00 +/- 169.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=150000, episode_reward=-497.60 +/- 3526.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=151000, episode_reward=1017.20 +/- 347.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=152000, episode_reward=718.00 +/- 844.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=153000, episode_reward=870.40 +/- 99.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=154000, episode_reward=1154.40 +/- 195.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=155000, episode_reward=1056.60 +/- 103.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=156000, episode_reward=943.80 +/- 273.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=157000, episode_reward=1211.60 +/- 206.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=158000, episode_reward=981.80 +/- 140.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=159000, episode_reward=1097.00 +/- 218.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=160000, episode_reward=1314.00 +/- 83.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=161000, episode_reward=1234.20 +/- 340.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=162000, episode_reward=-2291.40 +/- 6235.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=163000, episode_reward=979.40 +/- 303.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=164000, episode_reward=1039.80 +/- 66.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=165000, episode_reward=974.20 +/- 226.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=166000, episode_reward=1178.00 +/- 215.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=167000, episode_reward=1058.00 +/- 141.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=168000, episode_reward=1044.60 +/- 89.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=169000, episode_reward=1090.20 +/- 141.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=170000, episode_reward=1066.80 +/- 253.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=171000, episode_reward=961.00 +/- 288.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=172000, episode_reward=995.20 +/- 161.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=173000, episode_reward=1126.40 +/- 28.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=174000, episode_reward=-1674.60 +/- 5595.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=175000, episode_reward=1147.20 +/- 303.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=176000, episode_reward=1050.80 +/- 286.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=177000, episode_reward=1008.60 +/- 289.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=178000, episode_reward=991.00 +/- 88.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=179000, episode_reward=1152.80 +/- 250.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=180000, episode_reward=-1279.40 +/- 4563.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=181000, episode_reward=1073.60 +/- 373.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=182000, episode_reward=981.60 +/- 351.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=183000, episode_reward=1036.60 +/- 298.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=184000, episode_reward=-2162.80 +/- 6944.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=185000, episode_reward=1189.40 +/- 180.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=186000, episode_reward=1052.20 +/- 222.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=187000, episode_reward=1239.00 +/- 74.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=188000, episode_reward=1095.60 +/- 256.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=189000, episode_reward=-2008.60 +/- 6533.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=190000, episode_reward=1032.00 +/- 232.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=191000, episode_reward=976.60 +/- 328.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=192000, episode_reward=947.60 +/- 261.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=193000, episode_reward=943.60 +/- 261.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=194000, episode_reward=1058.40 +/- 98.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=195000, episode_reward=-180.40 +/- 2405.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=196000, episode_reward=1069.80 +/- 311.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=197000, episode_reward=-2218.00 +/- 6616.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=198000, episode_reward=949.60 +/- 163.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=199000, episode_reward=1151.60 +/- 199.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=200000, episode_reward=1036.60 +/- 205.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=201000, episode_reward=-2117.40 +/- 6294.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=202000, episode_reward=979.60 +/- 252.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=203000, episode_reward=1079.20 +/- 167.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=204000, episode_reward=-1013.20 +/- 4168.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=205000, episode_reward=-2546.20 +/- 7033.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=206000, episode_reward=1234.20 +/- 215.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=207000, episode_reward=-2249.20 +/- 6728.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=208000, episode_reward=-1816.40 +/- 6056.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=209000, episode_reward=38.40 +/- 2098.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=210000, episode_reward=-252.60 +/- 2472.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=211000, episode_reward=160.60 +/- 1688.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=212000, episode_reward=-2540.00 +/- 6456.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=213000, episode_reward=652.20 +/- 914.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=214000, episode_reward=-430.40 +/- 3306.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=215000, episode_reward=888.20 +/- 183.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=216000, episode_reward=920.00 +/- 264.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=217000, episode_reward=931.40 +/- 239.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=218000, episode_reward=-2169.60 +/- 7064.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=219000, episode_reward=1126.20 +/- 192.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=220000, episode_reward=1085.40 +/- 327.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=221000, episode_reward=1111.80 +/- 328.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=222000, episode_reward=1077.60 +/- 351.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=223000, episode_reward=813.20 +/- 381.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=224000, episode_reward=1071.40 +/- 190.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=225000, episode_reward=-619.20 +/- 3071.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=226000, episode_reward=1258.00 +/- 170.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=227000, episode_reward=887.40 +/- 55.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=228000, episode_reward=-769.40 +/- 3726.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=229000, episode_reward=1025.60 +/- 138.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=230000, episode_reward=-857.00 +/- 3776.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=231000, episode_reward=936.80 +/- 154.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=232000, episode_reward=1274.00 +/- 225.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=233000, episode_reward=1032.00 +/- 88.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=234000, episode_reward=1224.00 +/- 122.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=235000, episode_reward=976.20 +/- 230.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=236000, episode_reward=1237.20 +/- 135.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=237000, episode_reward=1124.60 +/- 296.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=238000, episode_reward=-1519.80 +/- 5235.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=239000, episode_reward=-871.00 +/- 3895.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=240000, episode_reward=1151.00 +/- 238.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=241000, episode_reward=963.80 +/- 223.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=242000, episode_reward=1116.80 +/- 132.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=243000, episode_reward=-571.20 +/- 3225.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=244000, episode_reward=951.00 +/- 368.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=245000, episode_reward=1085.20 +/- 282.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=246000, episode_reward=-1023.60 +/- 4552.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=247000, episode_reward=1165.20 +/- 267.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=248000, episode_reward=1019.00 +/- 280.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=249000, episode_reward=1172.20 +/- 135.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=250000, episode_reward=1099.40 +/- 163.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=251000, episode_reward=1087.60 +/- 276.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=252000, episode_reward=910.60 +/- 195.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=253000, episode_reward=991.80 +/- 335.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=254000, episode_reward=1044.20 +/- 178.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=255000, episode_reward=1116.80 +/- 227.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=256000, episode_reward=918.80 +/- 327.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=257000, episode_reward=1165.80 +/- 92.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=258000, episode_reward=1028.20 +/- 231.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=259000, episode_reward=1113.80 +/- 146.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=260000, episode_reward=142.60 +/- 2052.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=261000, episode_reward=973.20 +/- 120.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=262000, episode_reward=957.20 +/- 335.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=263000, episode_reward=1371.40 +/- 175.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=264000, episode_reward=-2252.20 +/- 6614.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=265000, episode_reward=940.60 +/- 199.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=266000, episode_reward=1041.00 +/- 352.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=267000, episode_reward=1173.20 +/- 208.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=268000, episode_reward=1045.00 +/- 226.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=269000, episode_reward=1240.60 +/- 225.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=270000, episode_reward=-1702.60 +/- 5782.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=271000, episode_reward=1203.00 +/- 208.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=272000, episode_reward=1270.60 +/- 147.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=273000, episode_reward=1153.80 +/- 206.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=274000, episode_reward=1206.60 +/- 212.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=275000, episode_reward=1021.60 +/- 366.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=276000, episode_reward=1001.00 +/- 339.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=277000, episode_reward=1081.80 +/- 258.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=278000, episode_reward=1143.80 +/- 180.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=279000, episode_reward=-1915.40 +/- 6697.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=280000, episode_reward=-1475.80 +/- 4951.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=281000, episode_reward=965.20 +/- 282.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=282000, episode_reward=1095.00 +/- 313.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=283000, episode_reward=1132.20 +/- 282.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=284000, episode_reward=1068.60 +/- 160.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=285000, episode_reward=944.20 +/- 38.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=286000, episode_reward=846.40 +/- 173.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=287000, episode_reward=1149.00 +/- 214.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=288000, episode_reward=1130.20 +/- 142.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=289000, episode_reward=1027.60 +/- 254.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=290000, episode_reward=1149.20 +/- 243.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=291000, episode_reward=-1219.40 +/- 4711.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=292000, episode_reward=1167.00 +/- 234.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=293000, episode_reward=1130.20 +/- 287.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=294000, episode_reward=1156.60 +/- 190.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=295000, episode_reward=1159.40 +/- 213.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=296000, episode_reward=970.80 +/- 222.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=297000, episode_reward=1050.20 +/- 267.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=298000, episode_reward=1181.80 +/- 252.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=299000, episode_reward=-1826.40 +/- 6475.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=300000, episode_reward=984.40 +/- 287.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=301000, episode_reward=1169.40 +/- 107.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=302000, episode_reward=1190.80 +/- 185.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=303000, episode_reward=1230.80 +/- 316.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=304000, episode_reward=-843.20 +/- 4129.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=305000, episode_reward=1142.80 +/- 139.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=306000, episode_reward=963.80 +/- 220.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=307000, episode_reward=-348.00 +/- 3043.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=308000, episode_reward=-1968.80 +/- 6134.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=309000, episode_reward=1293.40 +/- 139.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=310000, episode_reward=-332.00 +/- 2729.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=311000, episode_reward=-497.00 +/- 3028.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=312000, episode_reward=903.40 +/- 186.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=313000, episode_reward=-5289.60 +/- 8147.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=314000, episode_reward=1003.00 +/- 147.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=315000, episode_reward=-535.40 +/- 3124.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=316000, episode_reward=-377.80 +/- 2749.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=317000, episode_reward=1040.60 +/- 123.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=318000, episode_reward=1139.40 +/- 181.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=319000, episode_reward=1223.40 +/- 252.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=320000, episode_reward=1132.20 +/- 176.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=321000, episode_reward=1014.80 +/- 278.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=322000, episode_reward=1139.80 +/- 214.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=323000, episode_reward=939.00 +/- 157.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=324000, episode_reward=1152.80 +/- 202.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=325000, episode_reward=1119.20 +/- 217.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=326000, episode_reward=1110.40 +/- 92.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=327000, episode_reward=-858.60 +/- 3861.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=328000, episode_reward=1108.20 +/- 97.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=329000, episode_reward=1271.60 +/- 85.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=330000, episode_reward=1026.00 +/- 227.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=331000, episode_reward=1000.80 +/- 186.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=332000, episode_reward=810.20 +/- 458.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=333000, episode_reward=-583.00 +/- 3118.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=334000, episode_reward=-1226.60 +/- 4524.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=335000, episode_reward=1067.80 +/- 119.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=336000, episode_reward=1251.40 +/- 136.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=337000, episode_reward=1248.20 +/- 146.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=338000, episode_reward=945.00 +/- 157.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=339000, episode_reward=1011.60 +/- 201.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=340000, episode_reward=1128.60 +/- 244.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=341000, episode_reward=1028.20 +/- 436.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=342000, episode_reward=1233.60 +/- 316.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=343000, episode_reward=937.40 +/- 177.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=344000, episode_reward=1140.20 +/- 298.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=345000, episode_reward=1179.80 +/- 85.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=346000, episode_reward=932.20 +/- 153.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=347000, episode_reward=1106.40 +/- 192.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=348000, episode_reward=1098.20 +/- 206.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=349000, episode_reward=-1341.20 +/- 2820.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=350000, episode_reward=1096.60 +/- 226.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=351000, episode_reward=983.60 +/- 227.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=352000, episode_reward=797.00 +/- 309.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=353000, episode_reward=935.40 +/- 298.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=354000, episode_reward=684.40 +/- 179.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=355000, episode_reward=1142.80 +/- 367.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=356000, episode_reward=1071.00 +/- 244.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=357000, episode_reward=1069.60 +/- 202.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=358000, episode_reward=1136.20 +/- 418.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=359000, episode_reward=1028.20 +/- 165.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=360000, episode_reward=1145.80 +/- 69.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=361000, episode_reward=881.40 +/- 106.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=362000, episode_reward=1093.80 +/- 284.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=363000, episode_reward=-208.00 +/- 2701.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=364000, episode_reward=159.00 +/- 1502.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=365000, episode_reward=-2547.20 +/- 5394.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=366000, episode_reward=1066.40 +/- 149.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=367000, episode_reward=786.40 +/- 425.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=368000, episode_reward=967.40 +/- 116.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=369000, episode_reward=-1074.00 +/- 4622.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=370000, episode_reward=-3845.80 +/- 6165.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=371000, episode_reward=1084.20 +/- 382.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=372000, episode_reward=890.80 +/- 419.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=373000, episode_reward=986.00 +/- 353.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=374000, episode_reward=1232.40 +/- 161.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=375000, episode_reward=-186.00 +/- 2188.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=376000, episode_reward=190.00 +/- 1716.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=377000, episode_reward=1160.00 +/- 58.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=378000, episode_reward=900.80 +/- 209.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=379000, episode_reward=963.80 +/- 248.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=380000, episode_reward=1125.20 +/- 116.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=381000, episode_reward=1134.00 +/- 240.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=382000, episode_reward=-1931.40 +/- 5861.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=383000, episode_reward=992.80 +/- 191.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=384000, episode_reward=1016.40 +/- 211.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=385000, episode_reward=1161.80 +/- 197.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=386000, episode_reward=1244.40 +/- 159.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=387000, episode_reward=1046.20 +/- 114.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=388000, episode_reward=1124.20 +/- 295.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=389000, episode_reward=1169.60 +/- 170.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=390000, episode_reward=1007.80 +/- 409.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=391000, episode_reward=1106.20 +/- 159.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=392000, episode_reward=1052.20 +/- 497.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=393000, episode_reward=1225.20 +/- 294.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=394000, episode_reward=1128.20 +/- 87.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=395000, episode_reward=923.00 +/- 255.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=396000, episode_reward=1257.80 +/- 133.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=397000, episode_reward=925.00 +/- 323.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=398000, episode_reward=944.40 +/- 144.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=399000, episode_reward=1062.60 +/- 195.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=400000, episode_reward=1226.80 +/- 190.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=401000, episode_reward=1016.40 +/- 196.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=402000, episode_reward=1099.20 +/- 290.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=403000, episode_reward=975.80 +/- 545.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=404000, episode_reward=903.60 +/- 383.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=405000, episode_reward=862.20 +/- 341.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=406000, episode_reward=1141.20 +/- 334.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=407000, episode_reward=1157.00 +/- 336.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=408000, episode_reward=858.00 +/- 413.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=409000, episode_reward=951.20 +/- 278.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=410000, episode_reward=-2488.80 +/- 6749.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=411000, episode_reward=766.20 +/- 381.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=412000, episode_reward=995.00 +/- 208.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=413000, episode_reward=1066.60 +/- 251.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=414000, episode_reward=944.00 +/- 159.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=415000, episode_reward=571.60 +/- 710.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=416000, episode_reward=1032.80 +/- 277.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=417000, episode_reward=1063.20 +/- 83.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=418000, episode_reward=1135.00 +/- 124.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=419000, episode_reward=1126.40 +/- 176.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=420000, episode_reward=1159.80 +/- 216.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=421000, episode_reward=875.40 +/- 456.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=422000, episode_reward=749.40 +/- 1373.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=423000, episode_reward=509.60 +/- 1242.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=424000, episode_reward=1037.60 +/- 102.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=425000, episode_reward=969.00 +/- 244.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=426000, episode_reward=1152.20 +/- 165.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=427000, episode_reward=549.60 +/- 841.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=428000, episode_reward=866.00 +/- 335.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=429000, episode_reward=990.80 +/- 348.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=430000, episode_reward=1183.00 +/- 241.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=431000, episode_reward=753.40 +/- 226.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=432000, episode_reward=792.00 +/- 381.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=433000, episode_reward=1032.20 +/- 238.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=434000, episode_reward=1149.00 +/- 151.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=435000, episode_reward=873.40 +/- 74.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=436000, episode_reward=1081.40 +/- 202.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=437000, episode_reward=1106.40 +/- 137.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=438000, episode_reward=1035.00 +/- 171.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=439000, episode_reward=-1281.60 +/- 4762.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=440000, episode_reward=-1153.60 +/- 4633.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=441000, episode_reward=1129.80 +/- 292.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=442000, episode_reward=923.20 +/- 448.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=443000, episode_reward=1170.40 +/- 290.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=444000, episode_reward=-2497.60 +/- 6611.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=445000, episode_reward=-1565.60 +/- 5391.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=446000, episode_reward=1098.00 +/- 313.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=447000, episode_reward=-2232.80 +/- 6824.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=448000, episode_reward=937.00 +/- 110.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=449000, episode_reward=1111.40 +/- 253.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=450000, episode_reward=393.60 +/- 696.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=451000, episode_reward=998.60 +/- 403.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=452000, episode_reward=902.40 +/- 217.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=453000, episode_reward=-2311.60 +/- 6512.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=454000, episode_reward=1218.40 +/- 206.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=455000, episode_reward=-362.40 +/- 2750.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=456000, episode_reward=1134.80 +/- 190.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=457000, episode_reward=1020.40 +/- 223.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=458000, episode_reward=1347.20 +/- 175.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=459000, episode_reward=985.80 +/- 331.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=460000, episode_reward=-315.40 +/- 2923.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=461000, episode_reward=732.40 +/- 243.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=462000, episode_reward=897.40 +/- 294.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=463000, episode_reward=-530.20 +/- 3149.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=464000, episode_reward=879.60 +/- 200.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=465000, episode_reward=770.20 +/- 260.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=466000, episode_reward=1172.80 +/- 89.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=467000, episode_reward=1120.00 +/- 176.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=468000, episode_reward=1042.40 +/- 226.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=469000, episode_reward=997.00 +/- 188.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=470000, episode_reward=1157.80 +/- 129.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=471000, episode_reward=1075.40 +/- 224.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=472000, episode_reward=1201.60 +/- 364.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=473000, episode_reward=988.40 +/- 190.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=474000, episode_reward=-4002.40 +/- 6263.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=475000, episode_reward=1123.60 +/- 253.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=476000, episode_reward=918.40 +/- 415.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=477000, episode_reward=971.80 +/- 149.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=478000, episode_reward=606.20 +/- 369.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=479000, episode_reward=1160.00 +/- 306.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=480000, episode_reward=942.20 +/- 280.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=481000, episode_reward=989.40 +/- 364.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=482000, episode_reward=-735.60 +/- 3626.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=483000, episode_reward=936.60 +/- 338.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=484000, episode_reward=871.20 +/- 259.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=485000, episode_reward=927.80 +/- 485.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=486000, episode_reward=-2326.60 +/- 6892.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=487000, episode_reward=1134.00 +/- 267.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=488000, episode_reward=42.80 +/- 1526.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=489000, episode_reward=1035.20 +/- 126.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=490000, episode_reward=1178.20 +/- 224.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=491000, episode_reward=-239.00 +/- 2316.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=492000, episode_reward=950.80 +/- 173.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=493000, episode_reward=-481.80 +/- 2501.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=494000, episode_reward=1171.60 +/- 294.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=495000, episode_reward=1107.20 +/- 154.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=496000, episode_reward=959.60 +/- 433.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=497000, episode_reward=-1088.60 +/- 4491.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=498000, episode_reward=44.40 +/- 2060.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=499000, episode_reward=-1507.00 +/- 4574.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=500000, episode_reward=1115.60 +/- 135.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Gamma = 0.99, Learning Rate = 0.0003, Activation Function = ReLU, Neurons = 8, Process noise = 0.1, Measurement noise = 0.0\n",
      "Eval num_timesteps=1000, episode_reward=-2366.00 +/- 55.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=-2392.20 +/- 69.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=3000, episode_reward=-2430.20 +/- 76.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=4000, episode_reward=-2419.80 +/- 73.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=5000, episode_reward=-2455.60 +/- 43.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=6000, episode_reward=-2403.40 +/- 39.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=7000, episode_reward=-2487.60 +/- 44.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=8000, episode_reward=-2460.20 +/- 86.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=9000, episode_reward=-2472.00 +/- 86.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=10000, episode_reward=-2445.40 +/- 69.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=11000, episode_reward=-2407.80 +/- 50.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=12000, episode_reward=-2436.20 +/- 36.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=13000, episode_reward=-2449.80 +/- 33.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=14000, episode_reward=-2404.80 +/- 77.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=15000, episode_reward=-2374.20 +/- 119.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=16000, episode_reward=-2405.00 +/- 56.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=17000, episode_reward=-2352.20 +/- 61.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=18000, episode_reward=-2370.60 +/- 94.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=19000, episode_reward=-2427.80 +/- 45.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=20000, episode_reward=-2418.60 +/- 68.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=21000, episode_reward=-2396.00 +/- 111.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=22000, episode_reward=-2354.60 +/- 74.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=23000, episode_reward=-2330.60 +/- 118.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=24000, episode_reward=-2464.20 +/- 89.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=25000, episode_reward=-2395.00 +/- 33.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=26000, episode_reward=-2434.20 +/- 83.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=27000, episode_reward=-2303.00 +/- 112.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=28000, episode_reward=-2381.40 +/- 88.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=29000, episode_reward=-2433.60 +/- 50.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=30000, episode_reward=-2441.80 +/- 100.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=31000, episode_reward=-2380.80 +/- 29.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=32000, episode_reward=-2440.00 +/- 112.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=33000, episode_reward=-2410.60 +/- 59.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=34000, episode_reward=-2356.40 +/- 44.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=35000, episode_reward=-2480.40 +/- 78.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=36000, episode_reward=-2350.00 +/- 73.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=37000, episode_reward=-2389.20 +/- 60.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=38000, episode_reward=-2376.20 +/- 89.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=39000, episode_reward=-2386.80 +/- 71.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=40000, episode_reward=-2388.60 +/- 67.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=41000, episode_reward=-2367.20 +/- 179.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=42000, episode_reward=-2377.00 +/- 46.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=43000, episode_reward=-2392.60 +/- 69.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=44000, episode_reward=-2433.00 +/- 48.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=45000, episode_reward=-2358.20 +/- 79.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=46000, episode_reward=-2394.20 +/- 38.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=47000, episode_reward=-2452.00 +/- 43.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=48000, episode_reward=-2403.20 +/- 101.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=49000, episode_reward=-2466.00 +/- 105.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=50000, episode_reward=-2428.20 +/- 108.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=51000, episode_reward=132.80 +/- 317.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=52000, episode_reward=-186.60 +/- 329.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=53000, episode_reward=127.40 +/- 279.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=54000, episode_reward=-59.40 +/- 251.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=55000, episode_reward=-632.80 +/- 830.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=56000, episode_reward=-6020.40 +/- 2289.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=57000, episode_reward=-6954.40 +/- 1262.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=58000, episode_reward=-369.80 +/- 661.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=59000, episode_reward=-138.60 +/- 215.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=60000, episode_reward=-414.60 +/- 275.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=61000, episode_reward=-170.20 +/- 586.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=62000, episode_reward=-309.60 +/- 695.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=63000, episode_reward=-181.00 +/- 486.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=64000, episode_reward=-361.40 +/- 555.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=65000, episode_reward=197.00 +/- 233.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=66000, episode_reward=37.40 +/- 331.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=67000, episode_reward=165.80 +/- 277.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=68000, episode_reward=-267.20 +/- 343.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=69000, episode_reward=-25.00 +/- 307.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=70000, episode_reward=47.40 +/- 345.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=71000, episode_reward=251.40 +/- 374.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=72000, episode_reward=-375.00 +/- 522.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=73000, episode_reward=-167.00 +/- 243.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=74000, episode_reward=-114.00 +/- 127.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=75000, episode_reward=-118.60 +/- 355.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=76000, episode_reward=-301.60 +/- 614.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=77000, episode_reward=100.80 +/- 494.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=78000, episode_reward=-87.00 +/- 449.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=79000, episode_reward=-66.40 +/- 292.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=80000, episode_reward=-139.00 +/- 336.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=81000, episode_reward=-333.00 +/- 557.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=82000, episode_reward=-139.00 +/- 389.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=83000, episode_reward=-462.00 +/- 705.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=84000, episode_reward=-279.40 +/- 456.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=85000, episode_reward=-589.00 +/- 1460.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=86000, episode_reward=-818.20 +/- 643.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=87000, episode_reward=-222.00 +/- 671.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=88000, episode_reward=132.20 +/- 143.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=89000, episode_reward=-354.80 +/- 517.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=90000, episode_reward=6.80 +/- 227.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=91000, episode_reward=-213.60 +/- 670.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=92000, episode_reward=-17.80 +/- 401.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=93000, episode_reward=-195.00 +/- 452.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=94000, episode_reward=79.80 +/- 449.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=95000, episode_reward=-22.80 +/- 440.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=96000, episode_reward=-61.60 +/- 220.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=97000, episode_reward=98.60 +/- 251.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=98000, episode_reward=41.60 +/- 334.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=99000, episode_reward=-235.40 +/- 557.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=100000, episode_reward=-737.80 +/- 357.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=101000, episode_reward=-219.80 +/- 353.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=102000, episode_reward=27.00 +/- 200.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=103000, episode_reward=15.20 +/- 334.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=104000, episode_reward=60.00 +/- 245.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=105000, episode_reward=331.40 +/- 180.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=106000, episode_reward=-21.80 +/- 133.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=107000, episode_reward=-136.00 +/- 190.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=108000, episode_reward=122.80 +/- 287.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=109000, episode_reward=-187.60 +/- 362.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=110000, episode_reward=-40.60 +/- 195.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=111000, episode_reward=-157.40 +/- 283.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=112000, episode_reward=28.60 +/- 194.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=113000, episode_reward=233.60 +/- 138.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=114000, episode_reward=-18.80 +/- 195.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=115000, episode_reward=-72.40 +/- 428.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=116000, episode_reward=72.40 +/- 218.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=117000, episode_reward=-110.00 +/- 347.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=118000, episode_reward=32.00 +/- 144.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=119000, episode_reward=-378.80 +/- 315.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=120000, episode_reward=105.00 +/- 116.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=121000, episode_reward=153.80 +/- 211.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=122000, episode_reward=7.60 +/- 328.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=123000, episode_reward=-79.20 +/- 123.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=124000, episode_reward=73.40 +/- 380.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=125000, episode_reward=78.20 +/- 458.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=126000, episode_reward=-113.60 +/- 195.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=127000, episode_reward=213.80 +/- 76.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=128000, episode_reward=203.60 +/- 58.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=129000, episode_reward=196.40 +/- 240.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=130000, episode_reward=297.60 +/- 146.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=131000, episode_reward=130.00 +/- 312.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=132000, episode_reward=-13.00 +/- 163.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=133000, episode_reward=-145.80 +/- 248.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=134000, episode_reward=66.80 +/- 323.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=135000, episode_reward=129.80 +/- 102.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=136000, episode_reward=30.20 +/- 168.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=137000, episode_reward=-135.80 +/- 283.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=138000, episode_reward=212.20 +/- 312.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=139000, episode_reward=142.80 +/- 153.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=140000, episode_reward=-40.80 +/- 183.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=141000, episode_reward=174.60 +/- 264.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=142000, episode_reward=44.40 +/- 234.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=143000, episode_reward=73.20 +/- 116.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=144000, episode_reward=24.20 +/- 142.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=145000, episode_reward=246.20 +/- 125.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=146000, episode_reward=158.00 +/- 307.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=147000, episode_reward=237.20 +/- 127.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=148000, episode_reward=55.60 +/- 352.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=149000, episode_reward=178.40 +/- 117.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=150000, episode_reward=345.20 +/- 81.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=151000, episode_reward=31.60 +/- 135.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=152000, episode_reward=121.20 +/- 155.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=153000, episode_reward=137.80 +/- 259.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=154000, episode_reward=34.20 +/- 171.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=155000, episode_reward=173.80 +/- 247.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=156000, episode_reward=66.80 +/- 225.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=157000, episode_reward=159.60 +/- 238.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=158000, episode_reward=132.40 +/- 277.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=159000, episode_reward=215.00 +/- 183.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=160000, episode_reward=74.40 +/- 244.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=161000, episode_reward=-56.00 +/- 182.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=162000, episode_reward=91.80 +/- 387.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=163000, episode_reward=48.60 +/- 137.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=164000, episode_reward=-95.00 +/- 302.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=165000, episode_reward=96.80 +/- 151.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=166000, episode_reward=106.80 +/- 137.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=167000, episode_reward=98.00 +/- 252.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=168000, episode_reward=-4.20 +/- 227.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=169000, episode_reward=273.60 +/- 231.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=170000, episode_reward=220.60 +/- 304.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=171000, episode_reward=164.60 +/- 249.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=172000, episode_reward=269.60 +/- 186.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=173000, episode_reward=141.80 +/- 130.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=174000, episode_reward=9.40 +/- 97.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=175000, episode_reward=43.60 +/- 255.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=176000, episode_reward=122.00 +/- 163.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=177000, episode_reward=168.00 +/- 193.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=178000, episode_reward=110.20 +/- 246.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=179000, episode_reward=327.20 +/- 165.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=180000, episode_reward=161.00 +/- 251.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=181000, episode_reward=146.80 +/- 180.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=182000, episode_reward=178.40 +/- 277.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=183000, episode_reward=90.40 +/- 365.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=184000, episode_reward=193.00 +/- 134.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=185000, episode_reward=146.80 +/- 279.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=186000, episode_reward=194.80 +/- 151.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=187000, episode_reward=156.20 +/- 227.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=188000, episode_reward=279.80 +/- 228.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=189000, episode_reward=-9.20 +/- 143.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=190000, episode_reward=254.80 +/- 188.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=191000, episode_reward=250.00 +/- 226.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=192000, episode_reward=320.60 +/- 154.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=193000, episode_reward=19.40 +/- 207.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=194000, episode_reward=-57.40 +/- 183.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=195000, episode_reward=212.20 +/- 225.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=196000, episode_reward=240.40 +/- 208.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=197000, episode_reward=172.80 +/- 282.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=198000, episode_reward=144.20 +/- 164.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=199000, episode_reward=81.80 +/- 150.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=200000, episode_reward=327.80 +/- 83.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=201000, episode_reward=305.40 +/- 126.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=202000, episode_reward=303.60 +/- 164.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=203000, episode_reward=144.40 +/- 277.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=204000, episode_reward=231.00 +/- 223.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=205000, episode_reward=174.00 +/- 186.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=206000, episode_reward=176.80 +/- 214.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=207000, episode_reward=199.80 +/- 101.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=208000, episode_reward=177.60 +/- 124.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=209000, episode_reward=316.60 +/- 156.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=210000, episode_reward=279.00 +/- 93.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=211000, episode_reward=213.40 +/- 196.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=212000, episode_reward=205.60 +/- 147.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=213000, episode_reward=222.60 +/- 121.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=214000, episode_reward=92.60 +/- 189.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=215000, episode_reward=173.60 +/- 82.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=216000, episode_reward=333.40 +/- 124.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=217000, episode_reward=259.00 +/- 145.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=218000, episode_reward=247.80 +/- 161.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=219000, episode_reward=420.20 +/- 128.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=220000, episode_reward=221.00 +/- 214.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=221000, episode_reward=227.60 +/- 243.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=222000, episode_reward=262.20 +/- 230.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=223000, episode_reward=32.40 +/- 219.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=224000, episode_reward=288.80 +/- 192.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=225000, episode_reward=238.00 +/- 171.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=226000, episode_reward=253.80 +/- 272.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=227000, episode_reward=214.40 +/- 153.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=228000, episode_reward=210.60 +/- 49.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=229000, episode_reward=120.20 +/- 181.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=230000, episode_reward=131.80 +/- 86.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=231000, episode_reward=208.80 +/- 93.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=232000, episode_reward=140.60 +/- 261.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=233000, episode_reward=377.60 +/- 163.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=234000, episode_reward=208.60 +/- 79.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=235000, episode_reward=342.60 +/- 228.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=236000, episode_reward=131.80 +/- 233.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=237000, episode_reward=363.20 +/- 131.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=238000, episode_reward=251.60 +/- 192.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=239000, episode_reward=214.80 +/- 211.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=240000, episode_reward=271.00 +/- 104.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=241000, episode_reward=112.00 +/- 219.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=242000, episode_reward=283.40 +/- 93.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=243000, episode_reward=367.20 +/- 157.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=244000, episode_reward=272.80 +/- 202.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=245000, episode_reward=277.80 +/- 242.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=246000, episode_reward=467.80 +/- 164.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=247000, episode_reward=168.20 +/- 167.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=248000, episode_reward=189.20 +/- 307.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=249000, episode_reward=345.40 +/- 163.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=250000, episode_reward=135.40 +/- 259.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=251000, episode_reward=263.00 +/- 113.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=252000, episode_reward=246.60 +/- 143.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=253000, episode_reward=298.60 +/- 125.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=254000, episode_reward=178.60 +/- 185.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=255000, episode_reward=298.80 +/- 134.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=256000, episode_reward=319.60 +/- 207.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=257000, episode_reward=177.20 +/- 234.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=258000, episode_reward=261.60 +/- 203.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=259000, episode_reward=320.20 +/- 107.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=260000, episode_reward=255.20 +/- 138.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=261000, episode_reward=393.20 +/- 140.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=262000, episode_reward=121.40 +/- 158.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=263000, episode_reward=142.00 +/- 160.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=264000, episode_reward=318.80 +/- 117.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=265000, episode_reward=405.20 +/- 267.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=266000, episode_reward=217.20 +/- 290.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=267000, episode_reward=111.60 +/- 181.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=268000, episode_reward=422.40 +/- 121.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=269000, episode_reward=380.00 +/- 230.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=270000, episode_reward=257.20 +/- 109.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=271000, episode_reward=333.60 +/- 154.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=272000, episode_reward=309.60 +/- 115.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=273000, episode_reward=279.60 +/- 77.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=274000, episode_reward=331.20 +/- 181.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=275000, episode_reward=163.20 +/- 117.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=276000, episode_reward=271.80 +/- 141.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=277000, episode_reward=311.80 +/- 184.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=278000, episode_reward=204.20 +/- 283.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=279000, episode_reward=274.80 +/- 88.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=280000, episode_reward=218.80 +/- 75.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=281000, episode_reward=82.80 +/- 114.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=282000, episode_reward=400.20 +/- 104.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=283000, episode_reward=166.80 +/- 149.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=284000, episode_reward=330.00 +/- 202.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=285000, episode_reward=321.40 +/- 239.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=286000, episode_reward=130.20 +/- 88.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=287000, episode_reward=246.80 +/- 140.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=288000, episode_reward=247.40 +/- 318.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=289000, episode_reward=275.20 +/- 86.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=290000, episode_reward=459.20 +/- 233.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=291000, episode_reward=164.40 +/- 94.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=292000, episode_reward=306.20 +/- 274.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=293000, episode_reward=348.40 +/- 177.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=294000, episode_reward=336.60 +/- 154.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=295000, episode_reward=69.40 +/- 112.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=296000, episode_reward=268.20 +/- 112.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=297000, episode_reward=330.20 +/- 179.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=298000, episode_reward=207.60 +/- 98.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=299000, episode_reward=246.40 +/- 113.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=300000, episode_reward=394.40 +/- 188.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=301000, episode_reward=178.40 +/- 126.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=302000, episode_reward=263.20 +/- 173.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=303000, episode_reward=242.80 +/- 144.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=304000, episode_reward=191.40 +/- 284.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=305000, episode_reward=201.40 +/- 231.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=306000, episode_reward=459.00 +/- 114.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=307000, episode_reward=371.00 +/- 176.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=308000, episode_reward=462.60 +/- 143.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=309000, episode_reward=247.80 +/- 23.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=310000, episode_reward=239.40 +/- 115.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=311000, episode_reward=284.00 +/- 186.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=312000, episode_reward=262.20 +/- 231.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=313000, episode_reward=391.00 +/- 141.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=314000, episode_reward=372.20 +/- 245.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=315000, episode_reward=267.20 +/- 167.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=316000, episode_reward=230.60 +/- 134.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=317000, episode_reward=296.20 +/- 44.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=318000, episode_reward=274.20 +/- 192.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=319000, episode_reward=140.40 +/- 122.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=320000, episode_reward=408.40 +/- 181.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=321000, episode_reward=282.80 +/- 220.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=322000, episode_reward=231.20 +/- 255.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=323000, episode_reward=147.00 +/- 190.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=324000, episode_reward=304.80 +/- 196.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=325000, episode_reward=314.80 +/- 178.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=326000, episode_reward=209.20 +/- 193.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=327000, episode_reward=314.00 +/- 80.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=328000, episode_reward=161.80 +/- 172.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=329000, episode_reward=209.20 +/- 163.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=330000, episode_reward=313.00 +/- 127.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=331000, episode_reward=111.40 +/- 210.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=332000, episode_reward=303.20 +/- 115.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=333000, episode_reward=487.60 +/- 133.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=334000, episode_reward=305.80 +/- 83.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=335000, episode_reward=88.60 +/- 181.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=336000, episode_reward=285.40 +/- 302.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=337000, episode_reward=35.80 +/- 140.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=338000, episode_reward=390.60 +/- 177.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=339000, episode_reward=346.60 +/- 263.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=340000, episode_reward=407.40 +/- 174.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=341000, episode_reward=170.40 +/- 239.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=342000, episode_reward=276.40 +/- 177.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=343000, episode_reward=288.60 +/- 186.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=344000, episode_reward=477.60 +/- 196.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=345000, episode_reward=190.00 +/- 173.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=346000, episode_reward=306.40 +/- 154.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=347000, episode_reward=324.20 +/- 152.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=348000, episode_reward=298.80 +/- 112.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=349000, episode_reward=248.80 +/- 206.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=350000, episode_reward=436.00 +/- 200.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=351000, episode_reward=387.00 +/- 257.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=352000, episode_reward=484.00 +/- 198.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=353000, episode_reward=419.60 +/- 107.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=354000, episode_reward=386.00 +/- 218.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=355000, episode_reward=304.20 +/- 201.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=356000, episode_reward=301.60 +/- 306.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=357000, episode_reward=367.60 +/- 261.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=358000, episode_reward=400.80 +/- 173.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=359000, episode_reward=690.60 +/- 89.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=360000, episode_reward=357.40 +/- 215.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=361000, episode_reward=565.80 +/- 199.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=362000, episode_reward=678.00 +/- 293.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=363000, episode_reward=582.60 +/- 194.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=364000, episode_reward=433.80 +/- 125.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=365000, episode_reward=461.00 +/- 205.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=366000, episode_reward=386.40 +/- 136.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=367000, episode_reward=496.60 +/- 145.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=368000, episode_reward=343.20 +/- 141.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=369000, episode_reward=551.80 +/- 146.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=370000, episode_reward=502.00 +/- 141.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=371000, episode_reward=652.40 +/- 146.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=372000, episode_reward=558.60 +/- 244.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=373000, episode_reward=507.40 +/- 246.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=374000, episode_reward=393.00 +/- 250.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=375000, episode_reward=674.00 +/- 132.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=376000, episode_reward=543.00 +/- 293.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=377000, episode_reward=497.20 +/- 304.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=378000, episode_reward=523.20 +/- 196.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=379000, episode_reward=482.20 +/- 80.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=380000, episode_reward=458.00 +/- 221.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=381000, episode_reward=406.00 +/- 222.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=382000, episode_reward=460.60 +/- 221.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=383000, episode_reward=654.20 +/- 209.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=384000, episode_reward=727.00 +/- 282.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=385000, episode_reward=557.00 +/- 168.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=386000, episode_reward=379.20 +/- 305.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=387000, episode_reward=518.60 +/- 227.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=388000, episode_reward=476.00 +/- 96.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=389000, episode_reward=634.00 +/- 267.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=390000, episode_reward=693.80 +/- 283.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=391000, episode_reward=583.60 +/- 79.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=392000, episode_reward=578.60 +/- 276.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=393000, episode_reward=505.00 +/- 194.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=394000, episode_reward=773.40 +/- 235.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=395000, episode_reward=634.20 +/- 303.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=396000, episode_reward=640.40 +/- 269.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=397000, episode_reward=551.80 +/- 225.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=398000, episode_reward=693.60 +/- 423.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=399000, episode_reward=703.80 +/- 225.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=400000, episode_reward=583.80 +/- 209.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=401000, episode_reward=767.00 +/- 125.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=402000, episode_reward=607.20 +/- 384.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=403000, episode_reward=783.00 +/- 232.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=404000, episode_reward=566.40 +/- 165.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=405000, episode_reward=953.40 +/- 95.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=406000, episode_reward=602.00 +/- 282.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=407000, episode_reward=773.20 +/- 66.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=408000, episode_reward=601.20 +/- 237.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=409000, episode_reward=637.60 +/- 224.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=410000, episode_reward=890.80 +/- 149.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=411000, episode_reward=625.40 +/- 308.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=412000, episode_reward=649.00 +/- 259.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=413000, episode_reward=834.40 +/- 143.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=414000, episode_reward=867.00 +/- 240.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=415000, episode_reward=909.40 +/- 290.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=416000, episode_reward=896.80 +/- 255.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=417000, episode_reward=860.60 +/- 267.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=418000, episode_reward=848.80 +/- 123.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=419000, episode_reward=575.80 +/- 181.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=420000, episode_reward=590.40 +/- 146.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=421000, episode_reward=640.20 +/- 206.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=422000, episode_reward=924.60 +/- 137.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=423000, episode_reward=940.80 +/- 332.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=424000, episode_reward=685.60 +/- 366.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=425000, episode_reward=736.60 +/- 168.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=426000, episode_reward=861.60 +/- 310.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=427000, episode_reward=666.00 +/- 317.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=428000, episode_reward=803.40 +/- 277.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=429000, episode_reward=694.00 +/- 195.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=430000, episode_reward=873.80 +/- 264.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=431000, episode_reward=751.20 +/- 163.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=432000, episode_reward=755.60 +/- 187.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=433000, episode_reward=854.20 +/- 286.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=434000, episode_reward=844.00 +/- 291.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=435000, episode_reward=880.20 +/- 320.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=436000, episode_reward=780.00 +/- 364.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=437000, episode_reward=820.40 +/- 352.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=438000, episode_reward=663.00 +/- 187.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=439000, episode_reward=835.40 +/- 297.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=440000, episode_reward=581.80 +/- 234.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=441000, episode_reward=830.60 +/- 175.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=442000, episode_reward=772.80 +/- 200.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=443000, episode_reward=631.00 +/- 228.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=444000, episode_reward=645.00 +/- 289.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=445000, episode_reward=682.00 +/- 135.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=446000, episode_reward=1039.60 +/- 107.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=447000, episode_reward=832.20 +/- 215.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=448000, episode_reward=850.20 +/- 291.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=449000, episode_reward=1038.20 +/- 189.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=450000, episode_reward=832.60 +/- 242.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=451000, episode_reward=717.20 +/- 225.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=452000, episode_reward=1014.80 +/- 156.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=453000, episode_reward=612.20 +/- 270.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=454000, episode_reward=479.00 +/- 102.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=455000, episode_reward=795.80 +/- 250.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=456000, episode_reward=1123.60 +/- 154.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=457000, episode_reward=993.40 +/- 103.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=458000, episode_reward=629.60 +/- 280.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=459000, episode_reward=830.00 +/- 238.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=460000, episode_reward=779.40 +/- 333.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=461000, episode_reward=624.40 +/- 301.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=462000, episode_reward=433.40 +/- 150.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=463000, episode_reward=835.40 +/- 115.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=464000, episode_reward=882.40 +/- 300.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=465000, episode_reward=747.40 +/- 224.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=466000, episode_reward=679.60 +/- 256.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=467000, episode_reward=581.40 +/- 360.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=468000, episode_reward=670.60 +/- 370.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=469000, episode_reward=838.60 +/- 301.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=470000, episode_reward=870.20 +/- 301.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=471000, episode_reward=687.40 +/- 130.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=472000, episode_reward=582.60 +/- 173.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=473000, episode_reward=608.60 +/- 68.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=474000, episode_reward=788.20 +/- 86.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=475000, episode_reward=761.20 +/- 199.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=476000, episode_reward=334.80 +/- 187.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=477000, episode_reward=822.20 +/- 208.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=478000, episode_reward=687.80 +/- 189.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=479000, episode_reward=741.80 +/- 238.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=480000, episode_reward=580.80 +/- 204.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=481000, episode_reward=352.80 +/- 200.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=482000, episode_reward=510.40 +/- 234.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=483000, episode_reward=604.20 +/- 263.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=484000, episode_reward=610.40 +/- 150.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=485000, episode_reward=520.80 +/- 312.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=486000, episode_reward=565.40 +/- 149.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=487000, episode_reward=527.60 +/- 104.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=488000, episode_reward=627.60 +/- 213.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=489000, episode_reward=375.60 +/- 202.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=490000, episode_reward=506.80 +/- 304.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=491000, episode_reward=391.80 +/- 125.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=492000, episode_reward=390.00 +/- 152.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=493000, episode_reward=204.80 +/- 109.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=494000, episode_reward=467.20 +/- 161.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=495000, episode_reward=112.20 +/- 122.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=496000, episode_reward=298.40 +/- 132.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=497000, episode_reward=274.80 +/- 259.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=498000, episode_reward=192.80 +/- 284.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=499000, episode_reward=330.80 +/- 109.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=500000, episode_reward=109.80 +/- 175.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Gamma = 0.99, Learning Rate = 0.0003, Activation Function = ReLU, Neurons = 32, Process noise = 0.1, Measurement noise = 0.0\n",
      "Eval num_timesteps=1000, episode_reward=-12.80 +/- 215.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=-569.60 +/- 794.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=3000, episode_reward=312.20 +/- 236.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4000, episode_reward=-134.40 +/- 395.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=5000, episode_reward=-441.80 +/- 1577.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=6000, episode_reward=-280.00 +/- 574.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=7000, episode_reward=-142.80 +/- 483.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=8000, episode_reward=-72.80 +/- 308.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=9000, episode_reward=-150.40 +/- 511.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=10000, episode_reward=-88.40 +/- 452.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=11000, episode_reward=24.40 +/- 261.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=12000, episode_reward=-71.40 +/- 233.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=13000, episode_reward=-233.00 +/- 265.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=14000, episode_reward=-1651.20 +/- 1723.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=15000, episode_reward=-172.40 +/- 455.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=16000, episode_reward=130.20 +/- 219.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=17000, episode_reward=-510.00 +/- 291.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=18000, episode_reward=60.00 +/- 133.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=19000, episode_reward=-1.80 +/- 252.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=20000, episode_reward=-292.00 +/- 811.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=21000, episode_reward=-75.80 +/- 331.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=22000, episode_reward=219.00 +/- 167.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=23000, episode_reward=11.80 +/- 189.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=24000, episode_reward=137.60 +/- 282.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=25000, episode_reward=-6.00 +/- 470.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=26000, episode_reward=81.80 +/- 191.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=27000, episode_reward=-273.20 +/- 419.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=28000, episode_reward=-206.00 +/- 392.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=29000, episode_reward=132.20 +/- 176.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=30000, episode_reward=-12.40 +/- 498.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=31000, episode_reward=-11.80 +/- 213.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=32000, episode_reward=-37.80 +/- 313.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=33000, episode_reward=-826.40 +/- 925.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=34000, episode_reward=-382.60 +/- 422.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=35000, episode_reward=-140.60 +/- 596.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=36000, episode_reward=90.60 +/- 224.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=37000, episode_reward=-155.80 +/- 306.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=38000, episode_reward=24.60 +/- 274.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=39000, episode_reward=-334.80 +/- 691.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=40000, episode_reward=-234.20 +/- 331.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=41000, episode_reward=-566.60 +/- 847.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=42000, episode_reward=-833.20 +/- 1342.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=43000, episode_reward=-41.40 +/- 316.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=44000, episode_reward=182.00 +/- 166.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=45000, episode_reward=-344.80 +/- 454.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=46000, episode_reward=61.20 +/- 248.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=47000, episode_reward=-332.80 +/- 464.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=48000, episode_reward=-605.20 +/- 903.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=49000, episode_reward=-85.20 +/- 404.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=50000, episode_reward=-602.00 +/- 377.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=51000, episode_reward=255.00 +/- 231.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=52000, episode_reward=-375.40 +/- 756.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=53000, episode_reward=-5422.60 +/- 2733.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=54000, episode_reward=-354.20 +/- 510.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=55000, episode_reward=-451.60 +/- 450.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=56000, episode_reward=194.00 +/- 248.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=57000, episode_reward=134.60 +/- 163.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=58000, episode_reward=356.00 +/- 206.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=59000, episode_reward=295.00 +/- 263.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=60000, episode_reward=327.40 +/- 127.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=61000, episode_reward=290.00 +/- 204.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=62000, episode_reward=296.40 +/- 190.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=63000, episode_reward=463.80 +/- 64.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=64000, episode_reward=268.40 +/- 75.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=65000, episode_reward=106.00 +/- 215.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=66000, episode_reward=420.00 +/- 242.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=67000, episode_reward=323.60 +/- 308.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=68000, episode_reward=500.00 +/- 136.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=69000, episode_reward=229.80 +/- 107.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=70000, episode_reward=317.80 +/- 128.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=71000, episode_reward=182.80 +/- 124.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=72000, episode_reward=478.40 +/- 206.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=73000, episode_reward=134.80 +/- 270.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=74000, episode_reward=338.00 +/- 233.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=75000, episode_reward=493.00 +/- 68.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=76000, episode_reward=285.60 +/- 218.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=77000, episode_reward=206.60 +/- 203.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=78000, episode_reward=294.00 +/- 241.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=79000, episode_reward=500.80 +/- 102.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=80000, episode_reward=308.80 +/- 211.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=81000, episode_reward=298.40 +/- 177.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=82000, episode_reward=246.60 +/- 108.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=83000, episode_reward=343.80 +/- 227.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=84000, episode_reward=288.40 +/- 171.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=85000, episode_reward=387.60 +/- 150.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=86000, episode_reward=319.60 +/- 98.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=87000, episode_reward=437.20 +/- 165.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=88000, episode_reward=388.00 +/- 204.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=89000, episode_reward=247.00 +/- 156.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=90000, episode_reward=345.00 +/- 177.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=91000, episode_reward=369.40 +/- 195.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=92000, episode_reward=324.20 +/- 49.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=93000, episode_reward=333.40 +/- 171.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=94000, episode_reward=359.20 +/- 147.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=95000, episode_reward=407.20 +/- 163.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=96000, episode_reward=286.40 +/- 155.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=97000, episode_reward=385.00 +/- 195.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=98000, episode_reward=241.00 +/- 273.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=99000, episode_reward=308.60 +/- 188.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=100000, episode_reward=340.20 +/- 136.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=101000, episode_reward=247.00 +/- 130.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=102000, episode_reward=333.40 +/- 170.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=103000, episode_reward=334.60 +/- 151.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=104000, episode_reward=404.80 +/- 258.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=105000, episode_reward=297.60 +/- 123.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=106000, episode_reward=307.20 +/- 164.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=107000, episode_reward=394.80 +/- 197.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=108000, episode_reward=316.60 +/- 140.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=109000, episode_reward=184.60 +/- 238.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=110000, episode_reward=373.20 +/- 177.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=111000, episode_reward=273.80 +/- 187.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=112000, episode_reward=347.40 +/- 148.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=113000, episode_reward=389.60 +/- 211.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=114000, episode_reward=293.80 +/- 226.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=115000, episode_reward=309.20 +/- 216.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=116000, episode_reward=283.00 +/- 215.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=117000, episode_reward=369.80 +/- 268.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=118000, episode_reward=299.40 +/- 181.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=119000, episode_reward=269.60 +/- 163.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=120000, episode_reward=170.40 +/- 210.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=121000, episode_reward=127.00 +/- 214.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=122000, episode_reward=372.60 +/- 259.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=123000, episode_reward=544.80 +/- 137.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=124000, episode_reward=279.40 +/- 219.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=125000, episode_reward=341.60 +/- 264.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=126000, episode_reward=195.80 +/- 136.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=127000, episode_reward=454.40 +/- 300.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=128000, episode_reward=492.60 +/- 140.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=129000, episode_reward=191.40 +/- 110.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=130000, episode_reward=192.60 +/- 240.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=131000, episode_reward=371.00 +/- 118.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=132000, episode_reward=430.80 +/- 153.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=133000, episode_reward=318.40 +/- 78.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=134000, episode_reward=400.20 +/- 222.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=135000, episode_reward=211.00 +/- 112.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=136000, episode_reward=250.00 +/- 193.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=137000, episode_reward=229.20 +/- 191.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=138000, episode_reward=476.60 +/- 142.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=139000, episode_reward=141.40 +/- 227.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=140000, episode_reward=322.00 +/- 103.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=141000, episode_reward=389.20 +/- 150.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=142000, episode_reward=441.80 +/- 76.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=143000, episode_reward=458.20 +/- 104.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=144000, episode_reward=281.40 +/- 82.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=145000, episode_reward=560.20 +/- 130.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=146000, episode_reward=413.80 +/- 125.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=147000, episode_reward=532.20 +/- 73.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=148000, episode_reward=306.40 +/- 102.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=149000, episode_reward=371.40 +/- 226.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=150000, episode_reward=448.40 +/- 73.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=151000, episode_reward=253.40 +/- 129.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=152000, episode_reward=258.80 +/- 103.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=153000, episode_reward=299.20 +/- 69.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=154000, episode_reward=527.00 +/- 150.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=155000, episode_reward=440.80 +/- 71.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=156000, episode_reward=381.60 +/- 242.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=157000, episode_reward=290.60 +/- 202.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=158000, episode_reward=517.40 +/- 114.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=159000, episode_reward=299.60 +/- 92.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=160000, episode_reward=227.00 +/- 224.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=161000, episode_reward=340.20 +/- 128.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=162000, episode_reward=400.20 +/- 176.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=163000, episode_reward=461.60 +/- 116.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=164000, episode_reward=277.40 +/- 231.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=165000, episode_reward=401.60 +/- 256.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=166000, episode_reward=266.40 +/- 159.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=167000, episode_reward=271.20 +/- 169.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=168000, episode_reward=260.20 +/- 228.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=169000, episode_reward=252.00 +/- 217.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=170000, episode_reward=341.20 +/- 123.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=171000, episode_reward=239.60 +/- 133.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=172000, episode_reward=426.80 +/- 90.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=173000, episode_reward=263.80 +/- 196.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=174000, episode_reward=408.20 +/- 102.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=175000, episode_reward=462.40 +/- 195.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=176000, episode_reward=422.00 +/- 95.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=177000, episode_reward=406.00 +/- 192.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=178000, episode_reward=548.20 +/- 130.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=179000, episode_reward=293.20 +/- 130.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=180000, episode_reward=268.60 +/- 276.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=181000, episode_reward=440.00 +/- 183.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=182000, episode_reward=277.00 +/- 158.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=183000, episode_reward=416.00 +/- 147.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=184000, episode_reward=334.00 +/- 98.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=185000, episode_reward=318.80 +/- 213.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=186000, episode_reward=366.20 +/- 166.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=187000, episode_reward=446.80 +/- 84.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=188000, episode_reward=448.80 +/- 262.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=189000, episode_reward=399.80 +/- 175.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=190000, episode_reward=351.80 +/- 121.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=191000, episode_reward=347.20 +/- 207.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=192000, episode_reward=480.20 +/- 170.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=193000, episode_reward=366.00 +/- 151.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=194000, episode_reward=467.40 +/- 143.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=195000, episode_reward=101.20 +/- 297.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=196000, episode_reward=196.00 +/- 110.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=197000, episode_reward=327.80 +/- 224.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=198000, episode_reward=244.60 +/- 186.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=199000, episode_reward=417.40 +/- 96.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=200000, episode_reward=346.20 +/- 183.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=201000, episode_reward=391.80 +/- 123.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=202000, episode_reward=331.60 +/- 232.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=203000, episode_reward=453.40 +/- 227.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=204000, episode_reward=317.20 +/- 94.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=205000, episode_reward=290.20 +/- 177.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=206000, episode_reward=368.60 +/- 124.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=207000, episode_reward=318.20 +/- 163.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=208000, episode_reward=321.20 +/- 258.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=209000, episode_reward=344.60 +/- 119.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=210000, episode_reward=227.40 +/- 238.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=211000, episode_reward=295.40 +/- 186.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=212000, episode_reward=398.80 +/- 129.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=213000, episode_reward=347.20 +/- 337.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=214000, episode_reward=228.20 +/- 167.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=215000, episode_reward=258.00 +/- 152.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=216000, episode_reward=427.40 +/- 130.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=217000, episode_reward=408.40 +/- 206.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=218000, episode_reward=306.80 +/- 160.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=219000, episode_reward=447.80 +/- 141.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=220000, episode_reward=335.80 +/- 170.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=221000, episode_reward=354.00 +/- 162.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=222000, episode_reward=414.20 +/- 245.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=223000, episode_reward=468.00 +/- 112.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=224000, episode_reward=236.60 +/- 124.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=225000, episode_reward=418.00 +/- 221.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=226000, episode_reward=339.60 +/- 112.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=227000, episode_reward=349.80 +/- 75.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=228000, episode_reward=339.40 +/- 71.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=229000, episode_reward=313.20 +/- 175.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=230000, episode_reward=274.20 +/- 136.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=231000, episode_reward=414.80 +/- 198.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=232000, episode_reward=381.80 +/- 130.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=233000, episode_reward=178.80 +/- 168.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=234000, episode_reward=328.60 +/- 103.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=235000, episode_reward=274.00 +/- 110.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=236000, episode_reward=298.60 +/- 209.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=237000, episode_reward=481.40 +/- 165.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=238000, episode_reward=476.80 +/- 142.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=239000, episode_reward=316.40 +/- 100.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=240000, episode_reward=273.80 +/- 192.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=241000, episode_reward=420.40 +/- 151.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=242000, episode_reward=335.40 +/- 225.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=243000, episode_reward=373.80 +/- 113.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=244000, episode_reward=464.00 +/- 208.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=245000, episode_reward=328.20 +/- 53.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=246000, episode_reward=182.40 +/- 199.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=247000, episode_reward=328.60 +/- 174.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=248000, episode_reward=493.20 +/- 242.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=249000, episode_reward=305.40 +/- 159.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=250000, episode_reward=241.60 +/- 248.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=251000, episode_reward=260.60 +/- 174.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=252000, episode_reward=395.20 +/- 235.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=253000, episode_reward=286.00 +/- 371.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=254000, episode_reward=323.00 +/- 113.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=255000, episode_reward=333.60 +/- 96.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=256000, episode_reward=296.80 +/- 170.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=257000, episode_reward=417.40 +/- 167.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=258000, episode_reward=257.20 +/- 115.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=259000, episode_reward=434.20 +/- 196.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=260000, episode_reward=308.00 +/- 186.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=261000, episode_reward=285.40 +/- 122.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=262000, episode_reward=270.40 +/- 159.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=263000, episode_reward=414.00 +/- 261.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=264000, episode_reward=204.40 +/- 65.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=265000, episode_reward=480.40 +/- 154.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=266000, episode_reward=290.40 +/- 137.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=267000, episode_reward=448.60 +/- 165.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=268000, episode_reward=288.80 +/- 53.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=269000, episode_reward=307.80 +/- 246.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=270000, episode_reward=374.80 +/- 171.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=271000, episode_reward=451.20 +/- 74.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=272000, episode_reward=374.20 +/- 209.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=273000, episode_reward=399.00 +/- 134.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=274000, episode_reward=404.60 +/- 113.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=275000, episode_reward=358.40 +/- 147.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=276000, episode_reward=278.20 +/- 237.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=277000, episode_reward=394.40 +/- 130.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=278000, episode_reward=204.40 +/- 239.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=279000, episode_reward=526.40 +/- 178.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=280000, episode_reward=408.40 +/- 110.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=281000, episode_reward=348.00 +/- 140.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=282000, episode_reward=521.80 +/- 68.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=283000, episode_reward=220.80 +/- 160.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=284000, episode_reward=242.80 +/- 99.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=285000, episode_reward=290.60 +/- 122.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=286000, episode_reward=375.60 +/- 80.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=287000, episode_reward=359.20 +/- 113.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=288000, episode_reward=288.40 +/- 150.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=289000, episode_reward=379.80 +/- 108.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=290000, episode_reward=422.60 +/- 91.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=291000, episode_reward=358.60 +/- 149.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=292000, episode_reward=360.20 +/- 151.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=293000, episode_reward=551.00 +/- 168.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=294000, episode_reward=435.80 +/- 166.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=295000, episode_reward=311.80 +/- 132.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=296000, episode_reward=450.40 +/- 85.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=297000, episode_reward=374.80 +/- 150.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=298000, episode_reward=291.40 +/- 114.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=299000, episode_reward=344.40 +/- 143.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=300000, episode_reward=355.20 +/- 294.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=301000, episode_reward=302.40 +/- 130.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=302000, episode_reward=272.20 +/- 192.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=303000, episode_reward=455.00 +/- 125.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=304000, episode_reward=317.20 +/- 156.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=305000, episode_reward=385.60 +/- 257.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=306000, episode_reward=340.80 +/- 221.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=307000, episode_reward=622.80 +/- 126.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=308000, episode_reward=462.00 +/- 229.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=309000, episode_reward=346.00 +/- 149.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=310000, episode_reward=195.20 +/- 132.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=311000, episode_reward=360.20 +/- 178.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=312000, episode_reward=409.40 +/- 222.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=313000, episode_reward=229.20 +/- 120.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=314000, episode_reward=336.00 +/- 198.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=315000, episode_reward=193.80 +/- 226.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=316000, episode_reward=282.20 +/- 205.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=317000, episode_reward=224.20 +/- 293.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=318000, episode_reward=426.20 +/- 220.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=319000, episode_reward=290.20 +/- 206.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=320000, episode_reward=368.40 +/- 178.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=321000, episode_reward=129.00 +/- 295.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=322000, episode_reward=320.60 +/- 175.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=323000, episode_reward=377.00 +/- 236.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=324000, episode_reward=310.80 +/- 77.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=325000, episode_reward=459.80 +/- 91.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=326000, episode_reward=401.40 +/- 146.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=327000, episode_reward=339.40 +/- 109.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=328000, episode_reward=493.00 +/- 152.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=329000, episode_reward=310.80 +/- 104.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=330000, episode_reward=294.60 +/- 100.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=331000, episode_reward=307.40 +/- 98.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=332000, episode_reward=299.40 +/- 135.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=333000, episode_reward=433.60 +/- 69.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=334000, episode_reward=351.60 +/- 118.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=335000, episode_reward=432.40 +/- 125.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=336000, episode_reward=374.40 +/- 146.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=337000, episode_reward=479.40 +/- 153.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=338000, episode_reward=456.20 +/- 101.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=339000, episode_reward=461.20 +/- 226.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=340000, episode_reward=293.00 +/- 228.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=341000, episode_reward=335.00 +/- 138.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=342000, episode_reward=345.20 +/- 94.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=343000, episode_reward=272.80 +/- 176.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=344000, episode_reward=444.20 +/- 156.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=345000, episode_reward=311.40 +/- 122.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=346000, episode_reward=316.60 +/- 116.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=347000, episode_reward=449.40 +/- 235.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=348000, episode_reward=179.00 +/- 174.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=349000, episode_reward=322.00 +/- 156.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=350000, episode_reward=191.40 +/- 186.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=351000, episode_reward=456.60 +/- 187.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=352000, episode_reward=388.00 +/- 264.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=353000, episode_reward=427.20 +/- 184.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=354000, episode_reward=361.40 +/- 343.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=355000, episode_reward=404.20 +/- 126.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=356000, episode_reward=348.60 +/- 156.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=357000, episode_reward=476.00 +/- 205.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=358000, episode_reward=346.80 +/- 184.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=359000, episode_reward=311.80 +/- 118.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=360000, episode_reward=407.60 +/- 101.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=361000, episode_reward=482.00 +/- 124.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=362000, episode_reward=342.20 +/- 101.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=363000, episode_reward=511.60 +/- 64.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=364000, episode_reward=311.00 +/- 139.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=365000, episode_reward=578.00 +/- 185.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=366000, episode_reward=474.00 +/- 127.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=367000, episode_reward=418.60 +/- 178.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=368000, episode_reward=522.00 +/- 165.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=369000, episode_reward=409.20 +/- 149.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=370000, episode_reward=408.20 +/- 138.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=371000, episode_reward=261.60 +/- 200.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=372000, episode_reward=303.80 +/- 189.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=373000, episode_reward=151.40 +/- 321.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=374000, episode_reward=263.60 +/- 92.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=375000, episode_reward=233.20 +/- 167.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=376000, episode_reward=381.20 +/- 152.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=377000, episode_reward=253.60 +/- 243.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=378000, episode_reward=274.60 +/- 182.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=379000, episode_reward=486.60 +/- 206.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=380000, episode_reward=451.40 +/- 165.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=381000, episode_reward=303.40 +/- 173.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=382000, episode_reward=102.20 +/- 230.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=383000, episode_reward=236.40 +/- 191.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=384000, episode_reward=271.60 +/- 246.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=385000, episode_reward=467.60 +/- 151.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=386000, episode_reward=372.80 +/- 168.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=387000, episode_reward=272.80 +/- 220.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=388000, episode_reward=473.00 +/- 184.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=389000, episode_reward=497.40 +/- 104.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=390000, episode_reward=268.60 +/- 191.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=391000, episode_reward=421.00 +/- 136.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=392000, episode_reward=365.40 +/- 140.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=393000, episode_reward=438.80 +/- 212.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=394000, episode_reward=496.00 +/- 125.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=395000, episode_reward=187.00 +/- 176.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=396000, episode_reward=373.40 +/- 89.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=397000, episode_reward=346.80 +/- 280.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=398000, episode_reward=340.00 +/- 263.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=399000, episode_reward=385.40 +/- 237.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=400000, episode_reward=396.20 +/- 271.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=401000, episode_reward=321.60 +/- 105.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=402000, episode_reward=446.60 +/- 194.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=403000, episode_reward=463.60 +/- 215.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=404000, episode_reward=434.40 +/- 126.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=405000, episode_reward=291.40 +/- 208.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=406000, episode_reward=162.00 +/- 134.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=407000, episode_reward=281.80 +/- 199.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=408000, episode_reward=268.60 +/- 111.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=409000, episode_reward=421.80 +/- 156.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=410000, episode_reward=335.60 +/- 258.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=411000, episode_reward=373.20 +/- 182.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=412000, episode_reward=423.80 +/- 142.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=413000, episode_reward=268.40 +/- 245.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=414000, episode_reward=247.80 +/- 264.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=415000, episode_reward=234.80 +/- 245.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=416000, episode_reward=244.80 +/- 107.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=417000, episode_reward=193.60 +/- 101.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=418000, episode_reward=400.40 +/- 135.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=419000, episode_reward=220.00 +/- 73.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=420000, episode_reward=446.60 +/- 154.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=421000, episode_reward=412.80 +/- 151.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=422000, episode_reward=456.40 +/- 132.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=423000, episode_reward=263.00 +/- 57.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=424000, episode_reward=233.20 +/- 188.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=425000, episode_reward=404.20 +/- 206.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=426000, episode_reward=263.20 +/- 226.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=427000, episode_reward=238.80 +/- 161.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=428000, episode_reward=274.80 +/- 137.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=429000, episode_reward=240.60 +/- 214.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=430000, episode_reward=291.20 +/- 124.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=431000, episode_reward=469.20 +/- 121.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=432000, episode_reward=299.00 +/- 270.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=433000, episode_reward=352.40 +/- 218.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=434000, episode_reward=310.60 +/- 155.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=435000, episode_reward=416.20 +/- 182.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=436000, episode_reward=312.60 +/- 105.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=437000, episode_reward=239.00 +/- 121.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=438000, episode_reward=276.40 +/- 191.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=439000, episode_reward=568.40 +/- 186.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=440000, episode_reward=293.40 +/- 206.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=441000, episode_reward=335.60 +/- 135.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=442000, episode_reward=196.80 +/- 219.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=443000, episode_reward=359.20 +/- 92.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=444000, episode_reward=223.00 +/- 201.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=445000, episode_reward=370.80 +/- 166.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=446000, episode_reward=378.80 +/- 196.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=447000, episode_reward=240.80 +/- 153.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=448000, episode_reward=401.80 +/- 182.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=449000, episode_reward=89.00 +/- 194.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=450000, episode_reward=333.40 +/- 110.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=451000, episode_reward=367.20 +/- 144.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=452000, episode_reward=308.80 +/- 249.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=453000, episode_reward=339.00 +/- 149.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=454000, episode_reward=265.20 +/- 127.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=455000, episode_reward=289.80 +/- 181.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=456000, episode_reward=490.60 +/- 189.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=457000, episode_reward=423.40 +/- 71.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=458000, episode_reward=320.20 +/- 87.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=459000, episode_reward=205.60 +/- 154.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=460000, episode_reward=400.60 +/- 151.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=461000, episode_reward=405.80 +/- 242.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=462000, episode_reward=243.00 +/- 114.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=463000, episode_reward=329.60 +/- 246.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=464000, episode_reward=355.60 +/- 247.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=465000, episode_reward=355.00 +/- 221.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=466000, episode_reward=460.40 +/- 48.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=467000, episode_reward=377.20 +/- 139.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=468000, episode_reward=310.60 +/- 214.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=469000, episode_reward=536.80 +/- 165.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=470000, episode_reward=240.40 +/- 138.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=471000, episode_reward=350.40 +/- 133.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=472000, episode_reward=217.60 +/- 86.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=473000, episode_reward=273.80 +/- 182.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=474000, episode_reward=230.60 +/- 88.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=475000, episode_reward=309.00 +/- 145.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=476000, episode_reward=295.40 +/- 181.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=477000, episode_reward=277.80 +/- 180.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=478000, episode_reward=308.00 +/- 164.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=479000, episode_reward=352.80 +/- 107.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=480000, episode_reward=242.80 +/- 185.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=481000, episode_reward=272.60 +/- 156.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=482000, episode_reward=393.40 +/- 45.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=483000, episode_reward=241.20 +/- 89.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=484000, episode_reward=282.40 +/- 242.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=485000, episode_reward=329.60 +/- 327.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=486000, episode_reward=271.00 +/- 159.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=487000, episode_reward=214.00 +/- 226.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=488000, episode_reward=255.40 +/- 183.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=489000, episode_reward=391.60 +/- 114.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=490000, episode_reward=36.40 +/- 273.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=491000, episode_reward=425.40 +/- 112.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=492000, episode_reward=127.00 +/- 190.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=493000, episode_reward=238.20 +/- 217.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=494000, episode_reward=371.40 +/- 346.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=495000, episode_reward=167.20 +/- 200.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=496000, episode_reward=342.60 +/- 76.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=497000, episode_reward=167.80 +/- 132.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=498000, episode_reward=193.80 +/- 263.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=499000, episode_reward=265.20 +/- 199.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=500000, episode_reward=344.00 +/- 205.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Gamma = 0.99, Learning Rate = 0.0003, Activation Function = ReLU, Neurons = 64, Process noise = 0.1, Measurement noise = 0.0\n",
      "Eval num_timesteps=1000, episode_reward=190.60 +/- 83.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=-22.00 +/- 164.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=3000, episode_reward=-610.60 +/- 1345.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=4000, episode_reward=2.00 +/- 194.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=5000, episode_reward=178.20 +/- 133.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=6000, episode_reward=112.20 +/- 114.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=7000, episode_reward=-7.80 +/- 247.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=8000, episode_reward=211.20 +/- 75.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=9000, episode_reward=-36.80 +/- 164.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=10000, episode_reward=136.40 +/- 128.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=11000, episode_reward=72.40 +/- 156.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=12000, episode_reward=-7.60 +/- 158.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=13000, episode_reward=-593.20 +/- 1451.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=14000, episode_reward=-8.20 +/- 193.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=15000, episode_reward=247.40 +/- 38.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=16000, episode_reward=77.20 +/- 166.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=17000, episode_reward=65.80 +/- 112.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=18000, episode_reward=74.20 +/- 158.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=19000, episode_reward=26.40 +/- 181.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=20000, episode_reward=71.40 +/- 149.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=21000, episode_reward=95.60 +/- 113.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=22000, episode_reward=92.00 +/- 176.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=23000, episode_reward=66.80 +/- 193.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=24000, episode_reward=124.00 +/- 151.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=25000, episode_reward=95.40 +/- 101.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=26000, episode_reward=71.80 +/- 171.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=27000, episode_reward=-16.60 +/- 84.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=28000, episode_reward=65.40 +/- 174.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=29000, episode_reward=76.00 +/- 190.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=30000, episode_reward=-126.20 +/- 47.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=31000, episode_reward=53.20 +/- 138.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=32000, episode_reward=-19.60 +/- 181.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=33000, episode_reward=13.00 +/- 197.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=34000, episode_reward=9.80 +/- 83.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=35000, episode_reward=100.20 +/- 196.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=36000, episode_reward=76.00 +/- 154.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=37000, episode_reward=-6.40 +/- 144.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=38000, episode_reward=83.80 +/- 153.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=39000, episode_reward=45.60 +/- 102.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=40000, episode_reward=170.40 +/- 41.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=41000, episode_reward=-2776.40 +/- 3485.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=42000, episode_reward=-12.20 +/- 122.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=43000, episode_reward=-868.20 +/- 1271.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=44000, episode_reward=5.20 +/- 179.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=45000, episode_reward=50.20 +/- 172.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=46000, episode_reward=-1934.40 +/- 3729.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=47000, episode_reward=65.20 +/- 136.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=48000, episode_reward=69.00 +/- 145.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=49000, episode_reward=5.80 +/- 146.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=50000, episode_reward=137.40 +/- 172.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=51000, episode_reward=113.60 +/- 143.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=52000, episode_reward=-6550.00 +/- 843.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=53000, episode_reward=250.20 +/- 187.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=54000, episode_reward=193.80 +/- 435.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=55000, episode_reward=264.80 +/- 229.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=56000, episode_reward=293.60 +/- 185.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=57000, episode_reward=251.00 +/- 98.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=58000, episode_reward=294.80 +/- 163.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=59000, episode_reward=291.40 +/- 129.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=60000, episode_reward=250.40 +/- 86.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=61000, episode_reward=250.40 +/- 168.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=62000, episode_reward=204.60 +/- 242.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=63000, episode_reward=168.40 +/- 232.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=64000, episode_reward=321.80 +/- 120.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=65000, episode_reward=292.80 +/- 202.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=66000, episode_reward=387.20 +/- 197.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=67000, episode_reward=439.60 +/- 370.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=68000, episode_reward=400.00 +/- 227.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=69000, episode_reward=314.80 +/- 148.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=70000, episode_reward=161.20 +/- 262.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=71000, episode_reward=343.40 +/- 174.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=72000, episode_reward=348.20 +/- 93.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=73000, episode_reward=327.20 +/- 104.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=74000, episode_reward=332.80 +/- 194.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=75000, episode_reward=402.20 +/- 121.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=76000, episode_reward=386.40 +/- 88.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=77000, episode_reward=302.80 +/- 131.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=78000, episode_reward=503.80 +/- 92.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=79000, episode_reward=384.00 +/- 121.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=80000, episode_reward=241.20 +/- 70.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=81000, episode_reward=314.80 +/- 276.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=82000, episode_reward=454.00 +/- 210.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=83000, episode_reward=375.20 +/- 146.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=84000, episode_reward=393.40 +/- 211.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=85000, episode_reward=563.80 +/- 104.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=86000, episode_reward=314.60 +/- 169.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=87000, episode_reward=250.00 +/- 157.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=88000, episode_reward=314.20 +/- 250.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=89000, episode_reward=311.20 +/- 142.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=90000, episode_reward=508.80 +/- 114.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=91000, episode_reward=392.60 +/- 42.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=92000, episode_reward=510.60 +/- 142.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=93000, episode_reward=225.40 +/- 317.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=94000, episode_reward=213.40 +/- 138.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=95000, episode_reward=249.80 +/- 168.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=96000, episode_reward=432.00 +/- 93.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=97000, episode_reward=186.60 +/- 269.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=98000, episode_reward=383.20 +/- 128.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=99000, episode_reward=326.60 +/- 86.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=100000, episode_reward=394.00 +/- 156.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=101000, episode_reward=112.00 +/- 305.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=102000, episode_reward=437.20 +/- 238.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=103000, episode_reward=350.00 +/- 127.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=104000, episode_reward=342.20 +/- 139.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=105000, episode_reward=361.00 +/- 238.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=106000, episode_reward=430.80 +/- 160.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=107000, episode_reward=296.60 +/- 210.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=108000, episode_reward=347.20 +/- 75.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=109000, episode_reward=505.00 +/- 139.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=110000, episode_reward=405.20 +/- 202.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=111000, episode_reward=285.20 +/- 212.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=112000, episode_reward=444.80 +/- 195.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=113000, episode_reward=444.80 +/- 128.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=114000, episode_reward=299.80 +/- 155.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=115000, episode_reward=422.00 +/- 155.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=116000, episode_reward=286.00 +/- 357.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=117000, episode_reward=145.00 +/- 267.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=118000, episode_reward=156.60 +/- 179.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=119000, episode_reward=261.00 +/- 119.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=120000, episode_reward=459.80 +/- 148.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=121000, episode_reward=521.80 +/- 100.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=122000, episode_reward=376.00 +/- 194.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=123000, episode_reward=213.00 +/- 276.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=124000, episode_reward=279.00 +/- 97.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=125000, episode_reward=288.40 +/- 142.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=126000, episode_reward=302.40 +/- 158.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=127000, episode_reward=337.60 +/- 75.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=128000, episode_reward=353.20 +/- 75.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=129000, episode_reward=259.80 +/- 174.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=130000, episode_reward=396.00 +/- 160.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=131000, episode_reward=404.00 +/- 119.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=132000, episode_reward=437.20 +/- 158.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=133000, episode_reward=319.80 +/- 163.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=134000, episode_reward=119.40 +/- 402.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=135000, episode_reward=429.40 +/- 104.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=136000, episode_reward=407.60 +/- 248.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=137000, episode_reward=319.60 +/- 180.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=138000, episode_reward=446.20 +/- 93.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=139000, episode_reward=292.20 +/- 161.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=140000, episode_reward=309.00 +/- 196.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=141000, episode_reward=455.20 +/- 216.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=142000, episode_reward=287.60 +/- 134.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=143000, episode_reward=439.00 +/- 98.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=144000, episode_reward=407.40 +/- 111.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=145000, episode_reward=388.20 +/- 192.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=146000, episode_reward=419.40 +/- 140.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=147000, episode_reward=350.40 +/- 218.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=148000, episode_reward=228.80 +/- 116.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=149000, episode_reward=530.60 +/- 99.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=150000, episode_reward=338.00 +/- 270.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=151000, episode_reward=361.60 +/- 68.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=152000, episode_reward=301.00 +/- 219.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=153000, episode_reward=242.80 +/- 157.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=154000, episode_reward=262.20 +/- 193.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=155000, episode_reward=335.60 +/- 147.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=156000, episode_reward=355.60 +/- 125.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=157000, episode_reward=410.60 +/- 166.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=158000, episode_reward=355.00 +/- 336.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=159000, episode_reward=197.00 +/- 106.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=160000, episode_reward=394.20 +/- 97.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=161000, episode_reward=282.20 +/- 164.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=162000, episode_reward=560.40 +/- 192.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=163000, episode_reward=355.80 +/- 109.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=164000, episode_reward=355.00 +/- 259.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=165000, episode_reward=317.40 +/- 218.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=166000, episode_reward=175.20 +/- 248.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=167000, episode_reward=380.20 +/- 111.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=168000, episode_reward=426.20 +/- 133.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=169000, episode_reward=490.40 +/- 212.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=170000, episode_reward=336.60 +/- 94.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=171000, episode_reward=411.60 +/- 237.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=172000, episode_reward=323.20 +/- 258.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=173000, episode_reward=354.00 +/- 294.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=174000, episode_reward=411.60 +/- 106.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=175000, episode_reward=375.00 +/- 156.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=176000, episode_reward=469.40 +/- 147.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=177000, episode_reward=352.40 +/- 167.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=178000, episode_reward=470.20 +/- 266.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=179000, episode_reward=560.20 +/- 215.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=180000, episode_reward=458.40 +/- 153.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=181000, episode_reward=287.80 +/- 186.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=182000, episode_reward=387.80 +/- 154.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=183000, episode_reward=321.20 +/- 252.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=184000, episode_reward=395.80 +/- 113.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=185000, episode_reward=171.60 +/- 107.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=186000, episode_reward=385.80 +/- 140.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=187000, episode_reward=387.60 +/- 64.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=188000, episode_reward=272.40 +/- 240.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=189000, episode_reward=511.40 +/- 166.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=190000, episode_reward=514.20 +/- 176.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=191000, episode_reward=422.60 +/- 276.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=192000, episode_reward=345.60 +/- 169.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=193000, episode_reward=180.80 +/- 175.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=194000, episode_reward=230.00 +/- 235.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=195000, episode_reward=275.00 +/- 236.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=196000, episode_reward=453.20 +/- 165.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=197000, episode_reward=421.00 +/- 118.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=198000, episode_reward=382.60 +/- 204.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=199000, episode_reward=325.40 +/- 241.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=200000, episode_reward=295.60 +/- 135.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=201000, episode_reward=381.60 +/- 157.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=202000, episode_reward=253.20 +/- 128.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=203000, episode_reward=346.40 +/- 237.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=204000, episode_reward=354.60 +/- 168.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=205000, episode_reward=270.40 +/- 130.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=206000, episode_reward=311.20 +/- 159.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=207000, episode_reward=267.60 +/- 126.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=208000, episode_reward=517.60 +/- 146.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=209000, episode_reward=354.80 +/- 217.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=210000, episode_reward=432.00 +/- 102.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=211000, episode_reward=476.60 +/- 161.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=212000, episode_reward=363.20 +/- 103.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=213000, episode_reward=390.20 +/- 172.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=214000, episode_reward=285.20 +/- 304.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=215000, episode_reward=334.40 +/- 124.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=216000, episode_reward=304.00 +/- 200.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=217000, episode_reward=335.20 +/- 84.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=218000, episode_reward=259.80 +/- 262.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=219000, episode_reward=246.40 +/- 127.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=220000, episode_reward=447.80 +/- 171.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=221000, episode_reward=193.40 +/- 162.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=222000, episode_reward=202.80 +/- 184.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=223000, episode_reward=430.40 +/- 118.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=224000, episode_reward=424.60 +/- 151.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=225000, episode_reward=151.60 +/- 227.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=226000, episode_reward=349.80 +/- 200.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=227000, episode_reward=331.00 +/- 81.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=228000, episode_reward=183.20 +/- 198.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=229000, episode_reward=460.80 +/- 122.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=230000, episode_reward=359.40 +/- 317.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=231000, episode_reward=386.40 +/- 199.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=232000, episode_reward=294.00 +/- 208.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=233000, episode_reward=231.40 +/- 220.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=234000, episode_reward=325.00 +/- 175.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=235000, episode_reward=283.80 +/- 147.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=236000, episode_reward=358.60 +/- 107.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=237000, episode_reward=540.40 +/- 139.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=238000, episode_reward=136.80 +/- 309.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=239000, episode_reward=307.20 +/- 147.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=240000, episode_reward=382.00 +/- 57.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=241000, episode_reward=234.60 +/- 190.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=242000, episode_reward=527.60 +/- 127.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=243000, episode_reward=498.80 +/- 41.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=244000, episode_reward=346.80 +/- 140.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=245000, episode_reward=395.60 +/- 78.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=246000, episode_reward=430.60 +/- 243.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=247000, episode_reward=299.60 +/- 136.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=248000, episode_reward=403.40 +/- 332.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=249000, episode_reward=194.40 +/- 155.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=250000, episode_reward=424.80 +/- 166.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=251000, episode_reward=275.80 +/- 221.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=252000, episode_reward=230.40 +/- 126.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=253000, episode_reward=332.80 +/- 252.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=254000, episode_reward=469.80 +/- 183.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=255000, episode_reward=337.00 +/- 244.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=256000, episode_reward=337.80 +/- 225.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=257000, episode_reward=426.00 +/- 263.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=258000, episode_reward=497.40 +/- 229.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=259000, episode_reward=350.20 +/- 130.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=260000, episode_reward=461.00 +/- 228.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=261000, episode_reward=496.60 +/- 151.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=262000, episode_reward=343.40 +/- 183.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=263000, episode_reward=267.40 +/- 68.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=264000, episode_reward=295.40 +/- 288.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=265000, episode_reward=493.40 +/- 100.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=266000, episode_reward=392.20 +/- 176.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=267000, episode_reward=324.40 +/- 260.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=268000, episode_reward=213.00 +/- 77.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=269000, episode_reward=382.00 +/- 165.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=270000, episode_reward=282.80 +/- 143.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=271000, episode_reward=372.00 +/- 106.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=272000, episode_reward=579.60 +/- 76.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=273000, episode_reward=564.20 +/- 93.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=274000, episode_reward=400.60 +/- 155.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=275000, episode_reward=420.00 +/- 224.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=276000, episode_reward=399.80 +/- 155.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=277000, episode_reward=482.60 +/- 114.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=278000, episode_reward=263.00 +/- 248.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=279000, episode_reward=352.80 +/- 126.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=280000, episode_reward=323.40 +/- 188.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=281000, episode_reward=359.40 +/- 199.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=282000, episode_reward=378.40 +/- 73.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=283000, episode_reward=388.80 +/- 161.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=284000, episode_reward=448.80 +/- 137.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=285000, episode_reward=255.40 +/- 232.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=286000, episode_reward=456.60 +/- 130.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=287000, episode_reward=314.60 +/- 42.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=288000, episode_reward=377.60 +/- 142.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=289000, episode_reward=218.40 +/- 59.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=290000, episode_reward=394.80 +/- 137.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=291000, episode_reward=350.00 +/- 178.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=292000, episode_reward=504.80 +/- 157.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=293000, episode_reward=499.80 +/- 150.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=294000, episode_reward=454.20 +/- 199.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=295000, episode_reward=309.60 +/- 246.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=296000, episode_reward=496.60 +/- 141.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=297000, episode_reward=447.00 +/- 270.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=298000, episode_reward=248.40 +/- 120.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=299000, episode_reward=415.40 +/- 213.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=300000, episode_reward=455.00 +/- 129.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=301000, episode_reward=481.20 +/- 133.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=302000, episode_reward=480.00 +/- 65.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=303000, episode_reward=437.60 +/- 43.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=304000, episode_reward=372.20 +/- 334.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=305000, episode_reward=217.80 +/- 270.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=306000, episode_reward=439.20 +/- 22.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=307000, episode_reward=337.40 +/- 135.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=308000, episode_reward=300.80 +/- 157.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=309000, episode_reward=302.40 +/- 50.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=310000, episode_reward=292.80 +/- 264.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=311000, episode_reward=452.40 +/- 166.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=312000, episode_reward=434.60 +/- 281.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=313000, episode_reward=482.80 +/- 231.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=314000, episode_reward=411.20 +/- 229.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=315000, episode_reward=345.40 +/- 309.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=316000, episode_reward=425.00 +/- 219.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=317000, episode_reward=511.80 +/- 225.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=318000, episode_reward=401.60 +/- 188.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=319000, episode_reward=248.80 +/- 224.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=320000, episode_reward=504.00 +/- 150.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=321000, episode_reward=292.20 +/- 117.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=322000, episode_reward=414.80 +/- 99.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=323000, episode_reward=475.60 +/- 159.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=324000, episode_reward=318.40 +/- 223.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=325000, episode_reward=517.80 +/- 173.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=326000, episode_reward=332.80 +/- 136.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=327000, episode_reward=444.80 +/- 127.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=328000, episode_reward=456.60 +/- 145.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=329000, episode_reward=337.60 +/- 165.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=330000, episode_reward=400.40 +/- 145.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=331000, episode_reward=448.20 +/- 227.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=332000, episode_reward=389.00 +/- 143.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=333000, episode_reward=406.80 +/- 140.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=334000, episode_reward=370.20 +/- 238.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=335000, episode_reward=341.00 +/- 106.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=336000, episode_reward=342.40 +/- 115.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=337000, episode_reward=393.60 +/- 161.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=338000, episode_reward=298.80 +/- 113.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=339000, episode_reward=278.00 +/- 167.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=340000, episode_reward=308.40 +/- 94.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=341000, episode_reward=449.20 +/- 195.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=342000, episode_reward=445.40 +/- 91.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=343000, episode_reward=500.20 +/- 252.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=344000, episode_reward=461.20 +/- 152.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=345000, episode_reward=428.20 +/- 204.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=346000, episode_reward=223.60 +/- 192.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=347000, episode_reward=269.80 +/- 186.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=348000, episode_reward=338.60 +/- 181.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=349000, episode_reward=321.80 +/- 76.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=350000, episode_reward=316.00 +/- 252.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=351000, episode_reward=318.20 +/- 274.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=352000, episode_reward=438.20 +/- 137.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=353000, episode_reward=221.00 +/- 266.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=354000, episode_reward=251.60 +/- 155.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=355000, episode_reward=231.40 +/- 98.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=356000, episode_reward=392.60 +/- 170.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=357000, episode_reward=342.60 +/- 101.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=358000, episode_reward=295.00 +/- 252.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=359000, episode_reward=378.00 +/- 136.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=360000, episode_reward=476.40 +/- 219.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=361000, episode_reward=349.00 +/- 205.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=362000, episode_reward=474.00 +/- 153.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=363000, episode_reward=304.80 +/- 283.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=364000, episode_reward=267.80 +/- 107.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=365000, episode_reward=321.40 +/- 233.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=366000, episode_reward=414.60 +/- 167.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=367000, episode_reward=457.60 +/- 108.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=368000, episode_reward=342.60 +/- 142.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=369000, episode_reward=379.20 +/- 167.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=370000, episode_reward=344.40 +/- 140.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=371000, episode_reward=436.60 +/- 206.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=372000, episode_reward=325.20 +/- 179.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=373000, episode_reward=294.20 +/- 189.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=374000, episode_reward=606.20 +/- 129.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=375000, episode_reward=373.60 +/- 143.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=376000, episode_reward=383.40 +/- 191.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=377000, episode_reward=519.60 +/- 124.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=378000, episode_reward=364.40 +/- 251.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=379000, episode_reward=320.20 +/- 248.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=380000, episode_reward=505.40 +/- 65.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=381000, episode_reward=312.20 +/- 157.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=382000, episode_reward=508.00 +/- 137.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=383000, episode_reward=422.20 +/- 101.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=384000, episode_reward=513.60 +/- 133.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=385000, episode_reward=495.40 +/- 214.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=386000, episode_reward=384.20 +/- 239.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=387000, episode_reward=433.60 +/- 107.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=388000, episode_reward=642.80 +/- 101.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=389000, episode_reward=634.00 +/- 81.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=390000, episode_reward=395.40 +/- 246.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=391000, episode_reward=577.00 +/- 146.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=392000, episode_reward=552.60 +/- 150.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=393000, episode_reward=524.60 +/- 163.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=394000, episode_reward=560.80 +/- 108.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=395000, episode_reward=293.60 +/- 181.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=396000, episode_reward=737.20 +/- 189.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=397000, episode_reward=629.60 +/- 138.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=398000, episode_reward=744.20 +/- 292.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=399000, episode_reward=686.00 +/- 290.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=400000, episode_reward=641.20 +/- 160.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=401000, episode_reward=371.00 +/- 261.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=402000, episode_reward=514.20 +/- 143.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=403000, episode_reward=456.80 +/- 223.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=404000, episode_reward=484.20 +/- 175.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=405000, episode_reward=404.40 +/- 210.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=406000, episode_reward=512.00 +/- 326.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=407000, episode_reward=666.80 +/- 183.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=408000, episode_reward=666.20 +/- 251.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=409000, episode_reward=566.00 +/- 163.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=410000, episode_reward=295.20 +/- 134.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=411000, episode_reward=584.60 +/- 197.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=412000, episode_reward=517.20 +/- 171.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=413000, episode_reward=639.80 +/- 262.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=414000, episode_reward=510.60 +/- 162.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=415000, episode_reward=665.20 +/- 192.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=416000, episode_reward=607.00 +/- 190.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=417000, episode_reward=196.40 +/- 175.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=418000, episode_reward=201.60 +/- 58.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=419000, episode_reward=433.40 +/- 85.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=420000, episode_reward=714.20 +/- 261.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=421000, episode_reward=590.80 +/- 244.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=422000, episode_reward=558.60 +/- 104.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=423000, episode_reward=584.20 +/- 162.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=424000, episode_reward=488.20 +/- 97.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=425000, episode_reward=528.60 +/- 114.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=426000, episode_reward=795.20 +/- 167.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=427000, episode_reward=494.20 +/- 190.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=428000, episode_reward=526.40 +/- 78.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=429000, episode_reward=595.00 +/- 207.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=430000, episode_reward=505.60 +/- 219.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=431000, episode_reward=600.20 +/- 295.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=432000, episode_reward=793.40 +/- 129.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=433000, episode_reward=558.40 +/- 211.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=434000, episode_reward=567.40 +/- 212.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=435000, episode_reward=884.40 +/- 56.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=436000, episode_reward=721.20 +/- 176.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=437000, episode_reward=435.80 +/- 122.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=438000, episode_reward=528.00 +/- 177.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=439000, episode_reward=695.40 +/- 256.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=440000, episode_reward=534.40 +/- 179.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=441000, episode_reward=700.00 +/- 184.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=442000, episode_reward=354.60 +/- 170.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=443000, episode_reward=641.60 +/- 153.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=444000, episode_reward=443.80 +/- 186.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=445000, episode_reward=415.20 +/- 185.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=446000, episode_reward=514.20 +/- 244.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=447000, episode_reward=754.40 +/- 302.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=448000, episode_reward=639.80 +/- 203.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=449000, episode_reward=954.20 +/- 124.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=450000, episode_reward=970.60 +/- 269.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=451000, episode_reward=815.40 +/- 217.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=452000, episode_reward=365.80 +/- 158.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=453000, episode_reward=708.80 +/- 84.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=454000, episode_reward=721.00 +/- 165.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=455000, episode_reward=644.00 +/- 157.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=456000, episode_reward=823.40 +/- 82.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=457000, episode_reward=899.80 +/- 265.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=458000, episode_reward=620.00 +/- 165.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=459000, episode_reward=557.00 +/- 111.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=460000, episode_reward=1087.60 +/- 182.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=461000, episode_reward=794.80 +/- 178.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=462000, episode_reward=482.00 +/- 66.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=463000, episode_reward=441.40 +/- 80.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=464000, episode_reward=711.80 +/- 260.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=465000, episode_reward=859.00 +/- 135.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=466000, episode_reward=626.80 +/- 57.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=467000, episode_reward=644.20 +/- 213.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=468000, episode_reward=879.00 +/- 320.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=469000, episode_reward=731.60 +/- 276.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=470000, episode_reward=1151.00 +/- 187.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=471000, episode_reward=625.60 +/- 180.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=472000, episode_reward=658.40 +/- 334.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=473000, episode_reward=698.20 +/- 327.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=474000, episode_reward=828.20 +/- 249.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=475000, episode_reward=1119.60 +/- 141.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=476000, episode_reward=1180.40 +/- 246.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=477000, episode_reward=1126.80 +/- 235.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=478000, episode_reward=1131.80 +/- 267.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=479000, episode_reward=954.20 +/- 94.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=480000, episode_reward=973.40 +/- 355.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=481000, episode_reward=1050.80 +/- 138.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=482000, episode_reward=1053.60 +/- 183.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=483000, episode_reward=1030.80 +/- 379.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=484000, episode_reward=600.80 +/- 226.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=485000, episode_reward=996.20 +/- 214.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=486000, episode_reward=1223.40 +/- 414.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=487000, episode_reward=1301.20 +/- 192.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=488000, episode_reward=765.60 +/- 290.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=489000, episode_reward=614.20 +/- 178.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=490000, episode_reward=1157.40 +/- 188.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=491000, episode_reward=1373.80 +/- 180.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=492000, episode_reward=1047.00 +/- 132.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=493000, episode_reward=1159.00 +/- 242.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=494000, episode_reward=1045.00 +/- 153.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=495000, episode_reward=752.80 +/- 192.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=496000, episode_reward=1060.80 +/- 251.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=497000, episode_reward=1282.00 +/- 155.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=498000, episode_reward=777.20 +/- 113.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=499000, episode_reward=927.60 +/- 188.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=500000, episode_reward=1178.80 +/- 271.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Gamma = 0.99, Learning Rate = 3e-05, Activation Function = Tanh, Neurons = 8, Process noise = 0.1, Measurement noise = 0.0\n",
      "Eval num_timesteps=1000, episode_reward=-3518.00 +/- 67.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=-3588.00 +/- 86.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=3000, episode_reward=-3505.00 +/- 100.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4000, episode_reward=-3510.00 +/- 110.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=5000, episode_reward=-3562.00 +/- 29.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=6000, episode_reward=-3495.00 +/- 93.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=7000, episode_reward=-3471.00 +/- 111.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=8000, episode_reward=-3475.00 +/- 99.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=9000, episode_reward=-3471.00 +/- 48.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=10000, episode_reward=-3422.00 +/- 45.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=11000, episode_reward=-3498.00 +/- 72.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=12000, episode_reward=-3462.00 +/- 42.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=13000, episode_reward=-3544.00 +/- 110.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=14000, episode_reward=-3481.00 +/- 59.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=15000, episode_reward=-3514.00 +/- 51.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=16000, episode_reward=-3498.00 +/- 61.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=17000, episode_reward=-3543.00 +/- 35.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=18000, episode_reward=-3525.00 +/- 55.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=19000, episode_reward=-3454.00 +/- 106.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=20000, episode_reward=-3495.00 +/- 122.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=21000, episode_reward=-3510.00 +/- 89.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=22000, episode_reward=-3457.00 +/- 69.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=23000, episode_reward=-3482.00 +/- 86.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=24000, episode_reward=-3464.00 +/- 30.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=25000, episode_reward=-3468.00 +/- 81.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=26000, episode_reward=-3539.00 +/- 69.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=27000, episode_reward=-3488.00 +/- 55.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=28000, episode_reward=-3511.00 +/- 89.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=29000, episode_reward=-3503.00 +/- 79.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=30000, episode_reward=-3484.00 +/- 55.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=31000, episode_reward=-3395.00 +/- 55.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=32000, episode_reward=-3442.00 +/- 66.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=33000, episode_reward=-3475.00 +/- 84.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=34000, episode_reward=-3558.00 +/- 74.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=35000, episode_reward=-3528.00 +/- 141.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=36000, episode_reward=-3508.00 +/- 48.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=37000, episode_reward=-3489.00 +/- 30.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=38000, episode_reward=-3513.00 +/- 30.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=39000, episode_reward=-3497.00 +/- 75.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=40000, episode_reward=-3464.00 +/- 33.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=41000, episode_reward=-3568.00 +/- 60.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=42000, episode_reward=-3462.00 +/- 76.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=43000, episode_reward=-3464.00 +/- 99.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=44000, episode_reward=-3488.00 +/- 101.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=45000, episode_reward=-3489.00 +/- 56.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=46000, episode_reward=-3491.00 +/- 61.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=47000, episode_reward=-3577.00 +/- 95.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=48000, episode_reward=-3419.00 +/- 36.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=49000, episode_reward=-3553.00 +/- 135.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=50000, episode_reward=-3474.00 +/- 36.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=51000, episode_reward=-3077.60 +/- 117.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=52000, episode_reward=-1902.60 +/- 175.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=53000, episode_reward=-1626.60 +/- 123.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=54000, episode_reward=-1133.80 +/- 107.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=55000, episode_reward=-1101.60 +/- 93.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=56000, episode_reward=-1171.60 +/- 68.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=57000, episode_reward=-1120.40 +/- 157.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=58000, episode_reward=-1083.80 +/- 94.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=59000, episode_reward=-1232.20 +/- 98.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=60000, episode_reward=-1281.40 +/- 131.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=61000, episode_reward=-2134.60 +/- 880.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=62000, episode_reward=-4938.20 +/- 1608.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=63000, episode_reward=-3663.80 +/- 2733.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=64000, episode_reward=-1318.60 +/- 178.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=65000, episode_reward=-1401.60 +/- 316.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=66000, episode_reward=-1604.40 +/- 265.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=67000, episode_reward=-1069.40 +/- 152.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=68000, episode_reward=-1165.40 +/- 208.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=69000, episode_reward=-958.60 +/- 171.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=70000, episode_reward=-937.20 +/- 137.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=71000, episode_reward=-906.60 +/- 192.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=72000, episode_reward=-833.80 +/- 155.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=73000, episode_reward=-624.20 +/- 166.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=74000, episode_reward=-478.00 +/- 125.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=75000, episode_reward=-363.60 +/- 103.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=76000, episode_reward=-291.20 +/- 79.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=77000, episode_reward=-296.80 +/- 165.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=78000, episode_reward=-523.80 +/- 257.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=79000, episode_reward=-506.00 +/- 213.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=80000, episode_reward=-332.80 +/- 121.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=81000, episode_reward=-758.40 +/- 151.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=82000, episode_reward=-973.60 +/- 451.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=83000, episode_reward=-1670.80 +/- 369.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=84000, episode_reward=-2464.00 +/- 326.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=85000, episode_reward=-5199.80 +/- 2028.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=86000, episode_reward=-6081.60 +/- 961.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=87000, episode_reward=-5889.20 +/- 2252.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=88000, episode_reward=-4300.80 +/- 1456.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=89000, episode_reward=-6365.00 +/- 2448.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=90000, episode_reward=-2141.40 +/- 466.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=91000, episode_reward=-1965.20 +/- 604.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=92000, episode_reward=-2000.00 +/- 202.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=93000, episode_reward=-1432.80 +/- 464.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=94000, episode_reward=-205.40 +/- 614.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=95000, episode_reward=-319.20 +/- 421.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=96000, episode_reward=-108.60 +/- 396.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=97000, episode_reward=-31.60 +/- 411.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=98000, episode_reward=-148.20 +/- 477.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=99000, episode_reward=-209.80 +/- 358.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=100000, episode_reward=-379.40 +/- 260.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=101000, episode_reward=274.20 +/- 120.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=102000, episode_reward=-186.40 +/- 442.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=103000, episode_reward=46.20 +/- 249.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=104000, episode_reward=219.00 +/- 159.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=105000, episode_reward=-41.60 +/- 307.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=106000, episode_reward=-54.00 +/- 207.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=107000, episode_reward=433.40 +/- 151.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=108000, episode_reward=144.40 +/- 174.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=109000, episode_reward=303.80 +/- 286.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=110000, episode_reward=-389.40 +/- 1014.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=111000, episode_reward=-684.20 +/- 741.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=112000, episode_reward=-640.00 +/- 834.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=113000, episode_reward=-233.40 +/- 468.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=114000, episode_reward=-502.60 +/- 800.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=115000, episode_reward=103.60 +/- 228.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=116000, episode_reward=-94.80 +/- 395.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=117000, episode_reward=-506.80 +/- 369.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=118000, episode_reward=-146.40 +/- 350.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=119000, episode_reward=-660.80 +/- 648.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=120000, episode_reward=-114.00 +/- 418.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=121000, episode_reward=-339.80 +/- 676.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=122000, episode_reward=-217.20 +/- 410.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=123000, episode_reward=-32.80 +/- 511.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=124000, episode_reward=20.20 +/- 311.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=125000, episode_reward=47.80 +/- 233.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=126000, episode_reward=-122.80 +/- 528.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=127000, episode_reward=-417.80 +/- 547.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=128000, episode_reward=-46.00 +/- 397.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=129000, episode_reward=-508.40 +/- 1237.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=130000, episode_reward=-193.80 +/- 510.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=131000, episode_reward=-177.60 +/- 377.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=132000, episode_reward=-615.40 +/- 625.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=133000, episode_reward=311.20 +/- 242.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=134000, episode_reward=297.60 +/- 203.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=135000, episode_reward=-143.80 +/- 548.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=136000, episode_reward=-389.00 +/- 672.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=137000, episode_reward=82.40 +/- 423.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=138000, episode_reward=-936.20 +/- 510.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=139000, episode_reward=-201.60 +/- 598.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=140000, episode_reward=85.00 +/- 299.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=141000, episode_reward=18.60 +/- 238.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=142000, episode_reward=-12.40 +/- 186.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=143000, episode_reward=-315.20 +/- 795.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=144000, episode_reward=-116.60 +/- 420.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=145000, episode_reward=-658.40 +/- 548.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=146000, episode_reward=215.80 +/- 341.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=147000, episode_reward=-21.60 +/- 449.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=148000, episode_reward=-30.80 +/- 416.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=149000, episode_reward=-562.40 +/- 724.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=150000, episode_reward=-127.20 +/- 427.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=151000, episode_reward=-155.00 +/- 525.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=152000, episode_reward=-46.20 +/- 489.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=153000, episode_reward=-365.20 +/- 378.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=154000, episode_reward=15.80 +/- 223.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=155000, episode_reward=-824.80 +/- 862.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=156000, episode_reward=47.60 +/- 249.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=157000, episode_reward=59.60 +/- 152.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=158000, episode_reward=122.00 +/- 382.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=159000, episode_reward=-32.40 +/- 281.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=160000, episode_reward=-146.60 +/- 657.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=161000, episode_reward=399.20 +/- 175.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=162000, episode_reward=277.20 +/- 168.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=163000, episode_reward=145.20 +/- 172.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=164000, episode_reward=-80.40 +/- 630.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=165000, episode_reward=-336.40 +/- 595.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=166000, episode_reward=-289.00 +/- 586.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=167000, episode_reward=38.00 +/- 240.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=168000, episode_reward=244.60 +/- 239.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=169000, episode_reward=-192.00 +/- 276.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=170000, episode_reward=-227.80 +/- 321.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=171000, episode_reward=50.20 +/- 395.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=172000, episode_reward=45.00 +/- 287.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=173000, episode_reward=265.00 +/- 267.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=174000, episode_reward=-99.60 +/- 326.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=175000, episode_reward=-211.80 +/- 278.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=176000, episode_reward=-1626.00 +/- 3399.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=177000, episode_reward=-50.80 +/- 409.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=178000, episode_reward=-551.00 +/- 1044.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=179000, episode_reward=194.80 +/- 39.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=180000, episode_reward=-142.00 +/- 360.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=181000, episode_reward=96.60 +/- 326.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=182000, episode_reward=-105.20 +/- 398.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=183000, episode_reward=-449.40 +/- 564.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=184000, episode_reward=268.20 +/- 281.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=185000, episode_reward=-135.40 +/- 549.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=186000, episode_reward=-205.60 +/- 672.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=187000, episode_reward=-14.20 +/- 388.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=188000, episode_reward=-263.40 +/- 837.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=189000, episode_reward=-0.80 +/- 390.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=190000, episode_reward=-329.80 +/- 642.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=191000, episode_reward=124.40 +/- 144.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=192000, episode_reward=34.00 +/- 178.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=193000, episode_reward=-252.40 +/- 518.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=194000, episode_reward=34.60 +/- 210.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=195000, episode_reward=36.20 +/- 123.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=196000, episode_reward=-234.80 +/- 820.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=197000, episode_reward=-135.20 +/- 230.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=198000, episode_reward=-84.20 +/- 411.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=199000, episode_reward=-347.00 +/- 519.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=200000, episode_reward=-353.80 +/- 465.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=201000, episode_reward=72.20 +/- 416.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=202000, episode_reward=207.60 +/- 206.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=203000, episode_reward=62.80 +/- 200.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=204000, episode_reward=-365.40 +/- 524.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=205000, episode_reward=-105.20 +/- 362.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=206000, episode_reward=109.80 +/- 338.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=207000, episode_reward=60.20 +/- 356.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=208000, episode_reward=42.60 +/- 121.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=209000, episode_reward=-193.40 +/- 234.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=210000, episode_reward=67.80 +/- 464.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=211000, episode_reward=-124.40 +/- 284.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=212000, episode_reward=109.20 +/- 92.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=213000, episode_reward=-158.00 +/- 366.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=214000, episode_reward=-141.00 +/- 107.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=215000, episode_reward=267.80 +/- 141.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=216000, episode_reward=-86.40 +/- 292.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=217000, episode_reward=35.20 +/- 317.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=218000, episode_reward=-75.80 +/- 522.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=219000, episode_reward=-108.00 +/- 407.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=220000, episode_reward=183.20 +/- 229.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=221000, episode_reward=200.00 +/- 173.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=222000, episode_reward=-80.60 +/- 345.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=223000, episode_reward=177.80 +/- 175.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=224000, episode_reward=385.80 +/- 223.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=225000, episode_reward=66.80 +/- 241.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=226000, episode_reward=-129.60 +/- 536.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=227000, episode_reward=-1.20 +/- 224.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=228000, episode_reward=98.40 +/- 203.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=229000, episode_reward=-152.80 +/- 147.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=230000, episode_reward=184.60 +/- 242.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=231000, episode_reward=208.60 +/- 363.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=232000, episode_reward=101.60 +/- 120.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=233000, episode_reward=43.00 +/- 137.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=234000, episode_reward=309.20 +/- 262.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=235000, episode_reward=115.80 +/- 300.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=236000, episode_reward=280.40 +/- 190.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=237000, episode_reward=134.40 +/- 202.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=238000, episode_reward=31.60 +/- 247.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=239000, episode_reward=73.60 +/- 245.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=240000, episode_reward=115.80 +/- 165.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=241000, episode_reward=-188.40 +/- 169.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=242000, episode_reward=-65.20 +/- 253.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=243000, episode_reward=-75.20 +/- 502.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=244000, episode_reward=36.40 +/- 228.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=245000, episode_reward=194.40 +/- 160.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=246000, episode_reward=-28.20 +/- 147.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=247000, episode_reward=129.80 +/- 151.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=248000, episode_reward=160.20 +/- 193.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=249000, episode_reward=182.60 +/- 247.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=250000, episode_reward=124.80 +/- 56.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=251000, episode_reward=-76.60 +/- 220.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=252000, episode_reward=-66.20 +/- 123.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=253000, episode_reward=208.00 +/- 266.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=254000, episode_reward=-80.00 +/- 152.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=255000, episode_reward=96.00 +/- 108.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=256000, episode_reward=93.60 +/- 238.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=257000, episode_reward=158.00 +/- 216.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=258000, episode_reward=309.40 +/- 127.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=259000, episode_reward=178.00 +/- 331.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=260000, episode_reward=-3.80 +/- 176.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=261000, episode_reward=79.40 +/- 320.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=262000, episode_reward=-4.00 +/- 224.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=263000, episode_reward=123.00 +/- 181.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=264000, episode_reward=-260.40 +/- 434.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=265000, episode_reward=373.20 +/- 175.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=266000, episode_reward=180.60 +/- 209.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=267000, episode_reward=28.00 +/- 208.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=268000, episode_reward=50.20 +/- 168.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=269000, episode_reward=-94.40 +/- 320.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=270000, episode_reward=203.80 +/- 262.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=271000, episode_reward=43.40 +/- 348.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=272000, episode_reward=-42.80 +/- 410.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=273000, episode_reward=-51.40 +/- 348.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=274000, episode_reward=200.00 +/- 183.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=275000, episode_reward=145.20 +/- 179.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=276000, episode_reward=110.20 +/- 248.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=277000, episode_reward=65.20 +/- 174.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=278000, episode_reward=258.40 +/- 134.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=279000, episode_reward=78.20 +/- 313.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=280000, episode_reward=-78.60 +/- 245.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=281000, episode_reward=255.40 +/- 293.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=282000, episode_reward=-33.60 +/- 169.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=283000, episode_reward=-226.80 +/- 349.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=284000, episode_reward=188.60 +/- 209.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=285000, episode_reward=97.60 +/- 314.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=286000, episode_reward=133.00 +/- 328.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=287000, episode_reward=101.80 +/- 219.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=288000, episode_reward=-2.40 +/- 192.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=289000, episode_reward=-31.40 +/- 163.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=290000, episode_reward=160.00 +/- 303.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=291000, episode_reward=-43.40 +/- 467.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=292000, episode_reward=-13.40 +/- 302.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=293000, episode_reward=-171.80 +/- 334.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=294000, episode_reward=-241.80 +/- 441.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=295000, episode_reward=164.00 +/- 407.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=296000, episode_reward=166.40 +/- 236.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=297000, episode_reward=226.40 +/- 297.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=298000, episode_reward=297.40 +/- 137.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=299000, episode_reward=121.80 +/- 220.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=300000, episode_reward=131.80 +/- 231.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=301000, episode_reward=16.40 +/- 292.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=302000, episode_reward=-140.00 +/- 254.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=303000, episode_reward=79.00 +/- 372.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=304000, episode_reward=219.20 +/- 399.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=305000, episode_reward=122.00 +/- 342.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=306000, episode_reward=137.80 +/- 121.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=307000, episode_reward=142.60 +/- 210.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=308000, episode_reward=127.40 +/- 258.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=309000, episode_reward=-29.00 +/- 418.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=310000, episode_reward=256.20 +/- 164.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=311000, episode_reward=-8.00 +/- 458.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=312000, episode_reward=79.20 +/- 239.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=313000, episode_reward=68.60 +/- 226.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=314000, episode_reward=146.60 +/- 232.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=315000, episode_reward=289.40 +/- 143.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=316000, episode_reward=337.80 +/- 170.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=317000, episode_reward=131.40 +/- 184.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=318000, episode_reward=357.80 +/- 336.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=319000, episode_reward=172.80 +/- 188.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=320000, episode_reward=237.80 +/- 190.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=321000, episode_reward=95.20 +/- 317.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=322000, episode_reward=137.60 +/- 175.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=323000, episode_reward=33.20 +/- 160.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=324000, episode_reward=281.20 +/- 162.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=325000, episode_reward=209.60 +/- 206.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=326000, episode_reward=-170.80 +/- 317.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=327000, episode_reward=198.00 +/- 216.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=328000, episode_reward=31.20 +/- 141.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=329000, episode_reward=-106.00 +/- 625.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=330000, episode_reward=275.60 +/- 291.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=331000, episode_reward=292.80 +/- 129.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=332000, episode_reward=-48.20 +/- 411.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=333000, episode_reward=167.20 +/- 179.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=334000, episode_reward=82.40 +/- 188.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=335000, episode_reward=339.00 +/- 77.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=336000, episode_reward=187.60 +/- 170.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=337000, episode_reward=118.00 +/- 85.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=338000, episode_reward=-23.00 +/- 322.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=339000, episode_reward=107.20 +/- 274.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=340000, episode_reward=63.40 +/- 147.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=341000, episode_reward=-130.80 +/- 402.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=342000, episode_reward=114.20 +/- 82.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=343000, episode_reward=147.00 +/- 267.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=344000, episode_reward=-112.60 +/- 286.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=345000, episode_reward=169.80 +/- 160.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=346000, episode_reward=38.40 +/- 401.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=347000, episode_reward=198.00 +/- 298.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=348000, episode_reward=179.60 +/- 332.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=349000, episode_reward=-291.60 +/- 386.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=350000, episode_reward=195.80 +/- 367.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=351000, episode_reward=188.40 +/- 108.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=352000, episode_reward=273.00 +/- 211.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=353000, episode_reward=163.00 +/- 380.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=354000, episode_reward=-86.40 +/- 439.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=355000, episode_reward=276.00 +/- 195.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=356000, episode_reward=-152.60 +/- 457.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=357000, episode_reward=187.80 +/- 260.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=358000, episode_reward=139.60 +/- 190.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=359000, episode_reward=125.60 +/- 183.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=360000, episode_reward=-44.40 +/- 299.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=361000, episode_reward=27.60 +/- 273.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=362000, episode_reward=267.80 +/- 201.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=363000, episode_reward=8.80 +/- 224.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=364000, episode_reward=283.20 +/- 219.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=365000, episode_reward=70.80 +/- 413.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=366000, episode_reward=29.60 +/- 305.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=367000, episode_reward=-24.40 +/- 388.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=368000, episode_reward=93.20 +/- 156.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=369000, episode_reward=113.80 +/- 277.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=370000, episode_reward=111.80 +/- 426.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=371000, episode_reward=-97.20 +/- 343.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=372000, episode_reward=221.20 +/- 211.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=373000, episode_reward=-114.00 +/- 475.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=374000, episode_reward=219.40 +/- 353.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=375000, episode_reward=107.20 +/- 163.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=376000, episode_reward=244.80 +/- 195.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=377000, episode_reward=235.60 +/- 170.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=378000, episode_reward=144.80 +/- 271.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=379000, episode_reward=-109.40 +/- 204.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=380000, episode_reward=270.20 +/- 356.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=381000, episode_reward=322.80 +/- 68.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=382000, episode_reward=-80.00 +/- 308.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=383000, episode_reward=131.60 +/- 340.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=384000, episode_reward=47.40 +/- 221.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=385000, episode_reward=142.60 +/- 288.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=386000, episode_reward=60.60 +/- 357.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=387000, episode_reward=-77.80 +/- 280.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=388000, episode_reward=207.60 +/- 142.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=389000, episode_reward=-171.80 +/- 311.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=390000, episode_reward=-280.40 +/- 428.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=391000, episode_reward=72.60 +/- 233.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=392000, episode_reward=258.00 +/- 224.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=393000, episode_reward=353.00 +/- 248.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=394000, episode_reward=136.00 +/- 291.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=395000, episode_reward=25.40 +/- 180.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=396000, episode_reward=174.00 +/- 240.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=397000, episode_reward=-90.60 +/- 261.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=398000, episode_reward=162.20 +/- 353.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=399000, episode_reward=90.40 +/- 469.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=400000, episode_reward=238.40 +/- 206.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=401000, episode_reward=107.60 +/- 419.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=402000, episode_reward=248.80 +/- 254.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=403000, episode_reward=234.00 +/- 248.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=404000, episode_reward=264.00 +/- 225.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=405000, episode_reward=15.60 +/- 396.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=406000, episode_reward=39.00 +/- 212.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=407000, episode_reward=159.00 +/- 142.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=408000, episode_reward=237.20 +/- 211.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=409000, episode_reward=210.60 +/- 142.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=410000, episode_reward=1.20 +/- 181.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=411000, episode_reward=158.40 +/- 176.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=412000, episode_reward=117.40 +/- 290.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=413000, episode_reward=26.80 +/- 249.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=414000, episode_reward=183.20 +/- 387.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=415000, episode_reward=277.20 +/- 110.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=416000, episode_reward=193.60 +/- 254.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=417000, episode_reward=59.80 +/- 250.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=418000, episode_reward=176.20 +/- 223.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=419000, episode_reward=37.20 +/- 203.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=420000, episode_reward=185.20 +/- 283.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=421000, episode_reward=43.60 +/- 147.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=422000, episode_reward=103.80 +/- 405.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=423000, episode_reward=159.60 +/- 224.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=424000, episode_reward=-41.40 +/- 195.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=425000, episode_reward=-189.20 +/- 340.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=426000, episode_reward=166.60 +/- 202.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=427000, episode_reward=-184.00 +/- 710.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=428000, episode_reward=269.00 +/- 173.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=429000, episode_reward=270.40 +/- 207.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=430000, episode_reward=26.00 +/- 106.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=431000, episode_reward=321.40 +/- 201.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=432000, episode_reward=119.60 +/- 353.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=433000, episode_reward=440.40 +/- 151.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=434000, episode_reward=104.80 +/- 186.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=435000, episode_reward=22.60 +/- 234.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=436000, episode_reward=293.20 +/- 154.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=437000, episode_reward=246.40 +/- 189.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=438000, episode_reward=238.00 +/- 179.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=439000, episode_reward=49.20 +/- 346.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=440000, episode_reward=235.00 +/- 228.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=441000, episode_reward=131.80 +/- 263.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=442000, episode_reward=-55.40 +/- 218.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=443000, episode_reward=-113.20 +/- 70.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=444000, episode_reward=64.40 +/- 284.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=445000, episode_reward=114.20 +/- 138.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=446000, episode_reward=-50.80 +/- 286.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=447000, episode_reward=123.20 +/- 232.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=448000, episode_reward=242.40 +/- 200.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=449000, episode_reward=-160.40 +/- 581.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=450000, episode_reward=103.80 +/- 252.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=451000, episode_reward=157.80 +/- 171.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=452000, episode_reward=156.00 +/- 92.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=453000, episode_reward=317.00 +/- 253.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=454000, episode_reward=154.20 +/- 239.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=455000, episode_reward=322.20 +/- 180.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=456000, episode_reward=143.00 +/- 142.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=457000, episode_reward=195.20 +/- 100.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=458000, episode_reward=185.20 +/- 196.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=459000, episode_reward=208.60 +/- 175.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=460000, episode_reward=73.20 +/- 206.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=461000, episode_reward=206.40 +/- 195.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=462000, episode_reward=254.60 +/- 161.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=463000, episode_reward=173.40 +/- 179.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=464000, episode_reward=-11.40 +/- 307.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=465000, episode_reward=390.60 +/- 182.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=466000, episode_reward=187.40 +/- 318.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=467000, episode_reward=285.60 +/- 124.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=468000, episode_reward=295.60 +/- 234.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=469000, episode_reward=63.40 +/- 245.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=470000, episode_reward=5.60 +/- 424.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=471000, episode_reward=227.80 +/- 285.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=472000, episode_reward=326.20 +/- 100.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=473000, episode_reward=-79.20 +/- 155.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=474000, episode_reward=268.80 +/- 216.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=475000, episode_reward=147.80 +/- 532.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=476000, episode_reward=223.40 +/- 279.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=477000, episode_reward=51.20 +/- 231.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=478000, episode_reward=415.60 +/- 89.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=479000, episode_reward=239.80 +/- 289.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=480000, episode_reward=57.60 +/- 156.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=481000, episode_reward=292.20 +/- 241.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=482000, episode_reward=58.60 +/- 232.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=483000, episode_reward=405.80 +/- 186.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=484000, episode_reward=94.80 +/- 183.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=485000, episode_reward=-6.00 +/- 519.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=486000, episode_reward=240.80 +/- 169.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=487000, episode_reward=-13.60 +/- 397.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=488000, episode_reward=156.00 +/- 137.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=489000, episode_reward=219.40 +/- 194.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=490000, episode_reward=245.80 +/- 96.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=491000, episode_reward=30.40 +/- 262.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=492000, episode_reward=128.00 +/- 137.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=493000, episode_reward=-68.60 +/- 321.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=494000, episode_reward=109.40 +/- 151.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=495000, episode_reward=398.20 +/- 122.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=496000, episode_reward=223.40 +/- 221.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=497000, episode_reward=345.40 +/- 296.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=498000, episode_reward=80.60 +/- 172.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=499000, episode_reward=209.60 +/- 160.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=500000, episode_reward=169.80 +/- 275.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Gamma = 0.99, Learning Rate = 3e-05, Activation Function = Tanh, Neurons = 32, Process noise = 0.1, Measurement noise = 0.0\n",
      "Eval num_timesteps=1000, episode_reward=-1624.40 +/- 124.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=-1524.80 +/- 122.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3000, episode_reward=-1638.40 +/- 98.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=4000, episode_reward=-1596.20 +/- 70.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=5000, episode_reward=-1503.20 +/- 100.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=6000, episode_reward=-1554.40 +/- 103.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=7000, episode_reward=-1539.40 +/- 131.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=8000, episode_reward=-1478.20 +/- 104.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=9000, episode_reward=-1413.20 +/- 110.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=10000, episode_reward=-1559.20 +/- 107.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=11000, episode_reward=-1661.80 +/- 63.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=12000, episode_reward=-1453.20 +/- 36.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=13000, episode_reward=-1565.40 +/- 52.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=14000, episode_reward=-1459.00 +/- 68.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=15000, episode_reward=-1547.40 +/- 129.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=16000, episode_reward=-1459.60 +/- 152.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=17000, episode_reward=-1500.00 +/- 44.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=18000, episode_reward=-1533.80 +/- 107.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=19000, episode_reward=-1539.80 +/- 111.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=20000, episode_reward=-1428.20 +/- 32.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=21000, episode_reward=-1545.60 +/- 110.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=22000, episode_reward=-1670.60 +/- 168.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=23000, episode_reward=-1490.80 +/- 96.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=24000, episode_reward=-1603.40 +/- 81.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=25000, episode_reward=-1555.80 +/- 55.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=26000, episode_reward=-1479.20 +/- 51.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=27000, episode_reward=-1474.80 +/- 115.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=28000, episode_reward=-1459.00 +/- 112.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=29000, episode_reward=-1564.60 +/- 143.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=30000, episode_reward=-1465.40 +/- 57.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=31000, episode_reward=-1523.40 +/- 84.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=32000, episode_reward=-1582.00 +/- 126.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=33000, episode_reward=-1573.20 +/- 114.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=34000, episode_reward=-1472.60 +/- 141.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=35000, episode_reward=-1514.60 +/- 146.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=36000, episode_reward=-1539.20 +/- 83.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=37000, episode_reward=-1534.40 +/- 117.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=38000, episode_reward=-1524.40 +/- 104.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=39000, episode_reward=-1555.80 +/- 135.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=40000, episode_reward=-1523.60 +/- 221.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=41000, episode_reward=-1564.80 +/- 108.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=42000, episode_reward=-1655.60 +/- 91.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=43000, episode_reward=-1549.80 +/- 142.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=44000, episode_reward=-1644.40 +/- 114.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=45000, episode_reward=-1597.00 +/- 83.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=46000, episode_reward=-1719.80 +/- 119.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=47000, episode_reward=-1502.60 +/- 128.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=48000, episode_reward=-1523.00 +/- 113.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=49000, episode_reward=-1526.00 +/- 148.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=50000, episode_reward=-1505.00 +/- 138.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=51000, episode_reward=-1597.80 +/- 165.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=52000, episode_reward=-1272.20 +/- 870.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=53000, episode_reward=-411.60 +/- 295.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=54000, episode_reward=-539.20 +/- 277.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=55000, episode_reward=-280.60 +/- 367.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=56000, episode_reward=-417.80 +/- 653.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=57000, episode_reward=57.60 +/- 235.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=58000, episode_reward=-248.00 +/- 836.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=59000, episode_reward=-619.00 +/- 557.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=60000, episode_reward=-463.40 +/- 476.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=61000, episode_reward=-646.00 +/- 586.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=62000, episode_reward=-1087.00 +/- 509.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=63000, episode_reward=-755.00 +/- 404.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=64000, episode_reward=-1049.80 +/- 1165.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=65000, episode_reward=-4840.20 +/- 2565.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=66000, episode_reward=-6886.40 +/- 1771.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=67000, episode_reward=-5199.40 +/- 2127.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=68000, episode_reward=-1992.60 +/- 733.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=69000, episode_reward=-2209.00 +/- 1104.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=70000, episode_reward=-1126.80 +/- 469.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=71000, episode_reward=-1188.60 +/- 156.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=72000, episode_reward=-1849.20 +/- 1038.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=73000, episode_reward=-1908.00 +/- 1685.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=74000, episode_reward=-1198.00 +/- 259.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=75000, episode_reward=-1431.00 +/- 428.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=76000, episode_reward=-2398.00 +/- 451.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=77000, episode_reward=-1678.60 +/- 923.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=78000, episode_reward=-2261.80 +/- 247.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=79000, episode_reward=-2766.60 +/- 608.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=80000, episode_reward=-3071.60 +/- 310.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=81000, episode_reward=-2471.40 +/- 461.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=82000, episode_reward=-2429.40 +/- 537.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=83000, episode_reward=-2602.60 +/- 417.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=84000, episode_reward=-2365.20 +/- 985.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=85000, episode_reward=-1944.20 +/- 218.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=86000, episode_reward=-1823.00 +/- 915.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=87000, episode_reward=-1781.00 +/- 434.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=88000, episode_reward=-3013.20 +/- 763.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=89000, episode_reward=-2458.20 +/- 256.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=90000, episode_reward=-2598.20 +/- 623.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=91000, episode_reward=-1931.20 +/- 443.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=92000, episode_reward=-2314.60 +/- 399.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=93000, episode_reward=-2113.20 +/- 443.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=94000, episode_reward=-2214.80 +/- 269.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=95000, episode_reward=-2291.00 +/- 389.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=96000, episode_reward=-1966.20 +/- 539.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=97000, episode_reward=-1389.40 +/- 382.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=98000, episode_reward=-1520.40 +/- 175.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=99000, episode_reward=-1197.60 +/- 675.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=100000, episode_reward=-1157.20 +/- 244.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=101000, episode_reward=-1073.80 +/- 243.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=102000, episode_reward=-807.40 +/- 276.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=103000, episode_reward=-974.60 +/- 433.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=104000, episode_reward=-994.20 +/- 195.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=105000, episode_reward=-702.40 +/- 286.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=106000, episode_reward=-644.40 +/- 318.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=107000, episode_reward=-554.20 +/- 346.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=108000, episode_reward=-547.00 +/- 220.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=109000, episode_reward=-463.80 +/- 326.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=110000, episode_reward=-562.60 +/- 148.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=111000, episode_reward=-608.60 +/- 272.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=112000, episode_reward=-1055.00 +/- 333.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=113000, episode_reward=-981.80 +/- 577.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=114000, episode_reward=-970.00 +/- 651.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=115000, episode_reward=-1254.00 +/- 723.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=116000, episode_reward=-1145.80 +/- 493.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=117000, episode_reward=-2160.80 +/- 247.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=118000, episode_reward=-1740.20 +/- 302.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=119000, episode_reward=-1762.60 +/- 766.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=120000, episode_reward=-753.40 +/- 1040.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=121000, episode_reward=-1662.40 +/- 444.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=122000, episode_reward=-1479.60 +/- 930.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=123000, episode_reward=-1560.40 +/- 717.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=124000, episode_reward=-1620.00 +/- 532.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=125000, episode_reward=-1838.60 +/- 427.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=126000, episode_reward=-1795.40 +/- 529.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=127000, episode_reward=-2199.60 +/- 535.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=128000, episode_reward=-2103.00 +/- 688.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=129000, episode_reward=-1893.00 +/- 1252.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=130000, episode_reward=-1300.40 +/- 588.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=131000, episode_reward=-1601.60 +/- 438.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=132000, episode_reward=-1427.20 +/- 238.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=133000, episode_reward=-1664.60 +/- 209.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=134000, episode_reward=-1296.60 +/- 728.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=135000, episode_reward=-1552.00 +/- 521.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=136000, episode_reward=-1160.00 +/- 516.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=137000, episode_reward=-1353.20 +/- 421.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=138000, episode_reward=-1197.80 +/- 465.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=139000, episode_reward=-1353.00 +/- 738.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=140000, episode_reward=-1392.20 +/- 451.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=141000, episode_reward=-1314.20 +/- 629.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=142000, episode_reward=-1791.80 +/- 619.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=143000, episode_reward=-797.60 +/- 352.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=144000, episode_reward=-1095.40 +/- 325.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=145000, episode_reward=-1097.20 +/- 274.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=146000, episode_reward=-689.60 +/- 512.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=147000, episode_reward=-500.00 +/- 481.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=148000, episode_reward=-1019.40 +/- 400.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=149000, episode_reward=-878.40 +/- 356.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=150000, episode_reward=-587.40 +/- 257.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=151000, episode_reward=-698.60 +/- 150.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=152000, episode_reward=-522.80 +/- 185.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=153000, episode_reward=-565.00 +/- 176.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=154000, episode_reward=-781.60 +/- 650.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=155000, episode_reward=-1083.40 +/- 258.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=156000, episode_reward=-687.20 +/- 207.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=157000, episode_reward=-858.20 +/- 394.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=158000, episode_reward=-800.00 +/- 338.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=159000, episode_reward=-506.20 +/- 295.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=160000, episode_reward=-291.20 +/- 81.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=161000, episode_reward=-655.00 +/- 275.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=162000, episode_reward=-549.60 +/- 391.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=163000, episode_reward=-590.40 +/- 348.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=164000, episode_reward=-556.20 +/- 236.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=165000, episode_reward=-680.00 +/- 397.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=166000, episode_reward=-588.40 +/- 343.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=167000, episode_reward=-853.00 +/- 360.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=168000, episode_reward=-553.60 +/- 350.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=169000, episode_reward=-675.00 +/- 340.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=170000, episode_reward=-659.80 +/- 352.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=171000, episode_reward=-631.20 +/- 294.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=172000, episode_reward=-564.00 +/- 454.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=173000, episode_reward=-465.60 +/- 244.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=174000, episode_reward=-482.40 +/- 150.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=175000, episode_reward=-548.00 +/- 110.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=176000, episode_reward=-523.00 +/- 217.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=177000, episode_reward=-499.80 +/- 338.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=178000, episode_reward=-596.60 +/- 197.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=179000, episode_reward=-442.80 +/- 193.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=180000, episode_reward=-755.60 +/- 284.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=181000, episode_reward=-415.60 +/- 307.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=182000, episode_reward=-483.60 +/- 146.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=183000, episode_reward=-583.40 +/- 177.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=184000, episode_reward=-670.80 +/- 129.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=185000, episode_reward=-582.20 +/- 234.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=186000, episode_reward=-671.80 +/- 269.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=187000, episode_reward=-485.40 +/- 122.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=188000, episode_reward=-413.20 +/- 201.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=189000, episode_reward=-452.80 +/- 359.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=190000, episode_reward=-579.00 +/- 240.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=191000, episode_reward=-475.00 +/- 183.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=192000, episode_reward=-564.00 +/- 277.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=193000, episode_reward=-648.60 +/- 215.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=194000, episode_reward=-513.40 +/- 266.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=195000, episode_reward=-469.20 +/- 440.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=196000, episode_reward=-621.20 +/- 272.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=197000, episode_reward=-618.80 +/- 199.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=198000, episode_reward=-570.80 +/- 232.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=199000, episode_reward=-784.60 +/- 230.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=200000, episode_reward=-717.20 +/- 119.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=201000, episode_reward=-860.40 +/- 175.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=202000, episode_reward=-518.20 +/- 291.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=203000, episode_reward=-545.00 +/- 227.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=204000, episode_reward=-784.60 +/- 204.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=205000, episode_reward=-428.60 +/- 242.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=206000, episode_reward=-557.20 +/- 353.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=207000, episode_reward=-778.40 +/- 185.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=208000, episode_reward=-460.20 +/- 197.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=209000, episode_reward=-714.40 +/- 119.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=210000, episode_reward=-512.80 +/- 207.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=211000, episode_reward=-532.00 +/- 233.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=212000, episode_reward=-542.00 +/- 167.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=213000, episode_reward=-534.80 +/- 213.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=214000, episode_reward=-760.60 +/- 249.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=215000, episode_reward=-683.00 +/- 238.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=216000, episode_reward=-467.00 +/- 84.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=217000, episode_reward=-570.20 +/- 270.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=218000, episode_reward=-381.60 +/- 240.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=219000, episode_reward=-417.80 +/- 201.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=220000, episode_reward=-512.40 +/- 318.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=221000, episode_reward=-516.80 +/- 105.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=222000, episode_reward=-748.20 +/- 325.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=223000, episode_reward=-609.00 +/- 301.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=224000, episode_reward=-577.60 +/- 191.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=225000, episode_reward=-594.80 +/- 91.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=226000, episode_reward=-313.40 +/- 335.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=227000, episode_reward=-510.60 +/- 106.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=228000, episode_reward=-628.80 +/- 229.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=229000, episode_reward=-630.80 +/- 238.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=230000, episode_reward=-534.20 +/- 92.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=231000, episode_reward=-318.20 +/- 52.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=232000, episode_reward=-535.20 +/- 206.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=233000, episode_reward=-588.60 +/- 145.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=234000, episode_reward=-648.40 +/- 183.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=235000, episode_reward=-431.20 +/- 142.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=236000, episode_reward=-446.60 +/- 167.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=237000, episode_reward=-392.00 +/- 97.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=238000, episode_reward=-577.00 +/- 282.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=239000, episode_reward=-363.80 +/- 175.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=240000, episode_reward=-679.80 +/- 221.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=241000, episode_reward=-433.20 +/- 237.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=242000, episode_reward=-503.40 +/- 92.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=243000, episode_reward=-486.60 +/- 286.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=244000, episode_reward=-514.60 +/- 168.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=245000, episode_reward=-435.80 +/- 193.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=246000, episode_reward=-548.20 +/- 138.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=247000, episode_reward=-456.40 +/- 130.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=248000, episode_reward=-417.60 +/- 232.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=249000, episode_reward=-372.20 +/- 260.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=250000, episode_reward=-451.20 +/- 87.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=251000, episode_reward=-451.80 +/- 353.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=252000, episode_reward=-447.20 +/- 360.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=253000, episode_reward=-280.80 +/- 167.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=254000, episode_reward=-209.00 +/- 177.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=255000, episode_reward=-382.40 +/- 120.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=256000, episode_reward=-303.60 +/- 214.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=257000, episode_reward=-486.00 +/- 181.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=258000, episode_reward=-319.80 +/- 250.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=259000, episode_reward=-298.80 +/- 159.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=260000, episode_reward=-107.60 +/- 113.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=261000, episode_reward=-295.80 +/- 433.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=262000, episode_reward=-278.80 +/- 228.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=263000, episode_reward=-158.00 +/- 238.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=264000, episode_reward=-452.20 +/- 172.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=265000, episode_reward=-191.20 +/- 295.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=266000, episode_reward=-409.40 +/- 342.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=267000, episode_reward=-400.00 +/- 88.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=268000, episode_reward=-378.60 +/- 280.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=269000, episode_reward=-548.60 +/- 171.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=270000, episode_reward=-304.60 +/- 212.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=271000, episode_reward=-322.20 +/- 166.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=272000, episode_reward=-357.80 +/- 127.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=273000, episode_reward=-227.80 +/- 93.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=274000, episode_reward=-358.60 +/- 161.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=275000, episode_reward=-301.00 +/- 262.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=276000, episode_reward=-268.60 +/- 188.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=277000, episode_reward=-263.00 +/- 289.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=278000, episode_reward=-328.80 +/- 102.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=279000, episode_reward=-500.80 +/- 114.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=280000, episode_reward=-380.00 +/- 267.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=281000, episode_reward=-301.80 +/- 274.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=282000, episode_reward=-340.60 +/- 200.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=283000, episode_reward=-264.40 +/- 283.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=284000, episode_reward=-88.20 +/- 157.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=285000, episode_reward=-653.60 +/- 64.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=286000, episode_reward=-360.40 +/- 233.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=287000, episode_reward=-365.40 +/- 172.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=288000, episode_reward=-484.40 +/- 152.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=289000, episode_reward=-408.60 +/- 351.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=290000, episode_reward=-356.60 +/- 227.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=291000, episode_reward=-414.00 +/- 524.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=292000, episode_reward=-339.00 +/- 259.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=293000, episode_reward=-259.20 +/- 236.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=294000, episode_reward=-368.40 +/- 159.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=295000, episode_reward=-304.80 +/- 140.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=296000, episode_reward=-521.20 +/- 134.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=297000, episode_reward=-478.60 +/- 294.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=298000, episode_reward=-475.80 +/- 236.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=299000, episode_reward=-552.60 +/- 265.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=300000, episode_reward=-202.60 +/- 260.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=301000, episode_reward=-440.60 +/- 320.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=302000, episode_reward=-410.20 +/- 229.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=303000, episode_reward=-209.20 +/- 245.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=304000, episode_reward=-274.00 +/- 268.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=305000, episode_reward=-316.80 +/- 433.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=306000, episode_reward=-272.80 +/- 276.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=307000, episode_reward=-402.60 +/- 328.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=308000, episode_reward=-322.40 +/- 264.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=309000, episode_reward=-328.80 +/- 248.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=310000, episode_reward=-245.60 +/- 163.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=311000, episode_reward=-159.40 +/- 335.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=312000, episode_reward=-287.80 +/- 196.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=313000, episode_reward=-406.00 +/- 169.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=314000, episode_reward=-29.40 +/- 108.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=315000, episode_reward=-67.40 +/- 176.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=316000, episode_reward=-403.80 +/- 185.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=317000, episode_reward=-43.80 +/- 163.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=318000, episode_reward=-318.80 +/- 120.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=319000, episode_reward=-293.60 +/- 281.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=320000, episode_reward=-195.60 +/- 199.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=321000, episode_reward=-317.40 +/- 285.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=322000, episode_reward=-364.60 +/- 435.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=323000, episode_reward=-357.60 +/- 147.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=324000, episode_reward=-98.40 +/- 254.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=325000, episode_reward=-440.60 +/- 201.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=326000, episode_reward=-153.60 +/- 232.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=327000, episode_reward=-358.80 +/- 123.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=328000, episode_reward=-246.20 +/- 250.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=329000, episode_reward=-211.20 +/- 166.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=330000, episode_reward=-398.80 +/- 148.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=331000, episode_reward=-346.40 +/- 222.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=332000, episode_reward=-451.60 +/- 161.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=333000, episode_reward=-303.80 +/- 162.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=334000, episode_reward=-142.40 +/- 333.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=335000, episode_reward=-316.80 +/- 149.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=336000, episode_reward=-209.40 +/- 97.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=337000, episode_reward=-259.60 +/- 295.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=338000, episode_reward=-252.20 +/- 228.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=339000, episode_reward=-213.80 +/- 235.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=340000, episode_reward=-131.40 +/- 364.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=341000, episode_reward=-435.00 +/- 142.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=342000, episode_reward=-332.40 +/- 136.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=343000, episode_reward=-442.80 +/- 193.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=344000, episode_reward=-352.00 +/- 218.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=345000, episode_reward=-154.60 +/- 76.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=346000, episode_reward=-392.20 +/- 188.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=347000, episode_reward=-234.40 +/- 157.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=348000, episode_reward=-473.60 +/- 212.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=349000, episode_reward=-121.20 +/- 240.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=350000, episode_reward=-150.00 +/- 323.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=351000, episode_reward=-466.60 +/- 260.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=352000, episode_reward=-276.20 +/- 295.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=353000, episode_reward=-419.00 +/- 183.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=354000, episode_reward=-79.80 +/- 262.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=355000, episode_reward=-272.40 +/- 318.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=356000, episode_reward=-172.20 +/- 144.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=357000, episode_reward=-315.00 +/- 156.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=358000, episode_reward=-375.20 +/- 297.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=359000, episode_reward=-479.60 +/- 156.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=360000, episode_reward=-300.20 +/- 84.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=361000, episode_reward=-206.40 +/- 273.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=362000, episode_reward=-393.20 +/- 97.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=363000, episode_reward=-186.60 +/- 171.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=364000, episode_reward=-245.80 +/- 310.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=365000, episode_reward=-174.00 +/- 175.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=366000, episode_reward=-160.00 +/- 152.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=367000, episode_reward=-434.20 +/- 298.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=368000, episode_reward=-330.80 +/- 146.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=369000, episode_reward=-56.60 +/- 219.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=370000, episode_reward=-165.00 +/- 128.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=371000, episode_reward=-276.60 +/- 231.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=372000, episode_reward=-223.60 +/- 198.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=373000, episode_reward=-304.00 +/- 193.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=374000, episode_reward=-179.40 +/- 67.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=375000, episode_reward=-364.60 +/- 195.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=376000, episode_reward=-134.80 +/- 101.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=377000, episode_reward=-214.40 +/- 76.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=378000, episode_reward=-154.60 +/- 367.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=379000, episode_reward=-351.20 +/- 126.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=380000, episode_reward=-217.00 +/- 184.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=381000, episode_reward=-363.00 +/- 341.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=382000, episode_reward=-361.20 +/- 263.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=383000, episode_reward=-511.00 +/- 147.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=384000, episode_reward=-167.20 +/- 107.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=385000, episode_reward=-80.20 +/- 138.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=386000, episode_reward=-164.80 +/- 317.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=387000, episode_reward=-57.40 +/- 201.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=388000, episode_reward=-333.80 +/- 374.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=389000, episode_reward=-350.40 +/- 494.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=390000, episode_reward=-198.00 +/- 195.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=391000, episode_reward=-335.60 +/- 173.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=392000, episode_reward=-158.40 +/- 193.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=393000, episode_reward=-254.00 +/- 202.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=394000, episode_reward=-133.60 +/- 275.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=395000, episode_reward=-616.20 +/- 320.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=396000, episode_reward=-127.80 +/- 217.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=397000, episode_reward=-169.20 +/- 68.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=398000, episode_reward=-266.80 +/- 154.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=399000, episode_reward=-287.00 +/- 256.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=400000, episode_reward=-306.80 +/- 252.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=401000, episode_reward=-345.20 +/- 274.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=402000, episode_reward=-132.20 +/- 229.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=403000, episode_reward=-169.00 +/- 122.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=404000, episode_reward=-188.80 +/- 308.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=405000, episode_reward=-277.60 +/- 176.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=406000, episode_reward=-240.20 +/- 76.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=407000, episode_reward=-230.60 +/- 252.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=408000, episode_reward=-373.60 +/- 172.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=409000, episode_reward=-27.40 +/- 264.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=410000, episode_reward=-307.80 +/- 93.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=411000, episode_reward=-389.60 +/- 57.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=412000, episode_reward=-158.40 +/- 94.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=413000, episode_reward=-390.80 +/- 81.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=414000, episode_reward=32.40 +/- 171.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=415000, episode_reward=-387.20 +/- 224.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=416000, episode_reward=-261.40 +/- 156.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=417000, episode_reward=-249.00 +/- 256.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=418000, episode_reward=-386.40 +/- 365.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=419000, episode_reward=-366.00 +/- 203.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=420000, episode_reward=-271.80 +/- 259.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=421000, episode_reward=-370.00 +/- 114.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=422000, episode_reward=-408.60 +/- 136.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=423000, episode_reward=-84.20 +/- 283.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=424000, episode_reward=-451.60 +/- 176.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=425000, episode_reward=-359.40 +/- 269.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=426000, episode_reward=-241.80 +/- 258.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=427000, episode_reward=-258.00 +/- 352.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=428000, episode_reward=-214.80 +/- 122.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=429000, episode_reward=-227.40 +/- 285.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=430000, episode_reward=-261.60 +/- 172.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=431000, episode_reward=-39.20 +/- 213.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=432000, episode_reward=-273.40 +/- 236.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=433000, episode_reward=-199.40 +/- 236.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=434000, episode_reward=-286.80 +/- 349.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=435000, episode_reward=-437.00 +/- 225.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=436000, episode_reward=-293.00 +/- 249.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=437000, episode_reward=-277.60 +/- 219.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=438000, episode_reward=-382.20 +/- 239.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=439000, episode_reward=-86.80 +/- 91.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=440000, episode_reward=-481.40 +/- 93.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=441000, episode_reward=-253.60 +/- 216.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=442000, episode_reward=-437.40 +/- 290.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=443000, episode_reward=-215.40 +/- 94.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=444000, episode_reward=-276.60 +/- 388.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=445000, episode_reward=-374.80 +/- 327.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=446000, episode_reward=-209.20 +/- 85.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=447000, episode_reward=-278.60 +/- 193.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=448000, episode_reward=-65.40 +/- 104.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=449000, episode_reward=-47.80 +/- 237.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=450000, episode_reward=-210.20 +/- 211.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=451000, episode_reward=-294.00 +/- 175.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=452000, episode_reward=-78.00 +/- 269.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=453000, episode_reward=-374.20 +/- 346.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=454000, episode_reward=-328.00 +/- 242.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=455000, episode_reward=-63.40 +/- 226.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=456000, episode_reward=-385.00 +/- 105.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=457000, episode_reward=-189.00 +/- 160.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=458000, episode_reward=-286.60 +/- 204.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=459000, episode_reward=-429.00 +/- 206.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=460000, episode_reward=-264.80 +/- 287.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=461000, episode_reward=-279.40 +/- 153.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=462000, episode_reward=-487.60 +/- 124.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=463000, episode_reward=-247.40 +/- 109.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=464000, episode_reward=-269.20 +/- 246.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=465000, episode_reward=-352.80 +/- 452.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=466000, episode_reward=-314.00 +/- 336.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=467000, episode_reward=-514.00 +/- 400.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=468000, episode_reward=-262.60 +/- 233.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=469000, episode_reward=-300.40 +/- 92.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=470000, episode_reward=-303.60 +/- 110.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=471000, episode_reward=-323.80 +/- 145.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=472000, episode_reward=-294.20 +/- 120.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=473000, episode_reward=-302.60 +/- 252.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=474000, episode_reward=-299.80 +/- 168.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=475000, episode_reward=-210.80 +/- 175.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=476000, episode_reward=-276.40 +/- 244.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=477000, episode_reward=-202.80 +/- 145.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=478000, episode_reward=-571.80 +/- 236.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=479000, episode_reward=-228.20 +/- 410.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=480000, episode_reward=-396.40 +/- 252.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=481000, episode_reward=-132.20 +/- 200.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=482000, episode_reward=-421.00 +/- 444.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=483000, episode_reward=-329.40 +/- 199.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=484000, episode_reward=-31.00 +/- 219.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=485000, episode_reward=-375.40 +/- 190.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=486000, episode_reward=-208.20 +/- 162.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=487000, episode_reward=-269.60 +/- 283.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=488000, episode_reward=-290.60 +/- 114.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=489000, episode_reward=-570.00 +/- 258.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=490000, episode_reward=-343.40 +/- 118.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=491000, episode_reward=-379.60 +/- 279.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=492000, episode_reward=-298.20 +/- 183.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=493000, episode_reward=-254.40 +/- 42.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=494000, episode_reward=-270.20 +/- 159.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=495000, episode_reward=-271.00 +/- 335.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=496000, episode_reward=-220.80 +/- 325.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=497000, episode_reward=-268.00 +/- 276.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=498000, episode_reward=-180.80 +/- 248.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=499000, episode_reward=-379.20 +/- 114.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=500000, episode_reward=-421.40 +/- 347.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Gamma = 0.99, Learning Rate = 3e-05, Activation Function = Tanh, Neurons = 64, Process noise = 0.1, Measurement noise = 0.0\n",
      "Eval num_timesteps=1000, episode_reward=-5814.60 +/- 1992.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=-6889.80 +/- 1619.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=3000, episode_reward=-7358.40 +/- 2041.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=4000, episode_reward=-5715.00 +/- 1082.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=5000, episode_reward=-5861.40 +/- 1552.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=6000, episode_reward=-5884.00 +/- 1986.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=7000, episode_reward=-6030.60 +/- 1715.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=8000, episode_reward=-7555.00 +/- 1559.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=9000, episode_reward=-5645.20 +/- 2700.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=10000, episode_reward=-6545.00 +/- 2627.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=11000, episode_reward=-5936.80 +/- 1591.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=12000, episode_reward=-7333.40 +/- 854.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=13000, episode_reward=-6113.80 +/- 1019.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=14000, episode_reward=-6089.20 +/- 1217.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=15000, episode_reward=-5891.20 +/- 1260.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=16000, episode_reward=-5041.40 +/- 2322.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=17000, episode_reward=-7197.60 +/- 927.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=18000, episode_reward=-5266.40 +/- 2146.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=19000, episode_reward=-4985.20 +/- 1987.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=20000, episode_reward=-5834.60 +/- 1085.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=21000, episode_reward=-6752.40 +/- 1636.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=22000, episode_reward=-5685.80 +/- 1465.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=23000, episode_reward=-7410.40 +/- 1748.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=24000, episode_reward=-6713.60 +/- 960.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=25000, episode_reward=-6399.00 +/- 1529.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=26000, episode_reward=-8167.20 +/- 2781.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=27000, episode_reward=-7145.80 +/- 1384.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=28000, episode_reward=-6580.40 +/- 1480.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=29000, episode_reward=-5541.80 +/- 1309.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=30000, episode_reward=-5449.40 +/- 990.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=31000, episode_reward=-5883.00 +/- 441.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=32000, episode_reward=-4024.20 +/- 1412.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=33000, episode_reward=-6159.60 +/- 1690.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=34000, episode_reward=-6046.80 +/- 973.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=35000, episode_reward=-5250.20 +/- 1528.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=36000, episode_reward=-7007.40 +/- 1399.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=37000, episode_reward=-5508.80 +/- 577.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=38000, episode_reward=-5491.20 +/- 2032.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=39000, episode_reward=-5129.60 +/- 1368.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=40000, episode_reward=-5557.60 +/- 1604.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=41000, episode_reward=-4913.40 +/- 1282.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=42000, episode_reward=-7540.00 +/- 1385.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=43000, episode_reward=-4299.80 +/- 1193.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=44000, episode_reward=-6701.00 +/- 1291.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=45000, episode_reward=-7699.20 +/- 1705.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=46000, episode_reward=-6279.60 +/- 654.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=47000, episode_reward=-5307.60 +/- 2051.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=48000, episode_reward=-5436.20 +/- 1425.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=49000, episode_reward=-7926.40 +/- 1085.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=50000, episode_reward=-6264.20 +/- 737.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=51000, episode_reward=-5785.80 +/- 397.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=52000, episode_reward=-504.20 +/- 214.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=53000, episode_reward=-225.00 +/- 86.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=54000, episode_reward=-49.00 +/- 250.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=55000, episode_reward=-551.60 +/- 149.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=56000, episode_reward=-698.40 +/- 196.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=57000, episode_reward=-1129.20 +/- 116.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=58000, episode_reward=-1299.60 +/- 493.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=59000, episode_reward=-1457.80 +/- 380.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=60000, episode_reward=-1918.80 +/- 335.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=61000, episode_reward=-1769.60 +/- 326.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=62000, episode_reward=-2106.60 +/- 207.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=63000, episode_reward=-1919.40 +/- 196.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=64000, episode_reward=-1584.20 +/- 473.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=65000, episode_reward=-1693.00 +/- 152.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=66000, episode_reward=-1462.80 +/- 223.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=67000, episode_reward=-1092.60 +/- 167.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=68000, episode_reward=-1213.40 +/- 203.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=69000, episode_reward=-986.20 +/- 311.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=70000, episode_reward=-820.20 +/- 252.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=71000, episode_reward=-1145.60 +/- 120.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=72000, episode_reward=-1029.00 +/- 426.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=73000, episode_reward=-1385.60 +/- 264.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=74000, episode_reward=-1179.80 +/- 139.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=75000, episode_reward=-1196.00 +/- 238.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=76000, episode_reward=-1080.00 +/- 351.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=77000, episode_reward=-752.40 +/- 417.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=78000, episode_reward=-878.40 +/- 120.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=79000, episode_reward=-899.60 +/- 271.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=80000, episode_reward=-886.40 +/- 115.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=81000, episode_reward=-1015.00 +/- 425.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=82000, episode_reward=-961.60 +/- 236.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=83000, episode_reward=-1205.60 +/- 292.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=84000, episode_reward=-1206.20 +/- 310.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=85000, episode_reward=-781.00 +/- 467.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=86000, episode_reward=-1200.80 +/- 297.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=87000, episode_reward=-1030.60 +/- 108.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=88000, episode_reward=-809.80 +/- 315.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=89000, episode_reward=-761.80 +/- 247.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=90000, episode_reward=-652.40 +/- 300.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=91000, episode_reward=-822.80 +/- 410.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=92000, episode_reward=-831.40 +/- 374.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=93000, episode_reward=-788.00 +/- 236.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=94000, episode_reward=-606.00 +/- 184.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=95000, episode_reward=-609.00 +/- 415.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=96000, episode_reward=-606.40 +/- 269.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=97000, episode_reward=-763.20 +/- 263.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=98000, episode_reward=-726.00 +/- 268.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=99000, episode_reward=-747.00 +/- 276.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=100000, episode_reward=-544.00 +/- 414.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=101000, episode_reward=-606.60 +/- 288.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=102000, episode_reward=-645.60 +/- 217.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=103000, episode_reward=-662.80 +/- 335.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=104000, episode_reward=-597.80 +/- 75.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=105000, episode_reward=-807.40 +/- 102.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=106000, episode_reward=-814.60 +/- 330.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=107000, episode_reward=-805.40 +/- 210.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=108000, episode_reward=-668.20 +/- 100.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=109000, episode_reward=-821.40 +/- 224.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=110000, episode_reward=-708.00 +/- 167.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=111000, episode_reward=-691.00 +/- 292.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=112000, episode_reward=-618.00 +/- 263.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=113000, episode_reward=-585.00 +/- 131.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=114000, episode_reward=-678.60 +/- 169.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=115000, episode_reward=-641.40 +/- 204.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=116000, episode_reward=-479.40 +/- 102.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=117000, episode_reward=-504.40 +/- 194.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=118000, episode_reward=-660.20 +/- 133.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=119000, episode_reward=-475.60 +/- 178.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=120000, episode_reward=-670.20 +/- 263.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=121000, episode_reward=-793.40 +/- 174.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=122000, episode_reward=-630.80 +/- 234.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=123000, episode_reward=-884.00 +/- 306.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=124000, episode_reward=-488.40 +/- 153.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=125000, episode_reward=-941.20 +/- 405.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=126000, episode_reward=-668.80 +/- 302.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=127000, episode_reward=-569.00 +/- 100.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=128000, episode_reward=-563.40 +/- 417.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=129000, episode_reward=-562.80 +/- 179.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=130000, episode_reward=-594.60 +/- 369.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=131000, episode_reward=-802.00 +/- 270.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=132000, episode_reward=-552.00 +/- 189.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=133000, episode_reward=-726.80 +/- 223.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=134000, episode_reward=-626.40 +/- 273.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=135000, episode_reward=-493.40 +/- 414.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=136000, episode_reward=-620.40 +/- 216.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=137000, episode_reward=-333.20 +/- 188.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=138000, episode_reward=-667.60 +/- 315.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=139000, episode_reward=-544.60 +/- 374.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=140000, episode_reward=-683.00 +/- 69.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=141000, episode_reward=-520.20 +/- 228.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=142000, episode_reward=-528.80 +/- 242.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=143000, episode_reward=-660.80 +/- 211.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=144000, episode_reward=-685.80 +/- 237.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=145000, episode_reward=-518.40 +/- 139.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=146000, episode_reward=-516.80 +/- 170.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=147000, episode_reward=-525.20 +/- 193.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=148000, episode_reward=-525.40 +/- 206.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=149000, episode_reward=-535.20 +/- 451.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=150000, episode_reward=-754.00 +/- 220.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=151000, episode_reward=-823.80 +/- 313.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=152000, episode_reward=-508.80 +/- 238.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=153000, episode_reward=-676.40 +/- 176.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=154000, episode_reward=-670.00 +/- 215.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=155000, episode_reward=-578.80 +/- 299.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=156000, episode_reward=-423.60 +/- 130.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=157000, episode_reward=-381.40 +/- 191.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=158000, episode_reward=-349.40 +/- 291.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=159000, episode_reward=-439.60 +/- 215.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=160000, episode_reward=-690.40 +/- 174.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=161000, episode_reward=-613.80 +/- 199.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=162000, episode_reward=-67.40 +/- 283.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=163000, episode_reward=-587.20 +/- 166.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=164000, episode_reward=-300.00 +/- 292.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=165000, episode_reward=-426.80 +/- 318.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=166000, episode_reward=-440.80 +/- 199.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=167000, episode_reward=-438.80 +/- 232.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=168000, episode_reward=-546.20 +/- 295.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=169000, episode_reward=-288.20 +/- 248.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=170000, episode_reward=-364.60 +/- 165.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=171000, episode_reward=-399.20 +/- 272.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=172000, episode_reward=-494.00 +/- 468.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=173000, episode_reward=-467.00 +/- 206.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=174000, episode_reward=-486.00 +/- 245.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=175000, episode_reward=-421.40 +/- 295.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=176000, episode_reward=-539.00 +/- 208.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=177000, episode_reward=-498.40 +/- 500.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=178000, episode_reward=-244.60 +/- 323.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=179000, episode_reward=-342.80 +/- 292.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=180000, episode_reward=-581.80 +/- 313.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=181000, episode_reward=-378.20 +/- 105.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=182000, episode_reward=-493.00 +/- 264.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=183000, episode_reward=-402.60 +/- 358.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=184000, episode_reward=-519.40 +/- 338.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=185000, episode_reward=-533.60 +/- 209.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=186000, episode_reward=-592.60 +/- 532.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=187000, episode_reward=-97.60 +/- 201.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=188000, episode_reward=-761.80 +/- 336.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=189000, episode_reward=-337.40 +/- 271.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=190000, episode_reward=-411.80 +/- 200.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=191000, episode_reward=-390.60 +/- 267.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=192000, episode_reward=-379.40 +/- 235.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=193000, episode_reward=-666.40 +/- 317.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=194000, episode_reward=-432.40 +/- 123.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=195000, episode_reward=-390.20 +/- 307.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=196000, episode_reward=-391.40 +/- 259.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=197000, episode_reward=-523.00 +/- 227.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=198000, episode_reward=-511.00 +/- 358.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=199000, episode_reward=-553.20 +/- 222.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=200000, episode_reward=-448.60 +/- 213.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=201000, episode_reward=-574.40 +/- 213.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=202000, episode_reward=-549.80 +/- 256.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=203000, episode_reward=-440.20 +/- 154.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=204000, episode_reward=-545.60 +/- 225.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=205000, episode_reward=-267.00 +/- 186.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=206000, episode_reward=-634.80 +/- 206.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=207000, episode_reward=-321.40 +/- 224.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=208000, episode_reward=-344.80 +/- 154.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=209000, episode_reward=-336.80 +/- 446.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=210000, episode_reward=-561.40 +/- 319.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=211000, episode_reward=-551.40 +/- 382.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=212000, episode_reward=-495.00 +/- 128.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=213000, episode_reward=-566.00 +/- 357.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=214000, episode_reward=-247.20 +/- 214.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=215000, episode_reward=-220.00 +/- 253.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=216000, episode_reward=-282.40 +/- 189.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=217000, episode_reward=-291.40 +/- 158.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=218000, episode_reward=-428.60 +/- 356.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=219000, episode_reward=-450.40 +/- 214.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=220000, episode_reward=-223.20 +/- 265.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=221000, episode_reward=-373.00 +/- 421.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=222000, episode_reward=-428.80 +/- 193.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=223000, episode_reward=-399.60 +/- 418.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=224000, episode_reward=-444.00 +/- 215.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=225000, episode_reward=-370.20 +/- 337.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=226000, episode_reward=-52.20 +/- 310.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=227000, episode_reward=-368.00 +/- 264.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=228000, episode_reward=-272.20 +/- 325.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=229000, episode_reward=-565.40 +/- 259.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=230000, episode_reward=-366.60 +/- 163.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=231000, episode_reward=-635.20 +/- 274.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=232000, episode_reward=-336.20 +/- 164.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=233000, episode_reward=-561.00 +/- 423.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=234000, episode_reward=-447.40 +/- 376.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=235000, episode_reward=-356.20 +/- 212.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=236000, episode_reward=-258.40 +/- 325.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=237000, episode_reward=-404.00 +/- 365.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=238000, episode_reward=-544.80 +/- 199.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=239000, episode_reward=-280.40 +/- 384.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=240000, episode_reward=-642.60 +/- 126.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=241000, episode_reward=-592.80 +/- 181.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=242000, episode_reward=-530.60 +/- 303.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=243000, episode_reward=-472.40 +/- 228.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=244000, episode_reward=-456.60 +/- 174.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=245000, episode_reward=-479.60 +/- 415.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=246000, episode_reward=-286.40 +/- 186.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=247000, episode_reward=-636.40 +/- 180.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=248000, episode_reward=-524.00 +/- 190.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=249000, episode_reward=-302.40 +/- 141.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=250000, episode_reward=-331.40 +/- 345.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=251000, episode_reward=-311.00 +/- 397.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=252000, episode_reward=-386.40 +/- 322.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=253000, episode_reward=-365.80 +/- 211.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=254000, episode_reward=-173.80 +/- 351.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=255000, episode_reward=-416.60 +/- 235.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=256000, episode_reward=-36.00 +/- 257.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=257000, episode_reward=-469.00 +/- 291.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=258000, episode_reward=-642.80 +/- 270.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=259000, episode_reward=-494.00 +/- 235.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=260000, episode_reward=-494.80 +/- 329.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=261000, episode_reward=-565.40 +/- 144.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=262000, episode_reward=-576.40 +/- 303.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=263000, episode_reward=-358.00 +/- 381.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=264000, episode_reward=-233.20 +/- 298.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=265000, episode_reward=-428.20 +/- 267.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=266000, episode_reward=-507.00 +/- 245.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=267000, episode_reward=-609.00 +/- 275.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=268000, episode_reward=-748.60 +/- 376.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=269000, episode_reward=-553.60 +/- 172.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=270000, episode_reward=-501.40 +/- 445.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=271000, episode_reward=-576.80 +/- 174.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=272000, episode_reward=-430.20 +/- 440.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=273000, episode_reward=-304.80 +/- 376.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=274000, episode_reward=-464.00 +/- 75.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=275000, episode_reward=-421.40 +/- 172.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=276000, episode_reward=-710.40 +/- 391.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=277000, episode_reward=-359.00 +/- 183.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=278000, episode_reward=-509.80 +/- 221.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=279000, episode_reward=-464.00 +/- 162.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=280000, episode_reward=-391.60 +/- 269.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=281000, episode_reward=-370.20 +/- 262.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=282000, episode_reward=-696.60 +/- 441.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=283000, episode_reward=-634.80 +/- 264.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=284000, episode_reward=-652.20 +/- 394.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=285000, episode_reward=-721.00 +/- 327.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=286000, episode_reward=-502.00 +/- 113.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=287000, episode_reward=-587.00 +/- 476.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=288000, episode_reward=-665.80 +/- 503.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=289000, episode_reward=-412.00 +/- 343.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=290000, episode_reward=-554.40 +/- 161.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=291000, episode_reward=-487.40 +/- 166.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=292000, episode_reward=-580.20 +/- 413.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=293000, episode_reward=-725.60 +/- 164.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=294000, episode_reward=-655.20 +/- 129.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=295000, episode_reward=-503.20 +/- 189.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=296000, episode_reward=-597.60 +/- 260.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=297000, episode_reward=-601.00 +/- 214.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=298000, episode_reward=-357.40 +/- 254.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=299000, episode_reward=-437.40 +/- 154.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=300000, episode_reward=-243.60 +/- 352.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=301000, episode_reward=-512.80 +/- 479.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=302000, episode_reward=-399.20 +/- 565.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=303000, episode_reward=-505.80 +/- 232.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=304000, episode_reward=-427.00 +/- 438.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=305000, episode_reward=-363.00 +/- 305.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=306000, episode_reward=-448.20 +/- 277.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=307000, episode_reward=-353.60 +/- 278.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=308000, episode_reward=-433.60 +/- 117.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=309000, episode_reward=-130.40 +/- 251.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=310000, episode_reward=-317.20 +/- 306.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=311000, episode_reward=-375.80 +/- 266.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=312000, episode_reward=138.80 +/- 148.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=313000, episode_reward=-361.20 +/- 222.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=314000, episode_reward=-589.60 +/- 403.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=315000, episode_reward=-423.40 +/- 282.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=316000, episode_reward=-656.80 +/- 172.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=317000, episode_reward=-295.60 +/- 317.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=318000, episode_reward=-545.60 +/- 442.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=319000, episode_reward=-261.20 +/- 205.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=320000, episode_reward=-281.00 +/- 266.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=321000, episode_reward=-479.80 +/- 81.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=322000, episode_reward=-737.80 +/- 103.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=323000, episode_reward=-462.60 +/- 185.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=324000, episode_reward=-482.00 +/- 201.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=325000, episode_reward=-360.80 +/- 285.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=326000, episode_reward=-368.00 +/- 357.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=327000, episode_reward=-433.00 +/- 210.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=328000, episode_reward=-235.80 +/- 149.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=329000, episode_reward=-412.00 +/- 234.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=330000, episode_reward=-385.40 +/- 443.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=331000, episode_reward=-341.40 +/- 117.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=332000, episode_reward=-483.00 +/- 386.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=333000, episode_reward=-252.00 +/- 450.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=334000, episode_reward=-206.80 +/- 156.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=335000, episode_reward=-427.00 +/- 253.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=336000, episode_reward=-240.40 +/- 78.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=337000, episode_reward=-624.80 +/- 237.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=338000, episode_reward=-437.00 +/- 190.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=339000, episode_reward=-391.60 +/- 371.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=340000, episode_reward=-401.80 +/- 283.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=341000, episode_reward=-481.40 +/- 264.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=342000, episode_reward=-776.40 +/- 155.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=343000, episode_reward=-279.40 +/- 371.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=344000, episode_reward=-607.80 +/- 349.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=345000, episode_reward=-460.00 +/- 361.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=346000, episode_reward=-597.20 +/- 303.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=347000, episode_reward=-635.00 +/- 468.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=348000, episode_reward=-559.80 +/- 319.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=349000, episode_reward=-329.00 +/- 447.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=350000, episode_reward=-234.20 +/- 281.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=351000, episode_reward=-520.20 +/- 431.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=352000, episode_reward=-518.80 +/- 334.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=353000, episode_reward=-716.40 +/- 296.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=354000, episode_reward=-576.40 +/- 570.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=355000, episode_reward=-679.20 +/- 477.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=356000, episode_reward=-457.80 +/- 370.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=357000, episode_reward=-259.60 +/- 201.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=358000, episode_reward=-157.20 +/- 244.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=359000, episode_reward=-698.40 +/- 259.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=360000, episode_reward=-191.80 +/- 446.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=361000, episode_reward=-346.20 +/- 318.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=362000, episode_reward=-368.40 +/- 302.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=363000, episode_reward=-535.80 +/- 344.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=364000, episode_reward=-633.80 +/- 245.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=365000, episode_reward=-448.00 +/- 147.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=366000, episode_reward=-522.20 +/- 184.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=367000, episode_reward=-228.00 +/- 275.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=368000, episode_reward=-386.20 +/- 87.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=369000, episode_reward=-202.80 +/- 240.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=370000, episode_reward=-437.40 +/- 337.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=371000, episode_reward=-442.60 +/- 191.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=372000, episode_reward=-474.80 +/- 308.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=373000, episode_reward=-504.40 +/- 272.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=374000, episode_reward=-375.20 +/- 301.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=375000, episode_reward=-165.60 +/- 334.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=376000, episode_reward=-158.40 +/- 160.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=377000, episode_reward=-87.80 +/- 227.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=378000, episode_reward=-408.60 +/- 345.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=379000, episode_reward=-521.20 +/- 147.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=380000, episode_reward=-470.40 +/- 330.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=381000, episode_reward=-285.00 +/- 338.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=382000, episode_reward=-418.20 +/- 264.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=383000, episode_reward=-353.20 +/- 218.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=384000, episode_reward=-279.00 +/- 340.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=385000, episode_reward=-525.40 +/- 166.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=386000, episode_reward=-285.20 +/- 242.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=387000, episode_reward=-717.00 +/- 593.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=388000, episode_reward=-360.00 +/- 276.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=389000, episode_reward=-758.40 +/- 209.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=390000, episode_reward=-437.20 +/- 264.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=391000, episode_reward=-355.80 +/- 445.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=392000, episode_reward=-420.00 +/- 97.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=393000, episode_reward=-329.20 +/- 236.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=394000, episode_reward=-512.80 +/- 357.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=395000, episode_reward=-538.80 +/- 189.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=396000, episode_reward=-235.20 +/- 327.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=397000, episode_reward=-269.80 +/- 411.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=398000, episode_reward=-411.40 +/- 612.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=399000, episode_reward=-213.00 +/- 159.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=400000, episode_reward=-71.00 +/- 295.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=401000, episode_reward=-456.40 +/- 498.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=402000, episode_reward=-196.00 +/- 194.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=403000, episode_reward=-308.60 +/- 194.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=404000, episode_reward=-477.20 +/- 369.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=405000, episode_reward=-499.60 +/- 290.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=406000, episode_reward=-419.80 +/- 321.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=407000, episode_reward=-715.80 +/- 266.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=408000, episode_reward=-540.80 +/- 353.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=409000, episode_reward=-117.60 +/- 242.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=410000, episode_reward=-181.80 +/- 139.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=411000, episode_reward=-332.40 +/- 343.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=412000, episode_reward=-489.00 +/- 165.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=413000, episode_reward=-377.60 +/- 301.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=414000, episode_reward=-664.20 +/- 252.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=415000, episode_reward=-499.00 +/- 144.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=416000, episode_reward=-40.60 +/- 345.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=417000, episode_reward=-468.80 +/- 579.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=418000, episode_reward=-614.40 +/- 252.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=419000, episode_reward=-381.40 +/- 260.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=420000, episode_reward=-442.20 +/- 486.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=421000, episode_reward=-309.20 +/- 341.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=422000, episode_reward=-431.80 +/- 267.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=423000, episode_reward=-214.20 +/- 86.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=424000, episode_reward=-248.00 +/- 243.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=425000, episode_reward=-236.80 +/- 130.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=426000, episode_reward=-233.00 +/- 117.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=427000, episode_reward=-507.00 +/- 236.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=428000, episode_reward=-140.80 +/- 188.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=429000, episode_reward=-633.60 +/- 120.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=430000, episode_reward=-264.80 +/- 117.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=431000, episode_reward=-311.80 +/- 228.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=432000, episode_reward=-471.40 +/- 445.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=433000, episode_reward=-158.60 +/- 334.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=434000, episode_reward=-323.40 +/- 148.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=435000, episode_reward=-45.20 +/- 320.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=436000, episode_reward=-515.00 +/- 361.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=437000, episode_reward=-301.00 +/- 208.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=438000, episode_reward=-250.80 +/- 212.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=439000, episode_reward=-386.00 +/- 520.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=440000, episode_reward=-212.40 +/- 135.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=441000, episode_reward=-363.60 +/- 105.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=442000, episode_reward=-253.20 +/- 305.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=443000, episode_reward=-546.80 +/- 485.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=444000, episode_reward=-324.40 +/- 542.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=445000, episode_reward=-297.40 +/- 321.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=446000, episode_reward=-534.40 +/- 156.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=447000, episode_reward=-376.40 +/- 374.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=448000, episode_reward=-238.40 +/- 480.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=449000, episode_reward=-490.80 +/- 392.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=450000, episode_reward=-708.40 +/- 219.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=451000, episode_reward=-295.40 +/- 260.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=452000, episode_reward=-352.40 +/- 182.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=453000, episode_reward=-341.80 +/- 261.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=454000, episode_reward=-234.40 +/- 392.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=455000, episode_reward=-174.20 +/- 211.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=456000, episode_reward=-295.20 +/- 276.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=457000, episode_reward=-209.80 +/- 377.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=458000, episode_reward=-444.20 +/- 563.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=459000, episode_reward=-171.40 +/- 212.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=460000, episode_reward=-503.00 +/- 509.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=461000, episode_reward=-551.80 +/- 309.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=462000, episode_reward=-596.80 +/- 400.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=463000, episode_reward=-582.20 +/- 291.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=464000, episode_reward=-369.00 +/- 227.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=465000, episode_reward=-301.80 +/- 208.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=466000, episode_reward=-487.00 +/- 302.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=467000, episode_reward=-547.00 +/- 191.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=468000, episode_reward=-538.60 +/- 312.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=469000, episode_reward=-232.40 +/- 219.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=470000, episode_reward=-526.40 +/- 333.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=471000, episode_reward=-200.60 +/- 332.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=472000, episode_reward=-210.40 +/- 248.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=473000, episode_reward=103.20 +/- 629.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=474000, episode_reward=-300.60 +/- 108.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=475000, episode_reward=-316.80 +/- 220.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=476000, episode_reward=-25.00 +/- 430.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=477000, episode_reward=152.80 +/- 383.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=478000, episode_reward=-354.60 +/- 462.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=479000, episode_reward=334.80 +/- 388.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=480000, episode_reward=-105.60 +/- 555.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=481000, episode_reward=-279.20 +/- 419.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=482000, episode_reward=57.40 +/- 330.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=483000, episode_reward=-281.40 +/- 478.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=484000, episode_reward=-23.20 +/- 448.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=485000, episode_reward=-13.40 +/- 112.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=486000, episode_reward=-482.80 +/- 315.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=487000, episode_reward=-213.20 +/- 230.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=488000, episode_reward=-70.00 +/- 353.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=489000, episode_reward=39.80 +/- 136.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=490000, episode_reward=-0.40 +/- 219.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=491000, episode_reward=-45.80 +/- 363.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=492000, episode_reward=311.80 +/- 268.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=493000, episode_reward=-26.60 +/- 435.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=494000, episode_reward=-194.00 +/- 356.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=495000, episode_reward=218.80 +/- 333.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=496000, episode_reward=18.00 +/- 159.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=497000, episode_reward=-328.40 +/- 330.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=498000, episode_reward=-400.00 +/- 400.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=499000, episode_reward=23.60 +/- 360.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=500000, episode_reward=-111.20 +/- 414.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Gamma = 0.99, Learning Rate = 3e-05, Activation Function = ReLU, Neurons = 8, Process noise = 0.1, Measurement noise = 0.0\n",
      "Eval num_timesteps=1000, episode_reward=-1506.00 +/- 57.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=-1454.00 +/- 73.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3000, episode_reward=-1497.00 +/- 133.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=4000, episode_reward=-1528.00 +/- 74.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=5000, episode_reward=-1594.00 +/- 115.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=6000, episode_reward=-1494.00 +/- 79.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=7000, episode_reward=-1576.00 +/- 101.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=8000, episode_reward=-1463.00 +/- 50.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=9000, episode_reward=-1414.00 +/- 56.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=10000, episode_reward=-1450.00 +/- 18.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=11000, episode_reward=-1452.00 +/- 35.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=12000, episode_reward=-1503.00 +/- 72.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=13000, episode_reward=-1527.00 +/- 84.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=14000, episode_reward=-1503.00 +/- 79.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=15000, episode_reward=-1529.00 +/- 75.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=16000, episode_reward=-1468.00 +/- 74.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=17000, episode_reward=-1497.00 +/- 67.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=18000, episode_reward=-1483.00 +/- 111.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=19000, episode_reward=-1471.00 +/- 76.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=20000, episode_reward=-1508.00 +/- 70.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=21000, episode_reward=-1475.00 +/- 41.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=22000, episode_reward=-1519.00 +/- 43.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=23000, episode_reward=-1452.00 +/- 41.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=24000, episode_reward=-1431.00 +/- 99.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=25000, episode_reward=-1483.00 +/- 131.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=26000, episode_reward=-1504.00 +/- 82.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=27000, episode_reward=-1502.00 +/- 55.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=28000, episode_reward=-1494.00 +/- 60.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=29000, episode_reward=-1492.00 +/- 42.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=30000, episode_reward=-1551.00 +/- 98.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=31000, episode_reward=-1444.00 +/- 87.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=32000, episode_reward=-1516.00 +/- 83.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=33000, episode_reward=-1512.00 +/- 55.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=34000, episode_reward=-1447.00 +/- 59.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=35000, episode_reward=-1532.00 +/- 68.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=36000, episode_reward=-1455.00 +/- 88.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=37000, episode_reward=-1464.00 +/- 97.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=38000, episode_reward=-1487.00 +/- 65.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=39000, episode_reward=-1475.00 +/- 52.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=40000, episode_reward=-1544.00 +/- 98.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=41000, episode_reward=-1622.00 +/- 87.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=42000, episode_reward=-1451.00 +/- 74.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=43000, episode_reward=-1457.00 +/- 106.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=44000, episode_reward=-1492.00 +/- 26.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=45000, episode_reward=-1430.00 +/- 106.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=46000, episode_reward=-1502.00 +/- 83.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=47000, episode_reward=-1482.00 +/- 95.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=48000, episode_reward=-1472.00 +/- 88.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=49000, episode_reward=-1487.00 +/- 65.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=50000, episode_reward=-1513.00 +/- 78.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=51000, episode_reward=-1578.00 +/- 58.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=52000, episode_reward=-1513.00 +/- 47.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=53000, episode_reward=-1472.00 +/- 51.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=54000, episode_reward=-1456.00 +/- 34.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=55000, episode_reward=-1439.00 +/- 96.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=56000, episode_reward=-1519.00 +/- 41.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=57000, episode_reward=-1498.00 +/- 64.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=58000, episode_reward=-1486.00 +/- 106.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=59000, episode_reward=-1343.00 +/- 310.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=60000, episode_reward=133.00 +/- 129.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=61000, episode_reward=439.80 +/- 207.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=62000, episode_reward=314.40 +/- 158.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=63000, episode_reward=488.20 +/- 155.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=64000, episode_reward=199.80 +/- 300.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=65000, episode_reward=232.40 +/- 122.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=66000, episode_reward=156.80 +/- 207.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=67000, episode_reward=92.40 +/- 90.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=68000, episode_reward=291.00 +/- 64.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=69000, episode_reward=118.20 +/- 87.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=70000, episode_reward=60.60 +/- 270.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=71000, episode_reward=-113.80 +/- 201.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=72000, episode_reward=129.40 +/- 245.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=73000, episode_reward=100.00 +/- 230.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=74000, episode_reward=-306.60 +/- 191.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=75000, episode_reward=-236.40 +/- 163.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=76000, episode_reward=-339.20 +/- 203.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=77000, episode_reward=-591.40 +/- 216.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=78000, episode_reward=-1119.80 +/- 387.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=79000, episode_reward=-1915.80 +/- 145.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=80000, episode_reward=-2794.40 +/- 366.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=81000, episode_reward=-2644.40 +/- 508.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=82000, episode_reward=-4602.40 +/- 1340.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=83000, episode_reward=-5099.20 +/- 879.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=84000, episode_reward=-2852.80 +/- 733.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=85000, episode_reward=-2170.40 +/- 321.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=86000, episode_reward=-1970.80 +/- 471.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=87000, episode_reward=-1725.00 +/- 222.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=88000, episode_reward=-1745.80 +/- 253.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=89000, episode_reward=-1170.80 +/- 368.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=90000, episode_reward=-1278.80 +/- 193.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=91000, episode_reward=-1096.00 +/- 355.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=92000, episode_reward=-1089.80 +/- 169.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=93000, episode_reward=-1175.60 +/- 216.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=94000, episode_reward=-1173.40 +/- 279.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=95000, episode_reward=-1129.40 +/- 221.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=96000, episode_reward=-862.60 +/- 119.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=97000, episode_reward=-1355.60 +/- 240.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=98000, episode_reward=-1319.20 +/- 177.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=99000, episode_reward=-1447.20 +/- 513.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=100000, episode_reward=-1699.20 +/- 214.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=101000, episode_reward=-2028.00 +/- 512.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=102000, episode_reward=-1998.60 +/- 215.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=103000, episode_reward=-1592.80 +/- 110.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=104000, episode_reward=-1594.00 +/- 172.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=105000, episode_reward=-1005.60 +/- 251.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=106000, episode_reward=-925.00 +/- 202.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=107000, episode_reward=-799.80 +/- 263.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=108000, episode_reward=-803.40 +/- 201.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=109000, episode_reward=-903.80 +/- 111.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=110000, episode_reward=-1013.60 +/- 277.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=111000, episode_reward=-995.80 +/- 240.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=112000, episode_reward=-950.40 +/- 369.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=113000, episode_reward=-936.60 +/- 451.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=114000, episode_reward=-1294.80 +/- 378.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=115000, episode_reward=-1308.80 +/- 357.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=116000, episode_reward=-1473.20 +/- 267.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=117000, episode_reward=-1512.40 +/- 159.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=118000, episode_reward=-1550.60 +/- 356.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=119000, episode_reward=-1340.80 +/- 755.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=120000, episode_reward=-2280.80 +/- 227.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=121000, episode_reward=-1815.20 +/- 503.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=122000, episode_reward=-2611.60 +/- 167.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=123000, episode_reward=-2457.40 +/- 629.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=124000, episode_reward=-3592.00 +/- 604.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=125000, episode_reward=-4100.20 +/- 803.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=126000, episode_reward=-4412.60 +/- 626.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=127000, episode_reward=-4764.00 +/- 585.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=128000, episode_reward=-4922.20 +/- 638.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=129000, episode_reward=-5244.60 +/- 694.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=130000, episode_reward=-4138.20 +/- 723.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=131000, episode_reward=-4643.60 +/- 788.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=132000, episode_reward=-4466.00 +/- 500.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=133000, episode_reward=-3964.00 +/- 505.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=134000, episode_reward=-3240.80 +/- 473.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=135000, episode_reward=-3313.00 +/- 294.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=136000, episode_reward=-2808.40 +/- 447.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=137000, episode_reward=-2428.80 +/- 325.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=138000, episode_reward=-2442.20 +/- 565.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=139000, episode_reward=-2819.00 +/- 465.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=140000, episode_reward=-2214.20 +/- 448.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=141000, episode_reward=-2026.40 +/- 595.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=142000, episode_reward=-1998.20 +/- 511.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=143000, episode_reward=-2276.20 +/- 321.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=144000, episode_reward=-2548.60 +/- 323.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=145000, episode_reward=-2147.20 +/- 313.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=146000, episode_reward=-2428.20 +/- 215.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=147000, episode_reward=-2398.00 +/- 542.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=148000, episode_reward=-2527.20 +/- 318.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=149000, episode_reward=-2640.20 +/- 517.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=150000, episode_reward=-2800.20 +/- 334.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=151000, episode_reward=-2352.80 +/- 367.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=152000, episode_reward=-2783.80 +/- 659.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=153000, episode_reward=-3006.00 +/- 620.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=154000, episode_reward=-2773.60 +/- 393.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=155000, episode_reward=-3196.20 +/- 637.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=156000, episode_reward=-3228.60 +/- 563.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=157000, episode_reward=-2941.60 +/- 781.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=158000, episode_reward=-3385.40 +/- 257.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=159000, episode_reward=-5337.20 +/- 926.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=160000, episode_reward=-4418.60 +/- 768.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=161000, episode_reward=-5156.60 +/- 1999.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=162000, episode_reward=-6253.20 +/- 704.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=163000, episode_reward=-6091.40 +/- 1015.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=164000, episode_reward=-5753.00 +/- 2048.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=165000, episode_reward=-6737.40 +/- 1865.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=166000, episode_reward=-5509.80 +/- 1582.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=167000, episode_reward=-7665.40 +/- 643.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=168000, episode_reward=-5796.60 +/- 1560.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=169000, episode_reward=-4807.40 +/- 1802.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=170000, episode_reward=-6442.20 +/- 2252.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=171000, episode_reward=-5861.80 +/- 2001.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=172000, episode_reward=-6355.40 +/- 1733.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=173000, episode_reward=-6317.40 +/- 679.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=174000, episode_reward=-5859.60 +/- 2304.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=175000, episode_reward=-6122.20 +/- 1188.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=176000, episode_reward=-5955.00 +/- 2105.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=177000, episode_reward=-5664.40 +/- 1536.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=178000, episode_reward=-6025.00 +/- 2632.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=179000, episode_reward=-5931.60 +/- 1430.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=180000, episode_reward=-4488.20 +/- 1651.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=181000, episode_reward=-6423.40 +/- 1496.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=182000, episode_reward=-6622.40 +/- 1074.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=183000, episode_reward=-7446.00 +/- 1052.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=184000, episode_reward=-4806.20 +/- 1821.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=185000, episode_reward=-6745.40 +/- 1801.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=186000, episode_reward=-4852.40 +/- 643.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=187000, episode_reward=-9377.00 +/- 2419.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=188000, episode_reward=-6252.60 +/- 480.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=189000, episode_reward=-5827.00 +/- 929.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=190000, episode_reward=-6912.80 +/- 2473.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=191000, episode_reward=-4492.80 +/- 1087.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=192000, episode_reward=-5753.00 +/- 2357.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=193000, episode_reward=-4831.00 +/- 1704.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=194000, episode_reward=-5546.80 +/- 1144.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=195000, episode_reward=-5700.60 +/- 1394.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=196000, episode_reward=-6685.80 +/- 2272.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=197000, episode_reward=-6457.60 +/- 1578.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=198000, episode_reward=-6676.00 +/- 1700.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=199000, episode_reward=-5369.20 +/- 724.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=200000, episode_reward=-8137.60 +/- 1874.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=201000, episode_reward=-5929.20 +/- 2302.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=202000, episode_reward=-5697.00 +/- 820.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=203000, episode_reward=-5713.80 +/- 1226.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=204000, episode_reward=-6661.80 +/- 1295.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=205000, episode_reward=-6548.40 +/- 1478.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=206000, episode_reward=-6144.00 +/- 1898.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=207000, episode_reward=-6272.60 +/- 1142.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=208000, episode_reward=-6069.40 +/- 1866.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=209000, episode_reward=-4984.40 +/- 2330.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=210000, episode_reward=-6554.40 +/- 2972.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=211000, episode_reward=-5577.80 +/- 1612.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=212000, episode_reward=-3491.00 +/- 1525.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=213000, episode_reward=-5939.20 +/- 1610.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=214000, episode_reward=-6233.40 +/- 1496.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=215000, episode_reward=-5201.20 +/- 941.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=216000, episode_reward=-6429.60 +/- 1334.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=217000, episode_reward=-6211.80 +/- 1480.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=218000, episode_reward=-4603.20 +/- 1866.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=219000, episode_reward=-7855.80 +/- 836.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=220000, episode_reward=-5673.20 +/- 767.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=221000, episode_reward=-5711.00 +/- 3221.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=222000, episode_reward=-8280.40 +/- 1127.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=223000, episode_reward=-7639.60 +/- 1654.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=224000, episode_reward=-5403.80 +/- 2246.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=225000, episode_reward=-6726.00 +/- 1495.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=226000, episode_reward=-5831.20 +/- 1995.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=227000, episode_reward=-6089.80 +/- 1426.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=228000, episode_reward=-6318.60 +/- 1080.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=229000, episode_reward=-4940.80 +/- 1446.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=230000, episode_reward=-7193.20 +/- 1185.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=231000, episode_reward=-5104.00 +/- 1088.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=232000, episode_reward=-6525.40 +/- 1527.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=233000, episode_reward=-7769.00 +/- 1323.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=234000, episode_reward=-6222.40 +/- 2259.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=235000, episode_reward=-6367.80 +/- 1174.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=236000, episode_reward=-6333.40 +/- 3399.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=237000, episode_reward=-4195.60 +/- 879.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=238000, episode_reward=-3753.20 +/- 623.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=239000, episode_reward=-3500.20 +/- 1425.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=240000, episode_reward=-4056.00 +/- 1013.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=241000, episode_reward=-2212.80 +/- 875.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=242000, episode_reward=-3567.60 +/- 656.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=243000, episode_reward=-4527.80 +/- 1219.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=244000, episode_reward=-4000.00 +/- 645.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=245000, episode_reward=-3307.60 +/- 851.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=246000, episode_reward=-2910.80 +/- 626.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=247000, episode_reward=-4008.40 +/- 606.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=248000, episode_reward=-2442.40 +/- 871.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=249000, episode_reward=-3996.40 +/- 680.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=250000, episode_reward=-3475.20 +/- 561.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=251000, episode_reward=-4033.60 +/- 1478.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=252000, episode_reward=-2314.80 +/- 452.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=253000, episode_reward=-3610.00 +/- 649.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=254000, episode_reward=-2792.60 +/- 739.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=255000, episode_reward=-3744.40 +/- 1107.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=256000, episode_reward=-3509.00 +/- 906.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=257000, episode_reward=-4522.20 +/- 2567.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=258000, episode_reward=-5355.00 +/- 1985.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=259000, episode_reward=-6841.20 +/- 1658.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=260000, episode_reward=-7830.80 +/- 1605.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=261000, episode_reward=-6229.80 +/- 1314.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=262000, episode_reward=-6404.60 +/- 2561.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=263000, episode_reward=-7305.40 +/- 1847.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=264000, episode_reward=-5812.20 +/- 2904.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=265000, episode_reward=-8023.00 +/- 2199.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=266000, episode_reward=-8683.60 +/- 1867.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=267000, episode_reward=-7902.80 +/- 1604.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=268000, episode_reward=-5844.40 +/- 807.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=269000, episode_reward=-7203.80 +/- 2531.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=270000, episode_reward=-7897.80 +/- 2654.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=271000, episode_reward=-7282.60 +/- 1293.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=272000, episode_reward=-8180.00 +/- 2120.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=273000, episode_reward=-5469.20 +/- 1657.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=274000, episode_reward=-6526.60 +/- 2162.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=275000, episode_reward=-6909.60 +/- 1604.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=276000, episode_reward=-6912.80 +/- 1372.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=277000, episode_reward=-5634.40 +/- 2109.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=278000, episode_reward=-7507.20 +/- 1286.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=279000, episode_reward=-6083.60 +/- 2387.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=280000, episode_reward=-6806.40 +/- 1948.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=281000, episode_reward=-7658.40 +/- 814.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=282000, episode_reward=-7317.40 +/- 2046.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=283000, episode_reward=-6325.40 +/- 1682.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=284000, episode_reward=-3303.60 +/- 925.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=285000, episode_reward=-3417.20 +/- 500.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=286000, episode_reward=-3225.60 +/- 1364.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=287000, episode_reward=-2545.60 +/- 905.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=288000, episode_reward=-2740.00 +/- 703.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=289000, episode_reward=-2892.40 +/- 1064.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=290000, episode_reward=-2376.40 +/- 971.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=291000, episode_reward=-3352.00 +/- 762.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=292000, episode_reward=-2130.20 +/- 995.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=293000, episode_reward=-3220.00 +/- 385.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=294000, episode_reward=-2829.80 +/- 850.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=295000, episode_reward=-2489.00 +/- 1241.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=296000, episode_reward=-2913.00 +/- 737.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=297000, episode_reward=-2376.80 +/- 627.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=298000, episode_reward=-837.20 +/- 335.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=299000, episode_reward=-1067.20 +/- 178.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=300000, episode_reward=-865.00 +/- 144.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=301000, episode_reward=-829.80 +/- 251.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=302000, episode_reward=-785.60 +/- 132.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=303000, episode_reward=-708.80 +/- 389.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=304000, episode_reward=-769.20 +/- 110.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=305000, episode_reward=-567.40 +/- 170.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=306000, episode_reward=-892.80 +/- 251.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=307000, episode_reward=-696.80 +/- 241.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=308000, episode_reward=-836.60 +/- 266.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=309000, episode_reward=-716.80 +/- 292.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=310000, episode_reward=-749.40 +/- 317.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=311000, episode_reward=-783.00 +/- 311.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=312000, episode_reward=-591.40 +/- 357.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=313000, episode_reward=-682.20 +/- 226.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=314000, episode_reward=-720.60 +/- 152.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=315000, episode_reward=-869.40 +/- 322.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=316000, episode_reward=-638.20 +/- 123.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=317000, episode_reward=-796.60 +/- 164.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=318000, episode_reward=-579.40 +/- 57.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=319000, episode_reward=-699.80 +/- 229.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=320000, episode_reward=-549.20 +/- 312.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=321000, episode_reward=-655.80 +/- 64.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=322000, episode_reward=-624.20 +/- 256.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=323000, episode_reward=-571.60 +/- 203.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=324000, episode_reward=-459.00 +/- 266.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=325000, episode_reward=-543.60 +/- 283.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=326000, episode_reward=-693.60 +/- 218.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=327000, episode_reward=-364.00 +/- 177.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=328000, episode_reward=-552.60 +/- 269.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=329000, episode_reward=-664.20 +/- 172.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=330000, episode_reward=-335.40 +/- 160.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=331000, episode_reward=-615.00 +/- 275.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=332000, episode_reward=-729.40 +/- 251.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=333000, episode_reward=-375.00 +/- 254.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=334000, episode_reward=-666.80 +/- 427.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=335000, episode_reward=-401.80 +/- 118.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=336000, episode_reward=-817.80 +/- 218.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=337000, episode_reward=-455.40 +/- 159.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=338000, episode_reward=-339.40 +/- 124.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=339000, episode_reward=-539.80 +/- 263.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=340000, episode_reward=-363.80 +/- 230.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=341000, episode_reward=-469.20 +/- 327.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=342000, episode_reward=-461.60 +/- 105.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=343000, episode_reward=-350.60 +/- 269.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=344000, episode_reward=-353.00 +/- 271.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=345000, episode_reward=-268.40 +/- 285.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=346000, episode_reward=-334.80 +/- 148.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=347000, episode_reward=-351.40 +/- 437.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=348000, episode_reward=-411.60 +/- 290.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=349000, episode_reward=-437.80 +/- 217.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=350000, episode_reward=-357.80 +/- 178.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=351000, episode_reward=-244.20 +/- 266.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=352000, episode_reward=-464.40 +/- 147.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=353000, episode_reward=-282.20 +/- 107.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=354000, episode_reward=-309.40 +/- 128.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=355000, episode_reward=-508.80 +/- 362.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=356000, episode_reward=-412.60 +/- 123.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=357000, episode_reward=-157.80 +/- 140.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=358000, episode_reward=-267.00 +/- 220.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=359000, episode_reward=-122.00 +/- 293.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=360000, episode_reward=-469.40 +/- 191.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=361000, episode_reward=-301.20 +/- 161.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=362000, episode_reward=-399.60 +/- 196.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=363000, episode_reward=-101.20 +/- 296.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=364000, episode_reward=-269.20 +/- 186.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=365000, episode_reward=-416.00 +/- 110.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=366000, episode_reward=-122.00 +/- 228.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=367000, episode_reward=-488.00 +/- 192.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=368000, episode_reward=-340.60 +/- 216.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=369000, episode_reward=-303.60 +/- 166.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=370000, episode_reward=-378.20 +/- 171.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=371000, episode_reward=-190.80 +/- 290.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=372000, episode_reward=-398.80 +/- 380.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=373000, episode_reward=-72.80 +/- 155.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=374000, episode_reward=-58.20 +/- 225.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=375000, episode_reward=-138.00 +/- 124.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=376000, episode_reward=-160.20 +/- 203.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=377000, episode_reward=-159.60 +/- 297.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=378000, episode_reward=-76.00 +/- 153.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=379000, episode_reward=-218.20 +/- 223.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=380000, episode_reward=-88.00 +/- 284.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=381000, episode_reward=-294.40 +/- 93.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=382000, episode_reward=-194.40 +/- 246.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=383000, episode_reward=-188.60 +/- 228.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=384000, episode_reward=-229.40 +/- 308.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=385000, episode_reward=-95.00 +/- 318.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=386000, episode_reward=-275.80 +/- 268.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=387000, episode_reward=-228.20 +/- 191.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=388000, episode_reward=-194.80 +/- 330.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=389000, episode_reward=-85.60 +/- 191.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=390000, episode_reward=-211.80 +/- 241.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=391000, episode_reward=-114.00 +/- 230.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=392000, episode_reward=-87.20 +/- 126.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=393000, episode_reward=-209.20 +/- 81.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=394000, episode_reward=-217.60 +/- 210.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=395000, episode_reward=-151.00 +/- 192.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=396000, episode_reward=-20.40 +/- 159.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=397000, episode_reward=-224.40 +/- 376.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=398000, episode_reward=-180.80 +/- 301.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=399000, episode_reward=-230.00 +/- 124.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=400000, episode_reward=1.40 +/- 205.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=401000, episode_reward=-113.60 +/- 198.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=402000, episode_reward=35.00 +/- 261.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=403000, episode_reward=-47.80 +/- 95.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=404000, episode_reward=-123.00 +/- 232.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=405000, episode_reward=-45.20 +/- 266.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=406000, episode_reward=-209.20 +/- 211.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=407000, episode_reward=-32.40 +/- 116.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=408000, episode_reward=-140.60 +/- 131.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=409000, episode_reward=-207.00 +/- 102.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=410000, episode_reward=-93.20 +/- 367.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=411000, episode_reward=-100.80 +/- 182.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=412000, episode_reward=22.60 +/- 48.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=413000, episode_reward=-160.20 +/- 163.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=414000, episode_reward=-228.20 +/- 98.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=415000, episode_reward=-153.00 +/- 221.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=416000, episode_reward=76.20 +/- 148.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=417000, episode_reward=9.40 +/- 201.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=418000, episode_reward=-136.20 +/- 248.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=419000, episode_reward=-194.20 +/- 393.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=420000, episode_reward=-42.40 +/- 157.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=421000, episode_reward=-65.40 +/- 208.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=422000, episode_reward=-213.00 +/- 244.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=423000, episode_reward=-165.40 +/- 348.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=424000, episode_reward=-7.40 +/- 202.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=425000, episode_reward=-36.40 +/- 150.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=426000, episode_reward=63.20 +/- 201.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=427000, episode_reward=-78.80 +/- 115.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=428000, episode_reward=-4.00 +/- 198.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=429000, episode_reward=17.20 +/- 216.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=430000, episode_reward=16.60 +/- 224.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=431000, episode_reward=36.60 +/- 321.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=432000, episode_reward=103.60 +/- 106.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=433000, episode_reward=-136.20 +/- 110.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=434000, episode_reward=120.80 +/- 129.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=435000, episode_reward=31.40 +/- 202.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=436000, episode_reward=-45.00 +/- 144.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=437000, episode_reward=42.00 +/- 179.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=438000, episode_reward=-42.20 +/- 228.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=439000, episode_reward=-87.40 +/- 179.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=440000, episode_reward=58.60 +/- 77.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=441000, episode_reward=134.40 +/- 163.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=442000, episode_reward=83.40 +/- 178.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=443000, episode_reward=32.20 +/- 229.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=444000, episode_reward=37.80 +/- 199.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=445000, episode_reward=-54.40 +/- 319.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=446000, episode_reward=199.60 +/- 303.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=447000, episode_reward=-141.20 +/- 57.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=448000, episode_reward=177.60 +/- 119.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=449000, episode_reward=-8.20 +/- 86.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=450000, episode_reward=139.20 +/- 245.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=451000, episode_reward=212.00 +/- 189.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=452000, episode_reward=-16.80 +/- 153.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=453000, episode_reward=38.80 +/- 192.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=454000, episode_reward=180.40 +/- 380.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=455000, episode_reward=-99.80 +/- 149.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=456000, episode_reward=161.60 +/- 114.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=457000, episode_reward=102.40 +/- 148.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=458000, episode_reward=125.40 +/- 217.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=459000, episode_reward=-82.40 +/- 178.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=460000, episode_reward=-10.60 +/- 112.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=461000, episode_reward=184.60 +/- 234.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=462000, episode_reward=119.60 +/- 116.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=463000, episode_reward=56.80 +/- 189.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=464000, episode_reward=24.00 +/- 94.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=465000, episode_reward=247.40 +/- 94.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=466000, episode_reward=-133.20 +/- 180.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=467000, episode_reward=78.40 +/- 114.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=468000, episode_reward=159.60 +/- 168.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=469000, episode_reward=24.00 +/- 183.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=470000, episode_reward=205.60 +/- 164.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=471000, episode_reward=9.40 +/- 147.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=472000, episode_reward=267.60 +/- 134.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=473000, episode_reward=42.40 +/- 219.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=474000, episode_reward=140.20 +/- 199.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=475000, episode_reward=-78.00 +/- 226.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=476000, episode_reward=80.80 +/- 40.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=477000, episode_reward=158.20 +/- 123.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=478000, episode_reward=-42.80 +/- 320.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=479000, episode_reward=103.80 +/- 149.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=480000, episode_reward=143.80 +/- 228.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=481000, episode_reward=259.40 +/- 168.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=482000, episode_reward=128.60 +/- 215.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=483000, episode_reward=205.60 +/- 267.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=484000, episode_reward=195.60 +/- 248.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=485000, episode_reward=34.60 +/- 160.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=486000, episode_reward=93.80 +/- 131.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=487000, episode_reward=-1.00 +/- 155.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=488000, episode_reward=154.80 +/- 86.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=489000, episode_reward=73.00 +/- 143.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=490000, episode_reward=104.20 +/- 177.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=491000, episode_reward=153.40 +/- 162.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=492000, episode_reward=110.80 +/- 173.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=493000, episode_reward=291.60 +/- 238.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=494000, episode_reward=0.40 +/- 146.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=495000, episode_reward=218.40 +/- 121.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=496000, episode_reward=266.60 +/- 98.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=497000, episode_reward=118.20 +/- 214.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=498000, episode_reward=51.20 +/- 167.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=499000, episode_reward=319.80 +/- 123.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=500000, episode_reward=386.80 +/- 149.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Gamma = 0.99, Learning Rate = 3e-05, Activation Function = ReLU, Neurons = 32, Process noise = 0.1, Measurement noise = 0.0\n",
      "Eval num_timesteps=1000, episode_reward=-2405.40 +/- 61.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=-2448.00 +/- 99.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=3000, episode_reward=-2426.00 +/- 80.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=4000, episode_reward=-2356.20 +/- 57.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=5000, episode_reward=-2485.40 +/- 57.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=6000, episode_reward=-2427.20 +/- 65.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=7000, episode_reward=-2390.60 +/- 28.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=8000, episode_reward=-2425.20 +/- 74.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=9000, episode_reward=-2421.00 +/- 54.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=10000, episode_reward=-2498.00 +/- 76.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=11000, episode_reward=-2466.00 +/- 90.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=12000, episode_reward=-2447.80 +/- 110.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=13000, episode_reward=-2501.80 +/- 77.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=14000, episode_reward=-2443.60 +/- 66.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=15000, episode_reward=-2407.40 +/- 60.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=16000, episode_reward=-2480.40 +/- 62.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=17000, episode_reward=-2442.80 +/- 77.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=18000, episode_reward=-2467.60 +/- 70.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=19000, episode_reward=-2466.20 +/- 118.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=20000, episode_reward=-2421.60 +/- 79.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=21000, episode_reward=-2397.80 +/- 41.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=22000, episode_reward=-2450.60 +/- 98.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=23000, episode_reward=-2442.00 +/- 84.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=24000, episode_reward=-2412.20 +/- 53.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=25000, episode_reward=-2494.40 +/- 112.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=26000, episode_reward=-2485.60 +/- 64.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=27000, episode_reward=-2516.20 +/- 56.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=28000, episode_reward=-2411.40 +/- 35.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=29000, episode_reward=-2449.80 +/- 122.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=30000, episode_reward=-2398.00 +/- 124.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=31000, episode_reward=-2442.40 +/- 101.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=32000, episode_reward=-2413.20 +/- 91.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=33000, episode_reward=-2411.20 +/- 53.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=34000, episode_reward=-2481.20 +/- 45.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=35000, episode_reward=-2480.80 +/- 29.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=36000, episode_reward=-2394.60 +/- 85.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=37000, episode_reward=-2476.60 +/- 79.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=38000, episode_reward=-2515.00 +/- 71.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=39000, episode_reward=-2496.40 +/- 73.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=40000, episode_reward=-2471.00 +/- 66.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=41000, episode_reward=-2482.40 +/- 62.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=42000, episode_reward=-2465.40 +/- 59.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=43000, episode_reward=-2460.80 +/- 95.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=44000, episode_reward=-2399.40 +/- 82.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=45000, episode_reward=-2463.80 +/- 39.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=46000, episode_reward=-2490.60 +/- 69.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=47000, episode_reward=-2350.00 +/- 99.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=48000, episode_reward=-2502.20 +/- 36.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=49000, episode_reward=-2508.80 +/- 99.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=50000, episode_reward=-2488.40 +/- 37.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=51000, episode_reward=-1400.20 +/- 80.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=52000, episode_reward=-1585.60 +/- 277.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=53000, episode_reward=-1072.40 +/- 328.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=54000, episode_reward=-909.20 +/- 498.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=55000, episode_reward=-237.40 +/- 302.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=56000, episode_reward=42.40 +/- 172.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=57000, episode_reward=206.20 +/- 215.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=58000, episode_reward=-41.00 +/- 310.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=59000, episode_reward=-185.60 +/- 544.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=60000, episode_reward=-177.40 +/- 370.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=61000, episode_reward=-865.40 +/- 1184.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=62000, episode_reward=205.00 +/- 303.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=63000, episode_reward=-4.80 +/- 408.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=64000, episode_reward=-204.80 +/- 528.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=65000, episode_reward=-300.80 +/- 605.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=66000, episode_reward=175.80 +/- 116.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=67000, episode_reward=-148.40 +/- 414.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=68000, episode_reward=-30.00 +/- 266.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=69000, episode_reward=61.60 +/- 495.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=70000, episode_reward=-146.60 +/- 435.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=71000, episode_reward=-684.00 +/- 549.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=72000, episode_reward=-312.00 +/- 488.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=73000, episode_reward=-105.40 +/- 293.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=74000, episode_reward=90.00 +/- 218.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=75000, episode_reward=108.40 +/- 152.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=76000, episode_reward=-107.20 +/- 567.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=77000, episode_reward=-102.40 +/- 413.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=78000, episode_reward=-231.40 +/- 537.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=79000, episode_reward=-747.60 +/- 923.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=80000, episode_reward=-174.00 +/- 227.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=81000, episode_reward=-344.40 +/- 651.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=82000, episode_reward=-169.00 +/- 423.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=83000, episode_reward=-150.00 +/- 415.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=84000, episode_reward=-831.20 +/- 917.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=85000, episode_reward=-1054.60 +/- 1639.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=86000, episode_reward=-455.80 +/- 778.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=87000, episode_reward=47.80 +/- 350.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=88000, episode_reward=101.40 +/- 220.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=89000, episode_reward=1.20 +/- 366.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=90000, episode_reward=213.80 +/- 387.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=91000, episode_reward=74.60 +/- 304.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=92000, episode_reward=-147.40 +/- 412.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=93000, episode_reward=-226.20 +/- 396.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=94000, episode_reward=357.80 +/- 210.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=95000, episode_reward=-361.80 +/- 426.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=96000, episode_reward=-406.20 +/- 518.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=97000, episode_reward=89.40 +/- 196.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=98000, episode_reward=3.20 +/- 337.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=99000, episode_reward=-298.00 +/- 464.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=100000, episode_reward=38.20 +/- 149.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=101000, episode_reward=83.00 +/- 388.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=102000, episode_reward=-416.60 +/- 1034.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=103000, episode_reward=60.00 +/- 198.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=104000, episode_reward=-37.20 +/- 434.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=105000, episode_reward=84.40 +/- 140.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=106000, episode_reward=-56.20 +/- 342.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=107000, episode_reward=-129.00 +/- 480.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=108000, episode_reward=218.40 +/- 269.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=109000, episode_reward=-412.80 +/- 264.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=110000, episode_reward=-290.40 +/- 522.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=111000, episode_reward=-461.60 +/- 745.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=112000, episode_reward=184.80 +/- 341.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=113000, episode_reward=-40.00 +/- 42.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=114000, episode_reward=-46.60 +/- 427.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=115000, episode_reward=-115.40 +/- 377.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=116000, episode_reward=-360.80 +/- 936.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=117000, episode_reward=-382.20 +/- 1469.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=118000, episode_reward=-449.40 +/- 733.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=119000, episode_reward=-473.40 +/- 468.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=120000, episode_reward=239.20 +/- 169.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=121000, episode_reward=211.20 +/- 320.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=122000, episode_reward=262.60 +/- 124.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=123000, episode_reward=69.40 +/- 237.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=124000, episode_reward=-62.60 +/- 197.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=125000, episode_reward=25.20 +/- 351.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=126000, episode_reward=171.60 +/- 172.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=127000, episode_reward=38.00 +/- 103.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=128000, episode_reward=5.40 +/- 192.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=129000, episode_reward=-116.20 +/- 292.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=130000, episode_reward=101.40 +/- 358.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=131000, episode_reward=-46.60 +/- 258.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=132000, episode_reward=-32.00 +/- 182.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=133000, episode_reward=258.20 +/- 190.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=134000, episode_reward=114.80 +/- 245.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=135000, episode_reward=48.20 +/- 220.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=136000, episode_reward=142.60 +/- 121.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=137000, episode_reward=128.80 +/- 219.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=138000, episode_reward=312.60 +/- 203.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=139000, episode_reward=115.80 +/- 281.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=140000, episode_reward=227.20 +/- 149.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=141000, episode_reward=140.80 +/- 178.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=142000, episode_reward=191.80 +/- 209.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=143000, episode_reward=81.40 +/- 102.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=144000, episode_reward=83.60 +/- 74.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=145000, episode_reward=301.00 +/- 149.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=146000, episode_reward=199.80 +/- 399.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=147000, episode_reward=231.40 +/- 194.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=148000, episode_reward=202.00 +/- 265.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=149000, episode_reward=213.60 +/- 199.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=150000, episode_reward=125.60 +/- 115.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=151000, episode_reward=221.80 +/- 201.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=152000, episode_reward=305.00 +/- 172.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=153000, episode_reward=305.20 +/- 241.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=154000, episode_reward=419.80 +/- 270.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=155000, episode_reward=244.80 +/- 106.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=156000, episode_reward=291.60 +/- 135.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=157000, episode_reward=287.20 +/- 138.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=158000, episode_reward=230.40 +/- 285.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=159000, episode_reward=131.00 +/- 210.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=160000, episode_reward=263.80 +/- 81.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=161000, episode_reward=362.60 +/- 138.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=162000, episode_reward=122.20 +/- 93.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=163000, episode_reward=332.20 +/- 169.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=164000, episode_reward=360.80 +/- 117.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=165000, episode_reward=348.80 +/- 152.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=166000, episode_reward=401.60 +/- 94.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=167000, episode_reward=287.60 +/- 219.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=168000, episode_reward=317.80 +/- 145.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=169000, episode_reward=173.00 +/- 109.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=170000, episode_reward=364.40 +/- 220.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=171000, episode_reward=280.00 +/- 146.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=172000, episode_reward=271.80 +/- 73.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=173000, episode_reward=213.00 +/- 85.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=174000, episode_reward=430.40 +/- 51.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=175000, episode_reward=212.40 +/- 204.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=176000, episode_reward=326.00 +/- 272.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=177000, episode_reward=294.40 +/- 163.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=178000, episode_reward=264.60 +/- 140.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=179000, episode_reward=99.40 +/- 102.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=180000, episode_reward=360.00 +/- 159.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=181000, episode_reward=95.60 +/- 242.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=182000, episode_reward=184.20 +/- 205.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=183000, episode_reward=276.60 +/- 146.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=184000, episode_reward=219.80 +/- 233.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=185000, episode_reward=277.80 +/- 73.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=186000, episode_reward=141.40 +/- 230.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=187000, episode_reward=288.00 +/- 159.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=188000, episode_reward=200.60 +/- 222.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=189000, episode_reward=338.40 +/- 154.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=190000, episode_reward=390.00 +/- 171.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=191000, episode_reward=204.60 +/- 154.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=192000, episode_reward=208.20 +/- 249.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=193000, episode_reward=411.20 +/- 114.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=194000, episode_reward=282.60 +/- 165.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=195000, episode_reward=209.20 +/- 209.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=196000, episode_reward=328.60 +/- 151.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=197000, episode_reward=216.40 +/- 130.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=198000, episode_reward=258.20 +/- 110.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=199000, episode_reward=205.20 +/- 208.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=200000, episode_reward=316.00 +/- 147.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=201000, episode_reward=284.80 +/- 183.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=202000, episode_reward=309.00 +/- 147.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=203000, episode_reward=181.80 +/- 157.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=204000, episode_reward=267.40 +/- 132.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=205000, episode_reward=297.40 +/- 216.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=206000, episode_reward=209.20 +/- 291.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=207000, episode_reward=298.20 +/- 124.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=208000, episode_reward=282.80 +/- 190.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=209000, episode_reward=247.60 +/- 192.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=210000, episode_reward=457.40 +/- 165.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=211000, episode_reward=138.00 +/- 144.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=212000, episode_reward=208.00 +/- 181.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=213000, episode_reward=272.00 +/- 117.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=214000, episode_reward=410.80 +/- 134.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=215000, episode_reward=331.60 +/- 197.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=216000, episode_reward=276.80 +/- 228.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=217000, episode_reward=251.20 +/- 209.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=218000, episode_reward=297.00 +/- 149.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=219000, episode_reward=183.40 +/- 201.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=220000, episode_reward=280.20 +/- 261.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=221000, episode_reward=189.60 +/- 141.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=222000, episode_reward=222.40 +/- 152.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=223000, episode_reward=259.80 +/- 172.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=224000, episode_reward=285.20 +/- 109.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=225000, episode_reward=316.60 +/- 221.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=226000, episode_reward=302.60 +/- 156.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=227000, episode_reward=315.20 +/- 169.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=228000, episode_reward=222.80 +/- 147.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=229000, episode_reward=220.00 +/- 73.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=230000, episode_reward=180.40 +/- 242.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=231000, episode_reward=289.40 +/- 208.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=232000, episode_reward=164.00 +/- 200.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=233000, episode_reward=131.00 +/- 45.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=234000, episode_reward=254.20 +/- 251.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=235000, episode_reward=61.00 +/- 162.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=236000, episode_reward=315.00 +/- 171.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=237000, episode_reward=233.00 +/- 126.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=238000, episode_reward=299.60 +/- 223.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=239000, episode_reward=154.20 +/- 97.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=240000, episode_reward=310.80 +/- 270.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=241000, episode_reward=335.40 +/- 100.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=242000, episode_reward=123.80 +/- 67.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=243000, episode_reward=222.60 +/- 189.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=244000, episode_reward=248.80 +/- 202.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=245000, episode_reward=97.60 +/- 275.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=246000, episode_reward=246.20 +/- 247.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=247000, episode_reward=397.80 +/- 97.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=248000, episode_reward=142.20 +/- 211.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=249000, episode_reward=211.20 +/- 140.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=250000, episode_reward=255.00 +/- 239.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=251000, episode_reward=328.60 +/- 63.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=252000, episode_reward=273.60 +/- 285.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=253000, episode_reward=293.20 +/- 112.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=254000, episode_reward=265.00 +/- 341.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=255000, episode_reward=256.20 +/- 119.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=256000, episode_reward=269.20 +/- 145.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=257000, episode_reward=411.40 +/- 215.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=258000, episode_reward=213.00 +/- 164.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=259000, episode_reward=122.20 +/- 112.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=260000, episode_reward=324.00 +/- 255.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=261000, episode_reward=-3.80 +/- 280.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=262000, episode_reward=275.80 +/- 146.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=263000, episode_reward=277.20 +/- 125.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=264000, episode_reward=273.60 +/- 132.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=265000, episode_reward=187.80 +/- 160.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=266000, episode_reward=299.20 +/- 132.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=267000, episode_reward=164.20 +/- 162.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=268000, episode_reward=179.20 +/- 106.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=269000, episode_reward=90.60 +/- 195.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=270000, episode_reward=210.80 +/- 145.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=271000, episode_reward=117.80 +/- 104.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=272000, episode_reward=363.40 +/- 175.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=273000, episode_reward=210.00 +/- 274.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=274000, episode_reward=278.60 +/- 165.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=275000, episode_reward=171.20 +/- 109.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=276000, episode_reward=292.80 +/- 169.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=277000, episode_reward=294.80 +/- 143.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=278000, episode_reward=302.20 +/- 152.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=279000, episode_reward=414.80 +/- 238.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=280000, episode_reward=18.60 +/- 205.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=281000, episode_reward=263.40 +/- 40.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=282000, episode_reward=350.00 +/- 116.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=283000, episode_reward=108.00 +/- 190.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=284000, episode_reward=239.40 +/- 226.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=285000, episode_reward=366.80 +/- 147.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=286000, episode_reward=270.80 +/- 204.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=287000, episode_reward=221.60 +/- 90.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=288000, episode_reward=233.00 +/- 143.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=289000, episode_reward=321.20 +/- 203.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=290000, episode_reward=180.20 +/- 113.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=291000, episode_reward=246.20 +/- 265.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=292000, episode_reward=230.80 +/- 240.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=293000, episode_reward=227.20 +/- 234.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=294000, episode_reward=400.00 +/- 237.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=295000, episode_reward=387.00 +/- 157.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=296000, episode_reward=215.80 +/- 181.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=297000, episode_reward=185.40 +/- 76.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=298000, episode_reward=183.20 +/- 185.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=299000, episode_reward=211.80 +/- 261.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=300000, episode_reward=216.40 +/- 240.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=301000, episode_reward=204.20 +/- 112.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=302000, episode_reward=279.60 +/- 180.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=303000, episode_reward=44.20 +/- 212.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=304000, episode_reward=147.40 +/- 208.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=305000, episode_reward=128.60 +/- 156.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=306000, episode_reward=287.60 +/- 197.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=307000, episode_reward=176.00 +/- 205.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=308000, episode_reward=274.80 +/- 175.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=309000, episode_reward=300.40 +/- 194.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=310000, episode_reward=325.40 +/- 54.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=311000, episode_reward=144.40 +/- 104.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=312000, episode_reward=223.40 +/- 109.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=313000, episode_reward=145.40 +/- 110.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=314000, episode_reward=251.80 +/- 159.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=315000, episode_reward=238.60 +/- 181.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=316000, episode_reward=250.20 +/- 271.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=317000, episode_reward=278.00 +/- 81.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=318000, episode_reward=224.80 +/- 178.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=319000, episode_reward=192.00 +/- 96.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=320000, episode_reward=385.60 +/- 153.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=321000, episode_reward=247.40 +/- 226.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=322000, episode_reward=350.60 +/- 179.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=323000, episode_reward=250.60 +/- 63.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=324000, episode_reward=233.20 +/- 205.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=325000, episode_reward=237.80 +/- 69.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=326000, episode_reward=186.80 +/- 291.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=327000, episode_reward=162.20 +/- 336.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=328000, episode_reward=137.60 +/- 110.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=329000, episode_reward=166.00 +/- 195.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=330000, episode_reward=173.40 +/- 286.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=331000, episode_reward=141.60 +/- 74.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=332000, episode_reward=216.40 +/- 92.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=333000, episode_reward=197.80 +/- 224.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=334000, episode_reward=75.00 +/- 261.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=335000, episode_reward=219.60 +/- 234.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=336000, episode_reward=211.60 +/- 214.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=337000, episode_reward=390.40 +/- 149.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=338000, episode_reward=321.00 +/- 152.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=339000, episode_reward=372.20 +/- 110.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=340000, episode_reward=113.60 +/- 141.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=341000, episode_reward=200.80 +/- 216.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=342000, episode_reward=283.80 +/- 163.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=343000, episode_reward=227.60 +/- 153.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=344000, episode_reward=158.40 +/- 209.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=345000, episode_reward=412.20 +/- 253.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=346000, episode_reward=99.80 +/- 180.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=347000, episode_reward=127.60 +/- 137.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=348000, episode_reward=325.60 +/- 192.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=349000, episode_reward=169.40 +/- 153.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=350000, episode_reward=281.80 +/- 168.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=351000, episode_reward=295.60 +/- 270.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=352000, episode_reward=178.00 +/- 245.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=353000, episode_reward=166.40 +/- 149.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=354000, episode_reward=296.60 +/- 140.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=355000, episode_reward=143.20 +/- 205.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=356000, episode_reward=212.00 +/- 236.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=357000, episode_reward=350.60 +/- 138.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=358000, episode_reward=306.20 +/- 252.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=359000, episode_reward=264.80 +/- 148.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=360000, episode_reward=212.80 +/- 215.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=361000, episode_reward=261.40 +/- 266.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=362000, episode_reward=253.80 +/- 186.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=363000, episode_reward=178.00 +/- 306.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=364000, episode_reward=248.00 +/- 110.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=365000, episode_reward=123.80 +/- 162.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=366000, episode_reward=210.60 +/- 215.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=367000, episode_reward=218.60 +/- 135.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=368000, episode_reward=360.00 +/- 42.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=369000, episode_reward=322.80 +/- 149.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=370000, episode_reward=381.80 +/- 96.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=371000, episode_reward=66.00 +/- 131.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=372000, episode_reward=178.60 +/- 319.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=373000, episode_reward=174.40 +/- 211.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=374000, episode_reward=201.40 +/- 265.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=375000, episode_reward=224.60 +/- 86.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=376000, episode_reward=296.00 +/- 111.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=377000, episode_reward=305.60 +/- 194.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=378000, episode_reward=133.80 +/- 184.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=379000, episode_reward=290.00 +/- 226.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=380000, episode_reward=262.20 +/- 320.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=381000, episode_reward=289.80 +/- 144.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=382000, episode_reward=274.20 +/- 169.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=383000, episode_reward=473.40 +/- 133.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=384000, episode_reward=225.60 +/- 194.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=385000, episode_reward=236.00 +/- 230.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=386000, episode_reward=347.80 +/- 153.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=387000, episode_reward=412.20 +/- 141.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=388000, episode_reward=289.20 +/- 207.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=389000, episode_reward=250.60 +/- 98.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=390000, episode_reward=178.40 +/- 165.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=391000, episode_reward=369.80 +/- 160.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=392000, episode_reward=406.00 +/- 115.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=393000, episode_reward=100.80 +/- 85.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=394000, episode_reward=161.60 +/- 306.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=395000, episode_reward=277.00 +/- 57.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=396000, episode_reward=295.60 +/- 133.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=397000, episode_reward=198.40 +/- 107.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=398000, episode_reward=273.40 +/- 199.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=399000, episode_reward=274.20 +/- 133.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=400000, episode_reward=237.60 +/- 218.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=401000, episode_reward=195.00 +/- 212.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=402000, episode_reward=145.80 +/- 135.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=403000, episode_reward=245.60 +/- 380.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=404000, episode_reward=173.80 +/- 171.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=405000, episode_reward=147.00 +/- 160.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=406000, episode_reward=342.00 +/- 77.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=407000, episode_reward=295.40 +/- 262.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=408000, episode_reward=185.80 +/- 159.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=409000, episode_reward=456.60 +/- 65.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=410000, episode_reward=107.20 +/- 135.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=411000, episode_reward=253.00 +/- 280.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=412000, episode_reward=82.40 +/- 150.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=413000, episode_reward=245.40 +/- 119.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=414000, episode_reward=341.40 +/- 294.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=415000, episode_reward=272.20 +/- 180.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=416000, episode_reward=348.20 +/- 286.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=417000, episode_reward=294.20 +/- 118.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=418000, episode_reward=183.00 +/- 159.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=419000, episode_reward=147.40 +/- 171.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=420000, episode_reward=116.40 +/- 189.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=421000, episode_reward=351.60 +/- 124.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=422000, episode_reward=224.20 +/- 278.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=423000, episode_reward=226.40 +/- 148.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=424000, episode_reward=200.40 +/- 121.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=425000, episode_reward=348.00 +/- 66.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=426000, episode_reward=303.00 +/- 174.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=427000, episode_reward=406.60 +/- 173.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=428000, episode_reward=175.40 +/- 88.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=429000, episode_reward=289.60 +/- 151.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=430000, episode_reward=174.40 +/- 127.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=431000, episode_reward=348.20 +/- 153.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=432000, episode_reward=123.20 +/- 99.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=433000, episode_reward=165.20 +/- 247.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=434000, episode_reward=147.20 +/- 258.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=435000, episode_reward=217.80 +/- 137.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=436000, episode_reward=345.20 +/- 164.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=437000, episode_reward=266.40 +/- 139.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=438000, episode_reward=277.80 +/- 199.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=439000, episode_reward=319.60 +/- 180.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=440000, episode_reward=160.00 +/- 43.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=441000, episode_reward=363.40 +/- 173.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=442000, episode_reward=115.80 +/- 224.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=443000, episode_reward=281.40 +/- 260.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=444000, episode_reward=199.60 +/- 187.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=445000, episode_reward=-7.60 +/- 204.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=446000, episode_reward=296.60 +/- 243.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=447000, episode_reward=250.80 +/- 185.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=448000, episode_reward=132.80 +/- 231.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=449000, episode_reward=305.40 +/- 137.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=450000, episode_reward=278.80 +/- 92.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=451000, episode_reward=207.40 +/- 165.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=452000, episode_reward=276.00 +/- 318.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=453000, episode_reward=269.60 +/- 140.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=454000, episode_reward=177.20 +/- 280.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=455000, episode_reward=265.60 +/- 147.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=456000, episode_reward=230.80 +/- 222.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=457000, episode_reward=273.60 +/- 157.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=458000, episode_reward=337.20 +/- 298.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=459000, episode_reward=182.00 +/- 175.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=460000, episode_reward=227.20 +/- 270.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=461000, episode_reward=275.00 +/- 250.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=462000, episode_reward=292.20 +/- 177.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=463000, episode_reward=17.40 +/- 97.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=464000, episode_reward=210.00 +/- 127.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=465000, episode_reward=167.60 +/- 218.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=466000, episode_reward=100.20 +/- 153.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=467000, episode_reward=208.60 +/- 291.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=468000, episode_reward=169.40 +/- 191.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=469000, episode_reward=192.00 +/- 151.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=470000, episode_reward=226.40 +/- 161.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=471000, episode_reward=82.40 +/- 339.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=472000, episode_reward=184.80 +/- 225.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=473000, episode_reward=269.80 +/- 232.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=474000, episode_reward=341.60 +/- 149.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=475000, episode_reward=254.80 +/- 294.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=476000, episode_reward=174.60 +/- 132.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=477000, episode_reward=263.00 +/- 58.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=478000, episode_reward=213.00 +/- 237.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=479000, episode_reward=152.80 +/- 227.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=480000, episode_reward=-52.00 +/- 157.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=481000, episode_reward=284.20 +/- 184.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=482000, episode_reward=128.80 +/- 161.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=483000, episode_reward=314.40 +/- 169.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=484000, episode_reward=159.00 +/- 236.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=485000, episode_reward=245.20 +/- 150.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=486000, episode_reward=287.00 +/- 177.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=487000, episode_reward=194.00 +/- 121.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=488000, episode_reward=349.40 +/- 223.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=489000, episode_reward=259.40 +/- 168.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=490000, episode_reward=382.60 +/- 109.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=491000, episode_reward=158.60 +/- 217.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=492000, episode_reward=77.20 +/- 88.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=493000, episode_reward=302.40 +/- 131.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=494000, episode_reward=350.20 +/- 105.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=495000, episode_reward=248.40 +/- 200.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=496000, episode_reward=363.60 +/- 150.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=497000, episode_reward=122.60 +/- 197.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=498000, episode_reward=212.40 +/- 76.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=499000, episode_reward=303.60 +/- 174.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=500000, episode_reward=422.20 +/- 174.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Gamma = 0.99, Learning Rate = 3e-05, Activation Function = ReLU, Neurons = 64, Process noise = 0.1, Measurement noise = 0.0\n",
      "Eval num_timesteps=1000, episode_reward=-1167.40 +/- 132.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=-1160.20 +/- 283.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3000, episode_reward=-1223.20 +/- 192.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=4000, episode_reward=-1235.40 +/- 211.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=5000, episode_reward=-1217.80 +/- 293.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=6000, episode_reward=-1258.40 +/- 94.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=7000, episode_reward=-1214.40 +/- 151.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=8000, episode_reward=-1369.40 +/- 147.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=9000, episode_reward=-1115.60 +/- 192.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=10000, episode_reward=-1353.80 +/- 169.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=11000, episode_reward=-1306.80 +/- 159.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=12000, episode_reward=-1256.60 +/- 276.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=13000, episode_reward=-1256.40 +/- 58.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=14000, episode_reward=-1215.80 +/- 181.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=15000, episode_reward=-1256.00 +/- 96.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=16000, episode_reward=-1214.40 +/- 210.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=17000, episode_reward=-1418.60 +/- 293.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=18000, episode_reward=-1221.40 +/- 112.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=19000, episode_reward=-1286.00 +/- 105.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=20000, episode_reward=-1322.80 +/- 159.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=21000, episode_reward=-1350.40 +/- 281.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=22000, episode_reward=-1345.00 +/- 135.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=23000, episode_reward=-1132.40 +/- 192.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=24000, episode_reward=-1247.20 +/- 233.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=25000, episode_reward=-1371.00 +/- 94.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=26000, episode_reward=-1356.20 +/- 188.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=27000, episode_reward=-1170.20 +/- 129.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=28000, episode_reward=-1425.80 +/- 95.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=29000, episode_reward=-1302.60 +/- 66.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=30000, episode_reward=-1279.80 +/- 220.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=31000, episode_reward=-1245.00 +/- 227.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=32000, episode_reward=-1290.20 +/- 126.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=33000, episode_reward=-1331.00 +/- 72.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=34000, episode_reward=-1151.20 +/- 73.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=35000, episode_reward=-1197.20 +/- 247.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=36000, episode_reward=-1425.80 +/- 148.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=37000, episode_reward=-1327.00 +/- 154.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=38000, episode_reward=-1156.00 +/- 193.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=39000, episode_reward=-1232.20 +/- 161.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=40000, episode_reward=-1303.40 +/- 187.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=41000, episode_reward=-1402.20 +/- 223.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=42000, episode_reward=-1241.00 +/- 111.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=43000, episode_reward=-1199.80 +/- 84.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=44000, episode_reward=-1229.00 +/- 109.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=45000, episode_reward=-1259.20 +/- 167.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=46000, episode_reward=-1402.80 +/- 173.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=47000, episode_reward=-1240.00 +/- 207.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=48000, episode_reward=-1295.40 +/- 84.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=49000, episode_reward=-1237.60 +/- 140.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=50000, episode_reward=-1292.00 +/- 136.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=51000, episode_reward=-913.00 +/- 340.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=52000, episode_reward=-349.80 +/- 885.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=53000, episode_reward=-1363.20 +/- 959.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=54000, episode_reward=-641.00 +/- 289.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=55000, episode_reward=-1189.40 +/- 1093.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=56000, episode_reward=-339.20 +/- 508.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=57000, episode_reward=-369.40 +/- 893.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=58000, episode_reward=-255.60 +/- 294.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=59000, episode_reward=-364.60 +/- 419.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=60000, episode_reward=135.20 +/- 385.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=61000, episode_reward=-206.60 +/- 363.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=62000, episode_reward=38.40 +/- 344.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=63000, episode_reward=-111.60 +/- 460.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=64000, episode_reward=-399.00 +/- 756.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=65000, episode_reward=125.80 +/- 240.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=66000, episode_reward=-395.80 +/- 743.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=67000, episode_reward=-91.20 +/- 437.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=68000, episode_reward=-28.00 +/- 242.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=69000, episode_reward=-30.80 +/- 503.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=70000, episode_reward=83.20 +/- 398.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=71000, episode_reward=-10.60 +/- 678.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=72000, episode_reward=-235.60 +/- 289.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=73000, episode_reward=-91.20 +/- 392.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=74000, episode_reward=113.40 +/- 201.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=75000, episode_reward=-169.40 +/- 534.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=76000, episode_reward=22.00 +/- 397.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=77000, episode_reward=127.00 +/- 387.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=78000, episode_reward=-358.20 +/- 584.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=79000, episode_reward=-193.20 +/- 351.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=80000, episode_reward=-89.40 +/- 289.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=81000, episode_reward=-187.40 +/- 477.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=82000, episode_reward=87.00 +/- 285.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=83000, episode_reward=113.80 +/- 266.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=84000, episode_reward=-137.60 +/- 348.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=85000, episode_reward=-100.60 +/- 425.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=86000, episode_reward=-34.40 +/- 260.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=87000, episode_reward=34.20 +/- 336.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=88000, episode_reward=8.60 +/- 290.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=89000, episode_reward=-85.00 +/- 283.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=90000, episode_reward=46.40 +/- 189.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=91000, episode_reward=38.40 +/- 187.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=92000, episode_reward=130.80 +/- 297.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=93000, episode_reward=61.20 +/- 328.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=94000, episode_reward=243.80 +/- 157.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=95000, episode_reward=202.20 +/- 190.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=96000, episode_reward=195.20 +/- 146.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=97000, episode_reward=56.40 +/- 132.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=98000, episode_reward=472.00 +/- 88.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=99000, episode_reward=143.60 +/- 341.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=100000, episode_reward=265.80 +/- 285.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=101000, episode_reward=169.80 +/- 99.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=102000, episode_reward=83.60 +/- 312.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=103000, episode_reward=142.20 +/- 264.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=104000, episode_reward=112.00 +/- 132.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=105000, episode_reward=233.80 +/- 195.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=106000, episode_reward=206.20 +/- 71.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=107000, episode_reward=366.00 +/- 186.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=108000, episode_reward=276.80 +/- 165.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=109000, episode_reward=362.40 +/- 160.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=110000, episode_reward=228.20 +/- 228.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=111000, episode_reward=144.20 +/- 289.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=112000, episode_reward=230.60 +/- 220.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=113000, episode_reward=230.00 +/- 243.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=114000, episode_reward=318.80 +/- 123.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=115000, episode_reward=80.00 +/- 169.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=116000, episode_reward=300.20 +/- 298.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=117000, episode_reward=180.20 +/- 323.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=118000, episode_reward=228.80 +/- 169.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=119000, episode_reward=237.60 +/- 178.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=120000, episode_reward=135.80 +/- 102.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=121000, episode_reward=297.80 +/- 239.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=122000, episode_reward=177.60 +/- 261.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=123000, episode_reward=445.60 +/- 197.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=124000, episode_reward=162.40 +/- 112.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=125000, episode_reward=434.00 +/- 176.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=126000, episode_reward=196.00 +/- 257.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=127000, episode_reward=287.80 +/- 166.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=128000, episode_reward=372.20 +/- 141.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=129000, episode_reward=388.20 +/- 195.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=130000, episode_reward=206.80 +/- 134.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=131000, episode_reward=156.60 +/- 152.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=132000, episode_reward=237.00 +/- 221.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=133000, episode_reward=13.80 +/- 229.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=134000, episode_reward=177.40 +/- 272.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=135000, episode_reward=248.00 +/- 169.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=136000, episode_reward=212.80 +/- 148.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=137000, episode_reward=234.40 +/- 225.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=138000, episode_reward=239.40 +/- 227.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=139000, episode_reward=312.60 +/- 95.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=140000, episode_reward=146.80 +/- 128.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=141000, episode_reward=290.60 +/- 205.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=142000, episode_reward=358.20 +/- 222.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=143000, episode_reward=40.80 +/- 232.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=144000, episode_reward=307.20 +/- 146.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=145000, episode_reward=200.40 +/- 200.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=146000, episode_reward=395.40 +/- 198.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=147000, episode_reward=165.20 +/- 134.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=148000, episode_reward=180.80 +/- 142.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=149000, episode_reward=215.00 +/- 182.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=150000, episode_reward=350.40 +/- 36.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=151000, episode_reward=272.20 +/- 296.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=152000, episode_reward=66.40 +/- 186.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=153000, episode_reward=367.80 +/- 198.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=154000, episode_reward=207.60 +/- 310.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=155000, episode_reward=71.60 +/- 93.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=156000, episode_reward=109.80 +/- 55.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=157000, episode_reward=221.40 +/- 204.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=158000, episode_reward=283.20 +/- 188.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=159000, episode_reward=265.00 +/- 173.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=160000, episode_reward=224.40 +/- 155.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=161000, episode_reward=215.20 +/- 142.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=162000, episode_reward=346.40 +/- 141.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=163000, episode_reward=355.80 +/- 100.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=164000, episode_reward=169.00 +/- 93.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=165000, episode_reward=231.60 +/- 179.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=166000, episode_reward=91.40 +/- 90.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=167000, episode_reward=227.20 +/- 232.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=168000, episode_reward=153.80 +/- 166.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=169000, episode_reward=154.00 +/- 142.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=170000, episode_reward=206.60 +/- 170.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=171000, episode_reward=238.40 +/- 162.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=172000, episode_reward=69.20 +/- 240.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=173000, episode_reward=146.60 +/- 199.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=174000, episode_reward=315.00 +/- 130.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=175000, episode_reward=249.20 +/- 122.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=176000, episode_reward=120.20 +/- 143.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=177000, episode_reward=359.80 +/- 123.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=178000, episode_reward=53.00 +/- 178.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=179000, episode_reward=205.40 +/- 197.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=180000, episode_reward=321.40 +/- 52.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=181000, episode_reward=311.60 +/- 283.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=182000, episode_reward=164.00 +/- 57.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=183000, episode_reward=237.20 +/- 239.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=184000, episode_reward=123.20 +/- 307.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=185000, episode_reward=231.60 +/- 236.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=186000, episode_reward=-5.40 +/- 144.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=187000, episode_reward=276.80 +/- 223.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=188000, episode_reward=307.60 +/- 131.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=189000, episode_reward=325.00 +/- 151.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=190000, episode_reward=136.40 +/- 311.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=191000, episode_reward=264.00 +/- 193.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=192000, episode_reward=306.00 +/- 157.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=193000, episode_reward=196.00 +/- 61.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=194000, episode_reward=118.20 +/- 199.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=195000, episode_reward=476.80 +/- 257.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=196000, episode_reward=126.80 +/- 73.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=197000, episode_reward=274.00 +/- 56.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=198000, episode_reward=388.60 +/- 176.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=199000, episode_reward=226.00 +/- 201.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=200000, episode_reward=231.80 +/- 256.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=201000, episode_reward=195.20 +/- 242.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=202000, episode_reward=205.00 +/- 192.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=203000, episode_reward=231.20 +/- 128.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=204000, episode_reward=176.40 +/- 123.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=205000, episode_reward=245.60 +/- 210.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=206000, episode_reward=342.00 +/- 147.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=207000, episode_reward=150.80 +/- 272.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=208000, episode_reward=182.80 +/- 245.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=209000, episode_reward=213.00 +/- 197.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=210000, episode_reward=182.20 +/- 222.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=211000, episode_reward=249.80 +/- 61.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=212000, episode_reward=245.40 +/- 188.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=213000, episode_reward=95.60 +/- 115.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=214000, episode_reward=206.80 +/- 287.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=215000, episode_reward=260.40 +/- 236.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=216000, episode_reward=238.40 +/- 120.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=217000, episode_reward=267.00 +/- 180.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=218000, episode_reward=241.20 +/- 131.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=219000, episode_reward=307.00 +/- 197.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=220000, episode_reward=195.80 +/- 242.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=221000, episode_reward=189.80 +/- 238.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=222000, episode_reward=290.60 +/- 176.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=223000, episode_reward=330.80 +/- 116.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=224000, episode_reward=129.40 +/- 166.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=225000, episode_reward=248.80 +/- 178.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=226000, episode_reward=291.20 +/- 177.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=227000, episode_reward=294.60 +/- 129.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=228000, episode_reward=294.00 +/- 156.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=229000, episode_reward=227.40 +/- 172.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=230000, episode_reward=173.80 +/- 233.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=231000, episode_reward=216.60 +/- 175.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=232000, episode_reward=272.60 +/- 205.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=233000, episode_reward=124.60 +/- 167.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=234000, episode_reward=157.40 +/- 41.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=235000, episode_reward=46.80 +/- 245.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=236000, episode_reward=119.00 +/- 221.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=237000, episode_reward=267.20 +/- 209.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=238000, episode_reward=188.20 +/- 96.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=239000, episode_reward=252.60 +/- 209.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=240000, episode_reward=134.40 +/- 219.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=241000, episode_reward=220.00 +/- 220.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=242000, episode_reward=181.20 +/- 110.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=243000, episode_reward=376.80 +/- 120.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=244000, episode_reward=114.60 +/- 225.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=245000, episode_reward=217.80 +/- 257.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=246000, episode_reward=176.20 +/- 253.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=247000, episode_reward=128.20 +/- 170.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=248000, episode_reward=372.80 +/- 220.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=249000, episode_reward=171.80 +/- 331.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=250000, episode_reward=133.00 +/- 138.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=251000, episode_reward=237.40 +/- 198.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=252000, episode_reward=259.20 +/- 79.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=253000, episode_reward=88.20 +/- 92.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=254000, episode_reward=313.40 +/- 104.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=255000, episode_reward=297.80 +/- 152.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=256000, episode_reward=320.20 +/- 200.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=257000, episode_reward=215.00 +/- 186.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=258000, episode_reward=264.00 +/- 174.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=259000, episode_reward=255.40 +/- 175.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=260000, episode_reward=291.60 +/- 172.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=261000, episode_reward=296.60 +/- 230.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=262000, episode_reward=-72.00 +/- 191.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=263000, episode_reward=128.40 +/- 200.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=264000, episode_reward=178.80 +/- 242.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=265000, episode_reward=289.60 +/- 126.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=266000, episode_reward=116.00 +/- 196.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=267000, episode_reward=267.80 +/- 107.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=268000, episode_reward=301.40 +/- 155.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=269000, episode_reward=242.40 +/- 149.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=270000, episode_reward=351.60 +/- 114.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=271000, episode_reward=110.80 +/- 137.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=272000, episode_reward=275.20 +/- 122.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=273000, episode_reward=199.20 +/- 216.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=274000, episode_reward=155.60 +/- 165.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=275000, episode_reward=293.00 +/- 82.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=276000, episode_reward=240.80 +/- 119.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=277000, episode_reward=-5.00 +/- 151.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=278000, episode_reward=225.40 +/- 267.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=279000, episode_reward=345.00 +/- 225.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=280000, episode_reward=104.00 +/- 167.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=281000, episode_reward=170.00 +/- 212.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=282000, episode_reward=296.80 +/- 242.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=283000, episode_reward=90.60 +/- 235.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=284000, episode_reward=164.40 +/- 102.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=285000, episode_reward=24.00 +/- 290.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=286000, episode_reward=180.00 +/- 164.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=287000, episode_reward=149.60 +/- 200.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=288000, episode_reward=207.20 +/- 126.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=289000, episode_reward=318.00 +/- 167.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=290000, episode_reward=219.60 +/- 136.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=291000, episode_reward=72.00 +/- 202.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=292000, episode_reward=283.80 +/- 98.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=293000, episode_reward=441.60 +/- 173.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=294000, episode_reward=269.60 +/- 258.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=295000, episode_reward=276.80 +/- 317.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=296000, episode_reward=335.40 +/- 253.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=297000, episode_reward=129.40 +/- 101.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=298000, episode_reward=202.00 +/- 158.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=299000, episode_reward=187.40 +/- 389.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=300000, episode_reward=24.40 +/- 190.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=301000, episode_reward=120.40 +/- 173.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=302000, episode_reward=235.20 +/- 228.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=303000, episode_reward=327.80 +/- 188.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=304000, episode_reward=189.80 +/- 97.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=305000, episode_reward=162.80 +/- 183.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=306000, episode_reward=152.80 +/- 166.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=307000, episode_reward=374.80 +/- 123.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=308000, episode_reward=98.60 +/- 148.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=309000, episode_reward=127.80 +/- 126.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=310000, episode_reward=267.40 +/- 132.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=311000, episode_reward=115.00 +/- 146.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=312000, episode_reward=71.20 +/- 258.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=313000, episode_reward=166.20 +/- 242.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=314000, episode_reward=270.60 +/- 87.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=315000, episode_reward=47.60 +/- 128.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=316000, episode_reward=121.00 +/- 96.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=317000, episode_reward=249.20 +/- 155.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=318000, episode_reward=183.80 +/- 110.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=319000, episode_reward=229.40 +/- 273.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=320000, episode_reward=107.20 +/- 246.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=321000, episode_reward=23.80 +/- 220.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=322000, episode_reward=404.80 +/- 207.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=323000, episode_reward=22.60 +/- 158.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=324000, episode_reward=334.20 +/- 191.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=325000, episode_reward=188.40 +/- 158.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=326000, episode_reward=28.00 +/- 218.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=327000, episode_reward=310.20 +/- 179.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=328000, episode_reward=155.40 +/- 192.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=329000, episode_reward=102.40 +/- 158.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=330000, episode_reward=287.20 +/- 157.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=331000, episode_reward=77.20 +/- 104.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=332000, episode_reward=-45.20 +/- 219.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=333000, episode_reward=257.40 +/- 119.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=334000, episode_reward=149.40 +/- 189.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=335000, episode_reward=148.60 +/- 176.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=336000, episode_reward=196.60 +/- 204.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=337000, episode_reward=311.60 +/- 144.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=338000, episode_reward=397.40 +/- 88.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=339000, episode_reward=225.00 +/- 158.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=340000, episode_reward=193.80 +/- 142.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=341000, episode_reward=192.80 +/- 253.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=342000, episode_reward=372.60 +/- 204.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=343000, episode_reward=277.80 +/- 163.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=344000, episode_reward=365.20 +/- 118.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=345000, episode_reward=295.80 +/- 116.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=346000, episode_reward=239.80 +/- 186.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=347000, episode_reward=169.20 +/- 213.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=348000, episode_reward=311.20 +/- 151.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=349000, episode_reward=193.80 +/- 191.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=350000, episode_reward=-153.80 +/- 253.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=351000, episode_reward=324.40 +/- 147.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=352000, episode_reward=148.00 +/- 76.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=353000, episode_reward=195.60 +/- 185.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=354000, episode_reward=159.60 +/- 176.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=355000, episode_reward=100.60 +/- 318.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=356000, episode_reward=167.00 +/- 109.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=357000, episode_reward=246.60 +/- 83.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=358000, episode_reward=219.80 +/- 80.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=359000, episode_reward=371.80 +/- 169.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=360000, episode_reward=269.40 +/- 272.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=361000, episode_reward=416.40 +/- 347.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=362000, episode_reward=4.60 +/- 227.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=363000, episode_reward=268.60 +/- 172.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=364000, episode_reward=128.80 +/- 184.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=365000, episode_reward=235.00 +/- 109.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=366000, episode_reward=169.00 +/- 362.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=367000, episode_reward=122.20 +/- 419.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=368000, episode_reward=-76.00 +/- 472.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=369000, episode_reward=231.40 +/- 269.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=370000, episode_reward=127.80 +/- 594.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=371000, episode_reward=-163.00 +/- 260.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=372000, episode_reward=-703.60 +/- 903.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=373000, episode_reward=-2113.80 +/- 1865.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=374000, episode_reward=-1592.40 +/- 1421.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=375000, episode_reward=-1010.60 +/- 465.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=376000, episode_reward=-576.60 +/- 1444.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=377000, episode_reward=-3193.40 +/- 1023.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=378000, episode_reward=-995.60 +/- 1403.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=379000, episode_reward=-1098.40 +/- 981.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=380000, episode_reward=-1046.80 +/- 835.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=381000, episode_reward=-1198.60 +/- 1262.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=382000, episode_reward=-1919.80 +/- 1522.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=383000, episode_reward=-1999.60 +/- 1771.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=384000, episode_reward=-506.40 +/- 555.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=385000, episode_reward=-96.20 +/- 195.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=386000, episode_reward=-240.40 +/- 286.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=387000, episode_reward=-1748.40 +/- 1082.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=388000, episode_reward=-256.40 +/- 537.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=389000, episode_reward=-431.80 +/- 388.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=390000, episode_reward=-748.00 +/- 319.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=391000, episode_reward=-248.00 +/- 375.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=392000, episode_reward=-719.20 +/- 762.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=393000, episode_reward=-844.80 +/- 608.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=394000, episode_reward=-612.20 +/- 373.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=395000, episode_reward=-530.40 +/- 308.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=396000, episode_reward=-1011.80 +/- 623.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=397000, episode_reward=-919.00 +/- 329.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=398000, episode_reward=-487.00 +/- 753.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=399000, episode_reward=-981.80 +/- 778.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=400000, episode_reward=-698.40 +/- 541.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=401000, episode_reward=-667.40 +/- 391.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=402000, episode_reward=-300.80 +/- 453.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=403000, episode_reward=-262.80 +/- 382.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=404000, episode_reward=-585.20 +/- 607.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=405000, episode_reward=-846.40 +/- 272.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=406000, episode_reward=-130.20 +/- 433.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=407000, episode_reward=-395.80 +/- 485.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=408000, episode_reward=-685.80 +/- 823.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=409000, episode_reward=-433.00 +/- 456.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=410000, episode_reward=-424.60 +/- 773.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=411000, episode_reward=-115.60 +/- 334.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=412000, episode_reward=-521.80 +/- 526.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=413000, episode_reward=-685.00 +/- 578.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=414000, episode_reward=78.20 +/- 322.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=415000, episode_reward=-132.00 +/- 326.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=416000, episode_reward=265.40 +/- 258.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=417000, episode_reward=-684.20 +/- 345.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=418000, episode_reward=-279.00 +/- 553.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=419000, episode_reward=-523.60 +/- 783.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=420000, episode_reward=-702.40 +/- 454.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=421000, episode_reward=-204.80 +/- 819.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=422000, episode_reward=-2.20 +/- 194.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=423000, episode_reward=-244.00 +/- 296.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=424000, episode_reward=170.80 +/- 514.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=425000, episode_reward=-105.80 +/- 359.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=426000, episode_reward=-86.80 +/- 449.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=427000, episode_reward=-78.40 +/- 250.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=428000, episode_reward=-101.80 +/- 530.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=429000, episode_reward=-238.80 +/- 583.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=430000, episode_reward=159.40 +/- 116.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=431000, episode_reward=-71.00 +/- 168.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=432000, episode_reward=115.20 +/- 273.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=433000, episode_reward=239.40 +/- 188.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=434000, episode_reward=66.80 +/- 162.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=435000, episode_reward=13.80 +/- 268.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=436000, episode_reward=-5.60 +/- 251.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=437000, episode_reward=186.60 +/- 151.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=438000, episode_reward=137.20 +/- 113.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=439000, episode_reward=92.80 +/- 218.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=440000, episode_reward=254.40 +/- 93.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=441000, episode_reward=252.60 +/- 53.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=442000, episode_reward=-159.20 +/- 362.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=443000, episode_reward=-42.00 +/- 217.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=444000, episode_reward=18.20 +/- 394.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=445000, episode_reward=159.00 +/- 172.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=446000, episode_reward=256.60 +/- 202.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=447000, episode_reward=182.40 +/- 144.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=448000, episode_reward=0.60 +/- 146.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=449000, episode_reward=131.80 +/- 390.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=450000, episode_reward=166.00 +/- 347.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=451000, episode_reward=196.40 +/- 128.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=452000, episode_reward=106.20 +/- 118.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=453000, episode_reward=256.40 +/- 144.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=454000, episode_reward=353.20 +/- 239.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=455000, episode_reward=304.20 +/- 191.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=456000, episode_reward=305.40 +/- 116.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=457000, episode_reward=165.40 +/- 154.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=458000, episode_reward=285.00 +/- 130.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=459000, episode_reward=332.40 +/- 189.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=460000, episode_reward=247.20 +/- 187.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=461000, episode_reward=171.80 +/- 102.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=462000, episode_reward=333.60 +/- 138.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=463000, episode_reward=157.40 +/- 97.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=464000, episode_reward=181.40 +/- 183.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=465000, episode_reward=234.00 +/- 134.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=466000, episode_reward=336.80 +/- 87.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=467000, episode_reward=293.20 +/- 173.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=468000, episode_reward=299.00 +/- 74.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=469000, episode_reward=251.80 +/- 85.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=470000, episode_reward=314.20 +/- 143.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=471000, episode_reward=191.00 +/- 221.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=472000, episode_reward=214.60 +/- 194.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=473000, episode_reward=127.60 +/- 199.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=474000, episode_reward=373.40 +/- 108.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=475000, episode_reward=216.80 +/- 226.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=476000, episode_reward=288.00 +/- 81.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=477000, episode_reward=284.40 +/- 230.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=478000, episode_reward=156.00 +/- 171.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=479000, episode_reward=216.40 +/- 203.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=480000, episode_reward=201.80 +/- 161.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=481000, episode_reward=162.80 +/- 107.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=482000, episode_reward=148.80 +/- 219.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=483000, episode_reward=54.20 +/- 200.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=484000, episode_reward=235.00 +/- 143.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=485000, episode_reward=250.60 +/- 200.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=486000, episode_reward=325.60 +/- 136.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=487000, episode_reward=238.40 +/- 246.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=488000, episode_reward=206.60 +/- 179.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=489000, episode_reward=87.60 +/- 146.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=490000, episode_reward=279.20 +/- 199.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=491000, episode_reward=256.80 +/- 113.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=492000, episode_reward=275.20 +/- 263.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=493000, episode_reward=134.00 +/- 149.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=494000, episode_reward=173.80 +/- 85.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=495000, episode_reward=79.80 +/- 104.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=496000, episode_reward=280.40 +/- 184.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=497000, episode_reward=248.80 +/- 242.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=498000, episode_reward=312.60 +/- 198.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=499000, episode_reward=220.40 +/- 148.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=500000, episode_reward=265.60 +/- 170.06\n",
      "Episode length: 100.00 +/- 0.00\n"
     ]
    }
   ],
   "source": [
    "### REINFORCEMENT LEARNING I ###\n",
    "### TRAIN, SAVE, EVALUATE MODEL - HYPERPARAMETER TUNING, NOISE LEVELS ###\n",
    "\n",
    "import gym\n",
    "import stable_baselines3 as sb\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "import pickle\n",
    "import torch as th\n",
    "\n",
    "total_timesteps = 1e6\n",
    "for ite in [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]:\n",
    "    for gamma in [0.9]: #[0.8, 0.9, 0.99]: # Default PPO = 0.99 / DQN = 0.99 / A2C = 0.99\n",
    "            for learningrate in [0.003]: #[0.003, 0.0003, 0.00003]: # Default PPO = 0.0003 / DQN = 0.0001 / A2C = 0.0007\n",
    "                for actfn in [th.nn.ReLU]: #[th.nn.Tanh, th.nn.ReLU]: # Default PPO = Tanh / DQN = ReLU / A2C = Tanh\n",
    "                    # The ``net_arch`` parameter allows to specify the amount and size of the hidden layers and how many\n",
    "                    # of them are shared between the policy network and the value network. It is assumed to be a list with the following\n",
    "                    # structure:\n",
    "                    # 1. An arbitrary length (zero allowed) number of integers each specifying the number of units in a shared layer.\n",
    "                    #    If the number of ints is zero, there will be no shared layers.\n",
    "                    # 2. An optional dict, to specify the following non-shared layers for the value network and the policy network.\n",
    "                    #    It is formatted like ``dict(vf=[<value layer sizes>], pi=[<policy layer sizes>])``.\n",
    "                    #    If it is missing any of the keys (pi or vf), no non-shared layers (empty list) is assumed.\n",
    "                    # For example to construct a network with one shared layer of size 55 followed by two non-shared layers for the value\n",
    "                    # network of size 255 and a single non-shared layer of size 128 for the policy network, the following layers_spec\n",
    "                    # would be used: ``[55, dict(vf=[255, 255], pi=[128])]``. A simple shared network topology with two layers of size 128\n",
    "                    # would be specified as [128, 128].\n",
    "                    for neurons in [8]: #[8, 32, 64]: # Default PPO = net_arch = [dict(vf=[64, 64], pi=[64, 64])] / DQN = net_arch = [64, 64] / A2C = net_arch = [dict(vf=[64, 64], pi=[64, 64])]\n",
    "                        for pn in [1]: #[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]: # Default = 1\n",
    "                            for mn in [0]: #[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]: # Default = 0\n",
    "                                print('Gamma = ' + str(gamma) + ', Learning Rate = ' + str(learningrate) + ', Activation Function = ' + str(actfn.__name__) +\n",
    "                                ', Neurons = ' + str(neurons) + ', Process noise = ' + str(0.1*pn) + ', Measurement noise = ' + str(0.1*mn))\n",
    "                                diag_model = pickle.load(open('diagnostics/model_' + 'pn' + str(pn) + '_mn' + str(mn), 'rb'))\n",
    "                                env = gym.make('Production-v0', diag_model = diag_model, process_noise = 0.1*pn, prod_levels = 5, forecast = 0)\n",
    "                                # Callback for best model\n",
    "                                best_callback = EvalCallback(env, best_model_save_path='./REV1/callback/PPOBEST' + str(ite) + '_fc0_l5_g' + str(gamma) + '_lr' + str(learningrate) + '_act' + str(actfn.__name__) + '_nn' + str(neurons) + '_pn' + str(pn) + '_mn' + str(mn),\n",
    "                                                                            log_path='./REV1/callback/PPOBEST' + str(ite) + '_fc0_l5_g' + str(gamma) + '_lr' + str(learningrate) + '_act' + str(actfn.__name__) + '_nn' + str(neurons) + '_pn' + str(pn) + '_mn' + str(mn),\n",
    "                                                        eval_freq=1000, deterministic=True, render=False)\n",
    "                                model = sb.DQN('MlpPolicy', env, gamma = gamma, learning_rate = learningrate, policy_kwargs = dict(activation_fn=actfn, net_arch=[dict(vf=[neurons, neurons], pi=[neurons, neurons])]), tensorboard_log=\"./REV1/tensorboard/\")\n",
    "                                #model = sb.PPO('MlpPolicy', env, gamma = gamma, learning_rate = learningrate, tensorboard_log=\"./REV1/tensorboard/\")\n",
    "                                model.learn(total_timesteps=total_timesteps, tb_log_name='PPOBEST' + str(ite) + '_fc0_l5_g' + str(gamma) + '_lr' + str(learningrate) + '_act' + str(actfn.__name__) + '_nn' + str(neurons) + '_pn' + str(pn) + '_mn' + str(mn),\n",
    "                                            callback = best_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gamma = 0.8, Learning Rate = 0.003, Activation Function = Tanh, Neurons = 8, Process noise = 0.1, Measurement noise = 0.0\n",
      "The average number of reactive maintenance interventions per episode is:  2.82\n",
      "The average number of preventive maintenance interventions per episode is:  3.55\n",
      "The average sum of inventory per episode is:  172.69\n",
      "The average sum of spare parts inventory per episode is:  3.04\n",
      "The average reward per episode is:  1174.12\n",
      "The average upper bound per episode is:  3017.1\n",
      "Gamma = 0.8, Learning Rate = 0.003, Activation Function = Tanh, Neurons = 32, Process noise = 0.1, Measurement noise = 0.0\n",
      "The average number of reactive maintenance interventions per episode is:  3.08\n",
      "The average number of preventive maintenance interventions per episode is:  3.05\n",
      "The average sum of inventory per episode is:  150.62\n",
      "The average sum of spare parts inventory per episode is:  5.4\n",
      "The average reward per episode is:  1149.63\n",
      "The average upper bound per episode is:  2976.3\n",
      "Gamma = 0.8, Learning Rate = 0.003, Activation Function = Tanh, Neurons = 64, Process noise = 0.1, Measurement noise = 0.0\n",
      "The average number of reactive maintenance interventions per episode is:  3.26\n",
      "The average number of preventive maintenance interventions per episode is:  3.0\n",
      "The average sum of inventory per episode is:  149.12\n",
      "The average sum of spare parts inventory per episode is:  5.15\n",
      "The average reward per episode is:  1109.87\n",
      "The average upper bound per episode is:  2992.6\n",
      "Gamma = 0.8, Learning Rate = 0.003, Activation Function = ReLU, Neurons = 8, Process noise = 0.1, Measurement noise = 0.0\n",
      "The average number of reactive maintenance interventions per episode is:  6.44\n",
      "The average number of preventive maintenance interventions per episode is:  0.0\n",
      "The average sum of inventory per episode is:  155.73\n",
      "The average sum of spare parts inventory per episode is:  0.0\n",
      "The average reward per episode is:  384.77\n",
      "The average upper bound per episode is:  2977.6\n",
      "Gamma = 0.8, Learning Rate = 0.003, Activation Function = ReLU, Neurons = 32, Process noise = 0.1, Measurement noise = 0.0\n",
      "The average number of reactive maintenance interventions per episode is:  2.76\n",
      "The average number of preventive maintenance interventions per episode is:  3.77\n",
      "The average sum of inventory per episode is:  181.84\n",
      "The average sum of spare parts inventory per episode is:  6.55\n",
      "The average reward per episode is:  1252.47\n",
      "The average upper bound per episode is:  3003.7\n",
      "Gamma = 0.8, Learning Rate = 0.003, Activation Function = ReLU, Neurons = 64, Process noise = 0.1, Measurement noise = 0.0\n",
      "The average number of reactive maintenance interventions per episode is:  2.53\n",
      "The average number of preventive maintenance interventions per episode is:  3.96\n",
      "The average sum of inventory per episode is:  183.59\n",
      "The average sum of spare parts inventory per episode is:  4.62\n",
      "The average reward per episode is:  1348.66\n",
      "The average upper bound per episode is:  2972.6\n",
      "Gamma = 0.8, Learning Rate = 0.0003, Activation Function = Tanh, Neurons = 8, Process noise = 0.1, Measurement noise = 0.0\n",
      "The average number of reactive maintenance interventions per episode is:  3.3\n",
      "The average number of preventive maintenance interventions per episode is:  3.17\n",
      "The average sum of inventory per episode is:  282.63\n",
      "The average sum of spare parts inventory per episode is:  0.0\n",
      "The average reward per episode is:  761.7\n",
      "The average upper bound per episode is:  3029.9\n",
      "Gamma = 0.8, Learning Rate = 0.0003, Activation Function = Tanh, Neurons = 32, Process noise = 0.1, Measurement noise = 0.0\n",
      "The average number of reactive maintenance interventions per episode is:  3.46\n",
      "The average number of preventive maintenance interventions per episode is:  2.75\n",
      "The average sum of inventory per episode is:  214.73\n",
      "The average sum of spare parts inventory per episode is:  4.63\n",
      "The average reward per episode is:  905.97\n",
      "The average upper bound per episode is:  2997.8\n",
      "Gamma = 0.8, Learning Rate = 0.0003, Activation Function = Tanh, Neurons = 64, Process noise = 0.1, Measurement noise = 0.0\n",
      "The average number of reactive maintenance interventions per episode is:  3.36\n",
      "The average number of preventive maintenance interventions per episode is:  2.94\n",
      "The average sum of inventory per episode is:  174.57\n",
      "The average sum of spare parts inventory per episode is:  4.18\n",
      "The average reward per episode is:  1008.15\n",
      "The average upper bound per episode is:  2990.9\n",
      "Gamma = 0.8, Learning Rate = 0.0003, Activation Function = ReLU, Neurons = 8, Process noise = 0.1, Measurement noise = 0.0\n",
      "The average number of reactive maintenance interventions per episode is:  6.65\n",
      "The average number of preventive maintenance interventions per episode is:  0.0\n",
      "The average sum of inventory per episode is:  262.62\n",
      "The average sum of spare parts inventory per episode is:  0.0\n",
      "The average reward per episode is:  286.47\n",
      "The average upper bound per episode is:  2976.8\n",
      "Gamma = 0.8, Learning Rate = 0.0003, Activation Function = ReLU, Neurons = 32, Process noise = 0.1, Measurement noise = 0.0\n",
      "The average number of reactive maintenance interventions per episode is:  6.29\n",
      "The average number of preventive maintenance interventions per episode is:  0.0\n",
      "The average sum of inventory per episode is:  149.31\n",
      "The average sum of spare parts inventory per episode is:  0.0\n",
      "The average reward per episode is:  406.06\n",
      "The average upper bound per episode is:  2988.5\n",
      "Gamma = 0.8, Learning Rate = 0.0003, Activation Function = ReLU, Neurons = 64, Process noise = 0.1, Measurement noise = 0.0\n",
      "The average number of reactive maintenance interventions per episode is:  4.99\n",
      "The average number of preventive maintenance interventions per episode is:  1.15\n",
      "The average sum of inventory per episode is:  133.19\n",
      "The average sum of spare parts inventory per episode is:  0.0\n",
      "The average reward per episode is:  565.42\n",
      "The average upper bound per episode is:  3005.3\n",
      "Gamma = 0.8, Learning Rate = 3e-05, Activation Function = Tanh, Neurons = 8, Process noise = 0.1, Measurement noise = 0.0\n",
      "The average number of reactive maintenance interventions per episode is:  5.61\n",
      "The average number of preventive maintenance interventions per episode is:  1.73\n",
      "The average sum of inventory per episode is:  385.54\n",
      "The average sum of spare parts inventory per episode is:  2.67\n",
      "The average reward per episode is:  80.61\n",
      "The average upper bound per episode is:  2954.0\n",
      "Gamma = 0.8, Learning Rate = 3e-05, Activation Function = Tanh, Neurons = 32, Process noise = 0.1, Measurement noise = 0.0\n",
      "The average number of reactive maintenance interventions per episode is:  5.87\n",
      "The average number of preventive maintenance interventions per episode is:  0.0\n",
      "The average sum of inventory per episode is:  124.98\n",
      "The average sum of spare parts inventory per episode is:  0.0\n",
      "The average reward per episode is:  362.65\n",
      "The average upper bound per episode is:  2975.0\n",
      "Gamma = 0.8, Learning Rate = 3e-05, Activation Function = Tanh, Neurons = 64, Process noise = 0.1, Measurement noise = 0.0\n",
      "The average number of reactive maintenance interventions per episode is:  4.05\n",
      "The average number of preventive maintenance interventions per episode is:  3.16\n",
      "The average sum of inventory per episode is:  214.13\n",
      "The average sum of spare parts inventory per episode is:  0.5\n",
      "The average reward per episode is:  445.75\n",
      "The average upper bound per episode is:  3013.8\n",
      "Gamma = 0.8, Learning Rate = 3e-05, Activation Function = ReLU, Neurons = 8, Process noise = 0.1, Measurement noise = 0.0\n",
      "The average number of reactive maintenance interventions per episode is:  6.78\n",
      "The average number of preventive maintenance interventions per episode is:  0.0\n",
      "The average sum of inventory per episode is:  338.03\n",
      "The average sum of spare parts inventory per episode is:  0.0\n",
      "The average reward per episode is:  76.04\n",
      "The average upper bound per episode is:  3002.4\n",
      "Gamma = 0.8, Learning Rate = 3e-05, Activation Function = ReLU, Neurons = 32, Process noise = 0.1, Measurement noise = 0.0\n",
      "The average number of reactive maintenance interventions per episode is:  6.66\n",
      "The average number of preventive maintenance interventions per episode is:  0.0\n",
      "The average sum of inventory per episode is:  291.38\n",
      "The average sum of spare parts inventory per episode is:  0.0\n",
      "The average reward per episode is:  250.76\n",
      "The average upper bound per episode is:  3007.0\n",
      "Gamma = 0.8, Learning Rate = 3e-05, Activation Function = ReLU, Neurons = 64, Process noise = 0.1, Measurement noise = 0.0\n",
      "The average number of reactive maintenance interventions per episode is:  6.71\n",
      "The average number of preventive maintenance interventions per episode is:  0.0\n",
      "The average sum of inventory per episode is:  264.59\n",
      "The average sum of spare parts inventory per episode is:  0.0\n",
      "The average reward per episode is:  274.15\n",
      "The average upper bound per episode is:  2993.1\n",
      "Gamma = 0.9, Learning Rate = 0.003, Activation Function = Tanh, Neurons = 8, Process noise = 0.1, Measurement noise = 0.0\n",
      "The average number of reactive maintenance interventions per episode is:  2.1\n",
      "The average number of preventive maintenance interventions per episode is:  4.59\n",
      "The average sum of inventory per episode is:  158.46\n",
      "The average sum of spare parts inventory per episode is:  6.83\n",
      "The average reward per episode is:  1310.46\n",
      "The average upper bound per episode is:  3004.2\n",
      "Gamma = 0.9, Learning Rate = 0.003, Activation Function = Tanh, Neurons = 32, Process noise = 0.1, Measurement noise = 0.0\n",
      "The average number of reactive maintenance interventions per episode is:  3.41\n",
      "The average number of preventive maintenance interventions per episode is:  3.83\n",
      "The average sum of inventory per episode is:  158.83\n",
      "The average sum of spare parts inventory per episode is:  4.19\n",
      "The average reward per episode is:  999.56\n",
      "The average upper bound per episode is:  3029.5\n",
      "Gamma = 0.9, Learning Rate = 0.003, Activation Function = Tanh, Neurons = 64, Process noise = 0.1, Measurement noise = 0.0\n",
      "The average number of reactive maintenance interventions per episode is:  2.45\n",
      "The average number of preventive maintenance interventions per episode is:  4.06\n",
      "The average sum of inventory per episode is:  153.89\n",
      "The average sum of spare parts inventory per episode is:  10.84\n",
      "The average reward per episode is:  1339.33\n",
      "The average upper bound per episode is:  3005.4\n",
      "Gamma = 0.9, Learning Rate = 0.003, Activation Function = ReLU, Neurons = 8, Process noise = 0.1, Measurement noise = 0.0\n",
      "The average number of reactive maintenance interventions per episode is:  6.36\n",
      "The average number of preventive maintenance interventions per episode is:  0.0\n",
      "The average sum of inventory per episode is:  160.8\n",
      "The average sum of spare parts inventory per episode is:  0.0\n",
      "The average reward per episode is:  401.37\n",
      "The average upper bound per episode is:  3006.9\n",
      "Gamma = 0.9, Learning Rate = 0.003, Activation Function = ReLU, Neurons = 32, Process noise = 0.1, Measurement noise = 0.0\n",
      "The average number of reactive maintenance interventions per episode is:  1.51\n",
      "The average number of preventive maintenance interventions per episode is:  5.63\n",
      "The average sum of inventory per episode is:  188.44\n",
      "The average sum of spare parts inventory per episode is:  10.15\n",
      "The average reward per episode is:  1568.9\n",
      "The average upper bound per episode is:  2990.0\n",
      "Gamma = 0.9, Learning Rate = 0.003, Activation Function = ReLU, Neurons = 64, Process noise = 0.1, Measurement noise = 0.0\n",
      "The average number of reactive maintenance interventions per episode is:  1.73\n",
      "The average number of preventive maintenance interventions per episode is:  5.32\n",
      "The average sum of inventory per episode is:  254.94\n",
      "The average sum of spare parts inventory per episode is:  6.2\n",
      "The average reward per episode is:  1529.85\n",
      "The average upper bound per episode is:  3010.3\n",
      "Gamma = 0.9, Learning Rate = 0.0003, Activation Function = Tanh, Neurons = 8, Process noise = 0.1, Measurement noise = 0.0\n",
      "The average number of reactive maintenance interventions per episode is:  3.79\n",
      "The average number of preventive maintenance interventions per episode is:  2.99\n",
      "The average sum of inventory per episode is:  459.66\n",
      "The average sum of spare parts inventory per episode is:  0.0\n",
      "The average reward per episode is:  528.47\n",
      "The average upper bound per episode is:  2972.2\n",
      "Gamma = 0.9, Learning Rate = 0.0003, Activation Function = Tanh, Neurons = 32, Process noise = 0.1, Measurement noise = 0.0\n",
      "The average number of reactive maintenance interventions per episode is:  3.51\n",
      "The average number of preventive maintenance interventions per episode is:  3.28\n",
      "The average sum of inventory per episode is:  317.1\n",
      "The average sum of spare parts inventory per episode is:  0.0\n",
      "The average reward per episode is:  747.46\n",
      "The average upper bound per episode is:  2985.2\n",
      "Gamma = 0.9, Learning Rate = 0.0003, Activation Function = Tanh, Neurons = 64, Process noise = 0.1, Measurement noise = 0.0\n",
      "The average number of reactive maintenance interventions per episode is:  2.9\n",
      "The average number of preventive maintenance interventions per episode is:  3.69\n",
      "The average sum of inventory per episode is:  161.03\n",
      "The average sum of spare parts inventory per episode is:  0.92\n",
      "The average reward per episode is:  994.05\n",
      "The average upper bound per episode is:  2997.8\n",
      "Gamma = 0.9, Learning Rate = 0.0003, Activation Function = ReLU, Neurons = 8, Process noise = 0.1, Measurement noise = 0.0\n",
      "The average number of reactive maintenance interventions per episode is:  4.1\n",
      "The average number of preventive maintenance interventions per episode is:  3.37\n",
      "The average sum of inventory per episode is:  476.98\n",
      "The average sum of spare parts inventory per episode is:  0.0\n",
      "The average reward per episode is:  335.39\n",
      "The average upper bound per episode is:  2998.6\n",
      "Gamma = 0.9, Learning Rate = 0.0003, Activation Function = ReLU, Neurons = 32, Process noise = 0.1, Measurement noise = 0.0\n",
      "The average number of reactive maintenance interventions per episode is:  6.11\n",
      "The average number of preventive maintenance interventions per episode is:  0.0\n",
      "The average sum of inventory per episode is:  121.5\n",
      "The average sum of spare parts inventory per episode is:  0.0\n",
      "The average reward per episode is:  354.85\n",
      "The average upper bound per episode is:  3010.6\n",
      "Gamma = 0.9, Learning Rate = 0.0003, Activation Function = ReLU, Neurons = 64, Process noise = 0.1, Measurement noise = 0.0\n",
      "The average number of reactive maintenance interventions per episode is:  3.37\n",
      "The average number of preventive maintenance interventions per episode is:  3.5\n",
      "The average sum of inventory per episode is:  146.64\n",
      "The average sum of spare parts inventory per episode is:  1.14\n",
      "The average reward per episode is:  815.01\n",
      "The average upper bound per episode is:  2982.2\n",
      "Gamma = 0.9, Learning Rate = 3e-05, Activation Function = Tanh, Neurons = 8, Process noise = 0.1, Measurement noise = 0.0\n",
      "The average number of reactive maintenance interventions per episode is:  6.19\n",
      "The average number of preventive maintenance interventions per episode is:  0.0\n",
      "The average sum of inventory per episode is:  141.54\n",
      "The average sum of spare parts inventory per episode is:  0.0\n",
      "The average reward per episode is:  367.81\n",
      "The average upper bound per episode is:  3034.9\n",
      "Gamma = 0.9, Learning Rate = 3e-05, Activation Function = Tanh, Neurons = 32, Process noise = 0.1, Measurement noise = 0.0\n",
      "The average number of reactive maintenance interventions per episode is:  6.38\n",
      "The average number of preventive maintenance interventions per episode is:  0.32\n",
      "The average sum of inventory per episode is:  334.7\n",
      "The average sum of spare parts inventory per episode is:  0.0\n",
      "The average reward per episode is:  135.86\n",
      "The average upper bound per episode is:  2998.7\n",
      "Gamma = 0.9, Learning Rate = 3e-05, Activation Function = Tanh, Neurons = 64, Process noise = 0.1, Measurement noise = 0.0\n",
      "The average number of reactive maintenance interventions per episode is:  2.91\n",
      "The average number of preventive maintenance interventions per episode is:  3.58\n",
      "The average sum of inventory per episode is:  300.02\n",
      "The average sum of spare parts inventory per episode is:  0.0\n",
      "The average reward per episode is:  731.1\n",
      "The average upper bound per episode is:  2996.2\n",
      "Gamma = 0.9, Learning Rate = 3e-05, Activation Function = ReLU, Neurons = 8, Process noise = 0.1, Measurement noise = 0.0\n",
      "The average number of reactive maintenance interventions per episode is:  6.17\n",
      "The average number of preventive maintenance interventions per episode is:  0.0\n",
      "The average sum of inventory per episode is:  144.6\n",
      "The average sum of spare parts inventory per episode is:  0.0\n",
      "The average reward per episode is:  364.13\n",
      "The average upper bound per episode is:  3011.9\n",
      "Gamma = 0.9, Learning Rate = 3e-05, Activation Function = ReLU, Neurons = 32, Process noise = 0.1, Measurement noise = 0.0\n",
      "The average number of reactive maintenance interventions per episode is:  5.58\n",
      "The average number of preventive maintenance interventions per episode is:  0.0\n",
      "The average sum of inventory per episode is:  95.87\n",
      "The average sum of spare parts inventory per episode is:  0.0\n",
      "The average reward per episode is:  320.2\n",
      "The average upper bound per episode is:  3014.6\n",
      "Gamma = 0.9, Learning Rate = 3e-05, Activation Function = ReLU, Neurons = 64, Process noise = 0.1, Measurement noise = 0.0\n",
      "The average number of reactive maintenance interventions per episode is:  6.23\n",
      "The average number of preventive maintenance interventions per episode is:  0.0\n",
      "The average sum of inventory per episode is:  203.78\n",
      "The average sum of spare parts inventory per episode is:  0.0\n",
      "The average reward per episode is:  282.3\n",
      "The average upper bound per episode is:  2987.0\n",
      "Gamma = 0.99, Learning Rate = 0.003, Activation Function = Tanh, Neurons = 8, Process noise = 0.1, Measurement noise = 0.0\n",
      "The average number of reactive maintenance interventions per episode is:  2.56\n",
      "The average number of preventive maintenance interventions per episode is:  5.44\n",
      "The average sum of inventory per episode is:  423.32\n",
      "The average sum of spare parts inventory per episode is:  0.0\n",
      "The average reward per episode is:  620.09\n",
      "The average upper bound per episode is:  3010.0\n",
      "Gamma = 0.99, Learning Rate = 0.003, Activation Function = Tanh, Neurons = 32, Process noise = 0.1, Measurement noise = 0.0\n",
      "The average number of reactive maintenance interventions per episode is:  1.58\n",
      "The average number of preventive maintenance interventions per episode is:  5.32\n",
      "The average sum of inventory per episode is:  181.39\n",
      "The average sum of spare parts inventory per episode is:  5.03\n",
      "The average reward per episode is:  1422.77\n",
      "The average upper bound per episode is:  2991.8\n",
      "Gamma = 0.99, Learning Rate = 0.003, Activation Function = Tanh, Neurons = 64, Process noise = 0.1, Measurement noise = 0.0\n",
      "The average number of reactive maintenance interventions per episode is:  2.58\n",
      "The average number of preventive maintenance interventions per episode is:  3.76\n",
      "The average sum of inventory per episode is:  149.19\n",
      "The average sum of spare parts inventory per episode is:  2.76\n",
      "The average reward per episode is:  1225.55\n",
      "The average upper bound per episode is:  2985.4\n",
      "Gamma = 0.99, Learning Rate = 0.003, Activation Function = ReLU, Neurons = 8, Process noise = 0.1, Measurement noise = 0.0\n",
      "The average number of reactive maintenance interventions per episode is:  1.38\n",
      "The average number of preventive maintenance interventions per episode is:  6.28\n",
      "The average sum of inventory per episode is:  167.44\n",
      "The average sum of spare parts inventory per episode is:  12.1\n",
      "The average reward per episode is:  1527.45\n",
      "The average upper bound per episode is:  2997.8\n",
      "Gamma = 0.99, Learning Rate = 0.003, Activation Function = ReLU, Neurons = 32, Process noise = 0.1, Measurement noise = 0.0\n",
      "The average number of reactive maintenance interventions per episode is:  1.29\n",
      "The average number of preventive maintenance interventions per episode is:  6.0\n",
      "The average sum of inventory per episode is:  193.59\n",
      "The average sum of spare parts inventory per episode is:  11.92\n",
      "The average reward per episode is:  1582.83\n",
      "The average upper bound per episode is:  3003.2\n",
      "Gamma = 0.99, Learning Rate = 0.003, Activation Function = ReLU, Neurons = 64, Process noise = 0.1, Measurement noise = 0.0\n",
      "The average number of reactive maintenance interventions per episode is:  1.6\n",
      "The average number of preventive maintenance interventions per episode is:  5.67\n",
      "The average sum of inventory per episode is:  239.7\n",
      "The average sum of spare parts inventory per episode is:  6.85\n",
      "The average reward per episode is:  1540.7\n",
      "The average upper bound per episode is:  2982.8\n",
      "Gamma = 0.99, Learning Rate = 0.0003, Activation Function = Tanh, Neurons = 8, Process noise = 0.1, Measurement noise = 0.0\n",
      "The average number of reactive maintenance interventions per episode is:  2.43\n",
      "The average number of preventive maintenance interventions per episode is:  4.14\n",
      "The average sum of inventory per episode is:  305.5\n",
      "The average sum of spare parts inventory per episode is:  0.0\n",
      "The average reward per episode is:  843.72\n",
      "The average upper bound per episode is:  2972.4\n",
      "Gamma = 0.99, Learning Rate = 0.0003, Activation Function = Tanh, Neurons = 32, Process noise = 0.1, Measurement noise = 0.0\n",
      "The average number of reactive maintenance interventions per episode is:  1.77\n",
      "The average number of preventive maintenance interventions per episode is:  5.74\n",
      "The average sum of inventory per episode is:  257.41\n",
      "The average sum of spare parts inventory per episode is:  0.0\n",
      "The average reward per episode is:  838.01\n",
      "The average upper bound per episode is:  3019.8\n",
      "Gamma = 0.99, Learning Rate = 0.0003, Activation Function = Tanh, Neurons = 64, Process noise = 0.1, Measurement noise = 0.0\n",
      "The average number of reactive maintenance interventions per episode is:  1.19\n",
      "The average number of preventive maintenance interventions per episode is:  6.54\n",
      "The average sum of inventory per episode is:  228.51\n",
      "The average sum of spare parts inventory per episode is:  0.0\n",
      "The average reward per episode is:  932.41\n",
      "The average upper bound per episode is:  2979.7\n",
      "Gamma = 0.99, Learning Rate = 0.0003, Activation Function = ReLU, Neurons = 8, Process noise = 0.1, Measurement noise = 0.0\n",
      "The average number of reactive maintenance interventions per episode is:  3.04\n",
      "The average number of preventive maintenance interventions per episode is:  3.75\n",
      "The average sum of inventory per episode is:  223.67\n",
      "The average sum of spare parts inventory per episode is:  0.0\n",
      "The average reward per episode is:  748.03\n",
      "The average upper bound per episode is:  3040.6\n",
      "Gamma = 0.99, Learning Rate = 0.0003, Activation Function = ReLU, Neurons = 32, Process noise = 0.1, Measurement noise = 0.0\n",
      "The average number of reactive maintenance interventions per episode is:  6.16\n",
      "The average number of preventive maintenance interventions per episode is:  0.0\n",
      "The average sum of inventory per episode is:  142.27\n",
      "The average sum of spare parts inventory per episode is:  0.0\n",
      "The average reward per episode is:  414.98\n",
      "The average upper bound per episode is:  3016.0\n",
      "Gamma = 0.99, Learning Rate = 0.0003, Activation Function = ReLU, Neurons = 64, Process noise = 0.1, Measurement noise = 0.0\n",
      "The average number of reactive maintenance interventions per episode is:  1.89\n",
      "The average number of preventive maintenance interventions per episode is:  4.32\n",
      "The average sum of inventory per episode is:  118.57\n",
      "The average sum of spare parts inventory per episode is:  7.49\n",
      "The average reward per episode is:  1276.39\n",
      "The average upper bound per episode is:  2982.2\n",
      "Gamma = 0.99, Learning Rate = 3e-05, Activation Function = Tanh, Neurons = 8, Process noise = 0.1, Measurement noise = 0.0\n",
      "The average number of reactive maintenance interventions per episode is:  5.94\n",
      "The average number of preventive maintenance interventions per episode is:  1.17\n",
      "The average sum of inventory per episode is:  368.53\n",
      "The average sum of spare parts inventory per episode is:  1.76\n",
      "The average reward per episode is:  118.31\n",
      "The average upper bound per episode is:  3029.3\n",
      "Gamma = 0.99, Learning Rate = 3e-05, Activation Function = Tanh, Neurons = 32, Process noise = 0.1, Measurement noise = 0.0\n",
      "The average number of reactive maintenance interventions per episode is:  7.13\n",
      "The average number of preventive maintenance interventions per episode is:  0.0\n",
      "The average sum of inventory per episode is:  566.64\n",
      "The average sum of spare parts inventory per episode is:  0.0\n",
      "The average reward per episode is:  -248.33\n",
      "The average upper bound per episode is:  2982.0\n",
      "Gamma = 0.99, Learning Rate = 3e-05, Activation Function = Tanh, Neurons = 64, Process noise = 0.1, Measurement noise = 0.0\n",
      "The average number of reactive maintenance interventions per episode is:  6.07\n",
      "The average number of preventive maintenance interventions per episode is:  0.84\n",
      "The average sum of inventory per episode is:  622.39\n",
      "The average sum of spare parts inventory per episode is:  0.0\n",
      "The average reward per episode is:  -155.29\n",
      "The average upper bound per episode is:  2984.1\n",
      "Gamma = 0.99, Learning Rate = 3e-05, Activation Function = ReLU, Neurons = 8, Process noise = 0.1, Measurement noise = 0.0\n",
      "The average number of reactive maintenance interventions per episode is:  7.04\n",
      "The average number of preventive maintenance interventions per episode is:  0.0\n",
      "The average sum of inventory per episode is:  280.21\n",
      "The average sum of spare parts inventory per episode is:  0.0\n",
      "The average reward per episode is:  272.51\n",
      "The average upper bound per episode is:  3020.9\n",
      "Gamma = 0.99, Learning Rate = 3e-05, Activation Function = ReLU, Neurons = 32, Process noise = 0.1, Measurement noise = 0.0\n",
      "The average number of reactive maintenance interventions per episode is:  6.35\n",
      "The average number of preventive maintenance interventions per episode is:  0.0\n",
      "The average sum of inventory per episode is:  232.55\n",
      "The average sum of spare parts inventory per episode is:  0.0\n",
      "The average reward per episode is:  264.86\n",
      "The average upper bound per episode is:  3000.7\n",
      "Gamma = 0.99, Learning Rate = 3e-05, Activation Function = ReLU, Neurons = 64, Process noise = 0.1, Measurement noise = 0.0\n",
      "The average number of reactive maintenance interventions per episode is:  6.51\n",
      "The average number of preventive maintenance interventions per episode is:  0.0\n",
      "The average sum of inventory per episode is:  255.69\n",
      "The average sum of spare parts inventory per episode is:  0.0\n",
      "The average reward per episode is:  194.24\n",
      "The average upper bound per episode is:  2964.8\n"
     ]
    }
   ],
   "source": [
    "### REINFORCEMENT LEARNING II ###\n",
    "### TRY AND EVALUATE MY MODEL ###\n",
    "import pandas as pd\n",
    "from stable_baselines3 import PPO, DQN\n",
    "import gym\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "import torch as th\n",
    "\n",
    "for gamma in [0.8, 0.9, 0.99]: #[0.8, 0.9, 0.99]: # Default = 0.99\n",
    "        for learningrate in [0.003, 0.0003, 0.00003]: #[0.003, 0.0003, 0.00003]: # Default = 3e-4 = 0.0003\n",
    "            for actfn in [th.nn.Tanh, th.nn.ReLU]: #[th.nn.Tanh, th.nn.ReLU]: # Default = Tanh\n",
    "                for neurons in [8, 32, 64]: #[8, 32, 64]: # Default = 64\n",
    "                    for pn in [1]: #[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]: # Default = 1\n",
    "                        for mn in [0]: #[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]: # Default = 0\n",
    "                            print('Gamma = ' + str(gamma) + ', Learning Rate = ' + str(learningrate) + ', Activation Function = ' + str(actfn.__name__) +\n",
    "                                                ', Neurons = ' + str(neurons) + ', Process noise = ' + str(0.1*pn) + ', Measurement noise = ' + str(0.1*mn))\n",
    "                            diag_model = pickle.load(open('diagnostics/model_' + 'pn' + str(pn) + '_mn' + str(mn), 'rb'))\n",
    "                            env = gym.make('Production-v0', diag_model = diag_model, process_noise = 0.1*pn, prod_levels = 5, forecast = 0)\n",
    "                            model = DQN.load('./REV1/callback/DQN_fc0_l5_g' + str(gamma) + '_lr' + str(learningrate) + '_act' + str(actfn.__name__) + '_nn' + str(neurons) + '_pn' + str(pn) + '_mn' + str(mn) + '/best_model', env = env)\n",
    "                            \n",
    "                            # Set iterations\n",
    "                            iterations = 100\n",
    "                            # Initilaize Reward\n",
    "                            result_df = pd.DataFrame(np.nan, index=range(0,iterations), columns=['RM', 'PM', 'Inventory', 'Spare Parts Inventory', 'Reward', 'Upper'])\n",
    "                            # Calculate reward with no costs and fulfillment of all orders\n",
    "                            \n",
    "                            for i in range(iterations):\n",
    "                                # Initialize episode\n",
    "                                store = []\n",
    "                                obs = env.reset()\n",
    "                                done = False\n",
    "                                store.append([0, obs[0], env.breakdown, obs[1], obs[2], 0, done, env.old_order])\n",
    "                                # Compute one episode\n",
    "                                while not done:\n",
    "                                    # Get best action for state\n",
    "                                    action, _state = model.predict(obs, deterministic=True)\n",
    "                                    # Compute next state\n",
    "                                    #action = random.choice([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
    "                                    obs, reward, done, info = env.step(action)\n",
    "                                    # Store results of this episode\n",
    "                                    store.append([action, obs[0], env.breakdown, obs[1], obs[2], reward, done, env.old_order])\n",
    "                                eps_df = pd.DataFrame(store, columns=['action', 'health_rul', 'breakdown', 'inventory', 'sp_inventory', 'reward', 'done', 'old_order'])\n",
    "                                # Calculate nr. of reactive maintenance interventions by counting health 'resets' and substracting PM actions\n",
    "                                result_df.iloc[i]['RM'] = sum(eps_df['breakdown']==True)\n",
    "                                # Calculate nr. of preventive maintenance interventions\n",
    "                                result_df.iloc[i]['PM'] = sum(eps_df['action']== env.actions-1)\n",
    "                                # Calculate inventory\n",
    "                                result_df.iloc[i]['Inventory'] = sum(eps_df['inventory'])\n",
    "                                # Calculate spare parts inventory per period\n",
    "                                result_df.iloc[i]['Spare Parts Inventory'] = sum(eps_df['sp_inventory'])\n",
    "                                # Calculate reward\n",
    "                                result_df.iloc[i]['Reward'] = sum(eps_df['reward'])\n",
    "                                # Calculate reward with no costs and fulfillment of all orders\n",
    "                                result_df.iloc[i]['Upper'] = sum(eps_df['old_order']) * env.order_r\n",
    "\n",
    "                            print(\"The average number of reactive maintenance interventions per episode is: \", result_df['RM'].mean())\n",
    "                            print(\"The average number of preventive maintenance interventions per episode is: \", result_df['PM'].mean())\n",
    "                            print(\"The average sum of inventory per episode is: \", result_df['Inventory'].mean())\n",
    "                            print(\"The average sum of spare parts inventory per episode is: \", result_df['Spare Parts Inventory'].mean())\n",
    "                            print(\"The average reward per episode is: \", result_df['Reward'].mean())\n",
    "                            print(\"The average upper bound per episode is: \", result_df['Upper'].mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x900 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize Exponential Degradation\n",
    "import gym\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import random\n",
    "store = []\n",
    "diag_model = pickle.load(open('diagnostics/model_' + 'pn' + str(0) + '_mn' + str(0), 'rb'))\n",
    "env = gym.make('Production-v0', diag_model = diag_model, prod_levels = 5)\n",
    "obs = env.reset()\n",
    "done = False\n",
    "store.append([1])\n",
    "#while not done:\n",
    "for i in range(100):\n",
    "    # Compute next state\n",
    "    #action = random.choice([0, 1, 2, 3, 4])\n",
    "    obs, reward, done, info = env.step(4)\n",
    "    # Store results of this episode\n",
    "    store.append([env.true_health])\n",
    "eps_df = pd.DataFrame(store, columns=['health'])\n",
    "eps_df.plot(y='health')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x900 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "eps_df.plot(y='health_rul')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average number of reactive maintenance interventions per episode is:  5.92\n",
      "The average number of preventive maintenance interventions per episode is:  0.0\n",
      "The average mean time between failure per episode is:  14.887095238095235\n",
      "The standard deviation of the mean time between failure per episode is:  1.5405320836510183\n",
      "The average sum of inventory per episode is:  114.34\n",
      "The average sum of spare parts inventory per episode is:  0.0\n",
      "The average reward per episode is:  317.37\n",
      "The average upper bound per episode is:  2991.5\n"
     ]
    }
   ],
   "source": [
    "### REINFORCEMENT LEARNING III ###\n",
    "### EVALUATE REACTIVE MODEL ###\n",
    "\n",
    "import pandas as pd\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3 import PPO\n",
    "import numpy as np\n",
    "import math\n",
    "import gym\n",
    "\n",
    "env = gym.make('Production-v0', reactive_mode = True, forecast = 0, prod_levels = 5)\n",
    "# Initialize Reward\n",
    "# Set iterations\n",
    "iterations = 100\n",
    "result_df = pd.DataFrame(np.nan, index=range(0,iterations), columns=['RM', 'PM', 'MTBF', 'Inventory', 'Spare Parts Inventory', 'Reward', 'Upper'])\n",
    "\n",
    "for i in range(iterations):\n",
    "    # Initialize episode\n",
    "    store = []\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    store.append([0, env.true_health, env.breakdown, obs[0], obs[1], 0, done, env.old_order])\n",
    "    # Compute one episode\n",
    "    while not done:\n",
    "        # Get best action for state\n",
    "        action = min(env.prod_levels-1, max(0, round(env.old_order)))\n",
    "        # Compute next state\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        # Store results of this episode\n",
    "        store.append([action, env.true_health, env.breakdown, obs[0], obs[1], reward, done, env.old_order])\n",
    "    eps_df = pd.DataFrame(store, columns=['action', 'health', 'breakdown', 'inventory', 'sp_inventory', 'reward', 'done', 'old_order'])\n",
    "    # Calculate nr. of reactive maintenance interventions by counting health 'resets' and substracting PM actions\n",
    "    result_df.iloc[i]['RM'] = sum(eps_df['breakdown']==True)\n",
    "    # Calculate nr. of preventive maintenance interventions\n",
    "    result_df.iloc[i]['PM'] = sum(eps_df['action']== env.actions-1)\n",
    "    # Calculate mean time between failures\n",
    "    # Cut df after last breakdown\n",
    "    eps_df_trim = eps_df.iloc[:(np.where(eps_df['breakdown'].eq(True), eps_df.index, 0).max()+1)]\n",
    "    # Calculate MTBF by dividing periods where machine is running / breakdowns\n",
    "    result_df.iloc[i]['MTBF'] = (len(eps_df_trim) - sum(eps_df_trim['breakdown'] == True)) / sum(eps_df_trim['breakdown'] == True)\n",
    "    # Calculate inventory\n",
    "    result_df.iloc[i]['Inventory'] = sum(eps_df['inventory'])\n",
    "    # Calculate spare parts inventory per period\n",
    "    result_df.iloc[i]['Spare Parts Inventory'] = sum(eps_df['sp_inventory'])\n",
    "    # Calculate reward\n",
    "    result_df.iloc[i]['Reward'] = sum(eps_df['reward'])\n",
    "    # Calculate reward with no costs and fulfillment of all orders\n",
    "    result_df.iloc[i]['Upper'] = sum(eps_df['old_order']) * env.order_r\n",
    "\n",
    "print(\"The average number of reactive maintenance interventions per episode is: \", result_df['RM'].mean())\n",
    "print(\"The average number of preventive maintenance interventions per episode is: \", result_df['PM'].mean())\n",
    "print(\"The average mean time between failure per episode is: \", result_df['MTBF'].mean())\n",
    "print(\"The standard deviation of the mean time between failure per episode is: \", result_df['MTBF'].std())\n",
    "print(\"The average sum of inventory per episode is: \", result_df['Inventory'].mean())\n",
    "print(\"The average sum of spare parts inventory per episode is: \", result_df['Spare Parts Inventory'].mean())\n",
    "print(\"The average reward per episode is: \", result_df['Reward'].mean())\n",
    "print(\"The average upper bound per episode is: \", result_df['Upper'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Maintenance Interval:  11 Coefficient:  0.55\n",
      "The average number of reactive maintenance interventions per episode is:  1.65\n",
      "The average number of preventive maintenance interventions per episode is:  6.57\n",
      "The average sum of inventory per episode is:  156.97\n",
      "The average sum of spare parts inventory per episode is:  6.63\n",
      "The average reward per episode is:  1221.5\n",
      "The average upper bound per episode is:  3010.1\n"
     ]
    }
   ],
   "source": [
    "### REINFORCEMENT LEARNING IV ###\n",
    "### EVALUATE TIME-BASED PREVENTIVE MODEL ###\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gym\n",
    "import math\n",
    "\n",
    "env = gym.make('Production-v0', forecast = 0, prod_levels = 6)\n",
    "interval = range (11, 12)\n",
    "# Set iterations\n",
    "iterations = 100\n",
    "\n",
    "for k in interval:\n",
    "    # Initilaize Reward\n",
    "    result_df = pd.DataFrame(np.nan, index=range(0,100), columns=['RM', 'PM', 'Inventory', 'Spare Parts Inventory', 'Reward', 'Upper'])\n",
    "\n",
    "    for i in range(iterations):\n",
    "        # Initialize episode\n",
    "        store = []\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        store.append([0, obs[0], env.breakdown, obs[1], obs[2], 0, done, env.old_order])\n",
    "        # Compute one episode\n",
    "        while not done:\n",
    "            # One period before maintenance: action = order + spare part order\n",
    "            if env.scheduled_maintenance_counter == k-1:\n",
    "                action = min(env.prod_levels-1, max(0, round(env.old_order))) + env.prod_levels\n",
    "            # At period of mtbf: maintain\n",
    "            elif env.scheduled_maintenance_counter == k:\n",
    "                action = env.actions-1\n",
    "            # Else: action = order    \n",
    "            else:             \n",
    "                action = min(env.prod_levels-1, max(0, round(env.old_order)))\n",
    "            # Compute next state\n",
    "            obs, reward, done, info = env.step(action)\n",
    "            # Store results of this episode\n",
    "            store.append([action, obs[0], env.breakdown, obs[1], obs[2], reward, done, env.old_order])\n",
    "        eps_df = pd.DataFrame(store, columns=['action', 'health', 'breakdown', 'inventory', 'sp_inventory', 'reward', 'done', 'old_order'])\n",
    "        # Calculate nr. of reactive maintenance interventions by counting health 'resets' and substracting PM actions\n",
    "        result_df.iloc[i]['RM'] = sum(eps_df['breakdown']==True)\n",
    "        # Calculate nr. of preventive maintenance interventions\n",
    "        result_df.iloc[i]['PM'] = sum(eps_df['action']==env.actions-1)\n",
    "        # Calculate inventory\n",
    "        result_df.iloc[i]['Inventory'] = sum(eps_df['inventory'])\n",
    "        # Calculate spare parts inventory per period\n",
    "        result_df.iloc[i]['Spare Parts Inventory'] = sum(eps_df['sp_inventory'])\n",
    "        # Calculate reward\n",
    "        result_df.iloc[i]['Reward'] = sum(eps_df['reward'])\n",
    "        # Calculate reward with no costs and fulfillment of all orders\n",
    "        result_df.iloc[i]['Upper'] = sum(eps_df['old_order']) * env.order_r\n",
    "\n",
    "    print(\"\\n\", \"Maintenance Interval: \", k, \"Coefficient: \", 0+0.05*k)\n",
    "    print(\"The average number of reactive maintenance interventions per episode is: \", result_df['RM'].mean())\n",
    "    print(\"The average number of preventive maintenance interventions per episode is: \", result_df['PM'].mean())\n",
    "    print(\"The average sum of inventory per episode is: \", result_df['Inventory'].mean())\n",
    "    print(\"The average sum of spare parts inventory per episode is: \", result_df['Spare Parts Inventory'].mean())\n",
    "    print(\"The average reward per episode is: \", result_df['Reward'].mean())\n",
    "    print(\"The average upper bound per episode is: \", result_df['Upper'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "### REINFORCEMENT LEARNING Va ###\n",
    "### VISUALIZE STATE-ACTION ###\n",
    "import numpy as np\n",
    "state_action = []\n",
    "\n",
    "# Define observation grid\n",
    "grid_health = np.arange(0.0, 1.01, 0.01)\n",
    "grid_order = range(0, 5)\n",
    "grid_inventory = range(0, 10)\n",
    "grid_sp_inventory = [0, 1]\n",
    "\n",
    "# Loop through grid and store best action for each state\n",
    "for hlt in grid_health:\n",
    "    for ord in grid_order:\n",
    "        for inv in grid_inventory:\n",
    "            for sin in grid_sp_inventory:\n",
    "                # Predict\n",
    "                action, _state = model.predict((hlt, ord, inv, sin), deterministic=True)\n",
    "                state_action.append([hlt, ord, inv, sin, action])\n",
    "\n",
    "state_action_df = pd.DataFrame(state_action, columns=['health', 'order', 'inventory', 'sp_inventory', 'action'])\n",
    "state_action_df.to_excel(\"visuals/state_action.xlsx\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### REINFORCEMENT LEARNING Vb ###\n",
    "### VISUALIZE STATE-VALUE ###\n",
    "import numpy as np\n",
    "state_action = []\n",
    "\n",
    "# Define observation grid\n",
    "grid_health = np.arange(0.0, 1.01, 0.01)\n",
    "grid_order = range(0, 5)\n",
    "grid_inventory = range(0, 10)\n",
    "grid_sp_inventory = [0, 1]\n",
    "\n",
    "# Loop through grid and store best action for each state\n",
    "for hlt in grid_health:\n",
    "    for ord in grid_order:\n",
    "        for inv in grid_inventory:\n",
    "            for sin in grid_sp_inventory:\n",
    "                # Predict\n",
    "                obs, _ = model.policy.obs_to_tensor(hlt, ord, inv, sin)\n",
    "                value = model.policy.predict_values(obs).item()\n",
    "                state_action.append([hlt, ord, inv, sin, value])\n",
    "\n",
    "state_action_df = pd.DataFrame(state_action, columns=['health', 'order', 'inventory', 'sp_inventory', 'value'])\n",
    "state_action_df.to_excel(\"visuals/state_value.xlsx\") \n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6f69c5940b32a5cbabe45c9825076a627c6cdb9ede58cf4d0fa74ca6057ffe74"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
