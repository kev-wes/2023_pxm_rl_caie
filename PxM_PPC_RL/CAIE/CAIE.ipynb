{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process noise = 0.0, Measurement noise = 0.0\n",
      "Process noise = 0.1, Measurement noise = 0.0\n",
      "Process noise = 0.2, Measurement noise = 0.0\n",
      "Process noise = 0.30000000000000004, Measurement noise = 0.0\n",
      "Process noise = 0.4, Measurement noise = 0.0\n",
      "Process noise = 0.5, Measurement noise = 0.0\n",
      "Process noise = 0.6000000000000001, Measurement noise = 0.0\n",
      "Process noise = 0.7000000000000001, Measurement noise = 0.0\n",
      "Process noise = 0.8, Measurement noise = 0.0\n",
      "Process noise = 0.9, Measurement noise = 0.0\n",
      "Process noise = 1.0, Measurement noise = 0.0\n"
     ]
    }
   ],
   "source": [
    "### DATA-DRIVEN DIAGNOSTICS I ###\n",
    "### GENERATE DATA FOR DATA-DRIVEN MODEL ###\n",
    "\n",
    "import random\n",
    "from prog_models.models import BatteryCircuit\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "# Ignore warnings when machine exceeds its end of life\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\"\"\" Method that uses a physical machine model from the prog_models package and a current (health) state of the model and\n",
    "an action (i.e., intensity), which is performed for 100 time steps\n",
    "    Parameter:\n",
    "        machine             machine model from the prog_models package\n",
    "        state               current (health) state of the model\n",
    "        action              loading of the machine for the next 100 time steps\n",
    "    Return:\n",
    "        health                               \n",
    "    \"\"\"\n",
    "def produce_model(machine, states, action):\n",
    "        \n",
    "        # Define load of battery\n",
    "        def future_loading(t, x=None):\n",
    "            return {'i': action}\n",
    "\n",
    "        # Set current state of machine\n",
    "        machine.parameters['x0'] = states\n",
    "        # Simulate 100 steps\n",
    "        options = {\n",
    "            'save_freq': 100,  # Frequency at which results are saved\n",
    "            'dt': 2  # Timestep\n",
    "        }\n",
    "        (_, _, states, outputs, event_states) = machine.simulate_to(100, future_loading, **options)\n",
    "        health = event_states[-1]['EOD']\n",
    "        return(round(health, 2), states[-1], outputs[-1]['t'], outputs[-1]['v'])\n",
    "def reset_states(machine):\n",
    "    # Returns initial states of machine, e.g., {'tb': 18.95, 'qb': 7856.3254, 'qcp': 0, 'qcs': 0} for Battery\n",
    "    return(machine.default_parameters['x0'])\n",
    "   \n",
    "for pn in [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]: \n",
    "    for mn in [0]:\n",
    "        print('Process noise = ' + str(0.1*pn) + ', Measurement noise = ' + str(0.1*mn))\n",
    "        battery = BatteryCircuit(process_noise = 0.1*pn, measurement_noise = 0.1*mn)\n",
    "        states = reset_states(battery)\n",
    "        reset_counter = 0\n",
    "        dataset = []\n",
    "        for i in range(int(1e4)):\n",
    "            # If asset failed last period, reset all historical values\n",
    "            if reset_counter == 0: t = v = t_1 = v_1 = t_2 = v_2 = t_3 = v_3 = 0 \n",
    "            # Shift history by one time period\n",
    "            v_3 = v_2\n",
    "            t_3 = t_2\n",
    "            v_2 = v_1\n",
    "            t_2 = t_1\n",
    "            v_1 = v\n",
    "            t_1 = t\n",
    "\n",
    "            # Increment reset_counter\n",
    "            reset_counter = reset_counter + 1\n",
    "            # Compute new health, states, t, and v using last battery state and a random new action\n",
    "            health, states, t, v = produce_model(machine=battery, states=states, action=random.sample((0, 1, 2, 3, 4), 1)[0])\n",
    "            \n",
    "            if health <= 0: \n",
    "                # Reset battery states to initialize battery for next produce_model call\n",
    "                states = reset_states(battery)\n",
    "                # Initialize reset_counter\n",
    "                reset_counter = 0\n",
    "                # Sometimes produce_model returns weird or negative values as the end of life is exceeded\n",
    "                # Here, we just simply set it to zero to not confuse a later learner \n",
    "                health = 0\n",
    "\n",
    "            # append to two-dimensional list\n",
    "            dataset.append([t, v, t_1, v_1, t_2, v_2, t_3, v_3, health])\n",
    "\n",
    "        # Transform two-dim list to dataframe\n",
    "        dataset = pd.DataFrame(dataset, columns=['t', 'v', 't_1', 'v_1', 't_2', 'v_2', 't_3', 'v_3', 'health'])\n",
    "        # Denote machine runs to-failure with incrementing id\n",
    "        k = 0\n",
    "        for i in range(dataset.shape[0]):\n",
    "            if (dataset.iloc[i]['t_1'] == 0 and dataset.iloc[i]['v_1'] == 0):\n",
    "                j = 1\n",
    "                k = k + 1\n",
    "            dataset.loc[i, 'time'] = j\n",
    "            j = j + 1\n",
    "            dataset.loc[i, 'ID'] = k\n",
    "        dataset = dataset.sort_values(['ID', 'time'], ascending=[True, False])\n",
    "        # Assign RUL by counting upwards per serial number in the descended data frame\n",
    "        dataset['RUL'] = dataset.groupby((dataset['ID'] != dataset['ID'].shift(1)).cumsum()).cumcount()\n",
    "        dataset = dataset.sort_values(['ID', 'time'], ascending=True)\n",
    "\n",
    "        # Save it as pickle\n",
    "        dataset.to_pickle('diagnostics/data_' + 'pn' + str(pn) + '_mn' + str(mn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model based on RUL (prognostics = True) or health (diagnostics, prognostics = False)\n",
    "prognostics = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process noise = 0.0, Measurement noise = 0.0\n",
      "R2: [0.98346924 0.97869884 0.98271119 0.98084096 0.98029931]\n",
      "mae: [-0.0186088  -0.02097155 -0.01949765 -0.0201391  -0.0208858 ]\n",
      "mape: [-7.86658635e-02 -3.23583633e+11 -9.00719925e+09 -9.20286148e-02\n",
      " -1.71587146e+11]\n",
      "rmse: [-0.03669848 -0.04135376 -0.03768967 -0.03960828 -0.04014499]\n",
      "Process noise = 0.1, Measurement noise = 0.0\n",
      "R2: [0.94511725 0.94338909 0.94308702 0.95269309 0.92781427]\n",
      "mae: [-0.04355035 -0.0430732  -0.0434886  -0.0409953  -0.04892035]\n",
      "mape: [-1.57625987e+09 -6.75539944e+09 -2.31935381e+10 -3.03992975e+10\n",
      " -3.69295169e+11]\n",
      "rmse: [-0.06585241 -0.06812947 -0.06765209 -0.06181947 -0.07649472]\n",
      "Process noise = 0.2, Measurement noise = 0.0\n",
      "R2: [0.89160295 0.90719555 0.90106071 0.89986382 0.8983238 ]\n",
      "mae: [-0.06726765 -0.0624331  -0.0637716  -0.0658622  -0.0643069 ]\n",
      "mape: [-3.90912448e+11 -3.67493730e+11 -1.03605309e+12 -7.60657977e+11\n",
      " -1.41413028e+11]\n",
      "rmse: [-0.09406841 -0.08643758 -0.08901846 -0.09017457 -0.0908437 ]\n",
      "Process noise = 0.30000000000000004, Measurement noise = 0.0\n",
      "R2: [0.87037827 0.84902257 0.84585148 0.85446821 0.85283189]\n",
      "mae: [-0.07610885 -0.0816426  -0.0808236  -0.0798498  -0.0796984 ]\n",
      "mape: [-8.21906932e+10 -2.44320280e+11 -3.61413870e+11 -2.58281439e+11\n",
      " -2.56705179e+10]\n",
      "rmse: [-0.10279177 -0.11007041 -0.11067439 -0.10907084 -0.10878756]\n",
      "Process noise = 0.4, Measurement noise = 0.0\n",
      "R2: [0.83456832 0.83012227 0.80835482 0.83903277 0.81974217]\n",
      "mae: [-0.0846392  -0.0875222  -0.09059015 -0.08526315 -0.090239  ]\n",
      "mape: [-1.02456892e+11 -4.00820367e+11 -2.39816680e+11 -2.95661316e+11\n",
      " -6.73288144e+10]\n",
      "rmse: [-0.11565337 -0.11667704 -0.12263368 -0.11445298 -0.12049927]\n",
      "Process noise = 0.5, Measurement noise = 0.0\n",
      "R2: [0.82839126 0.80480565 0.82500873 0.8179684  0.817267  ]\n",
      "mae: [-0.09095045 -0.096345   -0.0892272  -0.09109585 -0.09089715]\n",
      "mape: [-4.76480841e+11 -2.50850499e+11 -2.85978576e+11 -7.34086739e+10\n",
      " -4.02171447e+11]\n",
      "rmse: [-0.11899326 -0.12706747 -0.11847659 -0.12179301 -0.12175318]\n",
      "Process noise = 0.6000000000000001, Measurement noise = 0.0\n",
      "R2: [0.78807567 0.79536683 0.79253112 0.78549367 0.7681572 ]\n",
      "mae: [-0.09885245 -0.09946715 -0.09709435 -0.1021844  -0.1052449 ]\n",
      "mape: [-3.47452711e+11 -4.54638382e+11 -8.87209127e+11 -2.08516663e+11\n",
      " -3.28762773e+11]\n",
      "rmse: [-0.13075345 -0.13061024 -0.12871161 -0.13304715 -0.13793793]\n",
      "Process noise = 0.7000000000000001, Measurement noise = 0.0\n",
      "R2: [0.77073703 0.76979583 0.75657293 0.74444239 0.7908058 ]\n",
      "mae: [-0.1031984  -0.1025792  -0.10822995 -0.11202985 -0.09902865]\n",
      "mape: [-2.76746197e+11 -4.12529726e+11 -1.64156206e+11 -5.88395291e+11\n",
      " -3.12099454e+11]\n",
      "rmse: [-0.13438963 -0.1351357  -0.14146253 -0.14412065 -0.1310333 ]\n",
      "Process noise = 0.8, Measurement noise = 0.0\n",
      "R2: [0.75441041 0.73836646 0.72956504 0.7436836  0.74272255]\n",
      "mae: [-0.1075912  -0.1102882  -0.11299515 -0.10926115 -0.1102889 ]\n",
      "mape: [-3.29888673e+11 -1.32315757e+12 -3.39571412e+11 -9.10177485e+11\n",
      " -5.11158558e+11]\n",
      "rmse: [-0.14059851 -0.14411492 -0.14701622 -0.1434903  -0.14296051]\n",
      "Process noise = 0.9, Measurement noise = 0.0\n",
      "R2: [0.7459057  0.72225387 0.71386912 0.73079549 0.71229312]\n",
      "mae: [-0.1088923  -0.115251   -0.11461225 -0.1123348  -0.1146464 ]\n",
      "mape: [-2.86203756e+11 -1.43439648e+11 -3.62990130e+11 -8.26410532e+10\n",
      " -6.39285967e+11]\n",
      "rmse: [-0.14232725 -0.14946078 -0.14982241 -0.14679103 -0.14974282]\n",
      "Process noise = 1.0, Measurement noise = 0.0\n",
      "R2: [0.70404433 0.7015158  0.69136757 0.70469365 0.71885872]\n",
      "mae: [-0.1154981  -0.1165566  -0.11575255 -0.11541465 -0.11638255]\n",
      "mape: [-1.49744688e+11 -5.21967197e+11 -5.96051411e+11 -2.15925084e+12\n",
      " -3.37769972e+11]\n",
      "rmse: [-0.15095345 -0.15119491 -0.15424514 -0.15225114 -0.15056938]\n"
     ]
    }
   ],
   "source": [
    "### DATA-DRIVEN DIAGNOSTICS II ###\n",
    "### FIT AND TEST MODEL ###\n",
    "from sklearn import tree, linear_model, kernel_ridge, svm, neighbors, gaussian_process, ensemble, neural_network\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_validate\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "score_data = []\n",
    "for pn in [0, 1, 2, 3, 4, 5, 6, 7 , 8, 9, 10]: \n",
    "    for mn in [0]:\n",
    "        print('Process noise = ' + str(0.1*pn) + ', Measurement noise = ' + str(0.1*mn))\n",
    "        dataset = pd.read_pickle('diagnostics/data_' + 'pn' + str(pn) + '_mn' + str(mn))\n",
    "        X = dataset[['t', 'v', 't_1', 'v_1', 't_2', 'v_2', 't_3', 'v_3']]\n",
    "        if prognostics:\n",
    "            y = dataset['RUL']\n",
    "        else:\n",
    "            y = dataset['health']\n",
    "        learner = ensemble.RandomForestRegressor()\n",
    "        scoring = {'r2': 'r2',\n",
    "                    'mae': 'neg_mean_absolute_error',\n",
    "                    'mape': 'neg_mean_absolute_percentage_error',\n",
    "                    'rmse': 'neg_root_mean_squared_error'}\n",
    "        scores = cross_validate(learner, X, y, scoring=scoring, cv=5) # default scoring R2\n",
    "        print(\"R2: \" + str(scores['test_r2']))\n",
    "        print(\"mae: \" + str(scores['test_mae']))\n",
    "        print(\"mape: \" + str(scores['test_mape']))\n",
    "        print(\"rmse: \" + str(scores['test_rmse']))\n",
    "        # Store measures\n",
    "        score_data.append(['model_' + 'pn' + str(pn) + '_mn' + str(mn), np.mean(scores['test_r2']),\n",
    "                            np.mean(scores['test_mae']), np.mean(scores['test_mape']), np.mean(scores['test_rmse'])])\n",
    "\n",
    "        # Fit on all data\n",
    "        model = ensemble.RandomForestRegressor().fit(X, y)\n",
    "        if prognostics:\n",
    "            pickle.dump(model, open('prognostics/model_' + 'pn' + str(pn) + '_mn' + str(mn), 'wb'))\n",
    "        else:\n",
    "            pickle.dump(model, open('diagnostics/model_' + 'pn' + str(pn) + '_mn' + str(mn), 'wb'))\n",
    "# Transform two-dim list to dataframe\n",
    "score_df = pd.DataFrame(score_data, columns=['model', 'r2', 'mae', 'mape', 'rmse'])\n",
    "if prognostics:\n",
    "    score_df.to_excel(\"prognostics/scores.xlsx\") \n",
    "else:\n",
    "    score_df.to_excel(\"diagnostics/scores.xlsx\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process noise = 0.0, Measurement noise = 0.0\n",
      "Process noise = 0.1, Measurement noise = 0.0\n",
      "Process noise = 0.2, Measurement noise = 0.0\n",
      "Process noise = 0.30000000000000004, Measurement noise = 0.0\n",
      "Process noise = 0.4, Measurement noise = 0.0\n",
      "Process noise = 0.5, Measurement noise = 0.0\n",
      "Process noise = 0.6000000000000001, Measurement noise = 0.0\n",
      "Process noise = 0.7000000000000001, Measurement noise = 0.0\n",
      "Process noise = 0.8, Measurement noise = 0.0\n",
      "Process noise = 0.9, Measurement noise = 0.0\n",
      "Process noise = 1.0, Measurement noise = 0.0\n"
     ]
    }
   ],
   "source": [
    "### DATA-DRIVEN DIAGNOSTICS IIIa ###\n",
    "### FIT, TEST, VISUALIZE MODEL USING TRAIN AND TEST SETS ###\n",
    "\n",
    "for pn in [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]: \n",
    "    for mn in [0]:\n",
    "        print('Process noise = ' + str(0+0.1*pn) + ', Measurement noise = ' + str(0+0.1*mn))\n",
    "        if prognostics:\n",
    "            dataset = pd.read_pickle('diagnostics/data_' + 'pn' + str(pn) + '_mn' + str(mn))\n",
    "            y = dataset['RUL']\n",
    "        else:\n",
    "            dataset = pd.read_pickle('prognostics/data_' + 'pn' + str(pn) + '_mn' + str(mn))\n",
    "            y = dataset['health']\n",
    "        X = dataset[['t', 'v', 't_1', 'v_1', 't_2', 'v_2', 't_3', 'v_3']]\n",
    "        # Find index of healthy machines\n",
    "        index_df = X.index[(X['t_1'] == 0) & (X['v_1'] == 0) & (X['t_2'] == 0) & (X['v_2'] == 0) & (X['t_3'] == 0) & (X['v_3'] == 0)].tolist()\n",
    "        index_test = round(len(index_df)*0.8)\n",
    "\n",
    "        # Create train and test set without disrupting machine runs to-failure\n",
    "        X_train = X.iloc[0:(index_df[index_test])]\n",
    "        y_train = y.iloc[0:(index_df[index_test])]\n",
    "        X_test = X.iloc[index_df[index_test]:(len(X))]\n",
    "        y_test = y.iloc[index_df[index_test]:(len(y))]\n",
    "\n",
    "        ## Train\n",
    "        model = ensemble.RandomForestRegressor().fit(X_train, y_train)\n",
    "        ## Predict\n",
    "        y_pred = pd.DataFrame(model.predict(X_test), columns=['Pred'])\n",
    "        ## Analyze\n",
    "        #reset index of each DataFrame\n",
    "        X_test.reset_index(drop=True, inplace=True)\n",
    "        y_test.reset_index(drop=True, inplace=True)\n",
    "        # Concat dataframes\n",
    "        test_df = pd.concat([X_test, y_test, y_pred], axis=1)\n",
    "        # Print for visualization (e.g., in R)\n",
    "        if prognostics:\n",
    "            test_df.to_excel(\"prognostics/test_results_\" + 'pn' + str(pn) + '_mn' + str(mn) + \".xlsx\") \n",
    "        else:\n",
    "            test_df.to_excel(\"diagnostics/test_results_\" + 'pn' + str(pn) + '_mn' + str(mn) + \".xlsx\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gamma = 0.9, Learning Rate = 0.003, Activation Function = ReLU, Neurons = 8, Process noise = 0.1, Measurement noise = 0.0\n",
      "Eval num_timesteps=1000, episode_reward=-645.60 +/- 206.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=-636.20 +/- 76.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3000, episode_reward=58.00 +/- 204.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4000, episode_reward=4.20 +/- 132.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=5000, episode_reward=-159.60 +/- 227.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=6000, episode_reward=-435.80 +/- 259.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=7000, episode_reward=322.80 +/- 131.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=8000, episode_reward=396.20 +/- 108.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=9000, episode_reward=49.40 +/- 138.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=10000, episode_reward=352.40 +/- 119.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=11000, episode_reward=333.20 +/- 141.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=12000, episode_reward=376.20 +/- 73.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=13000, episode_reward=228.00 +/- 174.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=14000, episode_reward=423.80 +/- 134.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=15000, episode_reward=300.40 +/- 255.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=16000, episode_reward=316.60 +/- 213.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=17000, episode_reward=374.20 +/- 231.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=18000, episode_reward=361.60 +/- 85.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=19000, episode_reward=655.00 +/- 217.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=20000, episode_reward=420.20 +/- 258.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=21000, episode_reward=1033.20 +/- 168.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=22000, episode_reward=839.80 +/- 392.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=23000, episode_reward=949.00 +/- 120.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=24000, episode_reward=1232.40 +/- 369.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=25000, episode_reward=1230.40 +/- 180.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=26000, episode_reward=987.00 +/- 347.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=27000, episode_reward=1225.60 +/- 272.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=28000, episode_reward=1007.00 +/- 260.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=29000, episode_reward=1246.40 +/- 166.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=30000, episode_reward=1204.40 +/- 196.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=31000, episode_reward=1206.00 +/- 276.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=32000, episode_reward=1456.80 +/- 238.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=33000, episode_reward=1315.80 +/- 294.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=34000, episode_reward=1479.60 +/- 173.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=35000, episode_reward=-804.80 +/- 4381.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=36000, episode_reward=1149.80 +/- 167.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=37000, episode_reward=1514.60 +/- 160.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=38000, episode_reward=1436.00 +/- 233.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=39000, episode_reward=1555.80 +/- 213.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=1301.40 +/- 263.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=41000, episode_reward=1401.60 +/- 200.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=42000, episode_reward=-192.60 +/- 3264.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=43000, episode_reward=1626.20 +/- 113.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=44000, episode_reward=1450.00 +/- 218.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=45000, episode_reward=1446.20 +/- 245.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=46000, episode_reward=1362.00 +/- 235.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=47000, episode_reward=1527.20 +/- 151.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=48000, episode_reward=1457.40 +/- 315.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=49000, episode_reward=1347.60 +/- 229.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=50000, episode_reward=1421.80 +/- 86.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=51000, episode_reward=1336.20 +/- 316.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=52000, episode_reward=1584.20 +/- 246.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=53000, episode_reward=-66.40 +/- 3276.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=54000, episode_reward=1409.20 +/- 489.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=55000, episode_reward=1512.00 +/- 199.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=56000, episode_reward=1391.20 +/- 264.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=57000, episode_reward=1589.80 +/- 211.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=58000, episode_reward=-891.40 +/- 4759.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=59000, episode_reward=1608.80 +/- 216.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=60000, episode_reward=1507.00 +/- 75.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=61000, episode_reward=1482.60 +/- 210.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=62000, episode_reward=1483.20 +/- 193.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=63000, episode_reward=1523.60 +/- 166.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=64000, episode_reward=1403.20 +/- 322.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=65000, episode_reward=1616.00 +/- 167.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=66000, episode_reward=1558.00 +/- 197.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=67000, episode_reward=1715.60 +/- 108.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=68000, episode_reward=1650.20 +/- 246.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=69000, episode_reward=1541.00 +/- 117.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=70000, episode_reward=1708.80 +/- 161.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=71000, episode_reward=1452.00 +/- 279.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=72000, episode_reward=1582.60 +/- 293.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=73000, episode_reward=1615.60 +/- 177.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=74000, episode_reward=1635.80 +/- 300.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=75000, episode_reward=1480.80 +/- 313.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=76000, episode_reward=1659.80 +/- 138.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=77000, episode_reward=1638.80 +/- 183.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=78000, episode_reward=1723.80 +/- 74.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=79000, episode_reward=1737.60 +/- 210.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=80000, episode_reward=1635.40 +/- 322.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=81000, episode_reward=1616.80 +/- 226.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=82000, episode_reward=1605.20 +/- 211.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=83000, episode_reward=1475.20 +/- 289.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=84000, episode_reward=1370.40 +/- 176.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=85000, episode_reward=1449.40 +/- 284.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=86000, episode_reward=1627.40 +/- 185.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=87000, episode_reward=1617.20 +/- 118.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=88000, episode_reward=1589.00 +/- 235.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=89000, episode_reward=1759.60 +/- 224.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=90000, episode_reward=1573.20 +/- 87.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=91000, episode_reward=1626.60 +/- 209.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=92000, episode_reward=1462.80 +/- 163.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=93000, episode_reward=1614.80 +/- 194.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=94000, episode_reward=1478.20 +/- 351.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=95000, episode_reward=1606.20 +/- 320.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=96000, episode_reward=1702.80 +/- 265.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=97000, episode_reward=1552.80 +/- 169.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=98000, episode_reward=1533.20 +/- 263.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=99000, episode_reward=1357.40 +/- 102.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=100000, episode_reward=1624.00 +/- 251.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=101000, episode_reward=1615.60 +/- 217.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=102000, episode_reward=1503.60 +/- 358.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=103000, episode_reward=1880.20 +/- 66.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=104000, episode_reward=1643.60 +/- 233.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=105000, episode_reward=1608.40 +/- 209.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=106000, episode_reward=1675.80 +/- 310.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=107000, episode_reward=1791.80 +/- 168.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=108000, episode_reward=1453.40 +/- 285.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=109000, episode_reward=1482.20 +/- 308.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=110000, episode_reward=1642.80 +/- 216.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=111000, episode_reward=1646.60 +/- 219.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=112000, episode_reward=1324.00 +/- 288.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=113000, episode_reward=1613.40 +/- 168.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=114000, episode_reward=1804.60 +/- 265.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=115000, episode_reward=1589.40 +/- 410.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=116000, episode_reward=1675.60 +/- 164.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=117000, episode_reward=1638.60 +/- 181.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=118000, episode_reward=1623.80 +/- 176.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=119000, episode_reward=1759.80 +/- 244.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=120000, episode_reward=1560.80 +/- 285.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=121000, episode_reward=1676.20 +/- 306.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=122000, episode_reward=1510.60 +/- 200.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=123000, episode_reward=1501.60 +/- 222.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=124000, episode_reward=1570.20 +/- 199.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=125000, episode_reward=1683.60 +/- 261.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=126000, episode_reward=1766.80 +/- 173.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=127000, episode_reward=1753.40 +/- 290.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=128000, episode_reward=1387.40 +/- 133.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=129000, episode_reward=1562.60 +/- 324.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=130000, episode_reward=1799.60 +/- 244.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=131000, episode_reward=1654.20 +/- 216.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=132000, episode_reward=1616.40 +/- 298.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=133000, episode_reward=1415.40 +/- 276.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=134000, episode_reward=1691.60 +/- 238.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=135000, episode_reward=1735.00 +/- 231.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=136000, episode_reward=1526.80 +/- 312.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=137000, episode_reward=1450.80 +/- 177.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=138000, episode_reward=1558.80 +/- 250.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=139000, episode_reward=1421.80 +/- 204.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=140000, episode_reward=1705.80 +/- 175.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=141000, episode_reward=1731.00 +/- 446.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=142000, episode_reward=1483.00 +/- 216.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=143000, episode_reward=1519.80 +/- 205.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=144000, episode_reward=1750.20 +/- 172.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=145000, episode_reward=1743.60 +/- 133.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=146000, episode_reward=1614.80 +/- 214.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=147000, episode_reward=1526.00 +/- 252.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=148000, episode_reward=1716.80 +/- 108.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=149000, episode_reward=1635.40 +/- 397.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=150000, episode_reward=1627.20 +/- 161.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=151000, episode_reward=1718.60 +/- 132.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=152000, episode_reward=1630.00 +/- 230.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=153000, episode_reward=1757.80 +/- 66.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=154000, episode_reward=1563.60 +/- 139.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=155000, episode_reward=1695.80 +/- 182.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=156000, episode_reward=1697.60 +/- 226.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=157000, episode_reward=1474.20 +/- 253.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=158000, episode_reward=1582.80 +/- 228.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=159000, episode_reward=1569.00 +/- 222.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=160000, episode_reward=1656.00 +/- 162.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=161000, episode_reward=1709.40 +/- 235.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=162000, episode_reward=1545.40 +/- 289.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=163000, episode_reward=1720.00 +/- 180.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=164000, episode_reward=1709.60 +/- 259.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=165000, episode_reward=1483.20 +/- 127.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=166000, episode_reward=1484.00 +/- 266.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=167000, episode_reward=1673.40 +/- 185.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=168000, episode_reward=1639.40 +/- 245.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=169000, episode_reward=1685.20 +/- 172.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=170000, episode_reward=1529.00 +/- 136.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=171000, episode_reward=1602.20 +/- 319.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=172000, episode_reward=1552.00 +/- 323.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=173000, episode_reward=1557.20 +/- 185.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=174000, episode_reward=1702.60 +/- 265.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=175000, episode_reward=1784.60 +/- 195.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=176000, episode_reward=1651.20 +/- 408.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=177000, episode_reward=1790.60 +/- 249.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=178000, episode_reward=1604.00 +/- 265.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=179000, episode_reward=1442.60 +/- 186.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=180000, episode_reward=1522.40 +/- 291.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=181000, episode_reward=1586.00 +/- 416.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=182000, episode_reward=1600.20 +/- 90.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=183000, episode_reward=1720.40 +/- 282.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=184000, episode_reward=1595.80 +/- 223.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=185000, episode_reward=1572.60 +/- 137.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=186000, episode_reward=1757.00 +/- 99.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=187000, episode_reward=1471.00 +/- 331.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=188000, episode_reward=1675.60 +/- 138.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=189000, episode_reward=1843.00 +/- 171.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=190000, episode_reward=1509.40 +/- 258.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=191000, episode_reward=1452.20 +/- 199.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=192000, episode_reward=1711.00 +/- 171.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=193000, episode_reward=1524.80 +/- 402.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=194000, episode_reward=1616.60 +/- 290.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=195000, episode_reward=1658.20 +/- 142.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=196000, episode_reward=1838.00 +/- 230.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=197000, episode_reward=1543.40 +/- 257.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=198000, episode_reward=1691.00 +/- 108.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=199000, episode_reward=1485.00 +/- 155.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=200000, episode_reward=1792.00 +/- 168.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=201000, episode_reward=1731.60 +/- 208.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=202000, episode_reward=1539.20 +/- 216.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=203000, episode_reward=1382.80 +/- 247.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=204000, episode_reward=1690.80 +/- 200.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=205000, episode_reward=1562.80 +/- 227.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=206000, episode_reward=1622.20 +/- 264.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=207000, episode_reward=1570.20 +/- 253.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=208000, episode_reward=1552.20 +/- 234.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=209000, episode_reward=1727.20 +/- 160.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=210000, episode_reward=1731.00 +/- 191.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=211000, episode_reward=1750.20 +/- 212.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=212000, episode_reward=1582.80 +/- 259.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=213000, episode_reward=1485.40 +/- 393.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=214000, episode_reward=1661.80 +/- 182.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=215000, episode_reward=1542.20 +/- 286.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=216000, episode_reward=1661.60 +/- 95.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=217000, episode_reward=1743.40 +/- 184.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=218000, episode_reward=1495.00 +/- 296.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=219000, episode_reward=1827.00 +/- 176.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=220000, episode_reward=1701.20 +/- 225.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=221000, episode_reward=1524.80 +/- 117.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=222000, episode_reward=1465.00 +/- 199.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=223000, episode_reward=1488.20 +/- 211.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=224000, episode_reward=1497.60 +/- 314.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=225000, episode_reward=1558.80 +/- 357.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=226000, episode_reward=1644.20 +/- 269.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=227000, episode_reward=1647.40 +/- 111.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=228000, episode_reward=1708.80 +/- 266.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=229000, episode_reward=1503.20 +/- 129.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=230000, episode_reward=1642.40 +/- 293.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=231000, episode_reward=1605.00 +/- 144.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=232000, episode_reward=1637.00 +/- 174.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=233000, episode_reward=1747.60 +/- 173.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=234000, episode_reward=1422.60 +/- 226.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=235000, episode_reward=1681.20 +/- 154.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=236000, episode_reward=1616.00 +/- 231.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=237000, episode_reward=1607.20 +/- 171.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=238000, episode_reward=1855.80 +/- 97.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=239000, episode_reward=1519.00 +/- 232.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=240000, episode_reward=1755.20 +/- 121.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=241000, episode_reward=1519.60 +/- 362.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=242000, episode_reward=1633.00 +/- 168.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=243000, episode_reward=1649.80 +/- 236.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=244000, episode_reward=1582.20 +/- 124.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=245000, episode_reward=1796.00 +/- 218.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=246000, episode_reward=1683.60 +/- 146.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=247000, episode_reward=1616.80 +/- 95.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=248000, episode_reward=1548.80 +/- 222.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=249000, episode_reward=1825.20 +/- 166.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=250000, episode_reward=1827.20 +/- 220.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=251000, episode_reward=1804.60 +/- 101.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=252000, episode_reward=1713.20 +/- 189.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=253000, episode_reward=1690.40 +/- 163.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=254000, episode_reward=1377.80 +/- 179.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=255000, episode_reward=1564.80 +/- 253.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=256000, episode_reward=1728.60 +/- 165.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=257000, episode_reward=1623.60 +/- 311.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=258000, episode_reward=1858.20 +/- 142.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=259000, episode_reward=1623.20 +/- 300.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=260000, episode_reward=1653.60 +/- 231.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=261000, episode_reward=1717.40 +/- 177.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=262000, episode_reward=1599.60 +/- 188.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=263000, episode_reward=1767.60 +/- 154.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=264000, episode_reward=1690.20 +/- 105.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=265000, episode_reward=1753.60 +/- 185.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=266000, episode_reward=1713.60 +/- 181.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=267000, episode_reward=1638.60 +/- 207.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=268000, episode_reward=1719.40 +/- 120.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=269000, episode_reward=1664.60 +/- 228.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=270000, episode_reward=1535.80 +/- 400.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=271000, episode_reward=1597.60 +/- 143.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=272000, episode_reward=1792.60 +/- 142.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=273000, episode_reward=1467.60 +/- 234.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=274000, episode_reward=1689.00 +/- 87.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=275000, episode_reward=1710.00 +/- 328.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=276000, episode_reward=1657.80 +/- 275.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=277000, episode_reward=1556.40 +/- 293.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=278000, episode_reward=1569.40 +/- 81.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=279000, episode_reward=1637.40 +/- 322.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=280000, episode_reward=1535.20 +/- 177.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=281000, episode_reward=1649.40 +/- 311.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=282000, episode_reward=1525.20 +/- 254.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=283000, episode_reward=1552.40 +/- 251.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=284000, episode_reward=1710.60 +/- 210.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=285000, episode_reward=1500.60 +/- 259.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=286000, episode_reward=1603.00 +/- 248.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=287000, episode_reward=1627.40 +/- 232.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=288000, episode_reward=1687.80 +/- 234.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=289000, episode_reward=1629.00 +/- 200.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=290000, episode_reward=1631.20 +/- 176.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=291000, episode_reward=1574.20 +/- 338.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=292000, episode_reward=1684.20 +/- 165.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=293000, episode_reward=1616.20 +/- 326.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=294000, episode_reward=1450.80 +/- 190.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=295000, episode_reward=1517.40 +/- 410.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=296000, episode_reward=1764.80 +/- 172.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=297000, episode_reward=1822.00 +/- 139.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=298000, episode_reward=1297.80 +/- 354.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=299000, episode_reward=1745.20 +/- 259.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=300000, episode_reward=1829.80 +/- 119.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=301000, episode_reward=1447.00 +/- 226.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=302000, episode_reward=1387.40 +/- 138.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=303000, episode_reward=1592.40 +/- 131.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=304000, episode_reward=1733.80 +/- 211.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=305000, episode_reward=1604.80 +/- 332.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=306000, episode_reward=1480.60 +/- 273.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=307000, episode_reward=1419.80 +/- 244.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=308000, episode_reward=1664.00 +/- 162.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=309000, episode_reward=1582.40 +/- 211.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=310000, episode_reward=1787.80 +/- 200.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=311000, episode_reward=1499.20 +/- 163.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=312000, episode_reward=1519.00 +/- 155.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=313000, episode_reward=1462.60 +/- 268.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=314000, episode_reward=1792.20 +/- 161.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=315000, episode_reward=1804.00 +/- 164.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=316000, episode_reward=1471.60 +/- 142.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=317000, episode_reward=1719.60 +/- 151.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=318000, episode_reward=1600.00 +/- 226.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=319000, episode_reward=1636.20 +/- 239.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=320000, episode_reward=1520.00 +/- 365.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=321000, episode_reward=1591.00 +/- 256.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=322000, episode_reward=1666.20 +/- 132.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=323000, episode_reward=1764.20 +/- 247.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=324000, episode_reward=1592.20 +/- 235.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=325000, episode_reward=1644.00 +/- 140.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=326000, episode_reward=1684.40 +/- 227.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=327000, episode_reward=1671.20 +/- 82.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=328000, episode_reward=1504.80 +/- 216.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=329000, episode_reward=1687.60 +/- 233.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=330000, episode_reward=1539.40 +/- 184.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=331000, episode_reward=1549.20 +/- 276.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=332000, episode_reward=1625.20 +/- 167.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=333000, episode_reward=1795.60 +/- 205.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=334000, episode_reward=1714.20 +/- 156.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=335000, episode_reward=1581.40 +/- 231.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=336000, episode_reward=1750.00 +/- 184.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=337000, episode_reward=1658.00 +/- 197.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=338000, episode_reward=1515.60 +/- 257.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=339000, episode_reward=1348.40 +/- 245.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=340000, episode_reward=1538.20 +/- 374.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=341000, episode_reward=1723.80 +/- 228.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=342000, episode_reward=1618.60 +/- 215.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=343000, episode_reward=1670.20 +/- 68.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=344000, episode_reward=1742.40 +/- 248.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=345000, episode_reward=1591.00 +/- 286.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=346000, episode_reward=1661.40 +/- 104.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=347000, episode_reward=1807.60 +/- 28.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=348000, episode_reward=1499.00 +/- 115.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=349000, episode_reward=1891.20 +/- 104.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=350000, episode_reward=1495.80 +/- 272.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=351000, episode_reward=1778.40 +/- 236.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=352000, episode_reward=1514.00 +/- 422.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=353000, episode_reward=1500.80 +/- 225.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=354000, episode_reward=1822.00 +/- 126.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=355000, episode_reward=1654.00 +/- 222.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=356000, episode_reward=1685.20 +/- 277.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=357000, episode_reward=1596.40 +/- 204.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=358000, episode_reward=1826.80 +/- 236.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=359000, episode_reward=1555.20 +/- 280.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=360000, episode_reward=1702.00 +/- 95.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=361000, episode_reward=1827.80 +/- 144.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=362000, episode_reward=1693.00 +/- 195.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=363000, episode_reward=1695.60 +/- 274.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=364000, episode_reward=1772.00 +/- 116.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=365000, episode_reward=1598.20 +/- 160.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=366000, episode_reward=1567.00 +/- 322.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=367000, episode_reward=1730.80 +/- 170.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=368000, episode_reward=1732.60 +/- 207.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=369000, episode_reward=1711.20 +/- 236.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=370000, episode_reward=1546.40 +/- 184.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=371000, episode_reward=1759.80 +/- 288.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=372000, episode_reward=1640.80 +/- 133.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=373000, episode_reward=1800.40 +/- 231.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=374000, episode_reward=1594.00 +/- 418.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=375000, episode_reward=1459.00 +/- 487.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=376000, episode_reward=1560.20 +/- 73.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=377000, episode_reward=1559.80 +/- 249.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=378000, episode_reward=1706.00 +/- 325.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=379000, episode_reward=1462.60 +/- 177.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=380000, episode_reward=1623.80 +/- 119.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=381000, episode_reward=1692.00 +/- 184.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=382000, episode_reward=1698.20 +/- 236.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=383000, episode_reward=1707.60 +/- 367.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=384000, episode_reward=1724.80 +/- 55.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=385000, episode_reward=1678.40 +/- 214.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=386000, episode_reward=1748.20 +/- 228.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=387000, episode_reward=1678.40 +/- 80.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=388000, episode_reward=1572.20 +/- 235.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=389000, episode_reward=1772.20 +/- 175.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=390000, episode_reward=1501.80 +/- 504.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=391000, episode_reward=1529.60 +/- 85.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=392000, episode_reward=1807.00 +/- 82.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=393000, episode_reward=1681.60 +/- 145.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=394000, episode_reward=1642.40 +/- 269.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=395000, episode_reward=1650.60 +/- 224.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=396000, episode_reward=1556.40 +/- 317.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=397000, episode_reward=1585.40 +/- 194.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=398000, episode_reward=1662.00 +/- 183.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=399000, episode_reward=1697.60 +/- 193.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=400000, episode_reward=1786.60 +/- 230.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=401000, episode_reward=1652.00 +/- 270.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=402000, episode_reward=1677.20 +/- 257.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=403000, episode_reward=1831.20 +/- 219.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=404000, episode_reward=1672.40 +/- 255.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=405000, episode_reward=1730.00 +/- 292.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=406000, episode_reward=1723.00 +/- 281.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=407000, episode_reward=1691.80 +/- 118.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=408000, episode_reward=1571.00 +/- 242.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=409000, episode_reward=1509.00 +/- 164.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=410000, episode_reward=1681.40 +/- 101.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=411000, episode_reward=1675.20 +/- 210.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=412000, episode_reward=1812.80 +/- 163.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=413000, episode_reward=1700.80 +/- 114.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=414000, episode_reward=1806.60 +/- 106.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=415000, episode_reward=1776.80 +/- 115.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=416000, episode_reward=1617.80 +/- 124.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=417000, episode_reward=1793.60 +/- 159.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=418000, episode_reward=1510.20 +/- 108.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=419000, episode_reward=1738.40 +/- 165.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=420000, episode_reward=1891.80 +/- 129.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=421000, episode_reward=1607.40 +/- 356.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=422000, episode_reward=1551.40 +/- 221.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=423000, episode_reward=1708.40 +/- 247.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=424000, episode_reward=1827.20 +/- 173.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=425000, episode_reward=1814.80 +/- 229.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=426000, episode_reward=1708.20 +/- 225.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=427000, episode_reward=1731.80 +/- 74.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=428000, episode_reward=1751.60 +/- 96.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=429000, episode_reward=1679.40 +/- 150.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=430000, episode_reward=1664.20 +/- 23.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=431000, episode_reward=1589.80 +/- 239.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=432000, episode_reward=1527.60 +/- 62.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=433000, episode_reward=1485.00 +/- 192.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=434000, episode_reward=1672.80 +/- 218.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=435000, episode_reward=1514.60 +/- 271.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=436000, episode_reward=1753.80 +/- 386.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=437000, episode_reward=1864.60 +/- 111.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=438000, episode_reward=1526.60 +/- 136.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=439000, episode_reward=1467.20 +/- 115.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=440000, episode_reward=1632.40 +/- 173.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=441000, episode_reward=1683.80 +/- 142.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=442000, episode_reward=1573.60 +/- 192.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=443000, episode_reward=1763.20 +/- 177.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=444000, episode_reward=1881.80 +/- 126.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=445000, episode_reward=1676.60 +/- 114.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=446000, episode_reward=1553.60 +/- 146.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=447000, episode_reward=1649.40 +/- 284.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=448000, episode_reward=1615.60 +/- 198.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=449000, episode_reward=1945.00 +/- 149.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=450000, episode_reward=1824.00 +/- 149.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=451000, episode_reward=1716.60 +/- 123.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=452000, episode_reward=1729.80 +/- 214.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=453000, episode_reward=1571.80 +/- 131.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=454000, episode_reward=1798.40 +/- 61.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=455000, episode_reward=1678.40 +/- 250.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=456000, episode_reward=1660.60 +/- 90.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=457000, episode_reward=1621.80 +/- 233.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=458000, episode_reward=1714.20 +/- 455.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=459000, episode_reward=1578.20 +/- 241.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=460000, episode_reward=1631.20 +/- 166.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=461000, episode_reward=1845.20 +/- 213.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=462000, episode_reward=1675.00 +/- 120.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=463000, episode_reward=1588.80 +/- 149.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=464000, episode_reward=1622.80 +/- 178.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=465000, episode_reward=1769.20 +/- 208.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=466000, episode_reward=1550.20 +/- 367.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=467000, episode_reward=1738.80 +/- 123.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=468000, episode_reward=1626.40 +/- 333.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=469000, episode_reward=1713.60 +/- 206.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=470000, episode_reward=1617.00 +/- 207.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=471000, episode_reward=1479.80 +/- 273.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=472000, episode_reward=1557.40 +/- 162.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=473000, episode_reward=1598.40 +/- 136.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=474000, episode_reward=1656.40 +/- 291.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=475000, episode_reward=1615.00 +/- 251.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=476000, episode_reward=1856.20 +/- 204.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=477000, episode_reward=1717.00 +/- 101.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=478000, episode_reward=1526.00 +/- 148.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=479000, episode_reward=1710.00 +/- 247.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=480000, episode_reward=1558.40 +/- 252.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=481000, episode_reward=1629.80 +/- 271.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=482000, episode_reward=1796.00 +/- 227.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=483000, episode_reward=1605.40 +/- 216.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=484000, episode_reward=1518.40 +/- 218.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=485000, episode_reward=1761.80 +/- 267.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=486000, episode_reward=1660.80 +/- 164.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=487000, episode_reward=1586.60 +/- 117.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=488000, episode_reward=1509.00 +/- 193.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=489000, episode_reward=1541.00 +/- 191.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=490000, episode_reward=1669.20 +/- 201.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=491000, episode_reward=1770.80 +/- 176.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=492000, episode_reward=1294.60 +/- 102.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=493000, episode_reward=1676.20 +/- 349.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=494000, episode_reward=1641.00 +/- 103.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=495000, episode_reward=1747.40 +/- 116.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=496000, episode_reward=1715.00 +/- 172.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=497000, episode_reward=1723.80 +/- 99.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=498000, episode_reward=1619.00 +/- 196.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=499000, episode_reward=1537.60 +/- 263.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=500000, episode_reward=1534.20 +/- 204.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=501000, episode_reward=1326.40 +/- 73.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=502000, episode_reward=1736.00 +/- 236.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=503000, episode_reward=1600.00 +/- 136.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=504000, episode_reward=1724.20 +/- 271.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=505000, episode_reward=1549.60 +/- 312.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=506000, episode_reward=1666.20 +/- 154.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=507000, episode_reward=1422.00 +/- 299.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=508000, episode_reward=1545.80 +/- 197.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=509000, episode_reward=1812.00 +/- 223.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=510000, episode_reward=1685.60 +/- 192.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=511000, episode_reward=1684.60 +/- 328.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=512000, episode_reward=1754.00 +/- 189.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=513000, episode_reward=1575.80 +/- 183.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=514000, episode_reward=1729.60 +/- 121.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=515000, episode_reward=1684.40 +/- 276.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=516000, episode_reward=1640.00 +/- 246.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=517000, episode_reward=1698.80 +/- 142.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=518000, episode_reward=1725.00 +/- 230.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=519000, episode_reward=1819.40 +/- 204.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=520000, episode_reward=1601.20 +/- 208.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=521000, episode_reward=1453.60 +/- 240.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=522000, episode_reward=1641.40 +/- 297.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=523000, episode_reward=1539.60 +/- 174.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=524000, episode_reward=1783.80 +/- 196.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=525000, episode_reward=1608.40 +/- 383.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=526000, episode_reward=1816.60 +/- 265.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=527000, episode_reward=1586.80 +/- 134.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=528000, episode_reward=1552.40 +/- 133.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=529000, episode_reward=1575.00 +/- 182.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=530000, episode_reward=1656.00 +/- 316.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=531000, episode_reward=1532.60 +/- 250.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=532000, episode_reward=1679.00 +/- 146.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=533000, episode_reward=1541.60 +/- 362.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=534000, episode_reward=1532.00 +/- 222.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=535000, episode_reward=1759.80 +/- 123.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=536000, episode_reward=1846.40 +/- 172.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=537000, episode_reward=1596.00 +/- 206.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=538000, episode_reward=1505.20 +/- 197.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=539000, episode_reward=1868.20 +/- 134.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=540000, episode_reward=1640.00 +/- 194.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=541000, episode_reward=1797.60 +/- 316.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=542000, episode_reward=1689.20 +/- 249.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=543000, episode_reward=1817.00 +/- 265.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=544000, episode_reward=1424.60 +/- 248.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=545000, episode_reward=1607.80 +/- 236.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=546000, episode_reward=1663.60 +/- 216.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=547000, episode_reward=1735.00 +/- 391.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=548000, episode_reward=1460.40 +/- 415.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=549000, episode_reward=1670.80 +/- 126.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=550000, episode_reward=1552.60 +/- 201.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=551000, episode_reward=1759.60 +/- 163.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=552000, episode_reward=1795.20 +/- 252.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=553000, episode_reward=1734.60 +/- 208.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=554000, episode_reward=1624.20 +/- 210.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=555000, episode_reward=1708.40 +/- 181.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=556000, episode_reward=1478.00 +/- 260.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=557000, episode_reward=1609.00 +/- 188.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=558000, episode_reward=1742.20 +/- 256.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=559000, episode_reward=1578.40 +/- 216.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=560000, episode_reward=1791.00 +/- 150.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=561000, episode_reward=1583.20 +/- 183.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=562000, episode_reward=1604.40 +/- 220.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=563000, episode_reward=1656.80 +/- 116.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=564000, episode_reward=1614.00 +/- 144.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=565000, episode_reward=1656.00 +/- 287.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=566000, episode_reward=1758.20 +/- 196.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=567000, episode_reward=1475.80 +/- 143.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=568000, episode_reward=1828.40 +/- 212.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=569000, episode_reward=1869.00 +/- 123.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=570000, episode_reward=1789.60 +/- 178.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=571000, episode_reward=1699.80 +/- 304.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=572000, episode_reward=1637.60 +/- 128.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=573000, episode_reward=1631.40 +/- 354.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=574000, episode_reward=1684.40 +/- 224.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=575000, episode_reward=1602.40 +/- 351.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=576000, episode_reward=1615.00 +/- 291.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=577000, episode_reward=1691.00 +/- 196.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=578000, episode_reward=1505.20 +/- 233.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=579000, episode_reward=1721.40 +/- 198.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=580000, episode_reward=1517.60 +/- 209.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=581000, episode_reward=1701.40 +/- 368.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=582000, episode_reward=1748.40 +/- 160.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=583000, episode_reward=1559.60 +/- 181.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=584000, episode_reward=1623.20 +/- 152.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=585000, episode_reward=1559.00 +/- 211.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=586000, episode_reward=1849.60 +/- 71.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=587000, episode_reward=1548.00 +/- 229.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=588000, episode_reward=1723.40 +/- 168.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=589000, episode_reward=1770.80 +/- 310.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=590000, episode_reward=1398.80 +/- 290.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=591000, episode_reward=1689.00 +/- 380.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=592000, episode_reward=1760.00 +/- 235.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=593000, episode_reward=1475.60 +/- 388.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=594000, episode_reward=1677.80 +/- 135.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=595000, episode_reward=1810.40 +/- 215.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=596000, episode_reward=1432.40 +/- 246.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=597000, episode_reward=1813.20 +/- 228.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=598000, episode_reward=1759.60 +/- 177.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=599000, episode_reward=1449.80 +/- 337.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=600000, episode_reward=1475.00 +/- 291.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=601000, episode_reward=1741.80 +/- 240.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=602000, episode_reward=1685.80 +/- 187.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=603000, episode_reward=1582.20 +/- 186.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=604000, episode_reward=1788.00 +/- 142.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=605000, episode_reward=1734.00 +/- 220.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=606000, episode_reward=1620.00 +/- 228.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=607000, episode_reward=1595.00 +/- 216.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=608000, episode_reward=1521.80 +/- 364.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=609000, episode_reward=1698.40 +/- 129.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=610000, episode_reward=1620.20 +/- 216.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=611000, episode_reward=1665.00 +/- 264.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=612000, episode_reward=1486.40 +/- 294.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=613000, episode_reward=1605.00 +/- 207.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=614000, episode_reward=1543.00 +/- 448.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=615000, episode_reward=1774.60 +/- 102.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=616000, episode_reward=1428.20 +/- 102.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=617000, episode_reward=1698.20 +/- 131.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=618000, episode_reward=1626.00 +/- 180.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=619000, episode_reward=1573.60 +/- 305.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=620000, episode_reward=1657.60 +/- 167.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=621000, episode_reward=1701.80 +/- 108.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=622000, episode_reward=1622.80 +/- 86.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=623000, episode_reward=1680.00 +/- 191.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=624000, episode_reward=1655.20 +/- 120.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=625000, episode_reward=1744.00 +/- 108.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=626000, episode_reward=1885.60 +/- 201.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=627000, episode_reward=1860.40 +/- 198.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=628000, episode_reward=1627.40 +/- 182.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=629000, episode_reward=1724.60 +/- 162.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=630000, episode_reward=1747.80 +/- 178.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=631000, episode_reward=1719.80 +/- 329.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=632000, episode_reward=1766.20 +/- 172.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=633000, episode_reward=1518.60 +/- 163.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=634000, episode_reward=1735.20 +/- 240.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=635000, episode_reward=1803.40 +/- 209.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=636000, episode_reward=1763.60 +/- 229.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=637000, episode_reward=1626.40 +/- 112.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=638000, episode_reward=1503.40 +/- 179.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=639000, episode_reward=1455.00 +/- 177.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=640000, episode_reward=1618.20 +/- 125.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=641000, episode_reward=1503.40 +/- 196.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=642000, episode_reward=1650.40 +/- 198.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=643000, episode_reward=1751.20 +/- 168.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=644000, episode_reward=1706.40 +/- 191.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=645000, episode_reward=1837.00 +/- 142.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=646000, episode_reward=1737.00 +/- 142.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=647000, episode_reward=1639.00 +/- 76.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=648000, episode_reward=1567.20 +/- 245.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=649000, episode_reward=1699.80 +/- 200.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=650000, episode_reward=1362.00 +/- 249.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=651000, episode_reward=1534.00 +/- 73.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=652000, episode_reward=1525.00 +/- 233.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=653000, episode_reward=1916.20 +/- 207.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=654000, episode_reward=1493.60 +/- 162.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=655000, episode_reward=1852.00 +/- 159.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=656000, episode_reward=1638.60 +/- 253.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=657000, episode_reward=1714.40 +/- 261.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=658000, episode_reward=1441.60 +/- 131.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=659000, episode_reward=1711.00 +/- 194.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=660000, episode_reward=1652.40 +/- 195.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=661000, episode_reward=1707.60 +/- 156.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=662000, episode_reward=1584.20 +/- 70.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=663000, episode_reward=1678.80 +/- 131.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=664000, episode_reward=1352.00 +/- 361.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=665000, episode_reward=1953.60 +/- 135.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=666000, episode_reward=1464.40 +/- 138.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=667000, episode_reward=1906.40 +/- 122.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=668000, episode_reward=1501.80 +/- 252.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=669000, episode_reward=1721.00 +/- 214.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=670000, episode_reward=1572.80 +/- 170.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=671000, episode_reward=1675.40 +/- 132.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=672000, episode_reward=1738.00 +/- 237.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=673000, episode_reward=1643.00 +/- 205.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=674000, episode_reward=1655.00 +/- 197.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=675000, episode_reward=1642.40 +/- 229.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=676000, episode_reward=1699.20 +/- 342.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=677000, episode_reward=1772.40 +/- 92.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=678000, episode_reward=1677.80 +/- 139.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=679000, episode_reward=1755.40 +/- 192.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=680000, episode_reward=1731.20 +/- 206.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=681000, episode_reward=1769.80 +/- 136.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=682000, episode_reward=1646.60 +/- 177.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=683000, episode_reward=1659.00 +/- 190.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=684000, episode_reward=1669.00 +/- 173.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=685000, episode_reward=1778.20 +/- 305.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=686000, episode_reward=1686.40 +/- 214.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=687000, episode_reward=1670.00 +/- 132.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=688000, episode_reward=1795.20 +/- 158.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=689000, episode_reward=1522.00 +/- 197.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=690000, episode_reward=1647.40 +/- 151.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=691000, episode_reward=1732.00 +/- 125.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=692000, episode_reward=1694.80 +/- 249.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=693000, episode_reward=1689.00 +/- 98.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=694000, episode_reward=1732.20 +/- 119.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=695000, episode_reward=1583.20 +/- 335.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=696000, episode_reward=1699.80 +/- 94.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=697000, episode_reward=1760.00 +/- 180.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=698000, episode_reward=1675.60 +/- 160.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=699000, episode_reward=1740.20 +/- 103.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=700000, episode_reward=1471.00 +/- 192.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=701000, episode_reward=1833.40 +/- 153.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=702000, episode_reward=1713.20 +/- 132.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=703000, episode_reward=1529.20 +/- 183.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=704000, episode_reward=1579.00 +/- 205.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=705000, episode_reward=1655.00 +/- 194.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=706000, episode_reward=1614.00 +/- 137.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=707000, episode_reward=1736.40 +/- 185.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=708000, episode_reward=1476.00 +/- 252.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=709000, episode_reward=1666.20 +/- 167.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=710000, episode_reward=1710.80 +/- 184.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=711000, episode_reward=1689.00 +/- 201.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=712000, episode_reward=1767.00 +/- 245.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=713000, episode_reward=1712.80 +/- 147.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=714000, episode_reward=1642.40 +/- 136.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=715000, episode_reward=1748.80 +/- 163.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=716000, episode_reward=1679.60 +/- 131.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=717000, episode_reward=1834.40 +/- 149.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=718000, episode_reward=1609.60 +/- 254.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=719000, episode_reward=1783.60 +/- 188.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=720000, episode_reward=1682.20 +/- 249.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=721000, episode_reward=1720.00 +/- 175.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=722000, episode_reward=1583.00 +/- 208.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=723000, episode_reward=1791.80 +/- 139.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=724000, episode_reward=1716.00 +/- 337.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=725000, episode_reward=1752.40 +/- 95.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=726000, episode_reward=1553.40 +/- 94.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=727000, episode_reward=1534.00 +/- 304.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=728000, episode_reward=1654.40 +/- 108.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=729000, episode_reward=1723.80 +/- 243.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=730000, episode_reward=1835.60 +/- 205.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=731000, episode_reward=1492.80 +/- 216.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=732000, episode_reward=1698.80 +/- 170.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=733000, episode_reward=1635.20 +/- 133.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=734000, episode_reward=1532.20 +/- 210.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=735000, episode_reward=1769.00 +/- 278.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=736000, episode_reward=1380.40 +/- 137.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=737000, episode_reward=1720.40 +/- 385.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=738000, episode_reward=1622.20 +/- 178.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=739000, episode_reward=1794.80 +/- 270.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=740000, episode_reward=1739.20 +/- 313.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=741000, episode_reward=1812.40 +/- 261.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=742000, episode_reward=1478.60 +/- 169.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=743000, episode_reward=1622.80 +/- 224.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=744000, episode_reward=1613.40 +/- 230.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=745000, episode_reward=1649.80 +/- 122.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=746000, episode_reward=1576.80 +/- 85.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=747000, episode_reward=1812.40 +/- 196.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=748000, episode_reward=1616.00 +/- 147.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=749000, episode_reward=1677.80 +/- 116.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=750000, episode_reward=1610.20 +/- 321.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=751000, episode_reward=1752.20 +/- 123.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=752000, episode_reward=1587.80 +/- 213.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=753000, episode_reward=1767.40 +/- 111.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=754000, episode_reward=1565.80 +/- 223.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=755000, episode_reward=1761.40 +/- 245.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=756000, episode_reward=1614.80 +/- 272.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=757000, episode_reward=1788.80 +/- 236.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=758000, episode_reward=1756.80 +/- 150.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=759000, episode_reward=1618.20 +/- 209.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=760000, episode_reward=1520.80 +/- 89.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=761000, episode_reward=1710.60 +/- 364.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=762000, episode_reward=1686.80 +/- 165.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=763000, episode_reward=1774.40 +/- 186.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=764000, episode_reward=1687.20 +/- 227.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=765000, episode_reward=1798.80 +/- 128.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=766000, episode_reward=1560.00 +/- 264.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=767000, episode_reward=1721.00 +/- 90.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=768000, episode_reward=1655.80 +/- 270.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=769000, episode_reward=1827.20 +/- 264.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=770000, episode_reward=1588.40 +/- 179.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=771000, episode_reward=1778.20 +/- 151.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=772000, episode_reward=1827.00 +/- 198.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=773000, episode_reward=1793.60 +/- 53.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=774000, episode_reward=1728.20 +/- 219.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=775000, episode_reward=1654.80 +/- 163.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=776000, episode_reward=1594.80 +/- 144.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=777000, episode_reward=1621.60 +/- 209.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=778000, episode_reward=1503.20 +/- 397.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=779000, episode_reward=1536.60 +/- 221.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=780000, episode_reward=1622.40 +/- 183.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=781000, episode_reward=1665.20 +/- 281.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=782000, episode_reward=1630.40 +/- 111.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=783000, episode_reward=1545.40 +/- 226.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=784000, episode_reward=1605.00 +/- 296.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=785000, episode_reward=1664.40 +/- 434.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=786000, episode_reward=1518.60 +/- 323.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=787000, episode_reward=1627.20 +/- 302.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=788000, episode_reward=1686.80 +/- 308.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=789000, episode_reward=1633.20 +/- 130.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=790000, episode_reward=1462.00 +/- 336.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=791000, episode_reward=1852.80 +/- 190.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=792000, episode_reward=1748.60 +/- 230.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=793000, episode_reward=1660.60 +/- 211.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=794000, episode_reward=1827.20 +/- 146.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=795000, episode_reward=1659.60 +/- 139.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=796000, episode_reward=1588.60 +/- 305.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=797000, episode_reward=1712.20 +/- 199.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=798000, episode_reward=1767.80 +/- 107.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=799000, episode_reward=1748.80 +/- 182.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=800000, episode_reward=1782.80 +/- 239.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=801000, episode_reward=1484.60 +/- 209.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=802000, episode_reward=1616.00 +/- 111.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=803000, episode_reward=1664.20 +/- 218.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=804000, episode_reward=1857.20 +/- 102.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=805000, episode_reward=1604.40 +/- 284.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=806000, episode_reward=1738.60 +/- 140.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=807000, episode_reward=1633.00 +/- 365.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=808000, episode_reward=1683.20 +/- 62.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=809000, episode_reward=1715.60 +/- 343.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=810000, episode_reward=1647.80 +/- 139.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=811000, episode_reward=1643.80 +/- 329.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=812000, episode_reward=1681.80 +/- 86.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=813000, episode_reward=1721.80 +/- 81.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=814000, episode_reward=1932.40 +/- 108.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=815000, episode_reward=1699.00 +/- 241.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=816000, episode_reward=1706.80 +/- 243.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=817000, episode_reward=1555.40 +/- 241.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=818000, episode_reward=1761.40 +/- 182.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=819000, episode_reward=1623.40 +/- 206.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=820000, episode_reward=1742.20 +/- 241.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=821000, episode_reward=1546.00 +/- 221.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=822000, episode_reward=1822.60 +/- 153.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=823000, episode_reward=1913.60 +/- 128.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=824000, episode_reward=1908.40 +/- 97.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=825000, episode_reward=1906.60 +/- 108.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=826000, episode_reward=1795.00 +/- 239.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=827000, episode_reward=1681.00 +/- 276.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=828000, episode_reward=1618.80 +/- 130.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=829000, episode_reward=1493.60 +/- 430.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=830000, episode_reward=1694.60 +/- 185.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=831000, episode_reward=1579.20 +/- 254.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=832000, episode_reward=1723.60 +/- 266.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=833000, episode_reward=1820.60 +/- 264.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=834000, episode_reward=1770.40 +/- 286.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=835000, episode_reward=1841.40 +/- 215.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=836000, episode_reward=1843.60 +/- 343.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=837000, episode_reward=1661.80 +/- 145.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=838000, episode_reward=1640.80 +/- 128.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=839000, episode_reward=1847.60 +/- 57.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=840000, episode_reward=1625.40 +/- 208.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=841000, episode_reward=1565.40 +/- 164.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=842000, episode_reward=1606.20 +/- 143.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=843000, episode_reward=1803.20 +/- 137.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=844000, episode_reward=1733.60 +/- 173.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=845000, episode_reward=1681.00 +/- 162.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=846000, episode_reward=2036.60 +/- 147.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=847000, episode_reward=1725.80 +/- 259.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=848000, episode_reward=1738.20 +/- 187.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=849000, episode_reward=1842.80 +/- 95.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=850000, episode_reward=1755.80 +/- 228.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=851000, episode_reward=1693.20 +/- 152.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=852000, episode_reward=1659.20 +/- 86.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=853000, episode_reward=1766.00 +/- 235.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=854000, episode_reward=1482.60 +/- 304.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=855000, episode_reward=1768.40 +/- 85.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=856000, episode_reward=1832.80 +/- 172.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=857000, episode_reward=1729.80 +/- 109.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=858000, episode_reward=1579.60 +/- 243.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=859000, episode_reward=1508.60 +/- 191.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=860000, episode_reward=1508.20 +/- 108.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=861000, episode_reward=1660.00 +/- 96.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=862000, episode_reward=1787.60 +/- 135.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=863000, episode_reward=1628.20 +/- 230.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=864000, episode_reward=1573.60 +/- 188.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=865000, episode_reward=1741.00 +/- 206.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=866000, episode_reward=1809.60 +/- 202.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=867000, episode_reward=1797.20 +/- 116.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=868000, episode_reward=1693.40 +/- 154.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=869000, episode_reward=1728.20 +/- 195.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=870000, episode_reward=1764.40 +/- 114.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=871000, episode_reward=1611.00 +/- 133.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=872000, episode_reward=1804.60 +/- 194.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=873000, episode_reward=1544.40 +/- 135.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=874000, episode_reward=1621.60 +/- 115.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=875000, episode_reward=1531.80 +/- 218.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=876000, episode_reward=1756.20 +/- 174.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=877000, episode_reward=1581.00 +/- 196.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=878000, episode_reward=1810.40 +/- 203.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=879000, episode_reward=1666.20 +/- 241.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=880000, episode_reward=1672.40 +/- 261.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=881000, episode_reward=1592.80 +/- 277.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=882000, episode_reward=1712.40 +/- 187.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=883000, episode_reward=1758.40 +/- 175.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=884000, episode_reward=1681.00 +/- 162.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=885000, episode_reward=1493.20 +/- 136.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=886000, episode_reward=1571.20 +/- 40.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=887000, episode_reward=1645.60 +/- 73.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=888000, episode_reward=1782.40 +/- 178.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=889000, episode_reward=1658.80 +/- 192.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=890000, episode_reward=1649.40 +/- 111.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=891000, episode_reward=1770.20 +/- 236.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=892000, episode_reward=1732.00 +/- 146.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=893000, episode_reward=1753.20 +/- 130.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=894000, episode_reward=1748.20 +/- 72.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=895000, episode_reward=1645.00 +/- 228.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=896000, episode_reward=1682.00 +/- 123.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=897000, episode_reward=1754.60 +/- 149.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=898000, episode_reward=1647.40 +/- 153.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=899000, episode_reward=1746.20 +/- 260.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=900000, episode_reward=1632.80 +/- 169.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=901000, episode_reward=1884.80 +/- 128.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=902000, episode_reward=1763.60 +/- 68.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=903000, episode_reward=1471.80 +/- 300.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=904000, episode_reward=1627.00 +/- 136.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=905000, episode_reward=1597.20 +/- 125.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=906000, episode_reward=1876.20 +/- 128.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=907000, episode_reward=1554.80 +/- 191.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=908000, episode_reward=1649.40 +/- 141.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=909000, episode_reward=1675.60 +/- 234.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=910000, episode_reward=1802.00 +/- 64.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=911000, episode_reward=1719.60 +/- 239.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=912000, episode_reward=1567.00 +/- 244.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=913000, episode_reward=1824.40 +/- 181.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=914000, episode_reward=1584.20 +/- 233.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=915000, episode_reward=1694.60 +/- 238.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=916000, episode_reward=1565.60 +/- 267.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=917000, episode_reward=1829.60 +/- 34.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=918000, episode_reward=1619.80 +/- 187.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=919000, episode_reward=1598.40 +/- 308.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=920000, episode_reward=1783.60 +/- 101.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=921000, episode_reward=1676.00 +/- 280.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=922000, episode_reward=1748.60 +/- 117.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=923000, episode_reward=1678.60 +/- 136.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=924000, episode_reward=1405.60 +/- 237.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=925000, episode_reward=1580.80 +/- 380.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=926000, episode_reward=1692.60 +/- 187.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=927000, episode_reward=1740.80 +/- 128.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=928000, episode_reward=1621.40 +/- 243.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=929000, episode_reward=1670.80 +/- 277.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=930000, episode_reward=1847.60 +/- 171.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=931000, episode_reward=1672.40 +/- 171.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=932000, episode_reward=1449.60 +/- 187.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=933000, episode_reward=1604.60 +/- 146.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=934000, episode_reward=1643.80 +/- 128.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=935000, episode_reward=1825.20 +/- 171.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=936000, episode_reward=1541.20 +/- 160.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=937000, episode_reward=1694.40 +/- 163.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=938000, episode_reward=1615.20 +/- 127.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=939000, episode_reward=1677.40 +/- 127.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=940000, episode_reward=1705.00 +/- 168.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=941000, episode_reward=1588.00 +/- 49.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=942000, episode_reward=1566.60 +/- 151.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=943000, episode_reward=1613.00 +/- 189.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=944000, episode_reward=1644.40 +/- 228.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=945000, episode_reward=1797.60 +/- 197.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=946000, episode_reward=1713.60 +/- 115.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=947000, episode_reward=1538.20 +/- 261.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=948000, episode_reward=1760.40 +/- 109.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=949000, episode_reward=1599.40 +/- 222.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=950000, episode_reward=1483.40 +/- 395.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=951000, episode_reward=1578.80 +/- 271.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=952000, episode_reward=1557.60 +/- 173.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=953000, episode_reward=1612.40 +/- 109.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=954000, episode_reward=1779.40 +/- 149.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=955000, episode_reward=1791.60 +/- 236.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=956000, episode_reward=1838.60 +/- 119.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=957000, episode_reward=1644.40 +/- 306.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=958000, episode_reward=1806.20 +/- 139.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=959000, episode_reward=1850.80 +/- 149.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=960000, episode_reward=1664.00 +/- 253.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=961000, episode_reward=1627.40 +/- 183.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=962000, episode_reward=1720.20 +/- 88.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=963000, episode_reward=1745.40 +/- 152.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=964000, episode_reward=1652.40 +/- 268.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=965000, episode_reward=1745.40 +/- 147.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=966000, episode_reward=1741.80 +/- 170.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=967000, episode_reward=1712.80 +/- 230.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=968000, episode_reward=1399.00 +/- 200.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=969000, episode_reward=1705.00 +/- 231.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=970000, episode_reward=1629.40 +/- 128.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=971000, episode_reward=1902.40 +/- 95.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=972000, episode_reward=1607.80 +/- 177.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=973000, episode_reward=1684.60 +/- 202.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=974000, episode_reward=1694.00 +/- 252.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=975000, episode_reward=1674.80 +/- 180.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=976000, episode_reward=1627.40 +/- 93.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=977000, episode_reward=1708.00 +/- 202.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=978000, episode_reward=1714.40 +/- 186.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=979000, episode_reward=1568.20 +/- 310.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=980000, episode_reward=1528.20 +/- 241.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=981000, episode_reward=1699.80 +/- 206.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=982000, episode_reward=1562.20 +/- 287.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=983000, episode_reward=1701.20 +/- 117.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=984000, episode_reward=1725.80 +/- 181.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=985000, episode_reward=1725.80 +/- 198.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=986000, episode_reward=1616.00 +/- 218.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=987000, episode_reward=1572.40 +/- 241.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=988000, episode_reward=1758.00 +/- 139.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=989000, episode_reward=1624.40 +/- 154.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=990000, episode_reward=1668.80 +/- 236.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=991000, episode_reward=1663.40 +/- 162.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=992000, episode_reward=1791.00 +/- 78.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=993000, episode_reward=1789.60 +/- 277.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=994000, episode_reward=1655.60 +/- 175.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=995000, episode_reward=1927.20 +/- 119.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=996000, episode_reward=1770.60 +/- 230.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=997000, episode_reward=1725.00 +/- 93.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=998000, episode_reward=1635.20 +/- 153.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=999000, episode_reward=1686.00 +/- 235.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1000000, episode_reward=1794.40 +/- 183.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1001000, episode_reward=1763.80 +/- 155.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Gamma = 0.9, Learning Rate = 0.003, Activation Function = ReLU, Neurons = 8, Process noise = 0.1, Measurement noise = 0.0\n",
      "Eval num_timesteps=1000, episode_reward=-27.40 +/- 361.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=-76.00 +/- 298.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=3000, episode_reward=305.20 +/- 78.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4000, episode_reward=414.00 +/- 110.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=5000, episode_reward=321.20 +/- 179.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=6000, episode_reward=322.80 +/- 205.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=7000, episode_reward=-5.40 +/- 308.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=8000, episode_reward=-300.60 +/- 439.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=9000, episode_reward=-40.00 +/- 397.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=10000, episode_reward=-19.80 +/- 159.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=11000, episode_reward=235.40 +/- 204.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=12000, episode_reward=186.80 +/- 286.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=13000, episode_reward=443.20 +/- 93.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=14000, episode_reward=145.00 +/- 206.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=15000, episode_reward=330.60 +/- 206.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=16000, episode_reward=160.80 +/- 221.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=17000, episode_reward=442.80 +/- 237.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=18000, episode_reward=508.60 +/- 242.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=19000, episode_reward=668.60 +/- 358.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=20000, episode_reward=700.80 +/- 313.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=21000, episode_reward=1355.80 +/- 91.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=22000, episode_reward=1138.60 +/- 207.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=23000, episode_reward=859.40 +/- 318.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=24000, episode_reward=810.60 +/- 270.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=25000, episode_reward=-2029.40 +/- 6552.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=26000, episode_reward=1192.80 +/- 364.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=27000, episode_reward=979.80 +/- 371.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=28000, episode_reward=-731.60 +/- 3881.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=29000, episode_reward=1402.40 +/- 218.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=30000, episode_reward=1488.60 +/- 184.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=31000, episode_reward=1392.00 +/- 506.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=32000, episode_reward=1562.60 +/- 292.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=33000, episode_reward=1263.40 +/- 284.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=34000, episode_reward=1244.40 +/- 202.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=35000, episode_reward=1459.80 +/- 327.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=36000, episode_reward=1413.20 +/- 196.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=37000, episode_reward=1226.40 +/- 216.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=38000, episode_reward=1565.20 +/- 136.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=39000, episode_reward=1164.80 +/- 266.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=40000, episode_reward=1030.40 +/- 325.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=41000, episode_reward=1622.60 +/- 77.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=42000, episode_reward=1151.00 +/- 216.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=43000, episode_reward=1303.80 +/- 208.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=44000, episode_reward=1321.20 +/- 269.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=45000, episode_reward=1367.60 +/- 287.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=46000, episode_reward=1759.60 +/- 156.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=47000, episode_reward=1421.00 +/- 215.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=48000, episode_reward=1377.80 +/- 398.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=49000, episode_reward=1441.20 +/- 448.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=50000, episode_reward=1442.00 +/- 286.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=51000, episode_reward=1355.00 +/- 203.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=52000, episode_reward=1410.20 +/- 73.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=53000, episode_reward=1516.00 +/- 66.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=54000, episode_reward=1306.80 +/- 455.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=55000, episode_reward=1356.40 +/- 179.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=56000, episode_reward=1263.80 +/- 319.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=57000, episode_reward=1183.00 +/- 222.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=58000, episode_reward=1281.60 +/- 360.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=59000, episode_reward=1267.00 +/- 198.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=60000, episode_reward=880.80 +/- 349.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=61000, episode_reward=977.80 +/- 438.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=62000, episode_reward=1390.80 +/- 174.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=63000, episode_reward=1625.20 +/- 351.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=64000, episode_reward=1436.80 +/- 102.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=65000, episode_reward=1670.20 +/- 165.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=66000, episode_reward=1343.80 +/- 214.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=67000, episode_reward=1541.00 +/- 229.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=68000, episode_reward=1766.40 +/- 336.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=69000, episode_reward=1365.60 +/- 308.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=70000, episode_reward=1449.20 +/- 234.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=71000, episode_reward=1538.40 +/- 277.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=72000, episode_reward=1373.40 +/- 292.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=73000, episode_reward=1537.80 +/- 145.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=74000, episode_reward=1530.20 +/- 125.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=75000, episode_reward=1560.80 +/- 178.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=76000, episode_reward=1370.60 +/- 185.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=77000, episode_reward=1503.80 +/- 239.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=78000, episode_reward=1701.40 +/- 130.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=79000, episode_reward=1438.00 +/- 171.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=80000, episode_reward=1496.40 +/- 350.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=81000, episode_reward=1592.20 +/- 108.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=82000, episode_reward=1182.60 +/- 155.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=83000, episode_reward=922.80 +/- 446.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=84000, episode_reward=1421.20 +/- 189.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=85000, episode_reward=1526.80 +/- 314.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=86000, episode_reward=1145.80 +/- 221.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=87000, episode_reward=1642.00 +/- 244.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=88000, episode_reward=1540.40 +/- 143.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=89000, episode_reward=1517.00 +/- 270.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=90000, episode_reward=1377.60 +/- 202.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=91000, episode_reward=1325.40 +/- 211.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=92000, episode_reward=1546.40 +/- 204.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=93000, episode_reward=1434.80 +/- 319.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=94000, episode_reward=1607.60 +/- 260.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=95000, episode_reward=1654.60 +/- 131.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=96000, episode_reward=1708.00 +/- 103.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=97000, episode_reward=1505.80 +/- 155.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=98000, episode_reward=1404.00 +/- 289.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=99000, episode_reward=1520.40 +/- 241.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=100000, episode_reward=1374.00 +/- 117.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=101000, episode_reward=1622.00 +/- 181.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=102000, episode_reward=1456.00 +/- 253.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=103000, episode_reward=1425.40 +/- 307.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=104000, episode_reward=1433.20 +/- 221.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=105000, episode_reward=1567.00 +/- 231.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=106000, episode_reward=1333.80 +/- 263.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=107000, episode_reward=1522.60 +/- 302.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=108000, episode_reward=1609.00 +/- 183.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=109000, episode_reward=1633.40 +/- 205.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=110000, episode_reward=1639.00 +/- 197.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=111000, episode_reward=1371.40 +/- 225.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=112000, episode_reward=1507.20 +/- 93.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=113000, episode_reward=1486.60 +/- 307.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=114000, episode_reward=1648.40 +/- 336.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=115000, episode_reward=1715.40 +/- 171.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=116000, episode_reward=1601.00 +/- 160.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=117000, episode_reward=1734.00 +/- 249.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=118000, episode_reward=1689.00 +/- 186.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=119000, episode_reward=1518.20 +/- 188.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=120000, episode_reward=1641.80 +/- 82.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=121000, episode_reward=1496.00 +/- 233.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=122000, episode_reward=1591.20 +/- 232.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=123000, episode_reward=1560.40 +/- 208.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=124000, episode_reward=1436.80 +/- 215.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=125000, episode_reward=1203.60 +/- 243.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=126000, episode_reward=977.80 +/- 234.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=127000, episode_reward=1376.40 +/- 373.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=128000, episode_reward=1527.40 +/- 133.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=129000, episode_reward=1466.60 +/- 206.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=130000, episode_reward=1393.80 +/- 237.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=131000, episode_reward=1547.60 +/- 206.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=132000, episode_reward=1520.60 +/- 135.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=133000, episode_reward=1236.20 +/- 391.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=134000, episode_reward=1660.40 +/- 178.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=135000, episode_reward=1625.40 +/- 267.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=136000, episode_reward=1606.40 +/- 179.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=137000, episode_reward=1254.40 +/- 238.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=138000, episode_reward=1545.00 +/- 146.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=139000, episode_reward=1256.40 +/- 189.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=140000, episode_reward=1559.40 +/- 190.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=141000, episode_reward=1373.00 +/- 196.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=142000, episode_reward=1775.40 +/- 103.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=143000, episode_reward=1489.40 +/- 165.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=144000, episode_reward=1449.00 +/- 458.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=145000, episode_reward=1529.80 +/- 212.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=146000, episode_reward=1440.20 +/- 273.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=147000, episode_reward=1645.20 +/- 242.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=148000, episode_reward=1610.80 +/- 242.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=149000, episode_reward=1707.60 +/- 185.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=150000, episode_reward=1577.00 +/- 164.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=151000, episode_reward=1544.20 +/- 207.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=152000, episode_reward=1654.40 +/- 302.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=153000, episode_reward=1548.40 +/- 90.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=154000, episode_reward=1430.40 +/- 267.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=155000, episode_reward=1269.40 +/- 377.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=156000, episode_reward=1713.20 +/- 81.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=157000, episode_reward=1390.60 +/- 427.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=158000, episode_reward=1694.80 +/- 161.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=159000, episode_reward=1658.20 +/- 164.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=160000, episode_reward=1277.60 +/- 201.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=161000, episode_reward=1802.40 +/- 257.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=162000, episode_reward=1518.60 +/- 106.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=163000, episode_reward=1476.60 +/- 240.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=164000, episode_reward=1630.80 +/- 147.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=165000, episode_reward=1472.80 +/- 79.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=166000, episode_reward=1692.80 +/- 207.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=167000, episode_reward=1465.60 +/- 146.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=168000, episode_reward=1517.00 +/- 152.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=169000, episode_reward=1698.00 +/- 384.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=170000, episode_reward=1741.80 +/- 218.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=171000, episode_reward=1565.60 +/- 148.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=172000, episode_reward=1450.00 +/- 124.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=173000, episode_reward=1691.20 +/- 170.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=174000, episode_reward=1760.00 +/- 134.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=175000, episode_reward=1614.60 +/- 152.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=176000, episode_reward=1554.20 +/- 229.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=177000, episode_reward=1550.20 +/- 174.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=178000, episode_reward=1594.00 +/- 162.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=179000, episode_reward=1141.60 +/- 313.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=180000, episode_reward=1360.40 +/- 235.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=181000, episode_reward=1656.60 +/- 97.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=182000, episode_reward=1577.40 +/- 96.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=183000, episode_reward=1448.60 +/- 269.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=184000, episode_reward=1511.80 +/- 120.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=185000, episode_reward=1439.40 +/- 186.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=186000, episode_reward=1373.00 +/- 173.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=187000, episode_reward=1675.80 +/- 201.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=188000, episode_reward=1262.00 +/- 230.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=189000, episode_reward=1621.20 +/- 144.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=190000, episode_reward=1642.40 +/- 163.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=191000, episode_reward=1596.00 +/- 98.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=192000, episode_reward=1383.80 +/- 283.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=193000, episode_reward=1646.00 +/- 138.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=194000, episode_reward=1783.80 +/- 92.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=195000, episode_reward=1392.80 +/- 167.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=196000, episode_reward=1580.20 +/- 291.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=197000, episode_reward=1402.60 +/- 218.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=198000, episode_reward=1458.80 +/- 227.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=199000, episode_reward=1541.20 +/- 331.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=200000, episode_reward=1694.00 +/- 187.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=201000, episode_reward=1520.20 +/- 181.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=202000, episode_reward=1318.20 +/- 422.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=203000, episode_reward=1367.60 +/- 267.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=204000, episode_reward=1490.80 +/- 361.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=205000, episode_reward=1557.20 +/- 216.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=206000, episode_reward=1572.60 +/- 198.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=207000, episode_reward=1419.60 +/- 113.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=208000, episode_reward=1676.20 +/- 181.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=209000, episode_reward=1365.40 +/- 248.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=210000, episode_reward=1532.00 +/- 117.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=211000, episode_reward=1511.20 +/- 171.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=212000, episode_reward=1268.20 +/- 232.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=213000, episode_reward=1555.60 +/- 116.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=214000, episode_reward=1537.20 +/- 233.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=215000, episode_reward=1617.00 +/- 138.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=216000, episode_reward=1546.80 +/- 128.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=217000, episode_reward=1608.20 +/- 240.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=218000, episode_reward=1566.60 +/- 152.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=219000, episode_reward=1678.60 +/- 183.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=220000, episode_reward=1509.20 +/- 168.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=221000, episode_reward=1487.60 +/- 238.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=222000, episode_reward=1436.00 +/- 294.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=223000, episode_reward=1495.00 +/- 289.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=224000, episode_reward=1631.40 +/- 220.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=225000, episode_reward=1500.40 +/- 176.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=226000, episode_reward=1545.20 +/- 101.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=227000, episode_reward=1552.60 +/- 96.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=228000, episode_reward=1521.40 +/- 203.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=229000, episode_reward=1417.00 +/- 246.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=230000, episode_reward=1430.00 +/- 170.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=231000, episode_reward=1572.20 +/- 218.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=232000, episode_reward=1580.20 +/- 304.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=233000, episode_reward=1475.80 +/- 89.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=234000, episode_reward=1613.00 +/- 60.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=235000, episode_reward=1537.20 +/- 183.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=236000, episode_reward=1377.20 +/- 263.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=237000, episode_reward=1466.80 +/- 136.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=238000, episode_reward=1702.40 +/- 48.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=239000, episode_reward=1888.20 +/- 207.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=240000, episode_reward=1504.40 +/- 261.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=241000, episode_reward=1426.60 +/- 303.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=242000, episode_reward=1605.40 +/- 152.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=243000, episode_reward=1448.20 +/- 263.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=244000, episode_reward=1392.60 +/- 191.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=245000, episode_reward=1433.80 +/- 386.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=246000, episode_reward=1635.60 +/- 159.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=247000, episode_reward=1559.80 +/- 217.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=248000, episode_reward=1480.80 +/- 63.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=249000, episode_reward=1274.40 +/- 240.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=250000, episode_reward=1296.80 +/- 288.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=251000, episode_reward=1342.00 +/- 188.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=252000, episode_reward=1603.20 +/- 223.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=253000, episode_reward=1463.80 +/- 368.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=254000, episode_reward=1726.80 +/- 240.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=255000, episode_reward=1343.40 +/- 221.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=256000, episode_reward=1611.00 +/- 343.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=257000, episode_reward=1784.20 +/- 138.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=258000, episode_reward=1624.20 +/- 349.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=259000, episode_reward=1469.20 +/- 419.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=260000, episode_reward=1342.00 +/- 329.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=261000, episode_reward=1628.20 +/- 84.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=262000, episode_reward=1532.60 +/- 225.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=263000, episode_reward=1695.80 +/- 192.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=264000, episode_reward=1798.20 +/- 217.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=265000, episode_reward=1441.40 +/- 106.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=266000, episode_reward=1558.40 +/- 279.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=267000, episode_reward=1451.20 +/- 295.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=268000, episode_reward=1486.00 +/- 275.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=269000, episode_reward=1784.00 +/- 147.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=270000, episode_reward=1321.80 +/- 260.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=271000, episode_reward=1536.80 +/- 201.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=272000, episode_reward=1411.60 +/- 224.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=273000, episode_reward=1382.60 +/- 370.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=274000, episode_reward=1462.60 +/- 168.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=275000, episode_reward=1320.60 +/- 313.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=276000, episode_reward=1622.60 +/- 264.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=277000, episode_reward=1565.80 +/- 210.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=278000, episode_reward=1512.60 +/- 220.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=279000, episode_reward=1618.00 +/- 153.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=280000, episode_reward=1390.80 +/- 183.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=281000, episode_reward=1600.60 +/- 270.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=282000, episode_reward=1616.60 +/- 265.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=283000, episode_reward=1590.60 +/- 319.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=284000, episode_reward=1670.00 +/- 353.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=285000, episode_reward=1396.40 +/- 167.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=286000, episode_reward=1510.40 +/- 387.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=287000, episode_reward=1453.80 +/- 403.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=288000, episode_reward=1201.40 +/- 342.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=289000, episode_reward=1714.40 +/- 238.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=290000, episode_reward=1405.40 +/- 290.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=291000, episode_reward=1665.80 +/- 200.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=292000, episode_reward=1414.40 +/- 224.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=293000, episode_reward=1428.20 +/- 224.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=294000, episode_reward=1358.40 +/- 113.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=295000, episode_reward=1580.00 +/- 120.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=296000, episode_reward=1728.80 +/- 280.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=297000, episode_reward=1449.60 +/- 229.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=298000, episode_reward=1531.20 +/- 189.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=299000, episode_reward=1819.80 +/- 206.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=300000, episode_reward=1739.00 +/- 197.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=301000, episode_reward=1540.00 +/- 403.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=302000, episode_reward=1712.20 +/- 126.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=303000, episode_reward=1438.00 +/- 300.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=304000, episode_reward=1531.60 +/- 162.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=305000, episode_reward=1702.80 +/- 142.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=306000, episode_reward=1645.40 +/- 76.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=307000, episode_reward=1387.20 +/- 433.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=308000, episode_reward=1412.60 +/- 276.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=309000, episode_reward=1809.40 +/- 173.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=310000, episode_reward=1611.60 +/- 417.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=311000, episode_reward=1708.60 +/- 295.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=312000, episode_reward=1531.40 +/- 246.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=313000, episode_reward=1265.80 +/- 303.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=314000, episode_reward=1653.00 +/- 149.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=315000, episode_reward=1421.80 +/- 155.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=316000, episode_reward=1157.80 +/- 354.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=317000, episode_reward=1484.80 +/- 154.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=318000, episode_reward=1571.80 +/- 260.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=319000, episode_reward=1488.40 +/- 106.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=320000, episode_reward=1320.20 +/- 196.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=321000, episode_reward=1552.20 +/- 204.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=322000, episode_reward=1618.60 +/- 198.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=323000, episode_reward=1590.20 +/- 248.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=324000, episode_reward=1379.20 +/- 128.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=325000, episode_reward=1547.20 +/- 93.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=326000, episode_reward=1476.20 +/- 189.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=327000, episode_reward=1564.60 +/- 117.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=328000, episode_reward=1536.20 +/- 247.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=329000, episode_reward=1665.60 +/- 214.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=330000, episode_reward=1626.00 +/- 199.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=331000, episode_reward=1668.00 +/- 210.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=332000, episode_reward=1610.80 +/- 346.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=333000, episode_reward=1503.20 +/- 343.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=334000, episode_reward=1758.20 +/- 194.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=335000, episode_reward=1450.80 +/- 114.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=336000, episode_reward=1606.60 +/- 217.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=337000, episode_reward=1536.60 +/- 116.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=338000, episode_reward=1387.60 +/- 232.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=339000, episode_reward=1479.40 +/- 129.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=340000, episode_reward=1499.40 +/- 340.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=341000, episode_reward=1447.80 +/- 161.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=342000, episode_reward=1676.80 +/- 62.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=343000, episode_reward=1600.40 +/- 122.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=344000, episode_reward=1575.80 +/- 107.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=345000, episode_reward=1664.20 +/- 279.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=346000, episode_reward=1566.80 +/- 269.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=347000, episode_reward=1621.40 +/- 153.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=348000, episode_reward=1471.20 +/- 245.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=349000, episode_reward=1664.60 +/- 224.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=350000, episode_reward=1517.00 +/- 148.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=351000, episode_reward=1684.00 +/- 150.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=352000, episode_reward=1549.00 +/- 140.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=353000, episode_reward=1682.00 +/- 248.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=354000, episode_reward=1666.00 +/- 76.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=355000, episode_reward=1574.80 +/- 339.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=356000, episode_reward=1609.00 +/- 99.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=357000, episode_reward=1483.80 +/- 186.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=358000, episode_reward=1497.20 +/- 217.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=359000, episode_reward=1636.60 +/- 245.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=360000, episode_reward=1576.00 +/- 265.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=361000, episode_reward=1473.20 +/- 140.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=362000, episode_reward=1596.80 +/- 241.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=363000, episode_reward=1715.20 +/- 98.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=364000, episode_reward=1635.00 +/- 218.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=365000, episode_reward=1493.80 +/- 330.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=366000, episode_reward=1543.80 +/- 67.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=367000, episode_reward=1633.00 +/- 189.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=368000, episode_reward=1600.60 +/- 216.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=369000, episode_reward=1618.40 +/- 249.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=370000, episode_reward=1258.00 +/- 57.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=371000, episode_reward=1661.20 +/- 174.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=372000, episode_reward=1609.60 +/- 292.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=373000, episode_reward=1436.80 +/- 259.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=374000, episode_reward=1724.60 +/- 162.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=375000, episode_reward=1397.00 +/- 265.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=376000, episode_reward=1624.00 +/- 54.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=377000, episode_reward=1515.20 +/- 257.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=378000, episode_reward=1508.20 +/- 148.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=379000, episode_reward=1546.20 +/- 154.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=380000, episode_reward=1642.40 +/- 250.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=381000, episode_reward=1496.80 +/- 211.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=382000, episode_reward=1283.00 +/- 290.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=383000, episode_reward=1546.60 +/- 273.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=384000, episode_reward=1535.00 +/- 366.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=385000, episode_reward=1622.80 +/- 273.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=386000, episode_reward=1587.20 +/- 221.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=387000, episode_reward=1662.80 +/- 472.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=388000, episode_reward=1393.20 +/- 382.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=389000, episode_reward=1459.20 +/- 419.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=390000, episode_reward=1574.00 +/- 231.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=391000, episode_reward=1480.00 +/- 168.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=392000, episode_reward=1623.60 +/- 274.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=393000, episode_reward=1759.00 +/- 150.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=394000, episode_reward=1511.80 +/- 220.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=395000, episode_reward=1786.80 +/- 161.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=396000, episode_reward=1521.60 +/- 237.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=397000, episode_reward=1487.40 +/- 169.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=398000, episode_reward=1340.80 +/- 180.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=399000, episode_reward=1653.60 +/- 95.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=400000, episode_reward=1630.80 +/- 232.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=401000, episode_reward=1719.00 +/- 221.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=402000, episode_reward=1766.00 +/- 227.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=403000, episode_reward=1502.20 +/- 383.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=404000, episode_reward=1600.80 +/- 322.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=405000, episode_reward=1614.20 +/- 135.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=406000, episode_reward=1632.80 +/- 267.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=407000, episode_reward=1647.00 +/- 234.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=408000, episode_reward=1434.80 +/- 240.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=409000, episode_reward=1632.60 +/- 140.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=410000, episode_reward=1640.60 +/- 133.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=411000, episode_reward=1596.40 +/- 382.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=412000, episode_reward=1624.20 +/- 167.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=413000, episode_reward=1530.40 +/- 355.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=414000, episode_reward=1530.60 +/- 300.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=415000, episode_reward=1630.60 +/- 105.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=416000, episode_reward=1460.60 +/- 198.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=417000, episode_reward=1649.60 +/- 193.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=418000, episode_reward=1465.60 +/- 274.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=419000, episode_reward=1607.60 +/- 132.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=420000, episode_reward=1445.00 +/- 223.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=421000, episode_reward=1460.60 +/- 240.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=422000, episode_reward=1500.00 +/- 293.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=423000, episode_reward=1363.80 +/- 304.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=424000, episode_reward=1626.60 +/- 247.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=425000, episode_reward=1726.80 +/- 359.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=426000, episode_reward=1780.80 +/- 170.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=427000, episode_reward=1599.40 +/- 150.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=428000, episode_reward=1522.40 +/- 333.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=429000, episode_reward=1480.00 +/- 260.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=430000, episode_reward=1482.80 +/- 186.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=431000, episode_reward=1456.60 +/- 299.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=432000, episode_reward=1604.80 +/- 156.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=433000, episode_reward=1525.40 +/- 170.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=434000, episode_reward=1709.00 +/- 207.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=435000, episode_reward=1658.40 +/- 178.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=436000, episode_reward=1637.00 +/- 197.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=437000, episode_reward=1764.80 +/- 134.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=438000, episode_reward=1577.20 +/- 269.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=439000, episode_reward=1440.00 +/- 241.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=440000, episode_reward=1649.00 +/- 326.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=441000, episode_reward=1357.20 +/- 262.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=442000, episode_reward=1532.00 +/- 88.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=443000, episode_reward=1725.20 +/- 271.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=444000, episode_reward=1685.00 +/- 155.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=445000, episode_reward=1666.60 +/- 80.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=446000, episode_reward=1564.60 +/- 413.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=447000, episode_reward=1658.60 +/- 163.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=448000, episode_reward=1835.80 +/- 86.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=449000, episode_reward=1489.00 +/- 254.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=450000, episode_reward=1502.80 +/- 175.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=451000, episode_reward=1507.60 +/- 63.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=452000, episode_reward=1528.00 +/- 185.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=453000, episode_reward=1525.80 +/- 239.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=454000, episode_reward=1578.80 +/- 203.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=455000, episode_reward=1840.60 +/- 233.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=456000, episode_reward=1437.80 +/- 377.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=457000, episode_reward=1513.60 +/- 148.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=458000, episode_reward=1549.60 +/- 285.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=459000, episode_reward=1372.20 +/- 277.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=460000, episode_reward=1780.20 +/- 284.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=461000, episode_reward=1633.60 +/- 150.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=462000, episode_reward=1493.60 +/- 130.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=463000, episode_reward=1732.00 +/- 134.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=464000, episode_reward=1714.60 +/- 187.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=465000, episode_reward=1642.40 +/- 248.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=466000, episode_reward=1643.40 +/- 153.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=467000, episode_reward=1328.40 +/- 192.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=468000, episode_reward=1582.80 +/- 225.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=469000, episode_reward=1517.00 +/- 148.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=470000, episode_reward=1545.00 +/- 267.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=471000, episode_reward=1682.60 +/- 163.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=472000, episode_reward=1594.80 +/- 264.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=473000, episode_reward=1494.40 +/- 63.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=474000, episode_reward=1357.60 +/- 367.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=475000, episode_reward=1610.60 +/- 155.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=476000, episode_reward=1486.00 +/- 232.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=477000, episode_reward=1539.80 +/- 241.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=478000, episode_reward=1568.00 +/- 192.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=479000, episode_reward=1504.20 +/- 177.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=480000, episode_reward=1379.80 +/- 244.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=481000, episode_reward=1766.80 +/- 165.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=482000, episode_reward=1831.40 +/- 198.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=483000, episode_reward=1708.20 +/- 176.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=484000, episode_reward=1598.80 +/- 213.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=485000, episode_reward=1521.00 +/- 349.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=486000, episode_reward=1786.20 +/- 134.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=487000, episode_reward=1652.20 +/- 191.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=488000, episode_reward=1597.60 +/- 334.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=489000, episode_reward=1639.00 +/- 53.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=490000, episode_reward=1777.20 +/- 234.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=491000, episode_reward=1634.40 +/- 216.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=492000, episode_reward=1619.80 +/- 253.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=493000, episode_reward=1606.60 +/- 408.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=494000, episode_reward=1363.60 +/- 167.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=495000, episode_reward=1465.20 +/- 169.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=496000, episode_reward=1609.80 +/- 246.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=497000, episode_reward=1617.40 +/- 332.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=498000, episode_reward=1688.40 +/- 236.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=499000, episode_reward=1468.60 +/- 274.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=500000, episode_reward=1646.20 +/- 149.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=501000, episode_reward=1646.60 +/- 164.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=502000, episode_reward=1659.00 +/- 133.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=503000, episode_reward=1702.60 +/- 118.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=504000, episode_reward=1522.40 +/- 234.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=505000, episode_reward=1495.40 +/- 175.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=506000, episode_reward=1578.60 +/- 226.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=507000, episode_reward=1443.40 +/- 273.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=508000, episode_reward=1615.60 +/- 112.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=509000, episode_reward=1599.00 +/- 325.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=510000, episode_reward=1667.40 +/- 170.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=511000, episode_reward=1517.40 +/- 240.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=512000, episode_reward=1609.60 +/- 146.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=513000, episode_reward=1839.20 +/- 155.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=514000, episode_reward=1442.60 +/- 263.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=515000, episode_reward=1708.00 +/- 209.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=516000, episode_reward=1645.40 +/- 198.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=517000, episode_reward=1503.60 +/- 218.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=518000, episode_reward=1838.00 +/- 206.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=519000, episode_reward=1562.80 +/- 255.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=520000, episode_reward=1572.80 +/- 375.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=521000, episode_reward=1393.40 +/- 195.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=522000, episode_reward=1584.60 +/- 337.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=523000, episode_reward=1640.20 +/- 135.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=524000, episode_reward=1567.00 +/- 135.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=525000, episode_reward=1664.20 +/- 118.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=526000, episode_reward=1830.80 +/- 103.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=527000, episode_reward=1596.00 +/- 153.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=528000, episode_reward=1482.00 +/- 134.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=529000, episode_reward=1582.00 +/- 348.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=530000, episode_reward=1773.00 +/- 186.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=531000, episode_reward=1647.00 +/- 230.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=532000, episode_reward=1629.80 +/- 87.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=533000, episode_reward=1737.80 +/- 167.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=534000, episode_reward=1633.60 +/- 175.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=535000, episode_reward=1513.80 +/- 151.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=536000, episode_reward=1446.80 +/- 405.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=537000, episode_reward=1688.20 +/- 187.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=538000, episode_reward=1264.60 +/- 247.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=539000, episode_reward=1475.60 +/- 186.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=540000, episode_reward=1551.20 +/- 180.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=541000, episode_reward=1472.00 +/- 93.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=542000, episode_reward=1680.20 +/- 204.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=543000, episode_reward=1589.40 +/- 164.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=544000, episode_reward=1564.00 +/- 170.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=545000, episode_reward=1711.80 +/- 266.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=546000, episode_reward=1628.00 +/- 144.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=547000, episode_reward=1547.00 +/- 144.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=548000, episode_reward=1699.20 +/- 190.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=549000, episode_reward=1607.60 +/- 179.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=550000, episode_reward=1546.20 +/- 218.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=551000, episode_reward=1560.20 +/- 331.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=552000, episode_reward=1619.00 +/- 159.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=553000, episode_reward=1741.00 +/- 111.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=554000, episode_reward=1702.80 +/- 146.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=555000, episode_reward=1613.40 +/- 470.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=556000, episode_reward=1548.00 +/- 88.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=557000, episode_reward=1688.40 +/- 197.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=558000, episode_reward=1776.80 +/- 211.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=559000, episode_reward=1766.80 +/- 149.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=560000, episode_reward=1563.80 +/- 213.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=561000, episode_reward=1804.20 +/- 144.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=562000, episode_reward=1655.40 +/- 211.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=563000, episode_reward=1639.20 +/- 193.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=564000, episode_reward=1575.40 +/- 160.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=565000, episode_reward=1632.80 +/- 169.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=566000, episode_reward=1708.40 +/- 192.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=567000, episode_reward=1578.80 +/- 243.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=568000, episode_reward=1359.80 +/- 208.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=569000, episode_reward=1527.40 +/- 240.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=570000, episode_reward=1550.60 +/- 173.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=571000, episode_reward=1628.40 +/- 150.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=572000, episode_reward=1721.00 +/- 171.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=573000, episode_reward=1749.60 +/- 192.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=574000, episode_reward=1594.20 +/- 256.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=575000, episode_reward=1844.40 +/- 67.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=576000, episode_reward=1667.60 +/- 170.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=577000, episode_reward=1745.20 +/- 125.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=578000, episode_reward=1630.80 +/- 159.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=579000, episode_reward=1629.20 +/- 249.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=580000, episode_reward=1641.00 +/- 190.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=581000, episode_reward=1755.20 +/- 132.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=582000, episode_reward=1888.80 +/- 144.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=583000, episode_reward=1616.40 +/- 262.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=584000, episode_reward=1567.20 +/- 263.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=585000, episode_reward=1757.80 +/- 53.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=586000, episode_reward=1649.80 +/- 130.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=587000, episode_reward=1784.60 +/- 189.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=588000, episode_reward=1665.60 +/- 233.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=589000, episode_reward=1566.40 +/- 190.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=590000, episode_reward=1477.00 +/- 140.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=591000, episode_reward=1624.00 +/- 181.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=592000, episode_reward=1699.60 +/- 155.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=593000, episode_reward=1733.20 +/- 196.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=594000, episode_reward=1846.00 +/- 124.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=595000, episode_reward=1650.60 +/- 222.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=596000, episode_reward=1713.60 +/- 188.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=597000, episode_reward=1494.20 +/- 296.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=598000, episode_reward=1488.60 +/- 170.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=599000, episode_reward=1790.60 +/- 143.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=600000, episode_reward=1738.40 +/- 264.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=601000, episode_reward=1787.40 +/- 167.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=602000, episode_reward=1581.40 +/- 297.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=603000, episode_reward=1775.60 +/- 223.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=604000, episode_reward=1395.60 +/- 339.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=605000, episode_reward=1673.00 +/- 166.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=606000, episode_reward=1675.80 +/- 286.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=607000, episode_reward=1524.20 +/- 138.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=608000, episode_reward=1539.20 +/- 210.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=609000, episode_reward=1365.60 +/- 108.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=610000, episode_reward=1445.80 +/- 376.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=611000, episode_reward=1644.00 +/- 182.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=612000, episode_reward=1756.80 +/- 162.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=613000, episode_reward=1696.00 +/- 149.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=614000, episode_reward=1667.60 +/- 92.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=615000, episode_reward=1446.20 +/- 221.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=616000, episode_reward=1494.80 +/- 320.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=617000, episode_reward=1728.60 +/- 189.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=618000, episode_reward=1496.40 +/- 111.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=619000, episode_reward=1572.80 +/- 358.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=620000, episode_reward=1554.60 +/- 221.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=621000, episode_reward=1616.40 +/- 139.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=622000, episode_reward=1565.80 +/- 163.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=623000, episode_reward=1539.60 +/- 223.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=624000, episode_reward=1759.00 +/- 286.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=625000, episode_reward=1598.60 +/- 320.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=626000, episode_reward=995.20 +/- 605.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=627000, episode_reward=1561.20 +/- 290.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=628000, episode_reward=1522.60 +/- 181.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=629000, episode_reward=1754.40 +/- 84.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=630000, episode_reward=1724.60 +/- 378.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=631000, episode_reward=1590.20 +/- 338.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=632000, episode_reward=1668.20 +/- 213.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=633000, episode_reward=1673.20 +/- 275.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=634000, episode_reward=1673.40 +/- 263.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=635000, episode_reward=1794.80 +/- 174.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=636000, episode_reward=1879.40 +/- 127.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=637000, episode_reward=1528.80 +/- 202.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=638000, episode_reward=1570.40 +/- 226.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=639000, episode_reward=1605.00 +/- 139.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=640000, episode_reward=1660.00 +/- 293.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=641000, episode_reward=1593.80 +/- 161.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=642000, episode_reward=1668.60 +/- 100.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=643000, episode_reward=1702.40 +/- 265.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=644000, episode_reward=1433.60 +/- 268.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=645000, episode_reward=1763.40 +/- 208.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=646000, episode_reward=1629.80 +/- 306.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=647000, episode_reward=1684.20 +/- 148.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=648000, episode_reward=1622.60 +/- 321.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=649000, episode_reward=1636.00 +/- 120.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=650000, episode_reward=1657.00 +/- 134.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=651000, episode_reward=1612.80 +/- 255.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=652000, episode_reward=1473.60 +/- 189.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=653000, episode_reward=1743.60 +/- 48.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=654000, episode_reward=1354.00 +/- 254.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=655000, episode_reward=1522.60 +/- 222.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=656000, episode_reward=1508.40 +/- 119.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=657000, episode_reward=1722.20 +/- 187.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=658000, episode_reward=1668.20 +/- 284.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=659000, episode_reward=1433.20 +/- 304.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=660000, episode_reward=1838.00 +/- 87.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=661000, episode_reward=1791.60 +/- 154.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=662000, episode_reward=1603.60 +/- 266.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=663000, episode_reward=1704.80 +/- 285.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=664000, episode_reward=1627.80 +/- 227.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=665000, episode_reward=1617.40 +/- 125.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=666000, episode_reward=1693.20 +/- 233.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=667000, episode_reward=1606.40 +/- 174.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=668000, episode_reward=1710.60 +/- 155.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=669000, episode_reward=1633.40 +/- 140.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=670000, episode_reward=1662.00 +/- 112.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=671000, episode_reward=1585.00 +/- 234.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=672000, episode_reward=1707.40 +/- 275.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=673000, episode_reward=1636.60 +/- 278.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=674000, episode_reward=1706.80 +/- 218.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=675000, episode_reward=1598.20 +/- 239.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=676000, episode_reward=1555.00 +/- 237.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=677000, episode_reward=1691.60 +/- 321.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=678000, episode_reward=1633.00 +/- 136.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=679000, episode_reward=1583.00 +/- 217.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=680000, episode_reward=1505.00 +/- 123.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=681000, episode_reward=1738.80 +/- 184.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=682000, episode_reward=1479.60 +/- 80.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=683000, episode_reward=1809.40 +/- 285.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=684000, episode_reward=1789.40 +/- 202.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=685000, episode_reward=1598.60 +/- 199.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=686000, episode_reward=1543.20 +/- 294.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=687000, episode_reward=1541.20 +/- 109.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=688000, episode_reward=1604.00 +/- 250.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=689000, episode_reward=1657.60 +/- 242.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=690000, episode_reward=1585.80 +/- 103.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=691000, episode_reward=1689.00 +/- 173.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=692000, episode_reward=1826.20 +/- 164.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=693000, episode_reward=1540.80 +/- 242.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=694000, episode_reward=1619.20 +/- 257.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=695000, episode_reward=1700.80 +/- 351.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=696000, episode_reward=1716.20 +/- 274.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=697000, episode_reward=1442.80 +/- 253.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=698000, episode_reward=1716.00 +/- 111.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=699000, episode_reward=1746.40 +/- 67.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=700000, episode_reward=1514.60 +/- 274.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=701000, episode_reward=1708.20 +/- 113.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=702000, episode_reward=1722.60 +/- 165.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=703000, episode_reward=1700.60 +/- 163.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=704000, episode_reward=1458.40 +/- 116.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=705000, episode_reward=1629.80 +/- 231.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=706000, episode_reward=1708.60 +/- 228.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=707000, episode_reward=1528.60 +/- 177.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=708000, episode_reward=1661.40 +/- 209.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=709000, episode_reward=1559.40 +/- 149.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=710000, episode_reward=1702.80 +/- 179.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=711000, episode_reward=1737.00 +/- 172.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=712000, episode_reward=1634.20 +/- 138.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=713000, episode_reward=1593.20 +/- 280.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=714000, episode_reward=1815.40 +/- 98.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=715000, episode_reward=1468.60 +/- 185.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=716000, episode_reward=1644.60 +/- 197.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=717000, episode_reward=1760.80 +/- 104.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=718000, episode_reward=1684.60 +/- 51.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=719000, episode_reward=1628.00 +/- 293.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=720000, episode_reward=1571.40 +/- 86.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=721000, episode_reward=1726.60 +/- 211.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=722000, episode_reward=1461.00 +/- 137.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=723000, episode_reward=1583.40 +/- 304.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=724000, episode_reward=1704.20 +/- 214.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=725000, episode_reward=1810.40 +/- 240.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=726000, episode_reward=1637.80 +/- 257.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=727000, episode_reward=1509.20 +/- 263.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=728000, episode_reward=1446.20 +/- 284.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=729000, episode_reward=1554.00 +/- 176.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=730000, episode_reward=1483.20 +/- 140.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=731000, episode_reward=1678.80 +/- 223.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=732000, episode_reward=1677.40 +/- 228.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=733000, episode_reward=1500.60 +/- 221.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=734000, episode_reward=1651.20 +/- 205.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=735000, episode_reward=1540.20 +/- 123.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=736000, episode_reward=1860.20 +/- 110.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=737000, episode_reward=1688.20 +/- 192.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=738000, episode_reward=1499.60 +/- 309.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=739000, episode_reward=1521.40 +/- 181.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=740000, episode_reward=1529.00 +/- 165.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=741000, episode_reward=1682.60 +/- 175.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=742000, episode_reward=1567.40 +/- 210.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=743000, episode_reward=1617.60 +/- 319.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=744000, episode_reward=1621.20 +/- 45.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=745000, episode_reward=1696.00 +/- 241.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=746000, episode_reward=1520.60 +/- 310.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=747000, episode_reward=1455.00 +/- 283.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=748000, episode_reward=1752.20 +/- 214.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=749000, episode_reward=1688.40 +/- 169.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=750000, episode_reward=1617.80 +/- 140.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=751000, episode_reward=1758.20 +/- 117.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=752000, episode_reward=1604.60 +/- 235.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=753000, episode_reward=1649.80 +/- 190.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=754000, episode_reward=1527.40 +/- 148.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=755000, episode_reward=1581.80 +/- 131.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=756000, episode_reward=1645.80 +/- 278.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=757000, episode_reward=1705.00 +/- 210.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=758000, episode_reward=1413.60 +/- 391.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=759000, episode_reward=1736.40 +/- 186.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=760000, episode_reward=1676.40 +/- 115.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=761000, episode_reward=1674.40 +/- 224.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=762000, episode_reward=1481.40 +/- 180.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=763000, episode_reward=1496.40 +/- 263.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=764000, episode_reward=1677.60 +/- 188.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=765000, episode_reward=1581.00 +/- 239.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=766000, episode_reward=1675.20 +/- 115.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=767000, episode_reward=1597.00 +/- 110.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=768000, episode_reward=1432.20 +/- 342.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=769000, episode_reward=1746.40 +/- 225.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=770000, episode_reward=1559.60 +/- 179.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=771000, episode_reward=1593.80 +/- 184.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=772000, episode_reward=1719.20 +/- 161.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=773000, episode_reward=1549.80 +/- 244.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=774000, episode_reward=1720.80 +/- 151.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=775000, episode_reward=1656.40 +/- 174.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=776000, episode_reward=1803.60 +/- 88.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=777000, episode_reward=1667.40 +/- 183.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=778000, episode_reward=1480.20 +/- 137.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=779000, episode_reward=1581.60 +/- 317.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=780000, episode_reward=1754.20 +/- 328.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=781000, episode_reward=1621.20 +/- 135.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=782000, episode_reward=1443.20 +/- 226.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=783000, episode_reward=1534.20 +/- 210.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=784000, episode_reward=1509.60 +/- 140.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=785000, episode_reward=1663.20 +/- 226.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=786000, episode_reward=1548.20 +/- 78.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=787000, episode_reward=1545.40 +/- 95.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=788000, episode_reward=1692.40 +/- 234.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=789000, episode_reward=1665.60 +/- 369.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=790000, episode_reward=1546.60 +/- 331.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=791000, episode_reward=1546.40 +/- 245.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=792000, episode_reward=1794.00 +/- 299.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=793000, episode_reward=1800.40 +/- 222.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=794000, episode_reward=1661.20 +/- 241.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=795000, episode_reward=1746.20 +/- 141.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=796000, episode_reward=1664.20 +/- 97.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=797000, episode_reward=1601.80 +/- 94.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=798000, episode_reward=1607.40 +/- 301.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=799000, episode_reward=1660.60 +/- 184.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=800000, episode_reward=1370.60 +/- 247.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=801000, episode_reward=1562.60 +/- 203.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=802000, episode_reward=1717.60 +/- 157.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=803000, episode_reward=1752.20 +/- 169.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=804000, episode_reward=1399.60 +/- 149.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=805000, episode_reward=1626.00 +/- 234.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=806000, episode_reward=1719.20 +/- 131.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=807000, episode_reward=1695.20 +/- 248.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=808000, episode_reward=1505.40 +/- 233.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=809000, episode_reward=1581.40 +/- 215.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=810000, episode_reward=1762.20 +/- 96.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=811000, episode_reward=1391.80 +/- 103.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=812000, episode_reward=1714.00 +/- 237.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=813000, episode_reward=1619.20 +/- 232.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=814000, episode_reward=1639.00 +/- 316.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=815000, episode_reward=1482.80 +/- 349.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=816000, episode_reward=1653.00 +/- 245.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=817000, episode_reward=1611.60 +/- 122.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=818000, episode_reward=1573.80 +/- 155.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=819000, episode_reward=1801.60 +/- 146.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=820000, episode_reward=1423.20 +/- 177.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=821000, episode_reward=1813.80 +/- 163.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=822000, episode_reward=1687.60 +/- 263.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=823000, episode_reward=1624.20 +/- 237.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=824000, episode_reward=1635.20 +/- 85.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=825000, episode_reward=1649.40 +/- 262.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=826000, episode_reward=1703.60 +/- 198.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=827000, episode_reward=1627.00 +/- 178.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=828000, episode_reward=1788.40 +/- 77.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=829000, episode_reward=1689.20 +/- 175.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=830000, episode_reward=1490.40 +/- 306.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=831000, episode_reward=1602.00 +/- 92.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=832000, episode_reward=1731.40 +/- 148.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=833000, episode_reward=1678.00 +/- 113.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=834000, episode_reward=1707.20 +/- 193.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=835000, episode_reward=1551.60 +/- 183.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=836000, episode_reward=1577.20 +/- 184.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=837000, episode_reward=1707.80 +/- 327.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=838000, episode_reward=1707.80 +/- 159.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=839000, episode_reward=1772.20 +/- 134.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=840000, episode_reward=1629.20 +/- 145.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=841000, episode_reward=1583.00 +/- 256.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=842000, episode_reward=1695.00 +/- 76.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=843000, episode_reward=1650.20 +/- 81.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=844000, episode_reward=1582.60 +/- 182.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=845000, episode_reward=1800.40 +/- 129.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=846000, episode_reward=1564.20 +/- 78.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=847000, episode_reward=1722.60 +/- 56.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=848000, episode_reward=1586.20 +/- 193.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=849000, episode_reward=1628.60 +/- 187.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=850000, episode_reward=1733.60 +/- 224.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=851000, episode_reward=1649.60 +/- 196.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=852000, episode_reward=1628.60 +/- 350.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=853000, episode_reward=1761.60 +/- 178.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=854000, episode_reward=1374.60 +/- 369.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=855000, episode_reward=1819.20 +/- 83.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=856000, episode_reward=1471.00 +/- 292.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=857000, episode_reward=1633.20 +/- 372.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=858000, episode_reward=1753.80 +/- 208.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=859000, episode_reward=1575.40 +/- 201.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=860000, episode_reward=1714.40 +/- 235.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=861000, episode_reward=1816.40 +/- 236.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=862000, episode_reward=1503.20 +/- 493.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=863000, episode_reward=1619.80 +/- 167.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=864000, episode_reward=1740.80 +/- 129.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=865000, episode_reward=1750.60 +/- 224.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=866000, episode_reward=1573.40 +/- 151.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=867000, episode_reward=1502.40 +/- 202.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=868000, episode_reward=1618.80 +/- 344.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=869000, episode_reward=1605.60 +/- 240.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=870000, episode_reward=1461.80 +/- 278.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=871000, episode_reward=1668.00 +/- 196.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=872000, episode_reward=1555.00 +/- 140.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=873000, episode_reward=1400.20 +/- 105.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=874000, episode_reward=1677.00 +/- 308.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=875000, episode_reward=1394.00 +/- 197.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=876000, episode_reward=1542.60 +/- 89.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=877000, episode_reward=1593.40 +/- 115.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=878000, episode_reward=1435.00 +/- 294.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=879000, episode_reward=1776.40 +/- 231.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=880000, episode_reward=1855.00 +/- 187.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=881000, episode_reward=1778.20 +/- 164.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=882000, episode_reward=1582.20 +/- 345.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=883000, episode_reward=1669.60 +/- 236.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=884000, episode_reward=1573.40 +/- 143.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=885000, episode_reward=1424.80 +/- 251.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=886000, episode_reward=1714.40 +/- 50.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=887000, episode_reward=1557.20 +/- 205.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=888000, episode_reward=1649.00 +/- 214.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=889000, episode_reward=1533.40 +/- 142.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=890000, episode_reward=1503.00 +/- 253.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=891000, episode_reward=1666.00 +/- 230.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=892000, episode_reward=1607.40 +/- 217.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=893000, episode_reward=1645.80 +/- 142.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=894000, episode_reward=1665.00 +/- 274.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=895000, episode_reward=1581.60 +/- 146.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=896000, episode_reward=1626.60 +/- 233.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=897000, episode_reward=1760.60 +/- 123.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=898000, episode_reward=1655.20 +/- 204.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=899000, episode_reward=1666.20 +/- 142.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=900000, episode_reward=1692.60 +/- 139.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=901000, episode_reward=1702.00 +/- 146.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=902000, episode_reward=1588.20 +/- 300.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=903000, episode_reward=1647.20 +/- 324.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=904000, episode_reward=1598.20 +/- 260.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=905000, episode_reward=1862.80 +/- 187.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=906000, episode_reward=1725.20 +/- 231.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=907000, episode_reward=1537.40 +/- 206.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=908000, episode_reward=1543.20 +/- 332.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=909000, episode_reward=1674.00 +/- 291.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=910000, episode_reward=1599.60 +/- 135.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=911000, episode_reward=1582.00 +/- 112.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=912000, episode_reward=1704.80 +/- 208.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=913000, episode_reward=1752.80 +/- 263.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=914000, episode_reward=1580.20 +/- 168.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=915000, episode_reward=1739.60 +/- 209.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=916000, episode_reward=1604.80 +/- 153.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=917000, episode_reward=1499.00 +/- 343.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=918000, episode_reward=1623.20 +/- 170.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=919000, episode_reward=1674.60 +/- 202.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=920000, episode_reward=1545.60 +/- 97.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=921000, episode_reward=1767.60 +/- 132.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=922000, episode_reward=1644.00 +/- 218.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=923000, episode_reward=1625.60 +/- 201.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=924000, episode_reward=1688.60 +/- 330.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=925000, episode_reward=1460.80 +/- 241.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=926000, episode_reward=1646.60 +/- 229.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=927000, episode_reward=1746.40 +/- 155.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=928000, episode_reward=1756.40 +/- 97.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=929000, episode_reward=1476.00 +/- 294.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=930000, episode_reward=1821.60 +/- 268.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=931000, episode_reward=1645.80 +/- 42.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=932000, episode_reward=1552.40 +/- 103.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=933000, episode_reward=1478.80 +/- 396.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=934000, episode_reward=1466.80 +/- 179.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=935000, episode_reward=1615.60 +/- 89.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=936000, episode_reward=1574.00 +/- 305.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=937000, episode_reward=1634.20 +/- 186.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=938000, episode_reward=1782.40 +/- 170.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=939000, episode_reward=1564.80 +/- 283.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=940000, episode_reward=1577.60 +/- 261.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=941000, episode_reward=1710.00 +/- 77.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=942000, episode_reward=1676.40 +/- 203.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=943000, episode_reward=1778.40 +/- 95.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=944000, episode_reward=1329.80 +/- 284.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=945000, episode_reward=1769.40 +/- 69.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=946000, episode_reward=1622.60 +/- 197.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=947000, episode_reward=1587.00 +/- 145.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=948000, episode_reward=1645.80 +/- 156.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=949000, episode_reward=1647.60 +/- 53.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=950000, episode_reward=1871.00 +/- 57.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=951000, episode_reward=1630.20 +/- 208.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=952000, episode_reward=1311.40 +/- 123.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=953000, episode_reward=1662.20 +/- 141.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=954000, episode_reward=1435.80 +/- 196.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=955000, episode_reward=1459.40 +/- 201.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=956000, episode_reward=1706.40 +/- 262.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=957000, episode_reward=1730.20 +/- 214.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=958000, episode_reward=1607.00 +/- 151.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=959000, episode_reward=1724.00 +/- 177.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=960000, episode_reward=1476.80 +/- 218.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=961000, episode_reward=1730.20 +/- 435.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=962000, episode_reward=1665.60 +/- 145.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=963000, episode_reward=1674.00 +/- 162.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=964000, episode_reward=1477.40 +/- 254.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=965000, episode_reward=1640.20 +/- 304.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=966000, episode_reward=1610.60 +/- 87.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=967000, episode_reward=1772.00 +/- 146.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=968000, episode_reward=1783.80 +/- 199.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=969000, episode_reward=1590.40 +/- 209.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=970000, episode_reward=1629.60 +/- 258.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=971000, episode_reward=1484.60 +/- 403.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=972000, episode_reward=1603.80 +/- 417.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=973000, episode_reward=1782.20 +/- 98.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=974000, episode_reward=1451.40 +/- 249.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=975000, episode_reward=1688.40 +/- 307.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=976000, episode_reward=1635.80 +/- 173.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=977000, episode_reward=1692.20 +/- 99.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=978000, episode_reward=1836.60 +/- 45.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=979000, episode_reward=1569.20 +/- 229.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=980000, episode_reward=1775.60 +/- 126.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=981000, episode_reward=1684.00 +/- 207.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=982000, episode_reward=1642.40 +/- 180.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=983000, episode_reward=1896.20 +/- 74.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=984000, episode_reward=1650.40 +/- 123.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=985000, episode_reward=1385.20 +/- 288.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=986000, episode_reward=1577.80 +/- 209.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=987000, episode_reward=1732.20 +/- 262.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=988000, episode_reward=1715.00 +/- 217.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=989000, episode_reward=1555.00 +/- 86.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=990000, episode_reward=1788.60 +/- 141.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=991000, episode_reward=1596.40 +/- 427.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=992000, episode_reward=1550.80 +/- 139.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=993000, episode_reward=1659.00 +/- 256.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=994000, episode_reward=1548.20 +/- 117.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=995000, episode_reward=1431.40 +/- 315.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=996000, episode_reward=1707.80 +/- 261.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=997000, episode_reward=1582.00 +/- 166.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=998000, episode_reward=1454.80 +/- 403.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=999000, episode_reward=1483.60 +/- 165.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1000000, episode_reward=1678.80 +/- 186.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1001000, episode_reward=1550.60 +/- 362.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Gamma = 0.9, Learning Rate = 0.003, Activation Function = ReLU, Neurons = 8, Process noise = 0.1, Measurement noise = 0.0\n",
      "Eval num_timesteps=1000, episode_reward=-1448.60 +/- 183.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=-1398.80 +/- 202.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3000, episode_reward=349.00 +/- 183.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4000, episode_reward=257.60 +/- 189.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=5000, episode_reward=-36.00 +/- 80.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=6000, episode_reward=2.00 +/- 212.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=7000, episode_reward=303.80 +/- 144.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=8000, episode_reward=309.80 +/- 85.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=9000, episode_reward=367.80 +/- 103.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=10000, episode_reward=537.60 +/- 112.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=11000, episode_reward=210.20 +/- 157.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=12000, episode_reward=380.80 +/- 140.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=13000, episode_reward=321.20 +/- 127.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=14000, episode_reward=210.00 +/- 158.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=15000, episode_reward=353.20 +/- 115.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=16000, episode_reward=288.20 +/- 76.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=17000, episode_reward=284.80 +/- 183.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=18000, episode_reward=341.40 +/- 71.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=19000, episode_reward=317.20 +/- 151.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=20000, episode_reward=246.00 +/- 211.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=21000, episode_reward=519.20 +/- 153.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=22000, episode_reward=305.40 +/- 129.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=23000, episode_reward=611.00 +/- 303.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=24000, episode_reward=531.40 +/- 229.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=25000, episode_reward=892.00 +/- 344.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=26000, episode_reward=1046.60 +/- 200.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=27000, episode_reward=953.00 +/- 305.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=28000, episode_reward=819.60 +/- 367.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=29000, episode_reward=1256.60 +/- 157.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=30000, episode_reward=1291.40 +/- 138.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=31000, episode_reward=1324.60 +/- 265.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=32000, episode_reward=1353.40 +/- 203.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=33000, episode_reward=976.20 +/- 545.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=34000, episode_reward=1129.00 +/- 200.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=35000, episode_reward=1325.40 +/- 416.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=36000, episode_reward=1366.40 +/- 344.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=37000, episode_reward=1383.00 +/- 278.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=38000, episode_reward=1429.60 +/- 306.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=39000, episode_reward=1331.20 +/- 277.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=40000, episode_reward=1134.60 +/- 220.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=41000, episode_reward=1361.60 +/- 216.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=42000, episode_reward=1424.40 +/- 299.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=43000, episode_reward=1225.40 +/- 119.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=44000, episode_reward=1597.60 +/- 118.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=45000, episode_reward=1320.00 +/- 394.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=46000, episode_reward=1359.80 +/- 97.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=47000, episode_reward=1452.40 +/- 365.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=48000, episode_reward=1565.80 +/- 169.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=49000, episode_reward=1542.60 +/- 202.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=50000, episode_reward=1451.20 +/- 285.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=51000, episode_reward=1571.20 +/- 437.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=52000, episode_reward=1633.00 +/- 208.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=53000, episode_reward=1748.40 +/- 194.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=54000, episode_reward=1864.00 +/- 129.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=55000, episode_reward=1658.00 +/- 188.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=56000, episode_reward=1680.60 +/- 243.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=57000, episode_reward=1568.40 +/- 242.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=58000, episode_reward=1738.40 +/- 158.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=59000, episode_reward=1689.20 +/- 328.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=60000, episode_reward=1486.20 +/- 73.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=61000, episode_reward=1491.20 +/- 160.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=62000, episode_reward=1453.00 +/- 160.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=63000, episode_reward=1420.80 +/- 197.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=64000, episode_reward=1819.60 +/- 238.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=65000, episode_reward=1610.80 +/- 274.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=66000, episode_reward=1632.00 +/- 107.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=67000, episode_reward=1567.20 +/- 270.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=68000, episode_reward=1705.80 +/- 216.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=69000, episode_reward=1326.00 +/- 22.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=70000, episode_reward=1634.00 +/- 225.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=71000, episode_reward=1664.40 +/- 170.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=72000, episode_reward=1433.20 +/- 185.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=73000, episode_reward=1640.20 +/- 176.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=74000, episode_reward=1401.20 +/- 345.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=75000, episode_reward=1684.00 +/- 128.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=76000, episode_reward=1586.20 +/- 236.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=77000, episode_reward=1640.60 +/- 159.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=78000, episode_reward=1553.60 +/- 249.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=79000, episode_reward=1573.40 +/- 152.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=80000, episode_reward=1660.00 +/- 188.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=81000, episode_reward=1857.20 +/- 170.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=82000, episode_reward=1497.00 +/- 248.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=83000, episode_reward=1772.20 +/- 278.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=84000, episode_reward=1551.00 +/- 294.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=85000, episode_reward=1623.40 +/- 189.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=86000, episode_reward=1613.00 +/- 213.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=87000, episode_reward=1478.20 +/- 141.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=88000, episode_reward=1742.20 +/- 223.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=89000, episode_reward=1753.80 +/- 108.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=90000, episode_reward=1555.60 +/- 287.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=91000, episode_reward=1592.20 +/- 180.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=92000, episode_reward=1638.20 +/- 160.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=93000, episode_reward=1774.20 +/- 195.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=94000, episode_reward=1613.20 +/- 167.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=95000, episode_reward=1741.60 +/- 155.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=96000, episode_reward=1717.40 +/- 189.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=97000, episode_reward=1671.80 +/- 182.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=98000, episode_reward=1435.40 +/- 266.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=99000, episode_reward=1596.20 +/- 169.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=100000, episode_reward=1578.00 +/- 146.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=101000, episode_reward=1841.40 +/- 250.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=102000, episode_reward=1429.00 +/- 162.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=103000, episode_reward=1803.00 +/- 178.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=104000, episode_reward=1527.40 +/- 288.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=105000, episode_reward=1832.00 +/- 192.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=106000, episode_reward=1758.20 +/- 205.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=107000, episode_reward=1688.20 +/- 223.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=108000, episode_reward=1509.80 +/- 251.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=109000, episode_reward=1539.60 +/- 103.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=110000, episode_reward=1538.40 +/- 250.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=111000, episode_reward=1577.00 +/- 179.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=112000, episode_reward=1603.40 +/- 189.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=113000, episode_reward=1606.80 +/- 182.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=114000, episode_reward=1578.60 +/- 71.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=115000, episode_reward=1697.20 +/- 293.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=116000, episode_reward=1860.80 +/- 134.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=117000, episode_reward=1680.80 +/- 233.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=118000, episode_reward=1658.80 +/- 95.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=119000, episode_reward=1873.00 +/- 148.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=120000, episode_reward=1433.60 +/- 154.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=121000, episode_reward=1364.00 +/- 182.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=122000, episode_reward=1673.00 +/- 91.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=123000, episode_reward=1698.40 +/- 195.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=124000, episode_reward=1689.20 +/- 267.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=125000, episode_reward=1668.20 +/- 282.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=126000, episode_reward=1628.80 +/- 370.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=127000, episode_reward=1476.40 +/- 530.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=128000, episode_reward=1689.20 +/- 314.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=129000, episode_reward=1826.00 +/- 182.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=130000, episode_reward=1668.40 +/- 131.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=131000, episode_reward=1537.20 +/- 304.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=132000, episode_reward=1536.60 +/- 284.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=133000, episode_reward=1581.60 +/- 133.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=134000, episode_reward=1704.80 +/- 142.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=135000, episode_reward=1570.20 +/- 173.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=136000, episode_reward=1657.80 +/- 65.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=137000, episode_reward=1505.00 +/- 188.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=138000, episode_reward=1560.40 +/- 159.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=139000, episode_reward=1624.00 +/- 144.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=140000, episode_reward=1466.60 +/- 154.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=141000, episode_reward=1654.60 +/- 141.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=142000, episode_reward=1521.40 +/- 76.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=143000, episode_reward=1692.60 +/- 134.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=144000, episode_reward=1709.20 +/- 269.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=145000, episode_reward=1576.00 +/- 98.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=146000, episode_reward=1684.60 +/- 185.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=147000, episode_reward=1498.60 +/- 218.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=148000, episode_reward=1586.40 +/- 297.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=149000, episode_reward=1560.20 +/- 193.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=150000, episode_reward=1407.60 +/- 310.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=151000, episode_reward=1639.80 +/- 183.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=152000, episode_reward=1404.00 +/- 226.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=153000, episode_reward=1619.60 +/- 173.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=154000, episode_reward=1653.40 +/- 174.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=155000, episode_reward=1712.40 +/- 237.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=156000, episode_reward=1732.20 +/- 178.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=157000, episode_reward=1638.60 +/- 190.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=158000, episode_reward=1557.20 +/- 228.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=159000, episode_reward=1652.00 +/- 82.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=160000, episode_reward=1700.40 +/- 127.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=161000, episode_reward=1649.00 +/- 120.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=162000, episode_reward=1666.40 +/- 253.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=163000, episode_reward=1417.20 +/- 329.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=164000, episode_reward=1645.60 +/- 240.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=165000, episode_reward=1640.60 +/- 118.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=166000, episode_reward=1616.20 +/- 155.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=167000, episode_reward=1471.40 +/- 198.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=168000, episode_reward=1745.20 +/- 106.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=169000, episode_reward=1729.00 +/- 175.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=170000, episode_reward=1538.40 +/- 244.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=171000, episode_reward=1652.20 +/- 195.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=172000, episode_reward=1599.20 +/- 132.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=173000, episode_reward=1664.00 +/- 180.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=174000, episode_reward=1618.60 +/- 282.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=175000, episode_reward=1683.00 +/- 152.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=176000, episode_reward=1666.00 +/- 191.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=177000, episode_reward=1653.20 +/- 180.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=178000, episode_reward=1763.40 +/- 172.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=179000, episode_reward=1586.40 +/- 178.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=180000, episode_reward=1771.60 +/- 174.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=181000, episode_reward=1519.00 +/- 240.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=182000, episode_reward=1480.20 +/- 222.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=183000, episode_reward=1511.60 +/- 216.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=184000, episode_reward=1624.00 +/- 197.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=185000, episode_reward=1793.00 +/- 172.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=186000, episode_reward=1652.00 +/- 294.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=187000, episode_reward=1617.40 +/- 194.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=188000, episode_reward=1593.60 +/- 109.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=189000, episode_reward=1732.20 +/- 148.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=190000, episode_reward=1723.20 +/- 88.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=191000, episode_reward=1721.20 +/- 369.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=192000, episode_reward=1604.60 +/- 169.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=193000, episode_reward=1670.60 +/- 246.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=194000, episode_reward=1183.40 +/- 278.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=195000, episode_reward=1726.80 +/- 238.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=196000, episode_reward=1516.40 +/- 278.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=197000, episode_reward=1896.40 +/- 135.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=198000, episode_reward=1512.40 +/- 285.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=199000, episode_reward=1728.80 +/- 178.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=200000, episode_reward=1723.40 +/- 131.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=201000, episode_reward=1761.60 +/- 108.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=202000, episode_reward=1703.00 +/- 113.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=203000, episode_reward=1538.00 +/- 284.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=204000, episode_reward=1692.40 +/- 261.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=205000, episode_reward=1755.80 +/- 174.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=206000, episode_reward=1476.40 +/- 52.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=207000, episode_reward=1838.20 +/- 202.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=208000, episode_reward=1427.40 +/- 300.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=209000, episode_reward=1718.40 +/- 174.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=210000, episode_reward=1533.20 +/- 182.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=211000, episode_reward=1647.40 +/- 215.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=212000, episode_reward=1618.60 +/- 307.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=213000, episode_reward=1635.80 +/- 221.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=214000, episode_reward=1538.80 +/- 270.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=215000, episode_reward=1604.60 +/- 244.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=216000, episode_reward=1616.40 +/- 155.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=217000, episode_reward=1691.80 +/- 163.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=218000, episode_reward=1777.20 +/- 183.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=219000, episode_reward=1772.60 +/- 193.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=220000, episode_reward=1628.80 +/- 149.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=221000, episode_reward=1695.60 +/- 191.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=222000, episode_reward=1868.80 +/- 96.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=223000, episode_reward=1565.00 +/- 265.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=224000, episode_reward=1390.40 +/- 242.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=225000, episode_reward=1459.20 +/- 59.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=226000, episode_reward=1619.80 +/- 276.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=227000, episode_reward=1499.40 +/- 346.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=228000, episode_reward=1543.00 +/- 284.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=229000, episode_reward=1419.00 +/- 114.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=230000, episode_reward=1579.20 +/- 276.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=231000, episode_reward=1393.60 +/- 354.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=232000, episode_reward=1757.40 +/- 144.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=233000, episode_reward=1534.60 +/- 156.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=234000, episode_reward=1373.00 +/- 274.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=235000, episode_reward=1760.80 +/- 338.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=236000, episode_reward=1614.40 +/- 173.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=237000, episode_reward=1664.00 +/- 160.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=238000, episode_reward=1692.60 +/- 135.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=239000, episode_reward=1707.40 +/- 208.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=240000, episode_reward=1468.80 +/- 136.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=241000, episode_reward=1635.20 +/- 279.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=242000, episode_reward=1489.80 +/- 148.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=243000, episode_reward=1615.40 +/- 207.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=244000, episode_reward=1719.40 +/- 251.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=245000, episode_reward=1573.80 +/- 134.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=246000, episode_reward=1536.60 +/- 332.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=247000, episode_reward=1646.40 +/- 139.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=248000, episode_reward=1811.80 +/- 162.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=249000, episode_reward=1590.40 +/- 124.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=250000, episode_reward=1598.60 +/- 177.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=251000, episode_reward=1695.80 +/- 238.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=252000, episode_reward=1742.20 +/- 196.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=253000, episode_reward=1675.80 +/- 167.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=254000, episode_reward=1551.40 +/- 168.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=255000, episode_reward=1647.00 +/- 342.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=256000, episode_reward=1832.00 +/- 178.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=257000, episode_reward=1686.20 +/- 165.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=258000, episode_reward=1711.40 +/- 343.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=259000, episode_reward=1676.20 +/- 94.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=260000, episode_reward=1425.80 +/- 259.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=261000, episode_reward=1551.60 +/- 249.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=262000, episode_reward=1756.40 +/- 78.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=263000, episode_reward=1701.00 +/- 210.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=264000, episode_reward=1643.40 +/- 285.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=265000, episode_reward=1503.00 +/- 341.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=266000, episode_reward=1676.80 +/- 71.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=267000, episode_reward=1765.40 +/- 120.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=268000, episode_reward=1376.80 +/- 166.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=269000, episode_reward=1413.80 +/- 218.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=270000, episode_reward=1684.20 +/- 263.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=271000, episode_reward=1558.40 +/- 383.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=272000, episode_reward=1655.40 +/- 285.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=273000, episode_reward=1478.40 +/- 208.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=274000, episode_reward=1632.80 +/- 345.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=275000, episode_reward=1697.00 +/- 163.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=276000, episode_reward=1643.60 +/- 214.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=277000, episode_reward=1433.80 +/- 374.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=278000, episode_reward=1651.40 +/- 268.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=279000, episode_reward=1583.00 +/- 133.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=280000, episode_reward=1557.40 +/- 184.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=281000, episode_reward=1386.20 +/- 467.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=282000, episode_reward=1657.20 +/- 158.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=283000, episode_reward=1543.00 +/- 355.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=284000, episode_reward=1678.80 +/- 83.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=285000, episode_reward=1805.20 +/- 131.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=286000, episode_reward=1665.60 +/- 251.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=287000, episode_reward=1623.80 +/- 69.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=288000, episode_reward=1501.00 +/- 329.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=289000, episode_reward=1619.60 +/- 336.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=290000, episode_reward=1511.80 +/- 266.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=291000, episode_reward=1627.60 +/- 56.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=292000, episode_reward=1554.20 +/- 241.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=293000, episode_reward=1620.60 +/- 335.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=294000, episode_reward=1202.40 +/- 515.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=295000, episode_reward=1514.20 +/- 311.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=296000, episode_reward=1571.80 +/- 106.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=297000, episode_reward=1733.40 +/- 122.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=298000, episode_reward=1743.20 +/- 181.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=299000, episode_reward=1625.20 +/- 322.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=300000, episode_reward=1498.20 +/- 203.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=301000, episode_reward=1741.20 +/- 180.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=302000, episode_reward=1638.40 +/- 225.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=303000, episode_reward=1781.20 +/- 83.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=304000, episode_reward=1815.80 +/- 131.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=305000, episode_reward=1738.60 +/- 103.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=306000, episode_reward=1518.40 +/- 258.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=307000, episode_reward=1473.80 +/- 232.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=308000, episode_reward=1382.20 +/- 379.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=309000, episode_reward=1588.40 +/- 346.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=310000, episode_reward=1370.80 +/- 159.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=311000, episode_reward=1537.60 +/- 304.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=312000, episode_reward=1526.20 +/- 164.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=313000, episode_reward=1621.40 +/- 189.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=314000, episode_reward=1438.20 +/- 161.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=315000, episode_reward=1517.60 +/- 303.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=316000, episode_reward=1707.80 +/- 231.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=317000, episode_reward=1659.60 +/- 145.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=318000, episode_reward=1631.80 +/- 143.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=319000, episode_reward=1557.80 +/- 164.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=320000, episode_reward=1630.80 +/- 255.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=321000, episode_reward=1678.20 +/- 296.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=322000, episode_reward=1703.80 +/- 262.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=323000, episode_reward=1539.60 +/- 290.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=324000, episode_reward=1650.80 +/- 252.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=325000, episode_reward=1565.00 +/- 90.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=326000, episode_reward=1632.20 +/- 160.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=327000, episode_reward=1589.00 +/- 304.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=328000, episode_reward=1678.80 +/- 104.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=329000, episode_reward=1738.60 +/- 109.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=330000, episode_reward=1588.60 +/- 295.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=331000, episode_reward=1702.60 +/- 158.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=332000, episode_reward=1269.80 +/- 285.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=333000, episode_reward=1406.60 +/- 264.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=334000, episode_reward=1429.20 +/- 228.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=335000, episode_reward=1413.00 +/- 136.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=336000, episode_reward=1637.80 +/- 94.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=337000, episode_reward=1534.40 +/- 115.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=338000, episode_reward=1840.20 +/- 164.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=339000, episode_reward=1522.20 +/- 114.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=340000, episode_reward=1743.40 +/- 167.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=341000, episode_reward=1575.80 +/- 156.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=342000, episode_reward=1650.00 +/- 229.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=343000, episode_reward=1729.00 +/- 219.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=344000, episode_reward=1572.80 +/- 106.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=345000, episode_reward=1601.80 +/- 185.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=346000, episode_reward=1751.80 +/- 315.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=347000, episode_reward=1878.00 +/- 48.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=348000, episode_reward=1681.40 +/- 307.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=349000, episode_reward=1758.20 +/- 165.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=350000, episode_reward=1624.60 +/- 226.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=351000, episode_reward=1810.00 +/- 130.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=352000, episode_reward=1570.80 +/- 308.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=353000, episode_reward=1473.40 +/- 310.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=354000, episode_reward=1690.40 +/- 318.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=355000, episode_reward=1358.00 +/- 183.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=356000, episode_reward=1589.60 +/- 170.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=357000, episode_reward=1676.80 +/- 147.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=358000, episode_reward=1665.20 +/- 176.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=359000, episode_reward=1414.80 +/- 164.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=360000, episode_reward=1631.60 +/- 149.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=361000, episode_reward=1677.60 +/- 332.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=362000, episode_reward=1574.80 +/- 221.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=363000, episode_reward=1620.60 +/- 224.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=364000, episode_reward=1548.00 +/- 230.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=365000, episode_reward=1718.00 +/- 231.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=366000, episode_reward=1535.60 +/- 329.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=367000, episode_reward=1711.60 +/- 204.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=368000, episode_reward=1640.20 +/- 273.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=369000, episode_reward=1492.20 +/- 246.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=370000, episode_reward=1684.00 +/- 223.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=371000, episode_reward=1506.80 +/- 279.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=372000, episode_reward=1649.00 +/- 76.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=373000, episode_reward=1696.80 +/- 268.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=374000, episode_reward=1896.60 +/- 134.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=375000, episode_reward=1675.40 +/- 164.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=376000, episode_reward=1490.20 +/- 173.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=377000, episode_reward=1647.60 +/- 162.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=378000, episode_reward=1676.80 +/- 344.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=379000, episode_reward=1640.80 +/- 75.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=380000, episode_reward=1608.60 +/- 128.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=381000, episode_reward=1541.00 +/- 224.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=382000, episode_reward=1454.60 +/- 273.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=383000, episode_reward=1740.40 +/- 110.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=384000, episode_reward=1354.20 +/- 188.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=385000, episode_reward=1633.80 +/- 238.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=386000, episode_reward=1491.60 +/- 140.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=387000, episode_reward=1722.40 +/- 135.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=388000, episode_reward=1638.40 +/- 225.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=389000, episode_reward=1670.00 +/- 174.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=390000, episode_reward=1604.60 +/- 338.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=391000, episode_reward=1675.40 +/- 181.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=392000, episode_reward=1564.20 +/- 112.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=393000, episode_reward=1641.20 +/- 315.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=394000, episode_reward=1491.40 +/- 283.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=395000, episode_reward=1494.60 +/- 284.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=396000, episode_reward=1761.80 +/- 223.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=397000, episode_reward=1631.60 +/- 305.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=398000, episode_reward=1454.40 +/- 328.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=399000, episode_reward=1458.80 +/- 197.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=400000, episode_reward=1441.20 +/- 211.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=401000, episode_reward=1688.60 +/- 153.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=402000, episode_reward=1607.40 +/- 272.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=403000, episode_reward=1612.60 +/- 108.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=404000, episode_reward=1812.80 +/- 181.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=405000, episode_reward=1633.20 +/- 132.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=406000, episode_reward=1428.80 +/- 164.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=407000, episode_reward=1543.40 +/- 272.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=408000, episode_reward=1530.60 +/- 212.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=409000, episode_reward=1409.20 +/- 199.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=410000, episode_reward=1516.40 +/- 149.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=411000, episode_reward=1548.40 +/- 166.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=412000, episode_reward=1535.40 +/- 327.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=413000, episode_reward=1728.00 +/- 155.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=414000, episode_reward=1532.20 +/- 234.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=415000, episode_reward=1528.60 +/- 221.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=416000, episode_reward=1672.60 +/- 140.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=417000, episode_reward=1655.20 +/- 95.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=418000, episode_reward=1693.20 +/- 118.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=419000, episode_reward=1687.60 +/- 126.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=420000, episode_reward=1483.20 +/- 256.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=421000, episode_reward=1776.80 +/- 158.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=422000, episode_reward=1384.80 +/- 284.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=423000, episode_reward=1754.20 +/- 238.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=424000, episode_reward=1696.00 +/- 300.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=425000, episode_reward=1703.40 +/- 152.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=426000, episode_reward=1518.60 +/- 86.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=427000, episode_reward=1612.80 +/- 233.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=428000, episode_reward=1492.20 +/- 161.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=429000, episode_reward=1629.80 +/- 200.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=430000, episode_reward=1569.20 +/- 147.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=431000, episode_reward=1524.40 +/- 298.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=432000, episode_reward=1661.60 +/- 301.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=433000, episode_reward=1753.40 +/- 222.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=434000, episode_reward=1658.60 +/- 128.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=435000, episode_reward=1565.60 +/- 171.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=436000, episode_reward=1703.60 +/- 169.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=437000, episode_reward=1443.00 +/- 483.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=438000, episode_reward=1640.40 +/- 103.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=439000, episode_reward=1708.80 +/- 133.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=440000, episode_reward=1764.20 +/- 139.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=441000, episode_reward=1662.00 +/- 392.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=442000, episode_reward=1709.20 +/- 261.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=443000, episode_reward=1586.60 +/- 215.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=444000, episode_reward=1699.40 +/- 186.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=445000, episode_reward=1616.00 +/- 235.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=446000, episode_reward=1780.60 +/- 121.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=447000, episode_reward=1731.00 +/- 221.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=448000, episode_reward=1686.00 +/- 72.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=449000, episode_reward=1894.00 +/- 78.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=450000, episode_reward=1516.00 +/- 235.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=451000, episode_reward=1735.40 +/- 102.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=452000, episode_reward=1668.20 +/- 178.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=453000, episode_reward=1669.00 +/- 264.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=454000, episode_reward=1833.00 +/- 249.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=455000, episode_reward=1799.20 +/- 191.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=456000, episode_reward=1515.80 +/- 434.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=457000, episode_reward=1689.80 +/- 119.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=458000, episode_reward=1607.00 +/- 217.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=459000, episode_reward=1579.60 +/- 298.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=460000, episode_reward=1744.40 +/- 97.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=461000, episode_reward=1682.60 +/- 146.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=462000, episode_reward=1563.60 +/- 156.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=463000, episode_reward=1611.40 +/- 227.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=464000, episode_reward=1378.80 +/- 241.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=465000, episode_reward=1538.00 +/- 340.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=466000, episode_reward=1563.00 +/- 264.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=467000, episode_reward=1568.00 +/- 295.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=468000, episode_reward=1540.00 +/- 226.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=469000, episode_reward=1637.20 +/- 111.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=470000, episode_reward=1893.20 +/- 261.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=471000, episode_reward=1652.80 +/- 244.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=472000, episode_reward=1547.80 +/- 340.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=473000, episode_reward=1880.00 +/- 134.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=474000, episode_reward=1585.20 +/- 212.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=475000, episode_reward=1568.60 +/- 161.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=476000, episode_reward=1722.00 +/- 217.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=477000, episode_reward=1579.60 +/- 210.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=478000, episode_reward=1594.60 +/- 347.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=479000, episode_reward=1904.60 +/- 109.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=480000, episode_reward=1656.60 +/- 253.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=481000, episode_reward=1848.80 +/- 106.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=482000, episode_reward=1509.20 +/- 189.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=483000, episode_reward=1711.80 +/- 309.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=484000, episode_reward=1607.20 +/- 216.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=485000, episode_reward=1686.40 +/- 174.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=486000, episode_reward=1834.40 +/- 164.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=487000, episode_reward=1635.40 +/- 286.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=488000, episode_reward=1614.00 +/- 242.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=489000, episode_reward=1770.60 +/- 225.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=490000, episode_reward=1610.00 +/- 215.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=491000, episode_reward=1622.00 +/- 276.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=492000, episode_reward=1629.80 +/- 173.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=493000, episode_reward=1623.20 +/- 106.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=494000, episode_reward=1627.00 +/- 172.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=495000, episode_reward=1426.00 +/- 384.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=496000, episode_reward=1817.80 +/- 112.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=497000, episode_reward=1656.20 +/- 174.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=498000, episode_reward=1682.60 +/- 84.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=499000, episode_reward=1812.80 +/- 198.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=500000, episode_reward=1714.40 +/- 101.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=501000, episode_reward=1398.20 +/- 259.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=502000, episode_reward=1637.80 +/- 256.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=503000, episode_reward=1518.40 +/- 241.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=504000, episode_reward=1692.80 +/- 216.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=505000, episode_reward=1565.40 +/- 376.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=506000, episode_reward=1597.60 +/- 127.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=507000, episode_reward=1630.60 +/- 162.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=508000, episode_reward=1637.40 +/- 278.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=509000, episode_reward=1579.00 +/- 279.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=510000, episode_reward=1689.20 +/- 205.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=511000, episode_reward=1575.00 +/- 273.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=512000, episode_reward=1631.40 +/- 213.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=513000, episode_reward=1636.80 +/- 257.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=514000, episode_reward=1694.00 +/- 166.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=515000, episode_reward=1558.80 +/- 212.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=516000, episode_reward=1590.60 +/- 177.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=517000, episode_reward=1702.40 +/- 290.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=518000, episode_reward=1558.40 +/- 94.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=519000, episode_reward=1699.20 +/- 144.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=520000, episode_reward=1718.40 +/- 265.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=521000, episode_reward=1698.20 +/- 234.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=522000, episode_reward=1619.00 +/- 245.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=523000, episode_reward=1666.60 +/- 195.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=524000, episode_reward=1400.80 +/- 161.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=525000, episode_reward=1714.20 +/- 202.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=526000, episode_reward=1768.80 +/- 80.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=527000, episode_reward=1661.20 +/- 249.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=528000, episode_reward=1884.60 +/- 123.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=529000, episode_reward=1482.80 +/- 237.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=530000, episode_reward=1604.00 +/- 198.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=531000, episode_reward=1590.80 +/- 231.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=532000, episode_reward=1607.20 +/- 166.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=533000, episode_reward=1773.80 +/- 71.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=534000, episode_reward=1642.20 +/- 199.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=535000, episode_reward=1526.40 +/- 109.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=536000, episode_reward=1807.00 +/- 127.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=537000, episode_reward=1784.40 +/- 190.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=538000, episode_reward=1755.60 +/- 206.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=539000, episode_reward=1472.00 +/- 226.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=540000, episode_reward=1503.80 +/- 190.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=541000, episode_reward=1695.60 +/- 138.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=542000, episode_reward=1743.20 +/- 244.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=543000, episode_reward=1769.20 +/- 189.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=544000, episode_reward=1602.40 +/- 173.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=545000, episode_reward=1744.00 +/- 231.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=546000, episode_reward=1576.20 +/- 235.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=547000, episode_reward=1647.20 +/- 175.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=548000, episode_reward=1722.80 +/- 225.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=549000, episode_reward=1824.20 +/- 233.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=550000, episode_reward=1545.00 +/- 197.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=551000, episode_reward=1736.80 +/- 210.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=552000, episode_reward=1757.40 +/- 229.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=553000, episode_reward=1753.00 +/- 185.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=554000, episode_reward=1791.20 +/- 188.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=555000, episode_reward=1839.60 +/- 197.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=556000, episode_reward=1560.80 +/- 172.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=557000, episode_reward=1656.00 +/- 335.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=558000, episode_reward=1849.00 +/- 204.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=559000, episode_reward=1790.40 +/- 109.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=560000, episode_reward=1354.20 +/- 263.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=561000, episode_reward=1683.60 +/- 173.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=562000, episode_reward=1640.00 +/- 342.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=563000, episode_reward=1717.60 +/- 244.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=564000, episode_reward=1659.00 +/- 233.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=565000, episode_reward=1660.00 +/- 170.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=566000, episode_reward=1529.40 +/- 164.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=567000, episode_reward=1382.20 +/- 242.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=568000, episode_reward=1661.20 +/- 164.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=569000, episode_reward=1812.80 +/- 202.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=570000, episode_reward=1691.60 +/- 190.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=571000, episode_reward=1781.00 +/- 137.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=572000, episode_reward=1820.20 +/- 284.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=573000, episode_reward=1642.60 +/- 326.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=574000, episode_reward=1513.60 +/- 267.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=575000, episode_reward=1648.20 +/- 159.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=576000, episode_reward=1666.00 +/- 72.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=577000, episode_reward=1700.80 +/- 49.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=578000, episode_reward=1671.40 +/- 102.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=579000, episode_reward=1413.40 +/- 150.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=580000, episode_reward=1439.40 +/- 239.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=581000, episode_reward=1574.00 +/- 330.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=582000, episode_reward=1763.20 +/- 201.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=583000, episode_reward=1502.40 +/- 305.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=584000, episode_reward=1663.00 +/- 219.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=585000, episode_reward=1609.80 +/- 364.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=586000, episode_reward=1434.20 +/- 164.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=587000, episode_reward=1590.20 +/- 136.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=588000, episode_reward=1764.20 +/- 370.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=589000, episode_reward=1640.00 +/- 294.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=590000, episode_reward=1789.20 +/- 309.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=591000, episode_reward=1599.60 +/- 219.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=592000, episode_reward=1721.20 +/- 133.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=593000, episode_reward=1467.00 +/- 217.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=594000, episode_reward=1755.00 +/- 121.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=595000, episode_reward=1529.00 +/- 160.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=596000, episode_reward=1789.40 +/- 131.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=597000, episode_reward=1962.00 +/- 136.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=598000, episode_reward=1774.00 +/- 294.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=599000, episode_reward=1634.20 +/- 90.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=600000, episode_reward=1490.40 +/- 381.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=601000, episode_reward=1764.20 +/- 275.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=602000, episode_reward=1705.20 +/- 323.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=603000, episode_reward=1700.80 +/- 218.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=604000, episode_reward=1539.40 +/- 378.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=605000, episode_reward=1418.00 +/- 191.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=606000, episode_reward=1619.20 +/- 265.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=607000, episode_reward=1670.40 +/- 281.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=608000, episode_reward=1612.00 +/- 107.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=609000, episode_reward=1881.20 +/- 149.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=610000, episode_reward=1644.20 +/- 275.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=611000, episode_reward=1770.40 +/- 205.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=612000, episode_reward=1638.00 +/- 277.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=613000, episode_reward=1764.20 +/- 100.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=614000, episode_reward=1639.60 +/- 318.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=615000, episode_reward=1815.60 +/- 186.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=616000, episode_reward=1608.60 +/- 273.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=617000, episode_reward=1672.40 +/- 234.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=618000, episode_reward=1740.40 +/- 206.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=619000, episode_reward=1670.00 +/- 207.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=620000, episode_reward=1775.60 +/- 163.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=621000, episode_reward=1793.20 +/- 282.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=622000, episode_reward=1708.60 +/- 222.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=623000, episode_reward=1597.80 +/- 213.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=624000, episode_reward=1509.60 +/- 226.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=625000, episode_reward=1695.20 +/- 159.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=626000, episode_reward=1607.20 +/- 220.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=627000, episode_reward=1464.20 +/- 249.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=628000, episode_reward=1836.80 +/- 212.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=629000, episode_reward=1619.20 +/- 280.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=630000, episode_reward=1448.40 +/- 291.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=631000, episode_reward=1647.60 +/- 160.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=632000, episode_reward=1508.40 +/- 370.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=633000, episode_reward=1592.40 +/- 153.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=634000, episode_reward=1648.20 +/- 151.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=635000, episode_reward=1847.20 +/- 182.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=636000, episode_reward=1459.40 +/- 411.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=637000, episode_reward=1846.00 +/- 180.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=638000, episode_reward=1583.60 +/- 234.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=639000, episode_reward=1751.80 +/- 270.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=640000, episode_reward=1727.80 +/- 206.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=641000, episode_reward=1606.00 +/- 309.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=642000, episode_reward=1748.40 +/- 88.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=643000, episode_reward=1718.00 +/- 234.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=644000, episode_reward=1860.40 +/- 253.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=645000, episode_reward=1639.60 +/- 79.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=646000, episode_reward=1714.20 +/- 148.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=647000, episode_reward=1684.20 +/- 68.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=648000, episode_reward=1593.00 +/- 204.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=649000, episode_reward=1530.20 +/- 256.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=650000, episode_reward=1537.80 +/- 157.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=651000, episode_reward=1688.80 +/- 237.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=652000, episode_reward=1617.00 +/- 166.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=653000, episode_reward=1581.20 +/- 292.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=654000, episode_reward=1520.80 +/- 287.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=655000, episode_reward=1599.00 +/- 167.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=656000, episode_reward=1608.20 +/- 366.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=657000, episode_reward=1748.20 +/- 218.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=658000, episode_reward=1711.80 +/- 222.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=659000, episode_reward=1840.00 +/- 243.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=660000, episode_reward=1682.60 +/- 138.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=661000, episode_reward=1560.00 +/- 255.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=662000, episode_reward=1627.00 +/- 240.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=663000, episode_reward=1486.80 +/- 198.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=664000, episode_reward=1586.00 +/- 237.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=665000, episode_reward=1448.60 +/- 384.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=666000, episode_reward=1635.40 +/- 144.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=667000, episode_reward=1764.00 +/- 126.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=668000, episode_reward=1715.40 +/- 286.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=669000, episode_reward=1742.80 +/- 270.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=670000, episode_reward=1815.60 +/- 175.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=671000, episode_reward=1502.80 +/- 255.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=672000, episode_reward=1627.00 +/- 156.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=673000, episode_reward=1724.00 +/- 194.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=674000, episode_reward=1830.40 +/- 111.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=675000, episode_reward=1644.40 +/- 258.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=676000, episode_reward=1812.80 +/- 209.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=677000, episode_reward=1695.80 +/- 182.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=678000, episode_reward=1694.40 +/- 135.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=679000, episode_reward=1806.40 +/- 140.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=680000, episode_reward=1508.20 +/- 258.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=681000, episode_reward=1735.40 +/- 275.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=682000, episode_reward=1389.20 +/- 359.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=683000, episode_reward=1633.80 +/- 253.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=684000, episode_reward=1804.00 +/- 248.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=685000, episode_reward=1668.60 +/- 342.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=686000, episode_reward=1793.40 +/- 87.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=687000, episode_reward=1645.40 +/- 214.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=688000, episode_reward=1428.80 +/- 295.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=689000, episode_reward=1509.40 +/- 218.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=690000, episode_reward=1775.40 +/- 109.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=691000, episode_reward=1757.00 +/- 190.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=692000, episode_reward=1348.40 +/- 213.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=693000, episode_reward=1768.60 +/- 314.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=694000, episode_reward=1624.00 +/- 321.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=695000, episode_reward=1746.00 +/- 300.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=696000, episode_reward=1781.60 +/- 331.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=697000, episode_reward=1643.00 +/- 230.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=698000, episode_reward=1579.20 +/- 355.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=699000, episode_reward=1786.20 +/- 233.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=700000, episode_reward=1624.40 +/- 171.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=701000, episode_reward=1699.20 +/- 249.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=702000, episode_reward=1615.60 +/- 213.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=703000, episode_reward=1619.40 +/- 160.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=704000, episode_reward=1637.40 +/- 271.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=705000, episode_reward=1657.40 +/- 210.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=706000, episode_reward=1651.60 +/- 89.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=707000, episode_reward=1772.00 +/- 62.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=708000, episode_reward=1729.60 +/- 152.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=709000, episode_reward=1715.80 +/- 280.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=710000, episode_reward=1593.20 +/- 299.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=711000, episode_reward=1664.00 +/- 197.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=712000, episode_reward=1770.20 +/- 143.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=713000, episode_reward=1647.40 +/- 233.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=714000, episode_reward=1682.40 +/- 160.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=715000, episode_reward=1604.60 +/- 136.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=716000, episode_reward=1699.80 +/- 177.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=717000, episode_reward=1736.60 +/- 148.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=718000, episode_reward=1730.20 +/- 330.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=719000, episode_reward=1738.00 +/- 221.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=720000, episode_reward=1522.80 +/- 361.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=721000, episode_reward=1676.40 +/- 286.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=722000, episode_reward=1776.20 +/- 226.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=723000, episode_reward=1621.60 +/- 166.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=724000, episode_reward=1650.20 +/- 191.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=725000, episode_reward=1754.40 +/- 378.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=726000, episode_reward=1686.80 +/- 213.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=727000, episode_reward=1642.40 +/- 144.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=728000, episode_reward=1708.20 +/- 34.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=729000, episode_reward=1748.80 +/- 189.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=730000, episode_reward=1783.80 +/- 319.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=731000, episode_reward=1804.20 +/- 142.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=732000, episode_reward=1586.60 +/- 175.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=733000, episode_reward=1650.20 +/- 183.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=734000, episode_reward=1639.80 +/- 203.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=735000, episode_reward=1724.40 +/- 156.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=736000, episode_reward=1625.60 +/- 330.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=737000, episode_reward=1740.80 +/- 214.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=738000, episode_reward=1710.60 +/- 233.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=739000, episode_reward=1644.80 +/- 181.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=740000, episode_reward=1648.20 +/- 142.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=741000, episode_reward=1670.20 +/- 233.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=742000, episode_reward=1749.60 +/- 124.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=743000, episode_reward=1677.60 +/- 248.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=744000, episode_reward=1770.00 +/- 258.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=745000, episode_reward=1632.00 +/- 188.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=746000, episode_reward=1799.00 +/- 219.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=747000, episode_reward=1732.40 +/- 134.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=748000, episode_reward=1779.60 +/- 343.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=749000, episode_reward=1686.00 +/- 280.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=750000, episode_reward=1435.40 +/- 324.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=751000, episode_reward=1418.80 +/- 246.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=752000, episode_reward=1569.80 +/- 237.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=753000, episode_reward=1765.00 +/- 146.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=754000, episode_reward=1510.40 +/- 125.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=755000, episode_reward=1643.20 +/- 217.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=756000, episode_reward=1591.40 +/- 325.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=757000, episode_reward=1607.00 +/- 267.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=758000, episode_reward=1323.40 +/- 230.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=759000, episode_reward=1651.60 +/- 189.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=760000, episode_reward=1532.60 +/- 114.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=761000, episode_reward=1641.20 +/- 254.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=762000, episode_reward=1594.00 +/- 307.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=763000, episode_reward=1597.00 +/- 253.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=764000, episode_reward=1540.20 +/- 272.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=765000, episode_reward=1616.40 +/- 110.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=766000, episode_reward=1960.60 +/- 224.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=767000, episode_reward=1488.80 +/- 250.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=768000, episode_reward=1482.60 +/- 147.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=769000, episode_reward=1709.20 +/- 335.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=770000, episode_reward=1657.60 +/- 296.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=771000, episode_reward=1868.80 +/- 116.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=772000, episode_reward=1462.80 +/- 264.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=773000, episode_reward=1627.60 +/- 175.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=774000, episode_reward=1382.60 +/- 397.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=775000, episode_reward=1816.80 +/- 166.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=776000, episode_reward=1715.20 +/- 198.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=777000, episode_reward=1717.20 +/- 288.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=778000, episode_reward=1810.20 +/- 277.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=779000, episode_reward=1534.20 +/- 240.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=780000, episode_reward=1667.00 +/- 319.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=781000, episode_reward=1549.60 +/- 314.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=782000, episode_reward=1426.00 +/- 167.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=783000, episode_reward=1387.80 +/- 301.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=784000, episode_reward=1624.00 +/- 278.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=785000, episode_reward=1619.80 +/- 202.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=786000, episode_reward=1636.80 +/- 293.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=787000, episode_reward=1528.60 +/- 48.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=788000, episode_reward=1630.00 +/- 141.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=789000, episode_reward=1653.60 +/- 99.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=790000, episode_reward=1504.40 +/- 192.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=791000, episode_reward=1592.20 +/- 174.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=792000, episode_reward=1736.00 +/- 190.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=793000, episode_reward=1752.20 +/- 77.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=794000, episode_reward=1746.20 +/- 101.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=795000, episode_reward=1671.60 +/- 213.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=796000, episode_reward=1740.40 +/- 229.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=797000, episode_reward=1601.40 +/- 228.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=798000, episode_reward=1526.00 +/- 164.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=799000, episode_reward=1552.40 +/- 266.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=800000, episode_reward=1570.80 +/- 273.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=801000, episode_reward=1781.20 +/- 226.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=802000, episode_reward=1527.00 +/- 167.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=803000, episode_reward=1695.20 +/- 101.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=804000, episode_reward=1733.40 +/- 329.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=805000, episode_reward=1876.80 +/- 167.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=806000, episode_reward=1594.60 +/- 188.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=807000, episode_reward=1778.80 +/- 394.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=808000, episode_reward=1539.80 +/- 344.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=809000, episode_reward=1854.00 +/- 101.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=810000, episode_reward=1701.60 +/- 320.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=811000, episode_reward=1712.60 +/- 95.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=812000, episode_reward=1687.60 +/- 118.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=813000, episode_reward=1762.40 +/- 122.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=814000, episode_reward=1760.40 +/- 142.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=815000, episode_reward=1906.80 +/- 249.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=816000, episode_reward=1604.60 +/- 164.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=817000, episode_reward=1766.20 +/- 358.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=818000, episode_reward=1710.00 +/- 151.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=819000, episode_reward=1603.80 +/- 191.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=820000, episode_reward=1453.40 +/- 308.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=821000, episode_reward=1590.00 +/- 234.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=822000, episode_reward=1726.60 +/- 285.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=823000, episode_reward=1658.00 +/- 174.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=824000, episode_reward=1555.20 +/- 215.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=825000, episode_reward=1873.60 +/- 92.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=826000, episode_reward=1643.00 +/- 373.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=827000, episode_reward=1770.00 +/- 185.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=828000, episode_reward=1705.00 +/- 440.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=829000, episode_reward=1591.00 +/- 282.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=830000, episode_reward=1795.80 +/- 301.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=831000, episode_reward=1506.20 +/- 407.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=832000, episode_reward=1650.40 +/- 472.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=833000, episode_reward=1511.40 +/- 210.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=834000, episode_reward=1486.00 +/- 312.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=835000, episode_reward=1704.60 +/- 206.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=836000, episode_reward=1773.40 +/- 109.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=837000, episode_reward=1614.80 +/- 146.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=838000, episode_reward=1771.60 +/- 239.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=839000, episode_reward=1557.40 +/- 392.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=840000, episode_reward=1471.60 +/- 324.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=841000, episode_reward=1708.40 +/- 186.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=842000, episode_reward=1784.20 +/- 115.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=843000, episode_reward=1706.20 +/- 129.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=844000, episode_reward=1602.80 +/- 250.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=845000, episode_reward=1640.40 +/- 183.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=846000, episode_reward=1567.20 +/- 157.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=847000, episode_reward=1829.60 +/- 208.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=848000, episode_reward=1803.80 +/- 253.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=849000, episode_reward=1730.00 +/- 104.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=850000, episode_reward=1535.20 +/- 55.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=851000, episode_reward=1822.60 +/- 318.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=852000, episode_reward=1690.20 +/- 350.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=853000, episode_reward=1653.20 +/- 236.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=854000, episode_reward=1624.60 +/- 159.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=855000, episode_reward=1648.20 +/- 164.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=856000, episode_reward=1752.20 +/- 153.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=857000, episode_reward=1676.80 +/- 184.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=858000, episode_reward=1595.80 +/- 113.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=859000, episode_reward=1672.20 +/- 175.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=860000, episode_reward=1578.40 +/- 125.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=861000, episode_reward=1644.40 +/- 148.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=862000, episode_reward=1399.60 +/- 249.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=863000, episode_reward=1645.20 +/- 301.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=864000, episode_reward=1785.80 +/- 201.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=865000, episode_reward=1769.80 +/- 101.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=866000, episode_reward=1815.60 +/- 196.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=867000, episode_reward=1807.20 +/- 154.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=868000, episode_reward=1791.80 +/- 179.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=869000, episode_reward=1679.00 +/- 190.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=870000, episode_reward=1613.40 +/- 180.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=871000, episode_reward=1755.80 +/- 192.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=872000, episode_reward=1587.20 +/- 210.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=873000, episode_reward=1775.80 +/- 183.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=874000, episode_reward=1597.80 +/- 140.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=875000, episode_reward=1765.80 +/- 233.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=876000, episode_reward=1732.40 +/- 159.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=877000, episode_reward=1621.80 +/- 102.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=878000, episode_reward=1702.80 +/- 142.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=879000, episode_reward=1641.00 +/- 36.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=880000, episode_reward=1635.00 +/- 141.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=881000, episode_reward=1745.20 +/- 188.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=882000, episode_reward=1821.20 +/- 176.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=883000, episode_reward=1743.60 +/- 157.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=884000, episode_reward=1556.20 +/- 220.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=885000, episode_reward=1634.60 +/- 233.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=886000, episode_reward=1573.40 +/- 232.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=887000, episode_reward=1707.60 +/- 30.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=888000, episode_reward=1670.60 +/- 115.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=889000, episode_reward=1733.20 +/- 194.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=890000, episode_reward=1766.80 +/- 210.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=891000, episode_reward=1603.20 +/- 343.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=892000, episode_reward=1697.20 +/- 221.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=893000, episode_reward=1574.20 +/- 158.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=894000, episode_reward=1717.80 +/- 167.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=895000, episode_reward=1600.60 +/- 158.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=896000, episode_reward=1611.40 +/- 176.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=897000, episode_reward=1673.00 +/- 216.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=898000, episode_reward=1774.20 +/- 347.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=899000, episode_reward=1607.40 +/- 150.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=900000, episode_reward=1699.20 +/- 208.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=901000, episode_reward=1702.20 +/- 137.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=902000, episode_reward=1694.00 +/- 198.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=903000, episode_reward=1737.60 +/- 181.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=904000, episode_reward=1899.80 +/- 277.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=905000, episode_reward=1502.40 +/- 104.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=906000, episode_reward=1659.20 +/- 286.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=907000, episode_reward=1678.20 +/- 210.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=908000, episode_reward=1493.80 +/- 143.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=909000, episode_reward=1605.40 +/- 190.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=910000, episode_reward=1285.80 +/- 304.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=911000, episode_reward=1646.20 +/- 199.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=912000, episode_reward=1562.40 +/- 173.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=913000, episode_reward=1608.00 +/- 189.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=914000, episode_reward=1643.80 +/- 151.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=915000, episode_reward=1408.60 +/- 99.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=916000, episode_reward=1619.20 +/- 161.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=917000, episode_reward=1618.80 +/- 190.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=918000, episode_reward=1780.80 +/- 165.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=919000, episode_reward=1733.80 +/- 161.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=920000, episode_reward=1607.60 +/- 175.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=921000, episode_reward=1622.00 +/- 327.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=922000, episode_reward=1521.20 +/- 252.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=923000, episode_reward=1468.00 +/- 225.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=924000, episode_reward=1704.40 +/- 219.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=925000, episode_reward=1724.00 +/- 123.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=926000, episode_reward=1695.20 +/- 392.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=927000, episode_reward=1645.40 +/- 245.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=928000, episode_reward=1758.40 +/- 152.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=929000, episode_reward=1416.80 +/- 335.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=930000, episode_reward=1668.00 +/- 346.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=931000, episode_reward=1662.00 +/- 332.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=932000, episode_reward=1940.60 +/- 31.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=933000, episode_reward=1630.00 +/- 154.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=934000, episode_reward=1655.40 +/- 327.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=935000, episode_reward=1699.40 +/- 124.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=936000, episode_reward=2046.20 +/- 48.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=937000, episode_reward=1631.60 +/- 354.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=938000, episode_reward=1581.80 +/- 262.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=939000, episode_reward=1631.40 +/- 234.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=940000, episode_reward=1850.40 +/- 109.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=941000, episode_reward=1695.20 +/- 163.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=942000, episode_reward=1627.80 +/- 209.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=943000, episode_reward=1445.00 +/- 262.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=944000, episode_reward=1780.40 +/- 163.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=945000, episode_reward=1721.40 +/- 179.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=946000, episode_reward=1452.80 +/- 470.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=947000, episode_reward=1692.00 +/- 272.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=948000, episode_reward=1797.60 +/- 150.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=949000, episode_reward=1653.60 +/- 86.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=950000, episode_reward=1453.40 +/- 382.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=951000, episode_reward=1745.00 +/- 267.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=952000, episode_reward=1563.20 +/- 249.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=953000, episode_reward=1633.20 +/- 210.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=954000, episode_reward=1737.60 +/- 171.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=955000, episode_reward=1711.60 +/- 220.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=956000, episode_reward=1657.00 +/- 124.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=957000, episode_reward=1596.40 +/- 73.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=958000, episode_reward=1569.80 +/- 94.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=959000, episode_reward=1532.00 +/- 68.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=960000, episode_reward=1681.80 +/- 200.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=961000, episode_reward=1646.00 +/- 161.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=962000, episode_reward=1700.20 +/- 135.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=963000, episode_reward=1621.20 +/- 168.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=964000, episode_reward=1658.00 +/- 283.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=965000, episode_reward=1490.80 +/- 302.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=966000, episode_reward=1646.20 +/- 226.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=967000, episode_reward=1619.60 +/- 234.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=968000, episode_reward=1573.20 +/- 180.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=969000, episode_reward=1860.20 +/- 218.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=970000, episode_reward=1639.80 +/- 446.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=971000, episode_reward=1418.60 +/- 303.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=972000, episode_reward=1316.80 +/- 199.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=973000, episode_reward=1647.20 +/- 203.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=974000, episode_reward=1423.20 +/- 328.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=975000, episode_reward=1723.60 +/- 231.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=976000, episode_reward=1595.60 +/- 165.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=977000, episode_reward=1672.80 +/- 282.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=978000, episode_reward=1768.60 +/- 160.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=979000, episode_reward=1709.20 +/- 229.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=980000, episode_reward=1577.60 +/- 235.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=981000, episode_reward=1656.00 +/- 209.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=982000, episode_reward=1680.40 +/- 152.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=983000, episode_reward=1737.00 +/- 211.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=984000, episode_reward=1811.60 +/- 52.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=985000, episode_reward=1662.60 +/- 221.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=986000, episode_reward=1591.60 +/- 314.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=987000, episode_reward=1731.00 +/- 92.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=988000, episode_reward=1765.80 +/- 171.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=989000, episode_reward=1665.80 +/- 186.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=990000, episode_reward=1693.00 +/- 196.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=991000, episode_reward=1688.80 +/- 209.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=992000, episode_reward=1689.40 +/- 227.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=993000, episode_reward=1403.40 +/- 248.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=994000, episode_reward=1539.40 +/- 186.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=995000, episode_reward=1546.00 +/- 87.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=996000, episode_reward=1717.80 +/- 203.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=997000, episode_reward=1490.00 +/- 414.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=998000, episode_reward=1767.40 +/- 119.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=999000, episode_reward=1801.40 +/- 249.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1000000, episode_reward=1645.80 +/- 303.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1001000, episode_reward=1510.00 +/- 179.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Gamma = 0.9, Learning Rate = 0.003, Activation Function = ReLU, Neurons = 8, Process noise = 0.1, Measurement noise = 0.0\n",
      "Eval num_timesteps=1000, episode_reward=29.00 +/- 168.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=-10.20 +/- 134.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=3000, episode_reward=312.20 +/- 178.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4000, episode_reward=299.00 +/- 232.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=5000, episode_reward=700.60 +/- 83.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=6000, episode_reward=403.00 +/- 126.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=7000, episode_reward=654.60 +/- 287.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=8000, episode_reward=718.60 +/- 224.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=9000, episode_reward=920.40 +/- 282.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=10000, episode_reward=1062.20 +/- 270.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=11000, episode_reward=195.00 +/- 177.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=12000, episode_reward=430.80 +/- 104.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=13000, episode_reward=435.00 +/- 159.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=14000, episode_reward=486.20 +/- 235.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=15000, episode_reward=705.20 +/- 256.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=16000, episode_reward=410.40 +/- 329.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=17000, episode_reward=522.60 +/- 149.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=18000, episode_reward=466.20 +/- 276.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=19000, episode_reward=1055.20 +/- 273.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=20000, episode_reward=929.40 +/- 177.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=21000, episode_reward=1070.60 +/- 336.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=22000, episode_reward=1126.20 +/- 280.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=23000, episode_reward=1096.80 +/- 168.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=24000, episode_reward=884.60 +/- 129.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=25000, episode_reward=1273.20 +/- 252.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=26000, episode_reward=1158.00 +/- 274.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=27000, episode_reward=948.00 +/- 317.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=28000, episode_reward=1231.20 +/- 169.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=29000, episode_reward=1025.00 +/- 193.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=30000, episode_reward=1290.60 +/- 265.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=31000, episode_reward=1273.00 +/- 221.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=32000, episode_reward=1168.60 +/- 277.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=33000, episode_reward=1221.80 +/- 238.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=34000, episode_reward=1047.00 +/- 304.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=35000, episode_reward=1185.40 +/- 182.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=36000, episode_reward=1187.20 +/- 274.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=37000, episode_reward=1176.20 +/- 102.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=38000, episode_reward=1213.00 +/- 138.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=39000, episode_reward=1257.60 +/- 262.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=40000, episode_reward=1410.60 +/- 211.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=41000, episode_reward=1265.20 +/- 373.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=42000, episode_reward=1386.00 +/- 220.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=43000, episode_reward=1404.40 +/- 406.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=44000, episode_reward=1443.40 +/- 135.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=45000, episode_reward=1349.40 +/- 250.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=46000, episode_reward=1538.20 +/- 82.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=47000, episode_reward=-259.60 +/- 2958.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=48000, episode_reward=1627.20 +/- 312.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=49000, episode_reward=1432.20 +/- 119.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=50000, episode_reward=1614.00 +/- 180.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=51000, episode_reward=1569.60 +/- 176.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=52000, episode_reward=1487.00 +/- 227.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=53000, episode_reward=1416.40 +/- 231.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=54000, episode_reward=1739.60 +/- 257.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=55000, episode_reward=1536.80 +/- 253.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=56000, episode_reward=1372.40 +/- 206.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=57000, episode_reward=1596.40 +/- 248.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=58000, episode_reward=1288.20 +/- 465.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=59000, episode_reward=1578.60 +/- 257.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=60000, episode_reward=1481.20 +/- 132.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=61000, episode_reward=1490.80 +/- 375.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=62000, episode_reward=1577.80 +/- 94.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=63000, episode_reward=1484.00 +/- 311.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=64000, episode_reward=1571.40 +/- 242.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=65000, episode_reward=1615.00 +/- 293.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=66000, episode_reward=1663.40 +/- 120.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=67000, episode_reward=1702.60 +/- 120.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=68000, episode_reward=1408.80 +/- 319.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=69000, episode_reward=1685.20 +/- 156.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=70000, episode_reward=-1602.40 +/- 6940.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=71000, episode_reward=1689.40 +/- 196.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=72000, episode_reward=1362.80 +/- 215.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=73000, episode_reward=1742.80 +/- 165.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=74000, episode_reward=1485.80 +/- 351.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=75000, episode_reward=1622.00 +/- 215.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=76000, episode_reward=1623.80 +/- 84.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=77000, episode_reward=1635.00 +/- 288.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=78000, episode_reward=1570.40 +/- 200.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=79000, episode_reward=1569.60 +/- 256.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=80000, episode_reward=1539.40 +/- 298.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=81000, episode_reward=1515.40 +/- 164.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=82000, episode_reward=1589.60 +/- 173.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=83000, episode_reward=1597.00 +/- 360.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=84000, episode_reward=1577.80 +/- 145.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=85000, episode_reward=313.40 +/- 2454.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=86000, episode_reward=1517.40 +/- 156.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=87000, episode_reward=1480.40 +/- 270.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=88000, episode_reward=1465.40 +/- 245.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=89000, episode_reward=1562.40 +/- 190.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=90000, episode_reward=1445.60 +/- 127.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=91000, episode_reward=1639.80 +/- 212.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=92000, episode_reward=1624.60 +/- 174.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=93000, episode_reward=1514.00 +/- 179.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=94000, episode_reward=1621.00 +/- 124.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=95000, episode_reward=1491.80 +/- 337.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=96000, episode_reward=1695.60 +/- 121.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=97000, episode_reward=1691.80 +/- 290.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=98000, episode_reward=1625.60 +/- 244.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=99000, episode_reward=1454.80 +/- 299.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=100000, episode_reward=1676.60 +/- 244.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=101000, episode_reward=1783.20 +/- 229.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=102000, episode_reward=1590.20 +/- 280.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=103000, episode_reward=1794.80 +/- 146.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=104000, episode_reward=1457.60 +/- 233.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=105000, episode_reward=1395.80 +/- 329.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=106000, episode_reward=1699.80 +/- 361.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=107000, episode_reward=1554.20 +/- 224.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=108000, episode_reward=1752.00 +/- 291.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=109000, episode_reward=581.80 +/- 2165.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=110000, episode_reward=1551.80 +/- 339.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=111000, episode_reward=1490.20 +/- 297.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=112000, episode_reward=1319.00 +/- 460.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=113000, episode_reward=1667.40 +/- 268.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=114000, episode_reward=1750.00 +/- 187.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=115000, episode_reward=1637.80 +/- 215.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=116000, episode_reward=1820.60 +/- 194.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=117000, episode_reward=1611.80 +/- 219.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=118000, episode_reward=1527.00 +/- 291.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=119000, episode_reward=1593.20 +/- 264.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=120000, episode_reward=1510.60 +/- 107.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=121000, episode_reward=1537.20 +/- 261.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=122000, episode_reward=1689.40 +/- 166.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=123000, episode_reward=1681.20 +/- 178.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=124000, episode_reward=1747.80 +/- 169.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=125000, episode_reward=1819.40 +/- 194.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=126000, episode_reward=1724.80 +/- 296.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=127000, episode_reward=1635.00 +/- 117.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=128000, episode_reward=1675.20 +/- 180.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=129000, episode_reward=1642.60 +/- 272.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=130000, episode_reward=1838.20 +/- 73.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=131000, episode_reward=1363.80 +/- 254.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=132000, episode_reward=1502.00 +/- 325.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=133000, episode_reward=1865.40 +/- 159.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=134000, episode_reward=1358.40 +/- 197.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=135000, episode_reward=1677.80 +/- 135.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=136000, episode_reward=1613.20 +/- 178.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=137000, episode_reward=1661.00 +/- 318.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=138000, episode_reward=1645.80 +/- 159.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=139000, episode_reward=1654.80 +/- 216.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=140000, episode_reward=1471.00 +/- 224.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=141000, episode_reward=1559.20 +/- 294.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=142000, episode_reward=1813.00 +/- 108.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=143000, episode_reward=1553.00 +/- 254.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=144000, episode_reward=1691.00 +/- 146.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=145000, episode_reward=1797.80 +/- 154.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=146000, episode_reward=1586.00 +/- 229.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=147000, episode_reward=1636.00 +/- 170.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=148000, episode_reward=1581.60 +/- 214.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=149000, episode_reward=1593.00 +/- 160.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=150000, episode_reward=1679.00 +/- 155.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=151000, episode_reward=1588.60 +/- 362.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=152000, episode_reward=1467.00 +/- 184.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=153000, episode_reward=1245.20 +/- 367.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=154000, episode_reward=1709.20 +/- 176.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=155000, episode_reward=1654.60 +/- 293.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=156000, episode_reward=1665.20 +/- 115.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=157000, episode_reward=1545.00 +/- 166.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=158000, episode_reward=1553.40 +/- 202.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=159000, episode_reward=1691.80 +/- 166.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=160000, episode_reward=1640.80 +/- 139.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=161000, episode_reward=1682.80 +/- 194.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=162000, episode_reward=1574.00 +/- 157.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=163000, episode_reward=1741.40 +/- 177.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=164000, episode_reward=1673.80 +/- 199.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=165000, episode_reward=1649.60 +/- 213.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=166000, episode_reward=1648.80 +/- 237.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=167000, episode_reward=1463.40 +/- 207.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=168000, episode_reward=1672.20 +/- 209.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=169000, episode_reward=1569.40 +/- 110.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=170000, episode_reward=1472.80 +/- 340.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=171000, episode_reward=1615.20 +/- 189.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=172000, episode_reward=1551.40 +/- 270.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=173000, episode_reward=1619.80 +/- 139.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=174000, episode_reward=1676.00 +/- 440.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=175000, episode_reward=1534.20 +/- 128.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=176000, episode_reward=1682.60 +/- 178.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=177000, episode_reward=1509.40 +/- 229.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=178000, episode_reward=1537.40 +/- 311.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=179000, episode_reward=1777.80 +/- 248.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=180000, episode_reward=1520.60 +/- 332.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=181000, episode_reward=1507.20 +/- 207.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=182000, episode_reward=1555.80 +/- 189.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=183000, episode_reward=1641.00 +/- 381.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=184000, episode_reward=1539.00 +/- 180.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=185000, episode_reward=1370.80 +/- 255.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=186000, episode_reward=1451.20 +/- 300.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=187000, episode_reward=1421.00 +/- 511.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=188000, episode_reward=1813.00 +/- 127.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=189000, episode_reward=1667.60 +/- 201.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=190000, episode_reward=1663.00 +/- 228.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=191000, episode_reward=1677.40 +/- 126.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=192000, episode_reward=1615.60 +/- 254.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=193000, episode_reward=1924.80 +/- 60.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=194000, episode_reward=1660.40 +/- 71.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=195000, episode_reward=1689.80 +/- 247.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=196000, episode_reward=1527.20 +/- 122.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=197000, episode_reward=1615.40 +/- 290.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=198000, episode_reward=1657.00 +/- 357.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=199000, episode_reward=1550.40 +/- 190.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=200000, episode_reward=1687.20 +/- 326.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=201000, episode_reward=1564.00 +/- 151.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=202000, episode_reward=1586.20 +/- 202.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=203000, episode_reward=1561.20 +/- 320.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=204000, episode_reward=1656.60 +/- 166.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=205000, episode_reward=1805.00 +/- 105.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=206000, episode_reward=1532.40 +/- 152.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=207000, episode_reward=1639.60 +/- 140.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=208000, episode_reward=1697.00 +/- 202.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=209000, episode_reward=1489.20 +/- 224.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=210000, episode_reward=1873.20 +/- 50.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=211000, episode_reward=1706.00 +/- 164.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=212000, episode_reward=1658.60 +/- 166.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=213000, episode_reward=1519.40 +/- 147.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=214000, episode_reward=1655.80 +/- 173.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=215000, episode_reward=1633.20 +/- 74.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=216000, episode_reward=1777.60 +/- 428.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=217000, episode_reward=1611.00 +/- 280.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=218000, episode_reward=1701.20 +/- 150.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=219000, episode_reward=1569.60 +/- 314.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=220000, episode_reward=1596.80 +/- 230.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=221000, episode_reward=1593.40 +/- 294.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=222000, episode_reward=1732.20 +/- 194.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=223000, episode_reward=1441.80 +/- 196.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=224000, episode_reward=1706.80 +/- 94.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=225000, episode_reward=1555.40 +/- 260.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=226000, episode_reward=1690.80 +/- 195.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=227000, episode_reward=1611.40 +/- 195.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=228000, episode_reward=1635.00 +/- 236.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=229000, episode_reward=1673.20 +/- 210.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=230000, episode_reward=1603.80 +/- 120.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=231000, episode_reward=1638.60 +/- 140.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=232000, episode_reward=1681.60 +/- 185.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=233000, episode_reward=1734.00 +/- 102.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=234000, episode_reward=1847.00 +/- 152.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=235000, episode_reward=1677.20 +/- 212.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=236000, episode_reward=1420.40 +/- 288.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=237000, episode_reward=1766.20 +/- 178.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=238000, episode_reward=1536.20 +/- 258.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=239000, episode_reward=1535.20 +/- 238.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=240000, episode_reward=1728.60 +/- 252.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=241000, episode_reward=1753.20 +/- 155.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=242000, episode_reward=1775.20 +/- 83.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=243000, episode_reward=1530.60 +/- 226.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=244000, episode_reward=1741.60 +/- 217.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=245000, episode_reward=1486.00 +/- 324.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=246000, episode_reward=1631.60 +/- 280.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=247000, episode_reward=1639.00 +/- 172.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=248000, episode_reward=1791.00 +/- 226.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=249000, episode_reward=1639.00 +/- 109.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=250000, episode_reward=1769.40 +/- 61.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=251000, episode_reward=1607.80 +/- 111.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=252000, episode_reward=1660.00 +/- 158.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=253000, episode_reward=1650.40 +/- 187.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=254000, episode_reward=1465.40 +/- 304.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=255000, episode_reward=1643.40 +/- 117.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=256000, episode_reward=1741.00 +/- 105.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=257000, episode_reward=1683.20 +/- 257.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=258000, episode_reward=1457.00 +/- 210.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=259000, episode_reward=1678.60 +/- 227.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=260000, episode_reward=1714.20 +/- 280.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=261000, episode_reward=1583.60 +/- 379.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=262000, episode_reward=1537.20 +/- 239.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=263000, episode_reward=1558.00 +/- 537.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=264000, episode_reward=1624.20 +/- 246.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=265000, episode_reward=1688.40 +/- 159.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=266000, episode_reward=1528.80 +/- 180.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=267000, episode_reward=1654.20 +/- 165.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=268000, episode_reward=1667.60 +/- 310.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=269000, episode_reward=1712.20 +/- 189.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=270000, episode_reward=1507.00 +/- 165.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=271000, episode_reward=1704.80 +/- 206.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=272000, episode_reward=1885.80 +/- 143.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=273000, episode_reward=1621.20 +/- 232.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=274000, episode_reward=1643.00 +/- 141.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=275000, episode_reward=1719.80 +/- 174.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=276000, episode_reward=1715.00 +/- 145.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=277000, episode_reward=1741.40 +/- 185.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=278000, episode_reward=1617.40 +/- 234.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=279000, episode_reward=1632.20 +/- 166.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=280000, episode_reward=1655.80 +/- 230.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=281000, episode_reward=1676.80 +/- 228.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=282000, episode_reward=1647.80 +/- 290.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=283000, episode_reward=1472.40 +/- 243.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=284000, episode_reward=1544.00 +/- 163.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=285000, episode_reward=1579.80 +/- 173.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=286000, episode_reward=1673.60 +/- 151.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=287000, episode_reward=1659.80 +/- 337.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=288000, episode_reward=1785.20 +/- 84.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=289000, episode_reward=1573.80 +/- 299.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=290000, episode_reward=1476.40 +/- 406.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=291000, episode_reward=1329.80 +/- 125.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=292000, episode_reward=1699.40 +/- 138.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=293000, episode_reward=1440.00 +/- 269.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=294000, episode_reward=1586.60 +/- 139.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=295000, episode_reward=1751.60 +/- 125.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=296000, episode_reward=1628.20 +/- 155.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=297000, episode_reward=1717.00 +/- 175.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=298000, episode_reward=1626.00 +/- 105.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=299000, episode_reward=1706.00 +/- 259.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=300000, episode_reward=22.00 +/- 3244.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=301000, episode_reward=1532.00 +/- 269.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=302000, episode_reward=1722.20 +/- 270.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=303000, episode_reward=1659.80 +/- 242.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=304000, episode_reward=1649.60 +/- 91.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=305000, episode_reward=1753.80 +/- 193.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=306000, episode_reward=1738.60 +/- 91.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=307000, episode_reward=1805.20 +/- 118.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=308000, episode_reward=1498.40 +/- 178.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=309000, episode_reward=1730.40 +/- 274.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=310000, episode_reward=1516.80 +/- 301.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=311000, episode_reward=1449.00 +/- 254.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=312000, episode_reward=1564.20 +/- 301.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=313000, episode_reward=1799.20 +/- 205.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=314000, episode_reward=1636.60 +/- 167.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=315000, episode_reward=1634.00 +/- 322.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=316000, episode_reward=1471.80 +/- 326.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=317000, episode_reward=1631.20 +/- 62.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=318000, episode_reward=1786.80 +/- 199.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=319000, episode_reward=1825.00 +/- 177.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=320000, episode_reward=1576.00 +/- 170.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=321000, episode_reward=1700.40 +/- 294.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=322000, episode_reward=1749.60 +/- 189.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=323000, episode_reward=1794.80 +/- 117.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=324000, episode_reward=1732.40 +/- 151.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=325000, episode_reward=1668.00 +/- 248.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=326000, episode_reward=1355.40 +/- 439.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=327000, episode_reward=1706.60 +/- 247.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=328000, episode_reward=1491.60 +/- 459.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=329000, episode_reward=1621.00 +/- 156.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=330000, episode_reward=1805.60 +/- 201.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=331000, episode_reward=1577.20 +/- 395.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=332000, episode_reward=1507.80 +/- 195.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=333000, episode_reward=1321.20 +/- 255.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=334000, episode_reward=1729.00 +/- 218.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=335000, episode_reward=1776.20 +/- 253.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=336000, episode_reward=1702.00 +/- 217.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=337000, episode_reward=1665.40 +/- 216.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=338000, episode_reward=1468.80 +/- 283.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=339000, episode_reward=1695.40 +/- 240.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=340000, episode_reward=1651.20 +/- 184.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=341000, episode_reward=1625.20 +/- 279.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=342000, episode_reward=1690.60 +/- 224.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=343000, episode_reward=1648.80 +/- 319.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=344000, episode_reward=1786.20 +/- 154.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=345000, episode_reward=1518.60 +/- 147.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=346000, episode_reward=1293.00 +/- 387.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=347000, episode_reward=1809.40 +/- 202.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=348000, episode_reward=1735.40 +/- 114.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=349000, episode_reward=1604.60 +/- 194.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=350000, episode_reward=1550.20 +/- 302.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=351000, episode_reward=1731.80 +/- 230.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=352000, episode_reward=1571.00 +/- 202.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=353000, episode_reward=1496.60 +/- 295.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=354000, episode_reward=1513.80 +/- 234.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=355000, episode_reward=1749.80 +/- 111.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=356000, episode_reward=1630.60 +/- 105.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=357000, episode_reward=1751.80 +/- 163.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=358000, episode_reward=1771.00 +/- 237.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=359000, episode_reward=1733.20 +/- 140.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=360000, episode_reward=1455.20 +/- 274.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=361000, episode_reward=1758.00 +/- 257.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=362000, episode_reward=1758.80 +/- 114.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=363000, episode_reward=1599.20 +/- 262.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=364000, episode_reward=1543.20 +/- 205.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=365000, episode_reward=1655.60 +/- 186.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=366000, episode_reward=1461.20 +/- 295.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=367000, episode_reward=1406.00 +/- 193.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=368000, episode_reward=1706.20 +/- 203.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=369000, episode_reward=1711.60 +/- 323.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=370000, episode_reward=1515.60 +/- 193.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=371000, episode_reward=1589.80 +/- 248.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=372000, episode_reward=1583.00 +/- 232.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=373000, episode_reward=1522.20 +/- 303.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=374000, episode_reward=1445.80 +/- 359.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=375000, episode_reward=1468.60 +/- 183.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=376000, episode_reward=1625.80 +/- 233.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=377000, episode_reward=1647.00 +/- 212.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=378000, episode_reward=1644.40 +/- 236.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=379000, episode_reward=1634.60 +/- 175.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=380000, episode_reward=1816.00 +/- 345.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=381000, episode_reward=1683.40 +/- 61.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=382000, episode_reward=1870.20 +/- 143.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=383000, episode_reward=1493.60 +/- 250.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=384000, episode_reward=1403.60 +/- 160.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=385000, episode_reward=1551.60 +/- 117.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=386000, episode_reward=1583.00 +/- 248.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=387000, episode_reward=1413.40 +/- 273.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=388000, episode_reward=1758.20 +/- 255.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=389000, episode_reward=1578.00 +/- 246.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=390000, episode_reward=1817.40 +/- 245.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=391000, episode_reward=1752.00 +/- 69.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=392000, episode_reward=1740.20 +/- 99.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=393000, episode_reward=1631.00 +/- 223.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=394000, episode_reward=1664.00 +/- 206.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=395000, episode_reward=1733.00 +/- 70.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=396000, episode_reward=1587.20 +/- 291.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=397000, episode_reward=1619.60 +/- 169.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=398000, episode_reward=1733.60 +/- 175.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=399000, episode_reward=1708.00 +/- 152.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=400000, episode_reward=1529.60 +/- 285.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=401000, episode_reward=1532.40 +/- 301.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=402000, episode_reward=1689.60 +/- 178.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=403000, episode_reward=1687.60 +/- 231.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=404000, episode_reward=1387.20 +/- 256.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=405000, episode_reward=1500.60 +/- 159.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=406000, episode_reward=1657.60 +/- 215.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=407000, episode_reward=1613.00 +/- 120.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=408000, episode_reward=1482.20 +/- 351.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=409000, episode_reward=1483.20 +/- 337.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=410000, episode_reward=1575.00 +/- 44.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=411000, episode_reward=1580.20 +/- 186.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=412000, episode_reward=1639.80 +/- 93.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=413000, episode_reward=1649.00 +/- 306.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=414000, episode_reward=1316.80 +/- 350.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=415000, episode_reward=1638.00 +/- 97.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=416000, episode_reward=1661.20 +/- 229.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=417000, episode_reward=1651.20 +/- 276.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=418000, episode_reward=1546.40 +/- 334.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=419000, episode_reward=1700.80 +/- 85.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=420000, episode_reward=1689.20 +/- 189.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=421000, episode_reward=1643.80 +/- 108.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=422000, episode_reward=1683.20 +/- 294.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=423000, episode_reward=1618.20 +/- 243.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=424000, episode_reward=1545.40 +/- 387.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=425000, episode_reward=1646.80 +/- 335.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=426000, episode_reward=1605.20 +/- 96.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=427000, episode_reward=1714.00 +/- 171.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=428000, episode_reward=1781.00 +/- 114.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=429000, episode_reward=1610.20 +/- 153.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=430000, episode_reward=1681.80 +/- 209.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=431000, episode_reward=1738.60 +/- 180.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=432000, episode_reward=1804.00 +/- 252.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=433000, episode_reward=1698.20 +/- 188.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=434000, episode_reward=1742.80 +/- 120.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=435000, episode_reward=1553.60 +/- 138.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=436000, episode_reward=1723.40 +/- 111.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=437000, episode_reward=1750.00 +/- 120.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=438000, episode_reward=1708.60 +/- 153.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=439000, episode_reward=1601.40 +/- 104.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=440000, episode_reward=1742.80 +/- 184.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=441000, episode_reward=1424.60 +/- 170.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=442000, episode_reward=1364.60 +/- 163.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=443000, episode_reward=1810.20 +/- 179.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=444000, episode_reward=1509.80 +/- 280.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=445000, episode_reward=1730.00 +/- 285.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=446000, episode_reward=1393.80 +/- 349.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=447000, episode_reward=1709.80 +/- 301.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=448000, episode_reward=1592.00 +/- 284.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=449000, episode_reward=1538.80 +/- 346.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=450000, episode_reward=1586.20 +/- 251.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=451000, episode_reward=1693.20 +/- 148.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=452000, episode_reward=1528.00 +/- 198.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=453000, episode_reward=1279.00 +/- 321.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=454000, episode_reward=1584.20 +/- 198.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=455000, episode_reward=1786.40 +/- 115.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=456000, episode_reward=1617.80 +/- 105.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=457000, episode_reward=1750.80 +/- 185.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=458000, episode_reward=1529.20 +/- 231.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=459000, episode_reward=1777.40 +/- 176.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=460000, episode_reward=1561.40 +/- 289.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=461000, episode_reward=1689.00 +/- 198.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=462000, episode_reward=1741.40 +/- 212.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=463000, episode_reward=1698.00 +/- 287.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=464000, episode_reward=1559.40 +/- 211.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=465000, episode_reward=1560.80 +/- 157.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=466000, episode_reward=1733.60 +/- 202.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=467000, episode_reward=1674.60 +/- 138.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=468000, episode_reward=1735.80 +/- 151.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=469000, episode_reward=1721.20 +/- 223.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=470000, episode_reward=1319.60 +/- 207.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=471000, episode_reward=1565.20 +/- 268.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=472000, episode_reward=1630.60 +/- 249.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=473000, episode_reward=1526.40 +/- 227.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=474000, episode_reward=1533.40 +/- 308.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=475000, episode_reward=1518.40 +/- 368.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=476000, episode_reward=1499.20 +/- 210.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=477000, episode_reward=1714.40 +/- 330.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=478000, episode_reward=1691.60 +/- 307.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=479000, episode_reward=1831.40 +/- 355.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=480000, episode_reward=1664.60 +/- 60.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=481000, episode_reward=1721.00 +/- 114.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=482000, episode_reward=1654.80 +/- 175.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=483000, episode_reward=1534.60 +/- 228.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=484000, episode_reward=1725.20 +/- 238.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=485000, episode_reward=1628.00 +/- 228.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=486000, episode_reward=1720.80 +/- 259.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=487000, episode_reward=1536.00 +/- 82.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=488000, episode_reward=1661.40 +/- 339.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=489000, episode_reward=1827.60 +/- 190.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=490000, episode_reward=1630.80 +/- 205.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=491000, episode_reward=1573.20 +/- 260.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=492000, episode_reward=1380.00 +/- 167.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=493000, episode_reward=1624.20 +/- 254.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=494000, episode_reward=1523.80 +/- 157.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=495000, episode_reward=1526.00 +/- 160.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=496000, episode_reward=1760.00 +/- 107.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=497000, episode_reward=1261.60 +/- 174.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=498000, episode_reward=1616.00 +/- 98.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=499000, episode_reward=1481.40 +/- 353.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=500000, episode_reward=1643.00 +/- 266.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=501000, episode_reward=1661.40 +/- 290.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=502000, episode_reward=1556.20 +/- 290.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=503000, episode_reward=1678.20 +/- 62.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=504000, episode_reward=1630.40 +/- 245.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=505000, episode_reward=1786.00 +/- 227.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=506000, episode_reward=1660.60 +/- 199.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=507000, episode_reward=1839.80 +/- 177.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=508000, episode_reward=1613.60 +/- 198.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=509000, episode_reward=1734.80 +/- 105.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=510000, episode_reward=1523.00 +/- 243.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=511000, episode_reward=1816.60 +/- 148.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=512000, episode_reward=1769.80 +/- 58.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=513000, episode_reward=1752.40 +/- 195.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=514000, episode_reward=1731.80 +/- 194.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=515000, episode_reward=1775.80 +/- 173.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=516000, episode_reward=1669.00 +/- 307.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=517000, episode_reward=1581.00 +/- 133.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=518000, episode_reward=1618.80 +/- 198.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=519000, episode_reward=1741.20 +/- 159.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=520000, episode_reward=1622.40 +/- 263.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=521000, episode_reward=1476.20 +/- 232.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=522000, episode_reward=1616.00 +/- 286.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=523000, episode_reward=1506.60 +/- 341.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=524000, episode_reward=1703.20 +/- 249.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=525000, episode_reward=1787.60 +/- 252.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=526000, episode_reward=1657.40 +/- 138.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=527000, episode_reward=1643.40 +/- 164.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=528000, episode_reward=1568.20 +/- 196.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=529000, episode_reward=1738.00 +/- 163.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=530000, episode_reward=1659.80 +/- 334.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=531000, episode_reward=1524.20 +/- 239.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=532000, episode_reward=1635.60 +/- 218.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=533000, episode_reward=1633.20 +/- 148.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=534000, episode_reward=1661.80 +/- 161.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=535000, episode_reward=1591.40 +/- 209.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=536000, episode_reward=1769.00 +/- 84.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=537000, episode_reward=1738.60 +/- 345.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=538000, episode_reward=1642.40 +/- 132.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=539000, episode_reward=1675.40 +/- 217.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=540000, episode_reward=1712.20 +/- 162.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=541000, episode_reward=1694.20 +/- 57.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=542000, episode_reward=1748.00 +/- 174.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=543000, episode_reward=1758.20 +/- 226.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=544000, episode_reward=1572.20 +/- 180.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=545000, episode_reward=1675.20 +/- 220.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=546000, episode_reward=1694.80 +/- 197.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=547000, episode_reward=1756.00 +/- 194.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=548000, episode_reward=1699.40 +/- 89.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=549000, episode_reward=1467.80 +/- 263.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=550000, episode_reward=1728.80 +/- 267.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=551000, episode_reward=1588.80 +/- 264.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=552000, episode_reward=1646.40 +/- 236.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=553000, episode_reward=1650.20 +/- 230.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=554000, episode_reward=1714.80 +/- 316.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=555000, episode_reward=1642.40 +/- 196.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=556000, episode_reward=1634.20 +/- 112.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=557000, episode_reward=1723.00 +/- 71.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=558000, episode_reward=1499.20 +/- 227.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=559000, episode_reward=1622.20 +/- 245.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=560000, episode_reward=1493.40 +/- 186.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=561000, episode_reward=1679.80 +/- 181.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=562000, episode_reward=1611.80 +/- 155.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=563000, episode_reward=1366.40 +/- 323.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=564000, episode_reward=1634.20 +/- 409.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=565000, episode_reward=1583.80 +/- 100.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=566000, episode_reward=1647.60 +/- 202.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=567000, episode_reward=1618.20 +/- 300.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=568000, episode_reward=1684.20 +/- 306.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=569000, episode_reward=1835.40 +/- 169.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=570000, episode_reward=1640.80 +/- 94.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=571000, episode_reward=1948.00 +/- 125.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=572000, episode_reward=1496.60 +/- 242.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=573000, episode_reward=1647.20 +/- 288.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=574000, episode_reward=1563.20 +/- 326.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=575000, episode_reward=1708.60 +/- 205.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=576000, episode_reward=1513.00 +/- 336.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=577000, episode_reward=1651.20 +/- 110.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=578000, episode_reward=1816.40 +/- 158.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=579000, episode_reward=1755.60 +/- 132.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=580000, episode_reward=1714.40 +/- 211.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=581000, episode_reward=1579.00 +/- 182.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=582000, episode_reward=1675.20 +/- 139.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=583000, episode_reward=1759.60 +/- 200.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=584000, episode_reward=1548.60 +/- 269.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=585000, episode_reward=1542.60 +/- 176.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=586000, episode_reward=1585.80 +/- 243.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=587000, episode_reward=1664.40 +/- 276.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=588000, episode_reward=1504.60 +/- 59.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=589000, episode_reward=1676.20 +/- 171.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=590000, episode_reward=1735.40 +/- 105.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=591000, episode_reward=1761.40 +/- 142.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=592000, episode_reward=1452.60 +/- 437.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=593000, episode_reward=1390.80 +/- 328.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=594000, episode_reward=1844.20 +/- 191.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=595000, episode_reward=1695.80 +/- 211.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=596000, episode_reward=1255.80 +/- 270.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=597000, episode_reward=1678.00 +/- 137.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=598000, episode_reward=1339.60 +/- 327.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=599000, episode_reward=1537.60 +/- 325.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=600000, episode_reward=1551.20 +/- 180.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=601000, episode_reward=1699.80 +/- 227.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=602000, episode_reward=1528.00 +/- 446.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=603000, episode_reward=1700.40 +/- 185.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=604000, episode_reward=1550.20 +/- 168.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=605000, episode_reward=1663.80 +/- 225.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=606000, episode_reward=1726.40 +/- 140.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=607000, episode_reward=1503.40 +/- 278.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=608000, episode_reward=1620.20 +/- 146.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=609000, episode_reward=1656.00 +/- 197.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=610000, episode_reward=1850.80 +/- 89.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=611000, episode_reward=1585.80 +/- 268.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=612000, episode_reward=1678.60 +/- 177.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=613000, episode_reward=1933.20 +/- 123.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=614000, episode_reward=1875.60 +/- 161.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=615000, episode_reward=1730.00 +/- 233.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=616000, episode_reward=1728.40 +/- 147.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=617000, episode_reward=1475.20 +/- 172.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=618000, episode_reward=1579.60 +/- 177.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=619000, episode_reward=1589.40 +/- 299.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=620000, episode_reward=1242.80 +/- 316.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=621000, episode_reward=1587.00 +/- 267.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=622000, episode_reward=1589.60 +/- 220.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=623000, episode_reward=1772.80 +/- 166.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=624000, episode_reward=1553.40 +/- 245.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=625000, episode_reward=1593.40 +/- 237.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=626000, episode_reward=1633.80 +/- 170.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=627000, episode_reward=1682.60 +/- 238.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=628000, episode_reward=1537.40 +/- 151.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=629000, episode_reward=1398.40 +/- 367.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=630000, episode_reward=1895.80 +/- 72.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=631000, episode_reward=1670.60 +/- 117.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=632000, episode_reward=1542.60 +/- 236.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=633000, episode_reward=1642.60 +/- 163.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=634000, episode_reward=1665.00 +/- 186.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=635000, episode_reward=1769.00 +/- 212.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=636000, episode_reward=1631.40 +/- 416.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=637000, episode_reward=1699.60 +/- 210.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=638000, episode_reward=1472.80 +/- 282.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=639000, episode_reward=1649.40 +/- 261.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=640000, episode_reward=1616.60 +/- 139.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=641000, episode_reward=1638.80 +/- 230.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=642000, episode_reward=1402.80 +/- 262.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=643000, episode_reward=1917.60 +/- 120.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=644000, episode_reward=1791.20 +/- 217.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=645000, episode_reward=1860.20 +/- 139.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=646000, episode_reward=1598.60 +/- 122.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=647000, episode_reward=1519.60 +/- 259.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=648000, episode_reward=1671.00 +/- 158.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=649000, episode_reward=1674.40 +/- 138.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=650000, episode_reward=1615.20 +/- 230.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=651000, episode_reward=1694.20 +/- 268.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=652000, episode_reward=1588.00 +/- 280.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=653000, episode_reward=1551.80 +/- 273.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=654000, episode_reward=1457.20 +/- 307.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=655000, episode_reward=1452.00 +/- 152.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=656000, episode_reward=1703.80 +/- 220.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=657000, episode_reward=1666.20 +/- 166.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=658000, episode_reward=1652.60 +/- 157.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=659000, episode_reward=1694.20 +/- 242.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=660000, episode_reward=1900.40 +/- 164.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=661000, episode_reward=1681.40 +/- 275.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=662000, episode_reward=1351.00 +/- 275.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=663000, episode_reward=1515.40 +/- 114.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=664000, episode_reward=1750.80 +/- 146.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=665000, episode_reward=1981.00 +/- 165.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=666000, episode_reward=1687.60 +/- 226.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=667000, episode_reward=1745.20 +/- 128.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=668000, episode_reward=1643.40 +/- 146.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=669000, episode_reward=1809.60 +/- 110.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=670000, episode_reward=1726.00 +/- 205.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=671000, episode_reward=1805.40 +/- 184.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=672000, episode_reward=1617.20 +/- 309.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=673000, episode_reward=1588.00 +/- 172.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=674000, episode_reward=1595.00 +/- 312.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=675000, episode_reward=1669.00 +/- 243.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=676000, episode_reward=1684.40 +/- 233.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=677000, episode_reward=1652.60 +/- 131.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=678000, episode_reward=1554.40 +/- 108.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=679000, episode_reward=1591.40 +/- 174.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=680000, episode_reward=1570.40 +/- 312.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=681000, episode_reward=1686.00 +/- 267.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=682000, episode_reward=1752.80 +/- 186.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=683000, episode_reward=1729.80 +/- 324.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=684000, episode_reward=1401.20 +/- 286.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=685000, episode_reward=1670.60 +/- 104.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=686000, episode_reward=1220.00 +/- 554.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=687000, episode_reward=1737.20 +/- 183.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=688000, episode_reward=1821.00 +/- 227.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=689000, episode_reward=1527.60 +/- 176.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=690000, episode_reward=1682.80 +/- 134.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=691000, episode_reward=1697.40 +/- 157.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=692000, episode_reward=1824.80 +/- 70.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=693000, episode_reward=1595.60 +/- 327.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=694000, episode_reward=1624.80 +/- 306.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=695000, episode_reward=1987.40 +/- 162.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=696000, episode_reward=1783.00 +/- 172.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=697000, episode_reward=1780.20 +/- 123.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=698000, episode_reward=1759.40 +/- 182.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=699000, episode_reward=1770.00 +/- 48.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=700000, episode_reward=1419.40 +/- 182.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=701000, episode_reward=1730.20 +/- 63.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=702000, episode_reward=1619.80 +/- 302.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=703000, episode_reward=1591.00 +/- 210.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=704000, episode_reward=1465.60 +/- 221.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=705000, episode_reward=1620.20 +/- 258.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=706000, episode_reward=1721.00 +/- 177.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=707000, episode_reward=1681.20 +/- 225.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=708000, episode_reward=1376.20 +/- 187.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=709000, episode_reward=1663.00 +/- 212.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=710000, episode_reward=1715.40 +/- 159.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=711000, episode_reward=1694.60 +/- 165.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=712000, episode_reward=1801.60 +/- 250.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=713000, episode_reward=1686.40 +/- 144.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=714000, episode_reward=1664.00 +/- 197.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=715000, episode_reward=1672.20 +/- 293.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=716000, episode_reward=1880.80 +/- 150.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=717000, episode_reward=1447.00 +/- 336.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=718000, episode_reward=1732.00 +/- 125.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=719000, episode_reward=1578.20 +/- 211.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=720000, episode_reward=1511.40 +/- 167.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=721000, episode_reward=1639.40 +/- 57.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=722000, episode_reward=1720.20 +/- 157.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=723000, episode_reward=1831.20 +/- 199.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=724000, episode_reward=1567.20 +/- 248.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=725000, episode_reward=1457.00 +/- 408.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=726000, episode_reward=1823.80 +/- 156.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=727000, episode_reward=1633.20 +/- 154.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=728000, episode_reward=1632.40 +/- 245.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=729000, episode_reward=1689.80 +/- 143.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=730000, episode_reward=1548.00 +/- 294.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=731000, episode_reward=1567.40 +/- 266.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=732000, episode_reward=1635.60 +/- 149.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=733000, episode_reward=1778.00 +/- 237.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=734000, episode_reward=1784.00 +/- 134.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=735000, episode_reward=1451.60 +/- 124.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=736000, episode_reward=1645.80 +/- 196.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=737000, episode_reward=1631.40 +/- 257.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=738000, episode_reward=1840.80 +/- 211.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=739000, episode_reward=1533.00 +/- 108.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=740000, episode_reward=1637.40 +/- 183.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=741000, episode_reward=1660.60 +/- 194.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=742000, episode_reward=1745.20 +/- 164.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=743000, episode_reward=1759.60 +/- 270.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=744000, episode_reward=1802.20 +/- 295.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=745000, episode_reward=1528.00 +/- 106.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=746000, episode_reward=1638.80 +/- 204.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=747000, episode_reward=1633.80 +/- 114.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=748000, episode_reward=1552.20 +/- 249.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=749000, episode_reward=1691.20 +/- 190.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=750000, episode_reward=1724.60 +/- 226.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=751000, episode_reward=1557.60 +/- 185.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=752000, episode_reward=1532.20 +/- 223.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=753000, episode_reward=1575.20 +/- 296.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=754000, episode_reward=1753.00 +/- 212.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=755000, episode_reward=1863.00 +/- 106.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=756000, episode_reward=1833.80 +/- 142.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=757000, episode_reward=1799.60 +/- 138.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=758000, episode_reward=1601.00 +/- 226.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=759000, episode_reward=1712.40 +/- 208.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=760000, episode_reward=1704.00 +/- 165.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=761000, episode_reward=1656.20 +/- 212.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=762000, episode_reward=1606.80 +/- 234.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=763000, episode_reward=1779.60 +/- 215.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=764000, episode_reward=1732.80 +/- 122.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=765000, episode_reward=1661.20 +/- 294.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=766000, episode_reward=1494.20 +/- 216.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=767000, episode_reward=1836.40 +/- 154.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=768000, episode_reward=1741.00 +/- 97.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=769000, episode_reward=1778.60 +/- 112.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=770000, episode_reward=1741.00 +/- 269.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=771000, episode_reward=1758.20 +/- 363.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=772000, episode_reward=1578.60 +/- 87.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=773000, episode_reward=1840.80 +/- 150.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=774000, episode_reward=1788.60 +/- 199.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=775000, episode_reward=1784.20 +/- 185.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=776000, episode_reward=1827.40 +/- 178.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=777000, episode_reward=1672.20 +/- 219.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=778000, episode_reward=1604.80 +/- 234.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=779000, episode_reward=1848.80 +/- 80.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=780000, episode_reward=1590.00 +/- 177.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=781000, episode_reward=1743.60 +/- 81.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=782000, episode_reward=1713.20 +/- 193.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=783000, episode_reward=1502.00 +/- 95.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=784000, episode_reward=1563.80 +/- 110.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=785000, episode_reward=1837.00 +/- 80.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=786000, episode_reward=1626.80 +/- 193.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=787000, episode_reward=1438.00 +/- 205.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=788000, episode_reward=1506.20 +/- 154.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=789000, episode_reward=1747.80 +/- 272.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=790000, episode_reward=1522.60 +/- 411.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=791000, episode_reward=1521.20 +/- 244.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=792000, episode_reward=1401.80 +/- 335.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=793000, episode_reward=1734.40 +/- 234.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=794000, episode_reward=1584.20 +/- 318.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=795000, episode_reward=1563.20 +/- 356.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=796000, episode_reward=1775.40 +/- 162.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=797000, episode_reward=1811.80 +/- 196.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=798000, episode_reward=1577.60 +/- 210.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=799000, episode_reward=1646.40 +/- 152.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=800000, episode_reward=1721.00 +/- 215.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=801000, episode_reward=1630.20 +/- 261.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=802000, episode_reward=1570.60 +/- 298.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=803000, episode_reward=1630.40 +/- 190.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=804000, episode_reward=1501.80 +/- 425.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=805000, episode_reward=1797.20 +/- 210.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=806000, episode_reward=1547.60 +/- 205.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=807000, episode_reward=1727.80 +/- 256.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=808000, episode_reward=1883.00 +/- 206.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=809000, episode_reward=1672.20 +/- 200.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=810000, episode_reward=1683.00 +/- 309.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=811000, episode_reward=1701.00 +/- 236.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=812000, episode_reward=1638.80 +/- 218.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=813000, episode_reward=1663.60 +/- 97.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=814000, episode_reward=1621.80 +/- 171.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=815000, episode_reward=1697.00 +/- 257.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=816000, episode_reward=1444.40 +/- 121.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=817000, episode_reward=1369.40 +/- 314.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=818000, episode_reward=1751.00 +/- 154.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=819000, episode_reward=1517.00 +/- 229.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=820000, episode_reward=1627.60 +/- 156.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=821000, episode_reward=1749.60 +/- 296.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=822000, episode_reward=1661.80 +/- 107.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=823000, episode_reward=1953.20 +/- 206.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=824000, episode_reward=1865.00 +/- 137.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=825000, episode_reward=1527.60 +/- 287.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=826000, episode_reward=1705.00 +/- 275.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=827000, episode_reward=1758.60 +/- 212.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=828000, episode_reward=1652.20 +/- 385.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=829000, episode_reward=1700.00 +/- 368.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=830000, episode_reward=1875.40 +/- 178.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=831000, episode_reward=1734.80 +/- 252.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=832000, episode_reward=1640.40 +/- 184.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=833000, episode_reward=1713.40 +/- 187.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=834000, episode_reward=1742.20 +/- 160.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=835000, episode_reward=1739.60 +/- 170.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=836000, episode_reward=1613.40 +/- 243.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=837000, episode_reward=1629.40 +/- 177.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=838000, episode_reward=1693.00 +/- 356.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=839000, episode_reward=1646.40 +/- 153.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=840000, episode_reward=1801.00 +/- 151.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=841000, episode_reward=1676.80 +/- 225.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=842000, episode_reward=1710.40 +/- 139.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=843000, episode_reward=1856.20 +/- 154.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=844000, episode_reward=1727.80 +/- 152.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=845000, episode_reward=1720.00 +/- 309.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=846000, episode_reward=1680.80 +/- 182.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=847000, episode_reward=1673.80 +/- 267.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=848000, episode_reward=1713.60 +/- 204.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=849000, episode_reward=1573.40 +/- 205.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=850000, episode_reward=1733.80 +/- 146.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=851000, episode_reward=1687.40 +/- 226.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=852000, episode_reward=1776.80 +/- 246.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=853000, episode_reward=1832.80 +/- 122.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=854000, episode_reward=1638.20 +/- 224.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=855000, episode_reward=1776.60 +/- 128.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=856000, episode_reward=1604.20 +/- 193.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=857000, episode_reward=1714.80 +/- 123.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=858000, episode_reward=1643.00 +/- 194.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=859000, episode_reward=1689.00 +/- 168.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=860000, episode_reward=1561.60 +/- 150.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=861000, episode_reward=1597.00 +/- 105.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=862000, episode_reward=1795.20 +/- 314.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=863000, episode_reward=1625.00 +/- 146.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=864000, episode_reward=1570.60 +/- 50.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=865000, episode_reward=1716.60 +/- 105.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=866000, episode_reward=1786.60 +/- 121.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=867000, episode_reward=1736.40 +/- 264.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=868000, episode_reward=1836.20 +/- 195.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=869000, episode_reward=1531.80 +/- 106.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=870000, episode_reward=1711.80 +/- 275.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=871000, episode_reward=1655.60 +/- 251.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=872000, episode_reward=1710.60 +/- 133.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=873000, episode_reward=1706.40 +/- 418.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=874000, episode_reward=1638.00 +/- 101.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=875000, episode_reward=1601.00 +/- 209.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=876000, episode_reward=1777.00 +/- 169.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=877000, episode_reward=1658.40 +/- 138.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=878000, episode_reward=1598.00 +/- 250.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=879000, episode_reward=1789.20 +/- 230.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=880000, episode_reward=1655.60 +/- 58.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=881000, episode_reward=1674.60 +/- 116.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=882000, episode_reward=1778.80 +/- 163.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=883000, episode_reward=1781.40 +/- 225.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=884000, episode_reward=1677.60 +/- 254.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=885000, episode_reward=1731.80 +/- 230.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=886000, episode_reward=1684.40 +/- 188.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=887000, episode_reward=1502.20 +/- 360.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=888000, episode_reward=1703.00 +/- 80.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=889000, episode_reward=1823.20 +/- 106.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=890000, episode_reward=1639.60 +/- 290.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=891000, episode_reward=1785.00 +/- 204.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=892000, episode_reward=1741.60 +/- 167.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=893000, episode_reward=1709.00 +/- 180.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=894000, episode_reward=1739.20 +/- 125.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=895000, episode_reward=1767.40 +/- 157.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=896000, episode_reward=1628.00 +/- 361.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=897000, episode_reward=1639.60 +/- 191.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=898000, episode_reward=1653.00 +/- 165.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=899000, episode_reward=1653.40 +/- 207.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=900000, episode_reward=1720.20 +/- 240.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=901000, episode_reward=1657.40 +/- 43.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=902000, episode_reward=1562.00 +/- 197.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=903000, episode_reward=1838.80 +/- 240.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=904000, episode_reward=1797.80 +/- 162.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=905000, episode_reward=1793.60 +/- 221.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=906000, episode_reward=1650.20 +/- 146.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=907000, episode_reward=1658.60 +/- 255.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=908000, episode_reward=1782.60 +/- 215.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=909000, episode_reward=1880.80 +/- 133.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=910000, episode_reward=1722.60 +/- 321.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=911000, episode_reward=1658.40 +/- 228.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=912000, episode_reward=1711.80 +/- 96.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=913000, episode_reward=1798.00 +/- 165.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=914000, episode_reward=1719.00 +/- 133.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=915000, episode_reward=1691.40 +/- 55.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=916000, episode_reward=1687.00 +/- 222.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=917000, episode_reward=1814.20 +/- 279.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=918000, episode_reward=1700.80 +/- 164.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=919000, episode_reward=1558.00 +/- 182.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=920000, episode_reward=1665.00 +/- 236.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=921000, episode_reward=1664.60 +/- 111.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=922000, episode_reward=1511.80 +/- 203.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=923000, episode_reward=1833.00 +/- 187.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=924000, episode_reward=1707.60 +/- 154.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=925000, episode_reward=1677.00 +/- 173.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=926000, episode_reward=1677.40 +/- 76.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=927000, episode_reward=1713.60 +/- 248.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=928000, episode_reward=1804.00 +/- 194.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=929000, episode_reward=1930.80 +/- 143.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=930000, episode_reward=1683.00 +/- 188.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=931000, episode_reward=1532.60 +/- 198.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=932000, episode_reward=1716.40 +/- 170.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=933000, episode_reward=1637.80 +/- 152.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=934000, episode_reward=1664.20 +/- 159.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=935000, episode_reward=1538.20 +/- 228.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=936000, episode_reward=1632.00 +/- 256.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=937000, episode_reward=1507.60 +/- 303.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=938000, episode_reward=1757.60 +/- 203.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=939000, episode_reward=1655.00 +/- 242.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=940000, episode_reward=1565.20 +/- 192.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=941000, episode_reward=1705.60 +/- 153.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=942000, episode_reward=1507.80 +/- 157.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=943000, episode_reward=1757.00 +/- 229.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=944000, episode_reward=1666.00 +/- 157.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=945000, episode_reward=1638.20 +/- 228.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=946000, episode_reward=1708.60 +/- 155.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=947000, episode_reward=1816.00 +/- 192.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=948000, episode_reward=1770.40 +/- 113.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=949000, episode_reward=1864.80 +/- 193.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=950000, episode_reward=1703.80 +/- 267.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=951000, episode_reward=1596.60 +/- 179.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=952000, episode_reward=1519.80 +/- 165.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=953000, episode_reward=1765.20 +/- 210.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=954000, episode_reward=1696.00 +/- 111.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=955000, episode_reward=1767.00 +/- 166.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=956000, episode_reward=1647.20 +/- 201.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=957000, episode_reward=1773.20 +/- 260.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=958000, episode_reward=1730.20 +/- 185.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=959000, episode_reward=1702.40 +/- 188.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=960000, episode_reward=1837.80 +/- 234.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=961000, episode_reward=1755.20 +/- 121.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=962000, episode_reward=1766.80 +/- 19.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=963000, episode_reward=1673.00 +/- 198.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=964000, episode_reward=1728.40 +/- 181.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=965000, episode_reward=1656.60 +/- 140.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=966000, episode_reward=1726.40 +/- 173.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=967000, episode_reward=1604.40 +/- 373.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=968000, episode_reward=1572.80 +/- 155.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=969000, episode_reward=1572.80 +/- 172.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=970000, episode_reward=1615.00 +/- 65.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=971000, episode_reward=1469.80 +/- 237.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=972000, episode_reward=1615.40 +/- 130.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=973000, episode_reward=1710.80 +/- 161.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=974000, episode_reward=1706.80 +/- 298.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=975000, episode_reward=1545.60 +/- 167.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=976000, episode_reward=1645.80 +/- 142.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=977000, episode_reward=1438.00 +/- 218.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=978000, episode_reward=1730.80 +/- 243.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=979000, episode_reward=1599.00 +/- 246.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=980000, episode_reward=1653.20 +/- 195.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=981000, episode_reward=1572.00 +/- 243.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=982000, episode_reward=1649.20 +/- 137.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=983000, episode_reward=1667.80 +/- 200.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=984000, episode_reward=1754.80 +/- 152.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=985000, episode_reward=1666.40 +/- 162.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=986000, episode_reward=1728.40 +/- 183.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=987000, episode_reward=1662.20 +/- 221.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=988000, episode_reward=1666.80 +/- 204.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=989000, episode_reward=1488.40 +/- 140.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=990000, episode_reward=1517.20 +/- 124.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=991000, episode_reward=1853.20 +/- 146.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=992000, episode_reward=1772.40 +/- 161.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=993000, episode_reward=1814.20 +/- 204.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=994000, episode_reward=1573.80 +/- 113.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=995000, episode_reward=1848.20 +/- 63.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=996000, episode_reward=1667.40 +/- 158.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=997000, episode_reward=1759.80 +/- 170.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=998000, episode_reward=1732.40 +/- 128.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=999000, episode_reward=1731.80 +/- 141.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1000000, episode_reward=1790.40 +/- 108.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1001000, episode_reward=1774.00 +/- 185.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Gamma = 0.9, Learning Rate = 0.003, Activation Function = ReLU, Neurons = 8, Process noise = 0.1, Measurement noise = 0.0\n",
      "Eval num_timesteps=1000, episode_reward=-19048.00 +/- 107.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=-18944.00 +/- 66.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3000, episode_reward=-225.60 +/- 317.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4000, episode_reward=-78.40 +/- 127.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=5000, episode_reward=315.80 +/- 120.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=6000, episode_reward=157.40 +/- 188.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=7000, episode_reward=395.00 +/- 152.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=8000, episode_reward=432.00 +/- 203.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=9000, episode_reward=485.20 +/- 101.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=10000, episode_reward=388.80 +/- 238.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=11000, episode_reward=322.80 +/- 71.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=12000, episode_reward=456.20 +/- 117.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=13000, episode_reward=326.80 +/- 191.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=14000, episode_reward=393.60 +/- 150.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=15000, episode_reward=495.00 +/- 110.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=16000, episode_reward=446.40 +/- 126.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=17000, episode_reward=392.00 +/- 211.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=18000, episode_reward=508.80 +/- 49.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=19000, episode_reward=478.20 +/- 203.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=20000, episode_reward=313.20 +/- 193.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=21000, episode_reward=329.20 +/- 172.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=22000, episode_reward=447.00 +/- 286.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=23000, episode_reward=821.80 +/- 171.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=24000, episode_reward=764.60 +/- 188.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=25000, episode_reward=870.20 +/- 191.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=26000, episode_reward=730.40 +/- 221.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=27000, episode_reward=1266.60 +/- 207.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=28000, episode_reward=1076.60 +/- 435.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=29000, episode_reward=980.20 +/- 225.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=30000, episode_reward=1174.80 +/- 103.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=31000, episode_reward=745.60 +/- 353.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=32000, episode_reward=1004.60 +/- 165.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=33000, episode_reward=1299.00 +/- 232.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=34000, episode_reward=1120.80 +/- 184.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=35000, episode_reward=1191.80 +/- 283.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=36000, episode_reward=1264.40 +/- 168.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=37000, episode_reward=1139.20 +/- 155.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=38000, episode_reward=1093.00 +/- 290.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=39000, episode_reward=649.80 +/- 195.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=40000, episode_reward=1226.00 +/- 196.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=41000, episode_reward=1029.80 +/- 316.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=42000, episode_reward=1044.00 +/- 221.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=43000, episode_reward=-606.60 +/- 3762.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=44000, episode_reward=1332.00 +/- 253.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=45000, episode_reward=1387.60 +/- 194.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=46000, episode_reward=1444.60 +/- 198.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=47000, episode_reward=1196.60 +/- 206.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=48000, episode_reward=1366.80 +/- 127.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=49000, episode_reward=1271.00 +/- 250.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=50000, episode_reward=-949.80 +/- 4628.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=51000, episode_reward=1358.60 +/- 175.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=52000, episode_reward=-1860.80 +/- 7222.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=53000, episode_reward=1468.00 +/- 219.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=54000, episode_reward=1456.60 +/- 162.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=55000, episode_reward=1572.80 +/- 195.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=56000, episode_reward=1601.20 +/- 241.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=57000, episode_reward=1353.80 +/- 185.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=58000, episode_reward=1488.00 +/- 358.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=59000, episode_reward=1605.80 +/- 158.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=60000, episode_reward=1680.80 +/- 169.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=61000, episode_reward=1461.00 +/- 83.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=62000, episode_reward=1618.40 +/- 350.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=63000, episode_reward=1612.60 +/- 254.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=64000, episode_reward=1451.40 +/- 290.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=65000, episode_reward=1635.00 +/- 302.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=66000, episode_reward=1609.40 +/- 166.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=67000, episode_reward=1457.00 +/- 244.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=68000, episode_reward=1658.60 +/- 165.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=69000, episode_reward=1679.80 +/- 144.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=70000, episode_reward=1501.00 +/- 187.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=71000, episode_reward=1618.40 +/- 269.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=72000, episode_reward=1738.80 +/- 190.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=73000, episode_reward=1615.40 +/- 239.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=74000, episode_reward=1625.80 +/- 161.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=75000, episode_reward=1583.00 +/- 122.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=76000, episode_reward=1559.80 +/- 195.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=77000, episode_reward=1665.60 +/- 155.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=78000, episode_reward=1663.20 +/- 380.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=79000, episode_reward=1660.00 +/- 159.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=80000, episode_reward=1503.20 +/- 188.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=81000, episode_reward=1526.20 +/- 314.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=82000, episode_reward=1614.80 +/- 77.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=83000, episode_reward=1606.00 +/- 277.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=84000, episode_reward=1657.20 +/- 182.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=85000, episode_reward=1864.20 +/- 109.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=86000, episode_reward=1524.60 +/- 266.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=87000, episode_reward=1301.20 +/- 220.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=88000, episode_reward=1780.60 +/- 128.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=89000, episode_reward=1346.80 +/- 276.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=90000, episode_reward=1599.00 +/- 145.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=91000, episode_reward=1558.80 +/- 231.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=92000, episode_reward=1600.60 +/- 130.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=93000, episode_reward=1565.20 +/- 178.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=94000, episode_reward=1662.80 +/- 195.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=95000, episode_reward=1614.00 +/- 165.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=96000, episode_reward=1406.00 +/- 212.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=97000, episode_reward=1677.80 +/- 175.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=98000, episode_reward=1451.40 +/- 234.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=99000, episode_reward=1389.40 +/- 291.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=100000, episode_reward=1329.20 +/- 262.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=101000, episode_reward=1738.60 +/- 177.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=102000, episode_reward=1363.20 +/- 64.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=103000, episode_reward=1595.80 +/- 276.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=104000, episode_reward=1637.00 +/- 151.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=105000, episode_reward=1628.00 +/- 376.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=106000, episode_reward=1528.40 +/- 358.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=107000, episode_reward=1629.20 +/- 166.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=108000, episode_reward=1731.00 +/- 168.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=109000, episode_reward=1516.40 +/- 345.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=110000, episode_reward=1692.80 +/- 177.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=111000, episode_reward=1388.00 +/- 237.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=112000, episode_reward=1482.20 +/- 427.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=113000, episode_reward=1614.80 +/- 148.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=114000, episode_reward=1594.40 +/- 138.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=115000, episode_reward=1633.40 +/- 106.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=116000, episode_reward=1670.20 +/- 210.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=117000, episode_reward=1568.00 +/- 159.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=118000, episode_reward=1622.80 +/- 277.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=119000, episode_reward=1574.40 +/- 250.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=120000, episode_reward=1574.80 +/- 134.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=121000, episode_reward=1627.60 +/- 268.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=122000, episode_reward=1564.60 +/- 241.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=123000, episode_reward=1673.40 +/- 254.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=124000, episode_reward=1588.40 +/- 355.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=125000, episode_reward=1464.60 +/- 192.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=126000, episode_reward=1416.60 +/- 327.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=127000, episode_reward=1665.60 +/- 169.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=128000, episode_reward=1723.00 +/- 337.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=129000, episode_reward=1498.60 +/- 286.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=130000, episode_reward=1502.00 +/- 281.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=131000, episode_reward=1286.60 +/- 343.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=132000, episode_reward=1396.20 +/- 222.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=133000, episode_reward=1582.60 +/- 370.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=134000, episode_reward=1584.80 +/- 189.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=135000, episode_reward=1825.00 +/- 302.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=136000, episode_reward=1662.40 +/- 232.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=137000, episode_reward=1744.20 +/- 152.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=138000, episode_reward=1591.20 +/- 186.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=139000, episode_reward=1476.60 +/- 336.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=140000, episode_reward=1572.00 +/- 188.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=141000, episode_reward=1716.40 +/- 276.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=142000, episode_reward=1562.80 +/- 264.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=143000, episode_reward=1624.80 +/- 175.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=144000, episode_reward=1551.80 +/- 405.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=145000, episode_reward=1476.40 +/- 310.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=146000, episode_reward=1482.20 +/- 187.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=147000, episode_reward=1573.20 +/- 63.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=148000, episode_reward=1671.80 +/- 89.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=149000, episode_reward=1716.60 +/- 163.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=150000, episode_reward=1757.00 +/- 145.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=151000, episode_reward=1556.80 +/- 246.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=152000, episode_reward=1477.00 +/- 328.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=153000, episode_reward=1760.40 +/- 226.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=154000, episode_reward=1695.20 +/- 201.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=155000, episode_reward=1494.80 +/- 109.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=156000, episode_reward=1394.00 +/- 164.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=157000, episode_reward=1597.80 +/- 300.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=158000, episode_reward=1569.20 +/- 122.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=159000, episode_reward=1487.20 +/- 379.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=160000, episode_reward=1090.20 +/- 297.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=161000, episode_reward=1712.20 +/- 194.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=162000, episode_reward=1520.00 +/- 291.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=163000, episode_reward=1627.00 +/- 248.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=164000, episode_reward=1620.80 +/- 338.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=165000, episode_reward=1394.80 +/- 353.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=166000, episode_reward=1470.00 +/- 315.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=167000, episode_reward=1576.20 +/- 214.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=168000, episode_reward=1683.60 +/- 370.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=169000, episode_reward=1334.40 +/- 594.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=170000, episode_reward=1583.60 +/- 235.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=171000, episode_reward=1512.20 +/- 300.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=172000, episode_reward=1762.00 +/- 231.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=173000, episode_reward=1460.00 +/- 177.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=174000, episode_reward=1701.40 +/- 290.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=175000, episode_reward=1797.00 +/- 147.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=176000, episode_reward=1474.20 +/- 308.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=177000, episode_reward=1510.60 +/- 248.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=178000, episode_reward=1531.20 +/- 278.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=179000, episode_reward=1363.60 +/- 155.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=180000, episode_reward=1537.40 +/- 200.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=181000, episode_reward=1753.40 +/- 95.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=182000, episode_reward=1651.60 +/- 233.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=183000, episode_reward=1380.40 +/- 155.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=184000, episode_reward=1772.00 +/- 242.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=185000, episode_reward=1684.20 +/- 251.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=186000, episode_reward=1659.20 +/- 209.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=187000, episode_reward=1721.40 +/- 151.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=188000, episode_reward=1488.00 +/- 396.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=189000, episode_reward=1800.20 +/- 251.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=190000, episode_reward=1615.00 +/- 226.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=191000, episode_reward=1755.00 +/- 119.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=192000, episode_reward=1715.80 +/- 186.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=193000, episode_reward=1552.20 +/- 149.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=194000, episode_reward=1541.80 +/- 138.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=195000, episode_reward=1640.20 +/- 222.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=196000, episode_reward=1577.40 +/- 178.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=197000, episode_reward=1685.00 +/- 279.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=198000, episode_reward=1654.80 +/- 187.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=199000, episode_reward=1509.00 +/- 86.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=200000, episode_reward=1747.00 +/- 266.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=201000, episode_reward=1565.20 +/- 168.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=202000, episode_reward=1483.80 +/- 285.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=203000, episode_reward=1659.80 +/- 91.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=204000, episode_reward=1495.80 +/- 277.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=205000, episode_reward=1401.20 +/- 72.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=206000, episode_reward=1538.80 +/- 192.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=207000, episode_reward=1677.20 +/- 174.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=208000, episode_reward=1646.40 +/- 53.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=209000, episode_reward=1645.60 +/- 300.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=210000, episode_reward=1655.60 +/- 221.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=211000, episode_reward=1588.20 +/- 178.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=212000, episode_reward=1674.80 +/- 105.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=213000, episode_reward=1388.80 +/- 409.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=214000, episode_reward=1787.20 +/- 190.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=215000, episode_reward=1462.60 +/- 250.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=216000, episode_reward=1683.00 +/- 225.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=217000, episode_reward=1631.60 +/- 348.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=218000, episode_reward=1797.60 +/- 272.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=219000, episode_reward=1775.80 +/- 102.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=220000, episode_reward=1754.40 +/- 71.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=221000, episode_reward=1580.80 +/- 159.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=222000, episode_reward=1560.00 +/- 124.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=223000, episode_reward=1515.00 +/- 397.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=224000, episode_reward=1501.60 +/- 148.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=225000, episode_reward=1646.20 +/- 200.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=226000, episode_reward=1640.60 +/- 264.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=227000, episode_reward=1690.40 +/- 108.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=228000, episode_reward=1375.60 +/- 282.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=229000, episode_reward=1597.00 +/- 288.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=230000, episode_reward=1375.00 +/- 154.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=231000, episode_reward=1700.40 +/- 221.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=232000, episode_reward=1539.20 +/- 422.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=233000, episode_reward=1626.20 +/- 143.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=234000, episode_reward=1486.00 +/- 112.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=235000, episode_reward=1605.60 +/- 78.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=236000, episode_reward=1576.20 +/- 172.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=237000, episode_reward=1637.60 +/- 241.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=238000, episode_reward=1526.80 +/- 165.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=239000, episode_reward=1614.80 +/- 397.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=240000, episode_reward=1549.20 +/- 112.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=241000, episode_reward=1652.00 +/- 139.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=242000, episode_reward=1643.00 +/- 143.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=243000, episode_reward=1716.40 +/- 171.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=244000, episode_reward=1634.80 +/- 154.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=245000, episode_reward=1570.20 +/- 238.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=246000, episode_reward=1314.40 +/- 226.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=247000, episode_reward=1669.20 +/- 125.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=248000, episode_reward=1442.20 +/- 121.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=249000, episode_reward=1754.20 +/- 110.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=250000, episode_reward=1454.00 +/- 157.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=251000, episode_reward=1544.80 +/- 352.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=252000, episode_reward=1477.60 +/- 374.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=253000, episode_reward=1554.40 +/- 205.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=254000, episode_reward=1619.40 +/- 242.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=255000, episode_reward=1540.80 +/- 183.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=256000, episode_reward=1428.80 +/- 193.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=257000, episode_reward=1543.80 +/- 182.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=258000, episode_reward=1634.20 +/- 188.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=259000, episode_reward=1566.40 +/- 125.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=260000, episode_reward=1545.80 +/- 285.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=261000, episode_reward=1522.20 +/- 263.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=262000, episode_reward=1518.20 +/- 313.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=263000, episode_reward=1671.60 +/- 198.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=264000, episode_reward=1637.40 +/- 372.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=265000, episode_reward=1643.40 +/- 213.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=266000, episode_reward=1835.80 +/- 187.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=267000, episode_reward=1520.00 +/- 330.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=268000, episode_reward=1528.40 +/- 124.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=269000, episode_reward=1524.40 +/- 144.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=270000, episode_reward=1729.00 +/- 196.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=271000, episode_reward=1745.00 +/- 131.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=272000, episode_reward=1505.20 +/- 195.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=273000, episode_reward=1673.40 +/- 257.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=274000, episode_reward=1717.40 +/- 198.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=275000, episode_reward=1524.60 +/- 111.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=276000, episode_reward=1635.40 +/- 288.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=277000, episode_reward=1488.80 +/- 231.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=278000, episode_reward=1583.20 +/- 193.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=279000, episode_reward=1731.40 +/- 127.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=280000, episode_reward=1750.40 +/- 207.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=281000, episode_reward=1586.80 +/- 260.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=282000, episode_reward=1534.40 +/- 195.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=283000, episode_reward=1415.20 +/- 265.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=284000, episode_reward=1644.60 +/- 226.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=285000, episode_reward=1605.00 +/- 190.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=286000, episode_reward=1492.60 +/- 368.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=287000, episode_reward=1746.60 +/- 222.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=288000, episode_reward=1561.80 +/- 177.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=289000, episode_reward=1606.60 +/- 169.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=290000, episode_reward=1753.40 +/- 193.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=291000, episode_reward=1719.60 +/- 97.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=292000, episode_reward=1760.00 +/- 139.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=293000, episode_reward=1691.80 +/- 128.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=294000, episode_reward=1540.40 +/- 244.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=295000, episode_reward=1551.80 +/- 167.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=296000, episode_reward=1491.00 +/- 327.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=297000, episode_reward=1691.00 +/- 179.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=298000, episode_reward=1749.80 +/- 177.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=299000, episode_reward=1647.80 +/- 263.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=300000, episode_reward=1584.60 +/- 188.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=301000, episode_reward=1664.20 +/- 198.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=302000, episode_reward=1383.80 +/- 249.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=303000, episode_reward=1642.00 +/- 255.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=304000, episode_reward=1601.20 +/- 269.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=305000, episode_reward=1502.40 +/- 165.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=306000, episode_reward=1731.00 +/- 176.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=307000, episode_reward=1692.40 +/- 141.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=308000, episode_reward=1496.00 +/- 333.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=309000, episode_reward=1631.00 +/- 328.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=310000, episode_reward=1527.40 +/- 190.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=311000, episode_reward=1675.00 +/- 227.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=312000, episode_reward=1560.60 +/- 233.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=313000, episode_reward=1754.60 +/- 123.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=314000, episode_reward=1676.40 +/- 126.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=315000, episode_reward=1527.00 +/- 89.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=316000, episode_reward=1608.80 +/- 112.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=317000, episode_reward=1382.60 +/- 453.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=318000, episode_reward=1451.00 +/- 274.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=319000, episode_reward=1706.80 +/- 266.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=320000, episode_reward=1552.20 +/- 121.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=321000, episode_reward=1586.60 +/- 389.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=322000, episode_reward=1538.20 +/- 280.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=323000, episode_reward=1669.00 +/- 98.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=324000, episode_reward=1510.40 +/- 227.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=325000, episode_reward=1576.40 +/- 167.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=326000, episode_reward=1731.40 +/- 102.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=327000, episode_reward=1576.20 +/- 171.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=328000, episode_reward=1656.00 +/- 268.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=329000, episode_reward=1619.80 +/- 158.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=330000, episode_reward=1672.00 +/- 190.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=331000, episode_reward=1622.40 +/- 58.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=332000, episode_reward=1673.60 +/- 272.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=333000, episode_reward=1599.00 +/- 178.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=334000, episode_reward=1647.60 +/- 164.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=335000, episode_reward=1599.80 +/- 334.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=336000, episode_reward=1647.00 +/- 247.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=337000, episode_reward=1748.20 +/- 160.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=338000, episode_reward=1509.00 +/- 459.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=339000, episode_reward=1583.20 +/- 331.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=340000, episode_reward=1475.60 +/- 269.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=341000, episode_reward=1760.20 +/- 88.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=342000, episode_reward=1496.60 +/- 204.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=343000, episode_reward=1489.60 +/- 291.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=344000, episode_reward=1742.00 +/- 215.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=345000, episode_reward=1308.80 +/- 203.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=346000, episode_reward=1628.00 +/- 215.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=347000, episode_reward=1722.60 +/- 180.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=348000, episode_reward=1679.00 +/- 316.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=349000, episode_reward=1648.80 +/- 252.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=350000, episode_reward=1667.80 +/- 83.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=351000, episode_reward=1838.20 +/- 141.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=352000, episode_reward=1513.00 +/- 152.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=353000, episode_reward=1744.80 +/- 180.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=354000, episode_reward=1495.80 +/- 216.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=355000, episode_reward=1635.80 +/- 168.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=356000, episode_reward=1504.60 +/- 337.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=357000, episode_reward=1601.60 +/- 349.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=358000, episode_reward=1705.00 +/- 121.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=359000, episode_reward=1534.40 +/- 219.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=360000, episode_reward=1683.60 +/- 154.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=361000, episode_reward=1657.60 +/- 130.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=362000, episode_reward=1548.40 +/- 254.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=363000, episode_reward=1696.60 +/- 216.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=364000, episode_reward=1581.00 +/- 178.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=365000, episode_reward=1661.20 +/- 184.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=366000, episode_reward=1629.40 +/- 188.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=367000, episode_reward=1682.60 +/- 218.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=368000, episode_reward=1726.00 +/- 171.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=369000, episode_reward=1680.40 +/- 117.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=370000, episode_reward=1678.60 +/- 286.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=371000, episode_reward=1670.20 +/- 253.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=372000, episode_reward=1569.20 +/- 212.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=373000, episode_reward=1566.80 +/- 286.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=374000, episode_reward=1677.40 +/- 129.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=375000, episode_reward=1612.80 +/- 293.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=376000, episode_reward=1574.80 +/- 315.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=377000, episode_reward=1673.40 +/- 157.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=378000, episode_reward=1543.80 +/- 232.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=379000, episode_reward=1511.60 +/- 199.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=380000, episode_reward=1766.20 +/- 185.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=381000, episode_reward=1576.20 +/- 172.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=382000, episode_reward=1689.40 +/- 202.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=383000, episode_reward=1739.80 +/- 159.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=384000, episode_reward=1393.20 +/- 113.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=385000, episode_reward=1578.60 +/- 150.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=386000, episode_reward=1344.60 +/- 376.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=387000, episode_reward=1559.80 +/- 89.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=388000, episode_reward=1587.00 +/- 266.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=389000, episode_reward=1517.20 +/- 184.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=390000, episode_reward=1478.60 +/- 376.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=391000, episode_reward=1647.60 +/- 168.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=392000, episode_reward=1669.60 +/- 139.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=393000, episode_reward=1607.00 +/- 231.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=394000, episode_reward=1669.00 +/- 199.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=395000, episode_reward=1748.80 +/- 242.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=396000, episode_reward=1794.40 +/- 208.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=397000, episode_reward=1806.20 +/- 224.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=398000, episode_reward=1807.80 +/- 151.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=399000, episode_reward=1632.80 +/- 171.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=400000, episode_reward=1694.20 +/- 262.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=401000, episode_reward=1572.80 +/- 187.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=402000, episode_reward=1595.00 +/- 62.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=403000, episode_reward=1669.20 +/- 174.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=404000, episode_reward=1710.80 +/- 228.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=405000, episode_reward=1723.20 +/- 167.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=406000, episode_reward=1720.80 +/- 223.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=407000, episode_reward=1808.00 +/- 169.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=408000, episode_reward=1724.60 +/- 231.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=409000, episode_reward=1637.40 +/- 149.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=410000, episode_reward=1606.00 +/- 147.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=411000, episode_reward=1543.20 +/- 206.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=412000, episode_reward=1436.00 +/- 272.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=413000, episode_reward=1672.60 +/- 166.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=414000, episode_reward=1527.60 +/- 328.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=415000, episode_reward=1432.80 +/- 228.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=416000, episode_reward=1656.60 +/- 189.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=417000, episode_reward=1653.40 +/- 199.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=418000, episode_reward=1639.40 +/- 298.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=419000, episode_reward=1781.20 +/- 62.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=420000, episode_reward=1551.20 +/- 247.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=421000, episode_reward=1616.00 +/- 186.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=422000, episode_reward=1659.20 +/- 278.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=423000, episode_reward=1779.80 +/- 189.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=424000, episode_reward=1489.20 +/- 158.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=425000, episode_reward=1632.00 +/- 386.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=426000, episode_reward=1677.80 +/- 188.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=427000, episode_reward=1512.60 +/- 186.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=428000, episode_reward=1670.00 +/- 181.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=429000, episode_reward=1829.40 +/- 162.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=430000, episode_reward=1664.40 +/- 262.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=431000, episode_reward=1699.80 +/- 196.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=432000, episode_reward=1793.00 +/- 201.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=433000, episode_reward=1772.20 +/- 256.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=434000, episode_reward=1638.60 +/- 93.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=435000, episode_reward=1696.20 +/- 281.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=436000, episode_reward=1559.00 +/- 440.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=437000, episode_reward=1675.20 +/- 262.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=438000, episode_reward=1600.40 +/- 337.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=439000, episode_reward=1711.40 +/- 191.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=440000, episode_reward=1676.80 +/- 231.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=441000, episode_reward=1584.80 +/- 174.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=442000, episode_reward=1625.20 +/- 449.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=443000, episode_reward=1654.60 +/- 239.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=444000, episode_reward=1800.80 +/- 260.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=445000, episode_reward=1690.20 +/- 140.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=446000, episode_reward=1829.20 +/- 126.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=447000, episode_reward=1641.40 +/- 142.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=448000, episode_reward=1754.40 +/- 264.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=449000, episode_reward=1753.80 +/- 234.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=450000, episode_reward=1598.80 +/- 328.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=451000, episode_reward=1556.20 +/- 277.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=452000, episode_reward=1430.60 +/- 264.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=453000, episode_reward=1592.20 +/- 234.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=454000, episode_reward=1519.40 +/- 214.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=455000, episode_reward=1677.80 +/- 180.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=456000, episode_reward=1544.40 +/- 400.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=457000, episode_reward=1721.40 +/- 187.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=458000, episode_reward=1570.20 +/- 155.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=459000, episode_reward=1768.60 +/- 272.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=460000, episode_reward=1693.20 +/- 209.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=461000, episode_reward=1766.60 +/- 76.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=462000, episode_reward=1761.80 +/- 152.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=463000, episode_reward=1718.00 +/- 92.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=464000, episode_reward=1796.60 +/- 187.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=465000, episode_reward=1657.00 +/- 222.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=466000, episode_reward=1633.80 +/- 233.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=467000, episode_reward=1713.60 +/- 263.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=468000, episode_reward=1556.00 +/- 200.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=469000, episode_reward=1774.80 +/- 145.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=470000, episode_reward=1688.20 +/- 252.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=471000, episode_reward=1787.20 +/- 156.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=472000, episode_reward=1844.80 +/- 91.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=473000, episode_reward=1576.80 +/- 230.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=474000, episode_reward=1706.20 +/- 262.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=475000, episode_reward=1719.60 +/- 124.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=476000, episode_reward=1692.20 +/- 396.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=477000, episode_reward=1754.00 +/- 146.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=478000, episode_reward=1756.80 +/- 249.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=479000, episode_reward=1747.00 +/- 242.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=480000, episode_reward=1724.60 +/- 36.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=481000, episode_reward=1719.20 +/- 245.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=482000, episode_reward=1524.80 +/- 299.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=483000, episode_reward=1514.40 +/- 196.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=484000, episode_reward=1714.00 +/- 160.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=485000, episode_reward=1625.00 +/- 218.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=486000, episode_reward=1704.00 +/- 146.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=487000, episode_reward=1664.80 +/- 323.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=488000, episode_reward=1436.60 +/- 405.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=489000, episode_reward=1546.40 +/- 137.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=490000, episode_reward=1846.80 +/- 210.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=491000, episode_reward=1592.00 +/- 294.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=492000, episode_reward=1672.00 +/- 275.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=493000, episode_reward=1841.20 +/- 323.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=494000, episode_reward=1764.60 +/- 150.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=495000, episode_reward=1703.80 +/- 229.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=496000, episode_reward=1640.00 +/- 180.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=497000, episode_reward=1682.80 +/- 224.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=498000, episode_reward=1571.00 +/- 212.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=499000, episode_reward=1697.80 +/- 190.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=500000, episode_reward=1823.00 +/- 184.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=501000, episode_reward=1796.80 +/- 247.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=502000, episode_reward=1836.00 +/- 146.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=503000, episode_reward=1605.00 +/- 196.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=504000, episode_reward=1510.00 +/- 190.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=505000, episode_reward=1615.40 +/- 78.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=506000, episode_reward=1825.20 +/- 155.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=507000, episode_reward=1810.40 +/- 250.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=508000, episode_reward=1527.20 +/- 193.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=509000, episode_reward=1757.40 +/- 26.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=510000, episode_reward=1761.60 +/- 273.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=511000, episode_reward=1789.40 +/- 175.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=512000, episode_reward=1649.20 +/- 264.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=513000, episode_reward=1687.60 +/- 227.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=514000, episode_reward=1528.60 +/- 233.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=515000, episode_reward=1606.00 +/- 119.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=516000, episode_reward=1762.60 +/- 137.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=517000, episode_reward=1648.20 +/- 142.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=518000, episode_reward=1837.60 +/- 210.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=519000, episode_reward=1662.60 +/- 352.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=520000, episode_reward=1625.80 +/- 319.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=521000, episode_reward=1769.00 +/- 120.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=522000, episode_reward=1745.60 +/- 299.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=523000, episode_reward=1477.80 +/- 279.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=524000, episode_reward=1644.80 +/- 98.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=525000, episode_reward=1715.00 +/- 86.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=526000, episode_reward=1772.80 +/- 253.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=527000, episode_reward=1642.40 +/- 177.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=528000, episode_reward=1536.60 +/- 209.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=529000, episode_reward=1824.20 +/- 118.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=530000, episode_reward=1744.20 +/- 141.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=531000, episode_reward=1764.80 +/- 173.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=532000, episode_reward=1731.80 +/- 168.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=533000, episode_reward=1663.00 +/- 138.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=534000, episode_reward=1621.60 +/- 227.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=535000, episode_reward=1775.00 +/- 117.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=536000, episode_reward=1689.40 +/- 137.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=537000, episode_reward=1558.20 +/- 186.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=538000, episode_reward=1811.60 +/- 164.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=539000, episode_reward=1872.80 +/- 102.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=540000, episode_reward=1827.80 +/- 158.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=541000, episode_reward=1728.60 +/- 208.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=542000, episode_reward=1715.20 +/- 198.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=543000, episode_reward=1681.60 +/- 285.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=544000, episode_reward=1795.60 +/- 170.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=545000, episode_reward=1694.60 +/- 203.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=546000, episode_reward=1991.80 +/- 366.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=547000, episode_reward=1779.20 +/- 365.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=548000, episode_reward=1635.20 +/- 388.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=549000, episode_reward=1509.20 +/- 497.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=550000, episode_reward=1720.00 +/- 297.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=551000, episode_reward=1756.20 +/- 264.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=552000, episode_reward=1611.00 +/- 350.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=553000, episode_reward=1705.60 +/- 212.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=554000, episode_reward=1272.20 +/- 231.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=555000, episode_reward=1416.00 +/- 274.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=556000, episode_reward=1618.00 +/- 141.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=557000, episode_reward=1767.20 +/- 158.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=558000, episode_reward=1698.60 +/- 252.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=559000, episode_reward=1673.40 +/- 354.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=560000, episode_reward=1722.60 +/- 236.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=561000, episode_reward=1773.20 +/- 125.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=562000, episode_reward=1690.20 +/- 143.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=563000, episode_reward=1782.20 +/- 108.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=564000, episode_reward=1688.20 +/- 190.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=565000, episode_reward=1635.20 +/- 147.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=566000, episode_reward=1838.20 +/- 158.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=567000, episode_reward=1735.40 +/- 269.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=568000, episode_reward=1804.80 +/- 215.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=569000, episode_reward=1785.60 +/- 393.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=570000, episode_reward=1588.80 +/- 126.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=571000, episode_reward=1550.40 +/- 319.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=572000, episode_reward=1464.00 +/- 345.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=573000, episode_reward=1495.40 +/- 333.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=574000, episode_reward=1802.20 +/- 374.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=575000, episode_reward=1673.20 +/- 287.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=576000, episode_reward=1402.20 +/- 331.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=577000, episode_reward=1665.00 +/- 206.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=578000, episode_reward=1718.20 +/- 182.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=579000, episode_reward=1581.40 +/- 237.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=580000, episode_reward=1750.60 +/- 306.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=581000, episode_reward=1397.00 +/- 293.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=582000, episode_reward=1649.80 +/- 163.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=583000, episode_reward=1636.80 +/- 259.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=584000, episode_reward=1447.80 +/- 241.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=585000, episode_reward=1538.40 +/- 194.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=586000, episode_reward=1842.20 +/- 199.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=587000, episode_reward=1775.20 +/- 227.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=588000, episode_reward=1762.60 +/- 191.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=589000, episode_reward=1746.80 +/- 190.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=590000, episode_reward=1764.00 +/- 322.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=591000, episode_reward=1867.20 +/- 159.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=592000, episode_reward=1649.80 +/- 157.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=593000, episode_reward=1687.40 +/- 225.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=594000, episode_reward=1415.60 +/- 284.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=595000, episode_reward=1852.60 +/- 308.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=596000, episode_reward=1259.00 +/- 162.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=597000, episode_reward=1733.00 +/- 193.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=598000, episode_reward=1540.80 +/- 223.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=599000, episode_reward=1455.20 +/- 146.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=600000, episode_reward=1573.00 +/- 252.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=601000, episode_reward=1750.40 +/- 265.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=602000, episode_reward=1539.60 +/- 224.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=603000, episode_reward=1438.20 +/- 311.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=604000, episode_reward=1593.00 +/- 84.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=605000, episode_reward=1741.20 +/- 222.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=606000, episode_reward=1547.40 +/- 213.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=607000, episode_reward=1866.60 +/- 134.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=608000, episode_reward=1525.40 +/- 385.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=609000, episode_reward=1599.80 +/- 176.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=610000, episode_reward=1615.20 +/- 177.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=611000, episode_reward=1642.80 +/- 304.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=612000, episode_reward=1806.20 +/- 94.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=613000, episode_reward=1348.80 +/- 208.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=614000, episode_reward=1542.20 +/- 209.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=615000, episode_reward=1507.00 +/- 319.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=616000, episode_reward=1641.40 +/- 280.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=617000, episode_reward=1495.00 +/- 338.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=618000, episode_reward=1544.60 +/- 172.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=619000, episode_reward=1767.60 +/- 197.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=620000, episode_reward=1664.60 +/- 277.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=621000, episode_reward=1813.80 +/- 94.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=622000, episode_reward=1520.40 +/- 149.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=623000, episode_reward=1784.20 +/- 237.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=624000, episode_reward=1707.60 +/- 125.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=625000, episode_reward=1751.60 +/- 285.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=626000, episode_reward=1458.80 +/- 258.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=627000, episode_reward=1808.80 +/- 108.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=628000, episode_reward=1625.60 +/- 194.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=629000, episode_reward=1583.80 +/- 220.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=630000, episode_reward=1584.40 +/- 154.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=631000, episode_reward=1579.80 +/- 336.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=632000, episode_reward=1709.80 +/- 179.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=633000, episode_reward=1677.60 +/- 256.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=634000, episode_reward=1727.60 +/- 262.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=635000, episode_reward=1673.20 +/- 132.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=636000, episode_reward=1701.60 +/- 194.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=637000, episode_reward=1674.00 +/- 327.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=638000, episode_reward=1523.20 +/- 385.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=639000, episode_reward=1528.20 +/- 240.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=640000, episode_reward=1563.00 +/- 307.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=641000, episode_reward=1633.60 +/- 346.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=642000, episode_reward=1414.40 +/- 209.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=643000, episode_reward=1697.20 +/- 142.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=644000, episode_reward=1639.80 +/- 96.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=645000, episode_reward=1533.20 +/- 183.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=646000, episode_reward=1786.00 +/- 182.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=647000, episode_reward=1744.40 +/- 49.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=648000, episode_reward=1618.40 +/- 294.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=649000, episode_reward=1405.60 +/- 314.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=650000, episode_reward=1760.00 +/- 249.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=651000, episode_reward=1725.60 +/- 177.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=652000, episode_reward=1733.80 +/- 226.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=653000, episode_reward=1773.80 +/- 195.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=654000, episode_reward=1677.60 +/- 197.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=655000, episode_reward=1415.40 +/- 159.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=656000, episode_reward=1821.20 +/- 233.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=657000, episode_reward=1641.40 +/- 235.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=658000, episode_reward=1565.00 +/- 195.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=659000, episode_reward=1768.20 +/- 163.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=660000, episode_reward=1503.00 +/- 291.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=661000, episode_reward=1519.80 +/- 258.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=662000, episode_reward=1626.00 +/- 222.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=663000, episode_reward=1683.60 +/- 359.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=664000, episode_reward=1778.20 +/- 230.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=665000, episode_reward=1436.80 +/- 263.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=666000, episode_reward=1698.40 +/- 218.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=667000, episode_reward=1551.00 +/- 308.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=668000, episode_reward=1653.60 +/- 185.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=669000, episode_reward=1648.80 +/- 262.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=670000, episode_reward=1550.20 +/- 213.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=671000, episode_reward=1595.40 +/- 265.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=672000, episode_reward=1376.20 +/- 392.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=673000, episode_reward=1747.00 +/- 283.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=674000, episode_reward=1699.60 +/- 191.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=675000, episode_reward=1525.40 +/- 248.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=676000, episode_reward=1537.80 +/- 262.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=677000, episode_reward=1751.80 +/- 383.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=678000, episode_reward=1682.20 +/- 132.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=679000, episode_reward=1713.20 +/- 192.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=680000, episode_reward=1780.40 +/- 155.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=681000, episode_reward=1607.00 +/- 230.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=682000, episode_reward=1673.40 +/- 258.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=683000, episode_reward=1664.80 +/- 185.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=684000, episode_reward=1557.80 +/- 198.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=685000, episode_reward=1718.80 +/- 81.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=686000, episode_reward=1729.60 +/- 294.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=687000, episode_reward=1806.00 +/- 309.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=688000, episode_reward=1701.60 +/- 280.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=689000, episode_reward=1491.60 +/- 267.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=690000, episode_reward=1536.60 +/- 288.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=691000, episode_reward=1611.80 +/- 221.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=692000, episode_reward=1637.00 +/- 68.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=693000, episode_reward=1794.40 +/- 155.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=694000, episode_reward=1490.40 +/- 283.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=695000, episode_reward=1589.60 +/- 224.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=696000, episode_reward=1527.60 +/- 142.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=697000, episode_reward=1705.80 +/- 154.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=698000, episode_reward=1356.80 +/- 355.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=699000, episode_reward=1458.00 +/- 174.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=700000, episode_reward=1770.00 +/- 261.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=701000, episode_reward=1618.80 +/- 192.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=702000, episode_reward=1721.20 +/- 247.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=703000, episode_reward=1606.40 +/- 343.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=704000, episode_reward=1570.00 +/- 322.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=705000, episode_reward=1538.40 +/- 323.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=706000, episode_reward=1656.20 +/- 102.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=707000, episode_reward=1624.60 +/- 213.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=708000, episode_reward=1747.40 +/- 139.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=709000, episode_reward=1638.00 +/- 228.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=710000, episode_reward=1736.00 +/- 181.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=711000, episode_reward=1532.80 +/- 304.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=712000, episode_reward=1713.00 +/- 376.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=713000, episode_reward=1653.00 +/- 250.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=714000, episode_reward=1711.00 +/- 206.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=715000, episode_reward=1648.40 +/- 313.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=716000, episode_reward=1841.40 +/- 66.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=717000, episode_reward=1665.40 +/- 202.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=718000, episode_reward=1674.40 +/- 176.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=719000, episode_reward=1643.20 +/- 301.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=720000, episode_reward=1298.00 +/- 349.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=721000, episode_reward=1514.20 +/- 225.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=722000, episode_reward=1710.80 +/- 30.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=723000, episode_reward=1816.60 +/- 149.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=724000, episode_reward=1608.20 +/- 238.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=725000, episode_reward=1839.00 +/- 119.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=726000, episode_reward=1376.40 +/- 185.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=727000, episode_reward=1579.80 +/- 287.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=728000, episode_reward=1671.80 +/- 224.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=729000, episode_reward=1725.00 +/- 168.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=730000, episode_reward=1441.20 +/- 300.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=731000, episode_reward=1739.20 +/- 267.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=732000, episode_reward=1648.20 +/- 277.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=733000, episode_reward=1731.40 +/- 195.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=734000, episode_reward=1737.80 +/- 248.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=735000, episode_reward=1638.60 +/- 342.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=736000, episode_reward=1551.00 +/- 98.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=737000, episode_reward=1706.60 +/- 81.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=738000, episode_reward=1597.40 +/- 204.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=739000, episode_reward=1671.60 +/- 233.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=740000, episode_reward=1718.20 +/- 132.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=741000, episode_reward=1736.40 +/- 255.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=742000, episode_reward=1625.00 +/- 296.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=743000, episode_reward=1805.60 +/- 353.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=744000, episode_reward=1747.80 +/- 324.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=745000, episode_reward=1516.00 +/- 334.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=746000, episode_reward=1621.00 +/- 353.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=747000, episode_reward=1795.00 +/- 186.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=748000, episode_reward=1721.00 +/- 218.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=749000, episode_reward=1701.00 +/- 194.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=750000, episode_reward=1808.00 +/- 140.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=751000, episode_reward=1653.80 +/- 153.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=752000, episode_reward=1750.60 +/- 201.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=753000, episode_reward=1589.40 +/- 190.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=754000, episode_reward=1783.00 +/- 234.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=755000, episode_reward=1670.60 +/- 178.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=756000, episode_reward=1697.40 +/- 83.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=757000, episode_reward=1781.80 +/- 292.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=758000, episode_reward=1643.60 +/- 179.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=759000, episode_reward=1717.20 +/- 163.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=760000, episode_reward=1676.80 +/- 205.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=761000, episode_reward=1719.80 +/- 89.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=762000, episode_reward=1528.60 +/- 213.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=763000, episode_reward=1642.60 +/- 222.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=764000, episode_reward=1609.20 +/- 374.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=765000, episode_reward=1819.80 +/- 96.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=766000, episode_reward=1824.20 +/- 150.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=767000, episode_reward=1671.00 +/- 359.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=768000, episode_reward=1653.40 +/- 364.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=769000, episode_reward=1701.20 +/- 289.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=770000, episode_reward=1697.00 +/- 249.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=771000, episode_reward=1576.00 +/- 330.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=772000, episode_reward=1727.80 +/- 119.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=773000, episode_reward=1689.60 +/- 113.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=774000, episode_reward=1688.80 +/- 263.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=775000, episode_reward=1532.40 +/- 207.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=776000, episode_reward=1535.60 +/- 146.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=777000, episode_reward=1609.20 +/- 212.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=778000, episode_reward=1575.80 +/- 216.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=779000, episode_reward=1715.00 +/- 207.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=780000, episode_reward=1698.00 +/- 70.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=781000, episode_reward=1662.60 +/- 383.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=782000, episode_reward=1702.40 +/- 174.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=783000, episode_reward=1649.40 +/- 286.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=784000, episode_reward=1677.20 +/- 186.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=785000, episode_reward=1845.60 +/- 100.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=786000, episode_reward=1888.20 +/- 146.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=787000, episode_reward=1750.40 +/- 176.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=788000, episode_reward=1773.60 +/- 244.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=789000, episode_reward=1825.40 +/- 330.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=790000, episode_reward=1772.80 +/- 124.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=791000, episode_reward=1641.80 +/- 125.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=792000, episode_reward=1667.40 +/- 201.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=793000, episode_reward=1476.20 +/- 163.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=794000, episode_reward=1553.40 +/- 233.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=795000, episode_reward=1494.60 +/- 87.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=796000, episode_reward=1718.40 +/- 296.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=797000, episode_reward=1811.20 +/- 136.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=798000, episode_reward=1714.20 +/- 209.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=799000, episode_reward=1616.80 +/- 215.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=800000, episode_reward=1526.60 +/- 143.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=801000, episode_reward=1747.00 +/- 118.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=802000, episode_reward=1633.40 +/- 264.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=803000, episode_reward=1675.80 +/- 147.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=804000, episode_reward=1561.40 +/- 183.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=805000, episode_reward=1676.60 +/- 264.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=806000, episode_reward=1517.40 +/- 246.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=807000, episode_reward=1730.40 +/- 260.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=808000, episode_reward=1593.60 +/- 210.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=809000, episode_reward=1642.00 +/- 192.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=810000, episode_reward=1706.60 +/- 245.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=811000, episode_reward=1643.00 +/- 219.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=812000, episode_reward=1662.80 +/- 204.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=813000, episode_reward=1707.00 +/- 380.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=814000, episode_reward=1604.00 +/- 144.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=815000, episode_reward=1618.40 +/- 98.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=816000, episode_reward=1759.00 +/- 295.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=817000, episode_reward=1687.40 +/- 261.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=818000, episode_reward=1620.40 +/- 248.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=819000, episode_reward=1674.20 +/- 263.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=820000, episode_reward=1700.40 +/- 169.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=821000, episode_reward=1847.80 +/- 221.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=822000, episode_reward=1780.60 +/- 162.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=823000, episode_reward=1740.60 +/- 116.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=824000, episode_reward=1703.60 +/- 114.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=825000, episode_reward=1464.20 +/- 386.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=826000, episode_reward=1583.20 +/- 207.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=827000, episode_reward=1775.20 +/- 214.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=828000, episode_reward=1619.00 +/- 353.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=829000, episode_reward=1759.60 +/- 98.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=830000, episode_reward=1592.40 +/- 296.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=831000, episode_reward=1577.60 +/- 211.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=832000, episode_reward=1765.60 +/- 213.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=833000, episode_reward=1744.40 +/- 154.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=834000, episode_reward=1645.40 +/- 253.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=835000, episode_reward=1673.80 +/- 238.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=836000, episode_reward=1769.80 +/- 147.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=837000, episode_reward=1544.00 +/- 320.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=838000, episode_reward=1589.40 +/- 267.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=839000, episode_reward=1794.60 +/- 190.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=840000, episode_reward=1680.40 +/- 182.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=841000, episode_reward=1675.20 +/- 73.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=842000, episode_reward=1708.40 +/- 89.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=843000, episode_reward=1599.40 +/- 281.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=844000, episode_reward=1764.00 +/- 171.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=845000, episode_reward=1756.20 +/- 207.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=846000, episode_reward=1741.40 +/- 197.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=847000, episode_reward=1865.80 +/- 64.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=848000, episode_reward=1613.20 +/- 224.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=849000, episode_reward=1638.00 +/- 266.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=850000, episode_reward=1765.20 +/- 248.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=851000, episode_reward=1975.00 +/- 154.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=852000, episode_reward=1622.20 +/- 59.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=853000, episode_reward=1703.20 +/- 127.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=854000, episode_reward=1636.60 +/- 340.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=855000, episode_reward=1840.40 +/- 78.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=856000, episode_reward=1561.60 +/- 85.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=857000, episode_reward=1435.00 +/- 137.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=858000, episode_reward=1728.40 +/- 116.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=859000, episode_reward=1760.20 +/- 117.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=860000, episode_reward=1788.40 +/- 318.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=861000, episode_reward=1804.80 +/- 101.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=862000, episode_reward=1674.40 +/- 223.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=863000, episode_reward=1709.00 +/- 166.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=864000, episode_reward=1785.40 +/- 217.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=865000, episode_reward=1636.20 +/- 246.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=866000, episode_reward=1640.20 +/- 478.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=867000, episode_reward=1705.60 +/- 133.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=868000, episode_reward=1735.40 +/- 341.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=869000, episode_reward=1623.40 +/- 210.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=870000, episode_reward=1529.00 +/- 236.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=871000, episode_reward=1655.40 +/- 447.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=872000, episode_reward=1599.60 +/- 141.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=873000, episode_reward=1472.20 +/- 235.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=874000, episode_reward=1651.00 +/- 203.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=875000, episode_reward=1596.20 +/- 310.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=876000, episode_reward=1549.60 +/- 196.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=877000, episode_reward=1835.00 +/- 120.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=878000, episode_reward=1729.80 +/- 156.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=879000, episode_reward=1833.20 +/- 147.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=880000, episode_reward=1629.40 +/- 229.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=881000, episode_reward=1710.20 +/- 217.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=882000, episode_reward=1696.20 +/- 159.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=883000, episode_reward=1874.60 +/- 155.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=884000, episode_reward=1676.00 +/- 258.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=885000, episode_reward=1840.00 +/- 57.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=886000, episode_reward=1913.20 +/- 198.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=887000, episode_reward=1680.00 +/- 270.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=888000, episode_reward=1753.20 +/- 162.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=889000, episode_reward=1715.80 +/- 288.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=890000, episode_reward=1754.00 +/- 186.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=891000, episode_reward=1887.80 +/- 119.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=892000, episode_reward=1666.20 +/- 408.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=893000, episode_reward=1748.20 +/- 121.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=894000, episode_reward=1741.00 +/- 228.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=895000, episode_reward=1741.00 +/- 206.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=896000, episode_reward=1698.00 +/- 136.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=897000, episode_reward=1664.60 +/- 172.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=898000, episode_reward=1509.80 +/- 350.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=899000, episode_reward=1501.20 +/- 204.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=900000, episode_reward=1789.20 +/- 123.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=901000, episode_reward=1844.60 +/- 93.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=902000, episode_reward=1776.40 +/- 122.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=903000, episode_reward=1621.20 +/- 294.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=904000, episode_reward=1781.60 +/- 135.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=905000, episode_reward=1695.20 +/- 210.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=906000, episode_reward=1541.00 +/- 268.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=907000, episode_reward=1814.00 +/- 264.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=908000, episode_reward=1754.40 +/- 140.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=909000, episode_reward=1709.80 +/- 69.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=910000, episode_reward=1650.80 +/- 213.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=911000, episode_reward=1568.20 +/- 131.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=912000, episode_reward=1732.00 +/- 154.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=913000, episode_reward=1700.00 +/- 232.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=914000, episode_reward=1546.40 +/- 219.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=915000, episode_reward=1678.80 +/- 273.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=916000, episode_reward=1713.20 +/- 216.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=917000, episode_reward=1742.40 +/- 182.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=918000, episode_reward=1751.60 +/- 177.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=919000, episode_reward=1631.20 +/- 150.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=920000, episode_reward=1690.20 +/- 404.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=921000, episode_reward=1751.40 +/- 173.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=922000, episode_reward=1719.20 +/- 175.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=923000, episode_reward=1569.80 +/- 137.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=924000, episode_reward=1447.80 +/- 208.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=925000, episode_reward=1623.80 +/- 162.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=926000, episode_reward=1399.60 +/- 192.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=927000, episode_reward=1538.00 +/- 335.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=928000, episode_reward=1721.40 +/- 115.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=929000, episode_reward=1700.60 +/- 154.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=930000, episode_reward=1813.00 +/- 145.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=931000, episode_reward=1604.60 +/- 231.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=932000, episode_reward=1834.60 +/- 228.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=933000, episode_reward=1886.60 +/- 295.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=934000, episode_reward=1514.40 +/- 275.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=935000, episode_reward=1455.60 +/- 418.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=936000, episode_reward=1675.00 +/- 350.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=937000, episode_reward=1596.80 +/- 366.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=938000, episode_reward=1621.80 +/- 349.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=939000, episode_reward=1663.40 +/- 104.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=940000, episode_reward=1765.00 +/- 186.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=941000, episode_reward=1448.80 +/- 168.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=942000, episode_reward=1769.40 +/- 203.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=943000, episode_reward=1498.80 +/- 349.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=944000, episode_reward=1893.20 +/- 199.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=945000, episode_reward=1545.20 +/- 215.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=946000, episode_reward=1573.80 +/- 161.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=947000, episode_reward=1768.20 +/- 204.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=948000, episode_reward=1679.20 +/- 348.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=949000, episode_reward=1789.60 +/- 180.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=950000, episode_reward=1771.80 +/- 203.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=951000, episode_reward=1717.60 +/- 163.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=952000, episode_reward=1901.20 +/- 145.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=953000, episode_reward=1684.00 +/- 84.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=954000, episode_reward=1786.80 +/- 191.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=955000, episode_reward=1650.60 +/- 273.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=956000, episode_reward=1762.20 +/- 77.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=957000, episode_reward=1730.80 +/- 240.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=958000, episode_reward=1715.40 +/- 141.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=959000, episode_reward=1805.80 +/- 147.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=960000, episode_reward=1819.60 +/- 224.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=961000, episode_reward=1872.40 +/- 133.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=962000, episode_reward=1756.00 +/- 164.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=963000, episode_reward=1652.00 +/- 122.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=964000, episode_reward=1706.20 +/- 270.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=965000, episode_reward=1747.80 +/- 231.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=966000, episode_reward=1752.60 +/- 140.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=967000, episode_reward=1632.80 +/- 383.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=968000, episode_reward=1717.80 +/- 312.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=969000, episode_reward=1685.20 +/- 169.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=970000, episode_reward=1894.80 +/- 115.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=971000, episode_reward=1719.00 +/- 249.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=972000, episode_reward=1446.20 +/- 165.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=973000, episode_reward=1661.60 +/- 197.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=974000, episode_reward=1769.60 +/- 154.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=975000, episode_reward=1610.00 +/- 287.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=976000, episode_reward=1544.80 +/- 309.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=977000, episode_reward=1660.80 +/- 184.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=978000, episode_reward=1418.00 +/- 310.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=979000, episode_reward=1503.20 +/- 232.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=980000, episode_reward=1819.40 +/- 85.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=981000, episode_reward=1673.80 +/- 249.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=982000, episode_reward=1642.20 +/- 387.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=983000, episode_reward=1776.40 +/- 243.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=984000, episode_reward=1695.00 +/- 157.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=985000, episode_reward=1632.40 +/- 132.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=986000, episode_reward=1713.40 +/- 268.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=987000, episode_reward=1662.20 +/- 136.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=988000, episode_reward=1643.80 +/- 199.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=989000, episode_reward=1592.00 +/- 290.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=990000, episode_reward=1544.80 +/- 144.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=991000, episode_reward=1869.80 +/- 177.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=992000, episode_reward=1639.20 +/- 141.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=993000, episode_reward=1850.40 +/- 124.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=994000, episode_reward=1890.00 +/- 100.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=995000, episode_reward=1596.80 +/- 284.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=996000, episode_reward=1833.00 +/- 240.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=997000, episode_reward=1666.00 +/- 167.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=998000, episode_reward=1644.40 +/- 450.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=999000, episode_reward=1515.80 +/- 264.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1000000, episode_reward=1762.00 +/- 308.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1001000, episode_reward=1647.80 +/- 205.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Gamma = 0.9, Learning Rate = 0.003, Activation Function = ReLU, Neurons = 8, Process noise = 0.1, Measurement noise = 0.0\n",
      "Eval num_timesteps=1000, episode_reward=-6087.40 +/- 1748.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=-5412.20 +/- 1854.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3000, episode_reward=-6216.00 +/- 2221.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=4000, episode_reward=-7435.20 +/- 1800.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=5000, episode_reward=359.40 +/- 117.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=6000, episode_reward=304.40 +/- 206.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=7000, episode_reward=377.40 +/- 409.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=8000, episode_reward=344.20 +/- 187.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=9000, episode_reward=437.60 +/- 118.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=10000, episode_reward=283.40 +/- 114.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=11000, episode_reward=372.40 +/- 157.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=12000, episode_reward=55.60 +/- 140.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=13000, episode_reward=470.60 +/- 307.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=14000, episode_reward=442.60 +/- 197.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=15000, episode_reward=236.80 +/- 93.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=16000, episode_reward=493.40 +/- 162.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=17000, episode_reward=241.20 +/- 169.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=18000, episode_reward=291.60 +/- 217.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=19000, episode_reward=289.80 +/- 348.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=20000, episode_reward=314.40 +/- 248.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=21000, episode_reward=606.80 +/- 170.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=22000, episode_reward=509.60 +/- 150.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=23000, episode_reward=749.80 +/- 222.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=24000, episode_reward=672.00 +/- 268.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=25000, episode_reward=868.20 +/- 279.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=26000, episode_reward=1026.40 +/- 351.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=27000, episode_reward=1205.60 +/- 318.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=28000, episode_reward=1082.00 +/- 319.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=29000, episode_reward=1213.80 +/- 235.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=30000, episode_reward=1260.80 +/- 357.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=31000, episode_reward=1176.60 +/- 378.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=32000, episode_reward=1281.00 +/- 223.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=33000, episode_reward=1209.80 +/- 323.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=34000, episode_reward=1017.00 +/- 213.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=35000, episode_reward=1028.80 +/- 99.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=36000, episode_reward=1515.00 +/- 226.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=37000, episode_reward=1411.60 +/- 101.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=38000, episode_reward=1428.00 +/- 160.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=39000, episode_reward=1584.40 +/- 188.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=1503.80 +/- 179.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=41000, episode_reward=1214.20 +/- 209.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=42000, episode_reward=1528.80 +/- 174.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=43000, episode_reward=1343.60 +/- 291.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=44000, episode_reward=1087.80 +/- 281.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=45000, episode_reward=1360.80 +/- 235.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=46000, episode_reward=1151.80 +/- 326.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=47000, episode_reward=1475.20 +/- 283.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=48000, episode_reward=1469.00 +/- 133.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=49000, episode_reward=1378.00 +/- 69.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=50000, episode_reward=1374.20 +/- 182.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=51000, episode_reward=1441.20 +/- 130.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=52000, episode_reward=1337.00 +/- 325.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=53000, episode_reward=1698.00 +/- 264.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=54000, episode_reward=1238.40 +/- 192.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=55000, episode_reward=1450.60 +/- 164.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=56000, episode_reward=1407.00 +/- 369.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=57000, episode_reward=1453.60 +/- 194.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=58000, episode_reward=1669.40 +/- 145.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=59000, episode_reward=1407.80 +/- 225.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=60000, episode_reward=1660.60 +/- 140.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=61000, episode_reward=1631.20 +/- 133.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=62000, episode_reward=1495.40 +/- 226.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=63000, episode_reward=1502.00 +/- 288.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=64000, episode_reward=1620.40 +/- 287.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=65000, episode_reward=1574.60 +/- 232.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=66000, episode_reward=1594.60 +/- 174.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=67000, episode_reward=1452.60 +/- 311.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=68000, episode_reward=1530.60 +/- 284.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=69000, episode_reward=1417.80 +/- 109.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=70000, episode_reward=1465.00 +/- 169.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=71000, episode_reward=1500.20 +/- 157.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=72000, episode_reward=1747.00 +/- 257.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=73000, episode_reward=1270.20 +/- 359.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=74000, episode_reward=1562.60 +/- 317.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=75000, episode_reward=1516.80 +/- 226.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=76000, episode_reward=1592.00 +/- 265.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=77000, episode_reward=1523.20 +/- 117.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=78000, episode_reward=1770.40 +/- 111.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=79000, episode_reward=1473.80 +/- 282.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=80000, episode_reward=1927.60 +/- 172.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=81000, episode_reward=1485.40 +/- 321.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=82000, episode_reward=1584.40 +/- 273.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=83000, episode_reward=1519.60 +/- 396.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=84000, episode_reward=1512.00 +/- 133.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=85000, episode_reward=1614.60 +/- 200.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=86000, episode_reward=1502.80 +/- 258.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=87000, episode_reward=1412.80 +/- 297.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=88000, episode_reward=1438.00 +/- 157.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=89000, episode_reward=1651.40 +/- 206.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=90000, episode_reward=1791.20 +/- 154.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=91000, episode_reward=1665.40 +/- 175.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=92000, episode_reward=1625.40 +/- 154.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=93000, episode_reward=1603.00 +/- 254.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=94000, episode_reward=1514.20 +/- 210.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=95000, episode_reward=1716.60 +/- 175.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=96000, episode_reward=1664.40 +/- 181.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=97000, episode_reward=1691.60 +/- 240.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=98000, episode_reward=1538.40 +/- 110.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=99000, episode_reward=1362.40 +/- 370.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=100000, episode_reward=1403.80 +/- 380.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=101000, episode_reward=1486.00 +/- 328.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=102000, episode_reward=1515.00 +/- 263.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=103000, episode_reward=1734.00 +/- 140.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=104000, episode_reward=1719.60 +/- 231.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=105000, episode_reward=1569.40 +/- 108.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=106000, episode_reward=1429.40 +/- 278.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=107000, episode_reward=1661.60 +/- 157.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=108000, episode_reward=1530.80 +/- 124.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=109000, episode_reward=1704.80 +/- 145.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=110000, episode_reward=1407.20 +/- 477.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=111000, episode_reward=1675.20 +/- 195.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=112000, episode_reward=1603.40 +/- 181.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=113000, episode_reward=1560.40 +/- 336.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=114000, episode_reward=1558.20 +/- 131.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=115000, episode_reward=1767.40 +/- 155.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=116000, episode_reward=1563.00 +/- 227.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=117000, episode_reward=1534.80 +/- 102.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=118000, episode_reward=1632.60 +/- 167.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=119000, episode_reward=1635.20 +/- 249.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=120000, episode_reward=1659.80 +/- 181.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=121000, episode_reward=1693.60 +/- 146.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=122000, episode_reward=1549.20 +/- 178.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=123000, episode_reward=1685.80 +/- 214.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=124000, episode_reward=1544.40 +/- 199.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=125000, episode_reward=1814.40 +/- 227.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=126000, episode_reward=1547.80 +/- 164.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=127000, episode_reward=1674.80 +/- 93.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=128000, episode_reward=1514.60 +/- 317.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=129000, episode_reward=1516.60 +/- 241.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=130000, episode_reward=1712.20 +/- 224.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=131000, episode_reward=1530.20 +/- 225.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=132000, episode_reward=1740.00 +/- 261.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=133000, episode_reward=1573.60 +/- 224.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=134000, episode_reward=1597.60 +/- 205.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=135000, episode_reward=1575.20 +/- 168.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=136000, episode_reward=1718.00 +/- 224.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=137000, episode_reward=1340.80 +/- 268.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=138000, episode_reward=1714.40 +/- 130.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=139000, episode_reward=1608.20 +/- 360.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=140000, episode_reward=1619.20 +/- 389.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=141000, episode_reward=1720.40 +/- 209.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=142000, episode_reward=1554.20 +/- 228.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=143000, episode_reward=1531.40 +/- 209.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=144000, episode_reward=1583.00 +/- 187.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=145000, episode_reward=1530.20 +/- 184.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=146000, episode_reward=1515.40 +/- 132.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=147000, episode_reward=1340.80 +/- 86.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=148000, episode_reward=1491.40 +/- 309.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=149000, episode_reward=1496.80 +/- 169.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=150000, episode_reward=1419.20 +/- 156.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=151000, episode_reward=1581.80 +/- 241.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=152000, episode_reward=1665.40 +/- 236.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=153000, episode_reward=1590.00 +/- 266.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=154000, episode_reward=1569.20 +/- 152.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=155000, episode_reward=1509.80 +/- 231.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=156000, episode_reward=1401.80 +/- 182.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=157000, episode_reward=1494.80 +/- 263.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=158000, episode_reward=1670.40 +/- 169.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=159000, episode_reward=1589.80 +/- 227.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=160000, episode_reward=1591.80 +/- 187.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=161000, episode_reward=1604.60 +/- 208.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=162000, episode_reward=1385.80 +/- 157.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=163000, episode_reward=1638.20 +/- 251.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=164000, episode_reward=1638.00 +/- 116.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=165000, episode_reward=1599.20 +/- 143.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=166000, episode_reward=1710.60 +/- 250.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=167000, episode_reward=1452.00 +/- 159.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=168000, episode_reward=1566.60 +/- 336.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=169000, episode_reward=1662.40 +/- 90.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=170000, episode_reward=1685.60 +/- 175.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=171000, episode_reward=1398.20 +/- 368.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=172000, episode_reward=1602.40 +/- 163.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=173000, episode_reward=1599.40 +/- 212.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=174000, episode_reward=1713.60 +/- 370.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=175000, episode_reward=1578.00 +/- 215.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=176000, episode_reward=1600.00 +/- 224.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=177000, episode_reward=1489.80 +/- 153.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=178000, episode_reward=1474.20 +/- 293.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=179000, episode_reward=1334.80 +/- 225.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=180000, episode_reward=1481.60 +/- 300.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=181000, episode_reward=1578.20 +/- 235.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=182000, episode_reward=1686.00 +/- 132.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=183000, episode_reward=1736.40 +/- 139.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=184000, episode_reward=1752.40 +/- 95.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=185000, episode_reward=1540.80 +/- 181.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=186000, episode_reward=1646.80 +/- 281.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=187000, episode_reward=1721.80 +/- 234.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=188000, episode_reward=1587.40 +/- 153.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=189000, episode_reward=1793.40 +/- 84.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=190000, episode_reward=1600.80 +/- 294.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=191000, episode_reward=1774.80 +/- 301.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=192000, episode_reward=1399.00 +/- 437.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=193000, episode_reward=1593.60 +/- 187.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=194000, episode_reward=1522.40 +/- 254.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=195000, episode_reward=1752.60 +/- 154.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=196000, episode_reward=1617.20 +/- 292.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=197000, episode_reward=1686.00 +/- 154.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=198000, episode_reward=1645.60 +/- 207.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=199000, episode_reward=1627.40 +/- 169.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=200000, episode_reward=1503.00 +/- 330.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=201000, episode_reward=1607.60 +/- 207.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=202000, episode_reward=1560.20 +/- 200.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=203000, episode_reward=1607.60 +/- 109.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=204000, episode_reward=1769.40 +/- 175.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=205000, episode_reward=1560.20 +/- 207.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=206000, episode_reward=1594.80 +/- 143.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=207000, episode_reward=1579.20 +/- 112.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=208000, episode_reward=1686.20 +/- 266.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=209000, episode_reward=1688.80 +/- 339.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=210000, episode_reward=1864.80 +/- 227.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=211000, episode_reward=1716.00 +/- 81.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=212000, episode_reward=1608.60 +/- 88.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=213000, episode_reward=1651.40 +/- 214.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=214000, episode_reward=1649.60 +/- 303.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=215000, episode_reward=1837.20 +/- 164.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=216000, episode_reward=1732.20 +/- 238.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=217000, episode_reward=1589.40 +/- 174.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=218000, episode_reward=1723.40 +/- 196.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=219000, episode_reward=1615.60 +/- 249.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=220000, episode_reward=1691.20 +/- 125.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=221000, episode_reward=1725.40 +/- 260.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=222000, episode_reward=1595.40 +/- 230.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=223000, episode_reward=1637.60 +/- 289.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=224000, episode_reward=1680.80 +/- 184.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=225000, episode_reward=1629.20 +/- 399.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=226000, episode_reward=1423.00 +/- 316.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=227000, episode_reward=1549.00 +/- 290.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=228000, episode_reward=1590.60 +/- 110.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=229000, episode_reward=1617.00 +/- 244.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=230000, episode_reward=1610.80 +/- 167.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=231000, episode_reward=1506.40 +/- 286.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=232000, episode_reward=1579.80 +/- 80.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=233000, episode_reward=1578.40 +/- 344.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=234000, episode_reward=1766.40 +/- 197.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=235000, episode_reward=1750.80 +/- 155.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=236000, episode_reward=1688.00 +/- 235.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=237000, episode_reward=1589.00 +/- 297.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=238000, episode_reward=1697.40 +/- 142.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=239000, episode_reward=1590.20 +/- 178.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=240000, episode_reward=1586.60 +/- 99.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=241000, episode_reward=1746.60 +/- 218.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=242000, episode_reward=1364.40 +/- 254.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=243000, episode_reward=1444.60 +/- 122.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=244000, episode_reward=1789.20 +/- 172.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=245000, episode_reward=1743.60 +/- 231.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=246000, episode_reward=1607.00 +/- 227.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=247000, episode_reward=1765.60 +/- 149.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=248000, episode_reward=1527.00 +/- 288.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=249000, episode_reward=1661.60 +/- 276.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=250000, episode_reward=1732.00 +/- 281.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=251000, episode_reward=1517.20 +/- 237.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=252000, episode_reward=1746.60 +/- 146.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=253000, episode_reward=1805.20 +/- 127.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=254000, episode_reward=1781.00 +/- 160.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=255000, episode_reward=1616.60 +/- 135.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=256000, episode_reward=1602.20 +/- 289.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=257000, episode_reward=1712.80 +/- 141.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=258000, episode_reward=1666.20 +/- 281.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=259000, episode_reward=1637.00 +/- 168.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=260000, episode_reward=1624.20 +/- 161.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=261000, episode_reward=1663.40 +/- 78.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=262000, episode_reward=1556.40 +/- 221.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=263000, episode_reward=1615.60 +/- 206.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=264000, episode_reward=1492.80 +/- 318.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=265000, episode_reward=1577.20 +/- 307.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=266000, episode_reward=1627.80 +/- 167.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=267000, episode_reward=1617.40 +/- 147.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=268000, episode_reward=1582.20 +/- 328.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=269000, episode_reward=1597.20 +/- 200.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=270000, episode_reward=1448.40 +/- 111.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=271000, episode_reward=1737.40 +/- 223.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=272000, episode_reward=1402.20 +/- 214.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=273000, episode_reward=1631.60 +/- 145.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=274000, episode_reward=1697.20 +/- 199.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=275000, episode_reward=1752.40 +/- 190.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=276000, episode_reward=1572.00 +/- 210.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=277000, episode_reward=1569.60 +/- 196.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=278000, episode_reward=1525.80 +/- 213.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=279000, episode_reward=1499.00 +/- 76.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=280000, episode_reward=1742.60 +/- 221.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=281000, episode_reward=1663.60 +/- 127.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=282000, episode_reward=1677.60 +/- 138.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=283000, episode_reward=1570.40 +/- 213.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=284000, episode_reward=1554.60 +/- 262.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=285000, episode_reward=1644.80 +/- 242.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=286000, episode_reward=1788.40 +/- 235.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=287000, episode_reward=1670.60 +/- 240.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=288000, episode_reward=1693.60 +/- 179.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=289000, episode_reward=1513.00 +/- 143.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=290000, episode_reward=1460.20 +/- 85.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=291000, episode_reward=1450.60 +/- 212.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=292000, episode_reward=1439.00 +/- 342.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=293000, episode_reward=1507.40 +/- 269.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=294000, episode_reward=1618.40 +/- 171.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=295000, episode_reward=1691.40 +/- 112.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=296000, episode_reward=1681.00 +/- 202.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=297000, episode_reward=1564.80 +/- 162.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=298000, episode_reward=1471.60 +/- 293.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=299000, episode_reward=1755.40 +/- 250.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=300000, episode_reward=1690.60 +/- 206.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=301000, episode_reward=1849.00 +/- 173.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=302000, episode_reward=1584.00 +/- 264.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=303000, episode_reward=1639.40 +/- 167.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=304000, episode_reward=1809.00 +/- 274.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=305000, episode_reward=1632.20 +/- 133.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=306000, episode_reward=1510.00 +/- 387.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=307000, episode_reward=1827.40 +/- 211.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=308000, episode_reward=1673.20 +/- 269.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=309000, episode_reward=1556.00 +/- 286.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=310000, episode_reward=1558.20 +/- 164.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=311000, episode_reward=1749.40 +/- 167.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=312000, episode_reward=1492.20 +/- 250.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=313000, episode_reward=1786.40 +/- 357.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=314000, episode_reward=1358.80 +/- 256.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=315000, episode_reward=1612.00 +/- 192.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=316000, episode_reward=1624.40 +/- 193.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=317000, episode_reward=1542.60 +/- 303.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=318000, episode_reward=1725.20 +/- 277.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=319000, episode_reward=1698.40 +/- 166.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=320000, episode_reward=1749.40 +/- 132.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=321000, episode_reward=1731.00 +/- 275.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=322000, episode_reward=1818.00 +/- 65.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=323000, episode_reward=1534.80 +/- 143.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=324000, episode_reward=1820.60 +/- 237.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=325000, episode_reward=1642.20 +/- 114.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=326000, episode_reward=1648.60 +/- 183.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=327000, episode_reward=1501.60 +/- 177.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=328000, episode_reward=1501.60 +/- 241.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=329000, episode_reward=1735.80 +/- 103.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=330000, episode_reward=1798.80 +/- 273.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=331000, episode_reward=1641.60 +/- 202.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=332000, episode_reward=1613.80 +/- 334.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=333000, episode_reward=1622.80 +/- 239.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=334000, episode_reward=1831.80 +/- 109.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=335000, episode_reward=1636.80 +/- 274.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=336000, episode_reward=1851.20 +/- 243.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=337000, episode_reward=1774.00 +/- 211.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=338000, episode_reward=1734.80 +/- 156.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=339000, episode_reward=1769.60 +/- 73.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=340000, episode_reward=1660.20 +/- 181.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=341000, episode_reward=1794.80 +/- 140.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=342000, episode_reward=1590.60 +/- 119.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=343000, episode_reward=1781.00 +/- 62.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=344000, episode_reward=1677.20 +/- 147.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=345000, episode_reward=1718.60 +/- 98.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=346000, episode_reward=1614.60 +/- 34.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=347000, episode_reward=1849.40 +/- 86.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=348000, episode_reward=1453.00 +/- 147.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=349000, episode_reward=1694.80 +/- 176.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=350000, episode_reward=1592.60 +/- 184.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=351000, episode_reward=1437.00 +/- 215.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=352000, episode_reward=1702.80 +/- 90.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=353000, episode_reward=1678.80 +/- 318.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=354000, episode_reward=1620.20 +/- 160.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=355000, episode_reward=1676.80 +/- 155.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=356000, episode_reward=1652.40 +/- 204.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=357000, episode_reward=1366.60 +/- 228.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=358000, episode_reward=1640.00 +/- 259.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=359000, episode_reward=1593.60 +/- 170.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=360000, episode_reward=1733.20 +/- 313.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=361000, episode_reward=1677.40 +/- 209.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=362000, episode_reward=1774.20 +/- 232.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=363000, episode_reward=1542.20 +/- 288.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=364000, episode_reward=1741.60 +/- 170.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=365000, episode_reward=1447.00 +/- 313.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=366000, episode_reward=1640.20 +/- 319.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=367000, episode_reward=1512.00 +/- 172.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=368000, episode_reward=1668.20 +/- 225.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=369000, episode_reward=1826.80 +/- 97.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=370000, episode_reward=1690.80 +/- 199.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=371000, episode_reward=1601.20 +/- 286.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=372000, episode_reward=1695.00 +/- 311.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=373000, episode_reward=1658.80 +/- 109.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=374000, episode_reward=1663.40 +/- 177.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=375000, episode_reward=1638.00 +/- 237.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=376000, episode_reward=1766.60 +/- 118.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=377000, episode_reward=1780.20 +/- 107.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=378000, episode_reward=1682.20 +/- 177.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=379000, episode_reward=1708.80 +/- 180.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=380000, episode_reward=1649.00 +/- 286.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=381000, episode_reward=1688.40 +/- 146.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=382000, episode_reward=1701.40 +/- 118.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=383000, episode_reward=1715.60 +/- 199.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=384000, episode_reward=1825.40 +/- 168.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=385000, episode_reward=1479.40 +/- 74.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=386000, episode_reward=1488.40 +/- 184.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=387000, episode_reward=1575.20 +/- 168.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=388000, episode_reward=1759.60 +/- 142.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=389000, episode_reward=1539.60 +/- 162.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=390000, episode_reward=1645.40 +/- 154.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=391000, episode_reward=1627.40 +/- 215.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=392000, episode_reward=1748.00 +/- 224.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=393000, episode_reward=1543.80 +/- 190.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=394000, episode_reward=1568.40 +/- 224.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=395000, episode_reward=1672.80 +/- 185.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=396000, episode_reward=1666.20 +/- 308.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=397000, episode_reward=1577.80 +/- 204.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=398000, episode_reward=1622.20 +/- 134.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=399000, episode_reward=1807.20 +/- 132.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=400000, episode_reward=1717.80 +/- 183.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=401000, episode_reward=1661.60 +/- 295.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=402000, episode_reward=1630.80 +/- 243.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=403000, episode_reward=1775.20 +/- 113.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=404000, episode_reward=1552.20 +/- 427.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=405000, episode_reward=1404.00 +/- 222.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=406000, episode_reward=1731.80 +/- 240.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=407000, episode_reward=1611.00 +/- 218.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=408000, episode_reward=1600.40 +/- 228.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=409000, episode_reward=1621.20 +/- 263.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=410000, episode_reward=1636.80 +/- 241.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=411000, episode_reward=1754.40 +/- 162.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=412000, episode_reward=1617.60 +/- 156.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=413000, episode_reward=1610.60 +/- 198.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=414000, episode_reward=1617.60 +/- 149.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=415000, episode_reward=1605.20 +/- 208.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=416000, episode_reward=1495.40 +/- 280.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=417000, episode_reward=1573.20 +/- 212.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=418000, episode_reward=1564.20 +/- 222.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=419000, episode_reward=1670.80 +/- 215.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=420000, episode_reward=1624.20 +/- 162.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=421000, episode_reward=1616.40 +/- 240.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=422000, episode_reward=1675.00 +/- 207.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=423000, episode_reward=1538.00 +/- 384.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=424000, episode_reward=1778.20 +/- 101.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=425000, episode_reward=1809.60 +/- 65.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=426000, episode_reward=1674.80 +/- 79.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=427000, episode_reward=1782.80 +/- 14.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=428000, episode_reward=1747.00 +/- 121.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=429000, episode_reward=1501.20 +/- 198.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=430000, episode_reward=1813.40 +/- 218.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=431000, episode_reward=1528.80 +/- 352.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=432000, episode_reward=1638.60 +/- 268.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=433000, episode_reward=1668.60 +/- 195.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=434000, episode_reward=1627.40 +/- 472.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=435000, episode_reward=1630.60 +/- 109.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=436000, episode_reward=1760.20 +/- 169.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=437000, episode_reward=1645.40 +/- 172.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=438000, episode_reward=1565.60 +/- 81.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=439000, episode_reward=1712.80 +/- 259.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=440000, episode_reward=1869.00 +/- 231.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=441000, episode_reward=1696.60 +/- 163.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=442000, episode_reward=1757.60 +/- 169.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=443000, episode_reward=1734.20 +/- 353.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=444000, episode_reward=1455.80 +/- 211.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=445000, episode_reward=1546.20 +/- 188.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=446000, episode_reward=1363.20 +/- 358.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=447000, episode_reward=1800.40 +/- 107.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=448000, episode_reward=1638.20 +/- 203.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=449000, episode_reward=1809.80 +/- 159.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=450000, episode_reward=1655.00 +/- 140.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=451000, episode_reward=1811.40 +/- 229.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=452000, episode_reward=1558.60 +/- 208.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=453000, episode_reward=1758.40 +/- 155.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=454000, episode_reward=1688.00 +/- 149.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=455000, episode_reward=1538.20 +/- 183.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=456000, episode_reward=1715.60 +/- 214.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=457000, episode_reward=1735.00 +/- 119.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=458000, episode_reward=1684.00 +/- 265.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=459000, episode_reward=1684.60 +/- 219.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=460000, episode_reward=1612.60 +/- 148.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=461000, episode_reward=1599.20 +/- 186.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=462000, episode_reward=1733.40 +/- 121.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=463000, episode_reward=1681.80 +/- 315.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=464000, episode_reward=1730.20 +/- 332.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=465000, episode_reward=1853.20 +/- 103.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=466000, episode_reward=1601.40 +/- 199.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=467000, episode_reward=1675.00 +/- 152.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=468000, episode_reward=1746.20 +/- 82.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=469000, episode_reward=1750.00 +/- 208.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=470000, episode_reward=1770.80 +/- 237.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=471000, episode_reward=1711.20 +/- 146.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=472000, episode_reward=1724.40 +/- 137.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=473000, episode_reward=1461.60 +/- 200.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=474000, episode_reward=1705.00 +/- 193.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=475000, episode_reward=1638.80 +/- 233.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=476000, episode_reward=1576.80 +/- 322.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=477000, episode_reward=1464.20 +/- 215.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=478000, episode_reward=1585.40 +/- 213.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=479000, episode_reward=1572.80 +/- 304.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=480000, episode_reward=1622.80 +/- 79.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=481000, episode_reward=1569.60 +/- 135.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=482000, episode_reward=1663.80 +/- 158.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=483000, episode_reward=1499.60 +/- 260.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=484000, episode_reward=1734.80 +/- 234.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=485000, episode_reward=1778.60 +/- 151.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=486000, episode_reward=1652.00 +/- 216.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=487000, episode_reward=1510.20 +/- 231.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=488000, episode_reward=1632.20 +/- 96.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=489000, episode_reward=1665.80 +/- 386.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=490000, episode_reward=1587.80 +/- 241.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=491000, episode_reward=1835.80 +/- 179.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=492000, episode_reward=1617.20 +/- 141.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=493000, episode_reward=1705.20 +/- 153.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=494000, episode_reward=1609.00 +/- 246.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=495000, episode_reward=1647.80 +/- 212.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=496000, episode_reward=1682.20 +/- 172.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=497000, episode_reward=1533.80 +/- 177.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=498000, episode_reward=1863.80 +/- 178.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=499000, episode_reward=1719.80 +/- 92.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=500000, episode_reward=1657.20 +/- 120.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=501000, episode_reward=1594.00 +/- 205.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=502000, episode_reward=1545.00 +/- 232.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=503000, episode_reward=1586.80 +/- 353.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=504000, episode_reward=1728.40 +/- 304.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=505000, episode_reward=1759.40 +/- 161.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=506000, episode_reward=1675.20 +/- 175.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=507000, episode_reward=1846.60 +/- 241.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=508000, episode_reward=1640.00 +/- 156.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=509000, episode_reward=1692.20 +/- 309.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=510000, episode_reward=1704.20 +/- 162.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=511000, episode_reward=1604.40 +/- 77.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=512000, episode_reward=1606.40 +/- 190.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=513000, episode_reward=1793.80 +/- 205.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=514000, episode_reward=1709.40 +/- 165.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=515000, episode_reward=1687.60 +/- 242.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=516000, episode_reward=1551.40 +/- 108.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=517000, episode_reward=1801.20 +/- 160.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=518000, episode_reward=1860.40 +/- 93.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=519000, episode_reward=1846.00 +/- 149.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=520000, episode_reward=1845.40 +/- 241.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=521000, episode_reward=1742.60 +/- 147.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=522000, episode_reward=1822.40 +/- 121.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=523000, episode_reward=1603.80 +/- 205.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=524000, episode_reward=1697.00 +/- 267.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=525000, episode_reward=1792.00 +/- 220.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=526000, episode_reward=1832.40 +/- 66.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=527000, episode_reward=1639.00 +/- 218.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=528000, episode_reward=1604.40 +/- 298.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=529000, episode_reward=1569.40 +/- 183.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=530000, episode_reward=1561.40 +/- 189.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=531000, episode_reward=1662.80 +/- 199.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=532000, episode_reward=1557.40 +/- 131.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=533000, episode_reward=1597.60 +/- 224.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=534000, episode_reward=1535.40 +/- 231.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=535000, episode_reward=1729.20 +/- 133.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=536000, episode_reward=1714.40 +/- 216.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=537000, episode_reward=1627.00 +/- 276.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=538000, episode_reward=1580.00 +/- 244.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=539000, episode_reward=1792.00 +/- 162.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=540000, episode_reward=1733.00 +/- 122.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=541000, episode_reward=1647.40 +/- 61.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=542000, episode_reward=1686.00 +/- 75.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=543000, episode_reward=1744.00 +/- 136.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=544000, episode_reward=1684.80 +/- 132.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=545000, episode_reward=1737.60 +/- 161.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=546000, episode_reward=1407.00 +/- 298.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=547000, episode_reward=1563.00 +/- 286.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=548000, episode_reward=1676.80 +/- 310.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=549000, episode_reward=1678.40 +/- 321.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=550000, episode_reward=1762.80 +/- 104.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=551000, episode_reward=1689.40 +/- 132.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=552000, episode_reward=1757.00 +/- 165.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=553000, episode_reward=1816.40 +/- 144.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=554000, episode_reward=1676.80 +/- 209.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=555000, episode_reward=1775.00 +/- 90.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=556000, episode_reward=1601.00 +/- 73.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=557000, episode_reward=1522.40 +/- 137.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=558000, episode_reward=1600.40 +/- 194.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=559000, episode_reward=1566.40 +/- 242.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=560000, episode_reward=1558.60 +/- 275.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=561000, episode_reward=1567.20 +/- 256.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=562000, episode_reward=1672.40 +/- 168.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=563000, episode_reward=1704.00 +/- 178.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=564000, episode_reward=1725.00 +/- 188.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=565000, episode_reward=1644.00 +/- 302.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=566000, episode_reward=1611.80 +/- 119.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=567000, episode_reward=1585.20 +/- 199.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=568000, episode_reward=1641.20 +/- 360.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=569000, episode_reward=1763.60 +/- 174.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=570000, episode_reward=1724.80 +/- 195.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=571000, episode_reward=1588.00 +/- 271.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=572000, episode_reward=1649.20 +/- 174.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=573000, episode_reward=1453.80 +/- 166.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=574000, episode_reward=1543.60 +/- 409.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=575000, episode_reward=1660.20 +/- 113.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=576000, episode_reward=1813.20 +/- 239.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=577000, episode_reward=1772.60 +/- 92.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=578000, episode_reward=1507.00 +/- 170.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=579000, episode_reward=1694.40 +/- 128.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=580000, episode_reward=1733.60 +/- 61.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=581000, episode_reward=1650.40 +/- 102.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=582000, episode_reward=1780.60 +/- 110.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=583000, episode_reward=1502.20 +/- 260.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=584000, episode_reward=1695.20 +/- 213.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=585000, episode_reward=1753.40 +/- 167.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=586000, episode_reward=1588.20 +/- 305.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=587000, episode_reward=1639.20 +/- 312.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=588000, episode_reward=1673.80 +/- 110.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=589000, episode_reward=1602.60 +/- 140.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=590000, episode_reward=1637.80 +/- 89.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=591000, episode_reward=1694.40 +/- 98.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=592000, episode_reward=1653.60 +/- 211.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=593000, episode_reward=1410.80 +/- 295.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=594000, episode_reward=1796.80 +/- 147.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=595000, episode_reward=1449.60 +/- 199.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=596000, episode_reward=1749.00 +/- 106.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=597000, episode_reward=1797.20 +/- 86.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=598000, episode_reward=1515.00 +/- 339.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=599000, episode_reward=1774.60 +/- 222.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=600000, episode_reward=1703.60 +/- 123.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=601000, episode_reward=1704.00 +/- 163.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=602000, episode_reward=1799.00 +/- 199.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=603000, episode_reward=1726.80 +/- 226.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=604000, episode_reward=1756.60 +/- 55.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=605000, episode_reward=1676.60 +/- 120.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=606000, episode_reward=1713.00 +/- 189.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=607000, episode_reward=1634.40 +/- 226.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=608000, episode_reward=1680.40 +/- 258.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=609000, episode_reward=1661.60 +/- 176.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=610000, episode_reward=1435.20 +/- 232.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=611000, episode_reward=1686.20 +/- 180.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=612000, episode_reward=1420.60 +/- 322.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=613000, episode_reward=1802.00 +/- 186.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=614000, episode_reward=1545.60 +/- 225.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=615000, episode_reward=1625.00 +/- 167.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=616000, episode_reward=1738.40 +/- 188.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=617000, episode_reward=1740.00 +/- 186.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=618000, episode_reward=1712.40 +/- 97.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=619000, episode_reward=1706.20 +/- 177.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=620000, episode_reward=1649.00 +/- 172.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=621000, episode_reward=1761.40 +/- 133.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=622000, episode_reward=1624.80 +/- 142.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=623000, episode_reward=1773.60 +/- 103.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=624000, episode_reward=1755.00 +/- 143.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=625000, episode_reward=1520.60 +/- 190.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=626000, episode_reward=1722.40 +/- 92.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=627000, episode_reward=1742.60 +/- 193.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=628000, episode_reward=1550.60 +/- 237.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=629000, episode_reward=1564.00 +/- 172.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=630000, episode_reward=1719.60 +/- 122.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=631000, episode_reward=1653.60 +/- 139.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=632000, episode_reward=1829.20 +/- 228.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=633000, episode_reward=1804.40 +/- 156.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=634000, episode_reward=1647.00 +/- 265.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=635000, episode_reward=1558.00 +/- 145.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=636000, episode_reward=1560.60 +/- 177.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=637000, episode_reward=1732.80 +/- 193.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=638000, episode_reward=1682.00 +/- 288.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=639000, episode_reward=1617.60 +/- 124.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=640000, episode_reward=1687.60 +/- 238.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=641000, episode_reward=1613.80 +/- 262.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=642000, episode_reward=1555.20 +/- 99.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=643000, episode_reward=1699.40 +/- 105.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=644000, episode_reward=1446.40 +/- 333.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=645000, episode_reward=1795.20 +/- 250.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=646000, episode_reward=1545.40 +/- 205.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=647000, episode_reward=1647.40 +/- 353.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=648000, episode_reward=1646.80 +/- 189.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=649000, episode_reward=1530.20 +/- 404.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=650000, episode_reward=1744.00 +/- 96.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=651000, episode_reward=1657.20 +/- 131.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=652000, episode_reward=1623.20 +/- 132.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=653000, episode_reward=1629.60 +/- 80.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=654000, episode_reward=1565.40 +/- 281.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=655000, episode_reward=1677.00 +/- 132.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=656000, episode_reward=1776.00 +/- 212.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=657000, episode_reward=1528.20 +/- 341.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=658000, episode_reward=1464.80 +/- 263.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=659000, episode_reward=1857.20 +/- 169.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=660000, episode_reward=1613.20 +/- 128.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=661000, episode_reward=1800.00 +/- 133.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=662000, episode_reward=1933.00 +/- 121.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=663000, episode_reward=1825.40 +/- 83.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=664000, episode_reward=1711.40 +/- 286.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=665000, episode_reward=1693.60 +/- 172.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=666000, episode_reward=1745.80 +/- 142.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=667000, episode_reward=1874.60 +/- 162.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=668000, episode_reward=1616.00 +/- 429.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=669000, episode_reward=1486.20 +/- 254.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=670000, episode_reward=1495.80 +/- 221.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=671000, episode_reward=1605.00 +/- 198.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=672000, episode_reward=1709.20 +/- 322.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=673000, episode_reward=1408.00 +/- 169.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=674000, episode_reward=1491.40 +/- 179.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=675000, episode_reward=1620.20 +/- 138.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=676000, episode_reward=1475.60 +/- 246.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=677000, episode_reward=1682.60 +/- 261.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=678000, episode_reward=1742.20 +/- 286.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=679000, episode_reward=1627.40 +/- 287.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=680000, episode_reward=1736.40 +/- 210.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=681000, episode_reward=1668.60 +/- 192.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=682000, episode_reward=1673.40 +/- 160.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=683000, episode_reward=1637.00 +/- 77.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=684000, episode_reward=1844.20 +/- 88.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=685000, episode_reward=1695.40 +/- 278.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=686000, episode_reward=1305.60 +/- 96.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=687000, episode_reward=1750.20 +/- 143.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=688000, episode_reward=1549.40 +/- 270.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=689000, episode_reward=1678.40 +/- 233.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=690000, episode_reward=1778.80 +/- 87.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=691000, episode_reward=1611.80 +/- 229.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=692000, episode_reward=1844.20 +/- 162.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=693000, episode_reward=1487.60 +/- 259.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=694000, episode_reward=1770.40 +/- 224.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=695000, episode_reward=1557.20 +/- 156.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=696000, episode_reward=1656.20 +/- 244.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=697000, episode_reward=1601.80 +/- 174.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=698000, episode_reward=1558.40 +/- 166.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=699000, episode_reward=1853.20 +/- 193.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=700000, episode_reward=1592.80 +/- 436.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=701000, episode_reward=1651.00 +/- 345.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=702000, episode_reward=1432.00 +/- 135.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=703000, episode_reward=1659.80 +/- 191.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=704000, episode_reward=1678.80 +/- 166.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=705000, episode_reward=1532.00 +/- 218.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=706000, episode_reward=1799.60 +/- 147.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=707000, episode_reward=1666.60 +/- 78.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=708000, episode_reward=1474.20 +/- 184.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=709000, episode_reward=1550.80 +/- 114.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=710000, episode_reward=1702.40 +/- 267.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=711000, episode_reward=1674.80 +/- 195.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=712000, episode_reward=1404.60 +/- 177.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=713000, episode_reward=1590.40 +/- 231.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=714000, episode_reward=1558.20 +/- 273.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=715000, episode_reward=1527.60 +/- 214.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=716000, episode_reward=1584.80 +/- 215.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=717000, episode_reward=1636.60 +/- 250.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=718000, episode_reward=1881.00 +/- 272.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=719000, episode_reward=1836.00 +/- 182.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=720000, episode_reward=1394.60 +/- 216.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=721000, episode_reward=1742.80 +/- 271.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=722000, episode_reward=1857.20 +/- 124.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=723000, episode_reward=1645.20 +/- 192.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=724000, episode_reward=1703.40 +/- 157.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=725000, episode_reward=1690.00 +/- 259.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=726000, episode_reward=1632.00 +/- 143.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=727000, episode_reward=1644.80 +/- 307.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=728000, episode_reward=1573.00 +/- 256.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=729000, episode_reward=1601.20 +/- 260.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=730000, episode_reward=1492.20 +/- 199.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=731000, episode_reward=1659.00 +/- 246.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=732000, episode_reward=1472.00 +/- 193.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=733000, episode_reward=1782.20 +/- 182.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=734000, episode_reward=1800.00 +/- 135.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=735000, episode_reward=1636.80 +/- 211.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=736000, episode_reward=1748.60 +/- 163.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=737000, episode_reward=1608.00 +/- 237.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=738000, episode_reward=1892.00 +/- 130.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=739000, episode_reward=1562.80 +/- 275.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=740000, episode_reward=1613.60 +/- 202.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=741000, episode_reward=1671.40 +/- 154.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=742000, episode_reward=1707.60 +/- 211.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=743000, episode_reward=1641.20 +/- 137.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=744000, episode_reward=1774.20 +/- 101.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=745000, episode_reward=1651.80 +/- 256.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=746000, episode_reward=1553.80 +/- 289.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=747000, episode_reward=1703.40 +/- 92.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=748000, episode_reward=1729.60 +/- 210.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=749000, episode_reward=1803.60 +/- 251.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=750000, episode_reward=1744.60 +/- 242.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=751000, episode_reward=1518.60 +/- 175.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=752000, episode_reward=1740.80 +/- 234.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=753000, episode_reward=1496.40 +/- 207.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=754000, episode_reward=1667.40 +/- 289.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=755000, episode_reward=1409.20 +/- 403.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=756000, episode_reward=1676.20 +/- 83.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=757000, episode_reward=1580.20 +/- 125.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=758000, episode_reward=1848.60 +/- 121.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=759000, episode_reward=1508.60 +/- 138.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=760000, episode_reward=1651.80 +/- 159.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=761000, episode_reward=1583.40 +/- 288.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=762000, episode_reward=1731.20 +/- 152.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=763000, episode_reward=1532.60 +/- 318.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=764000, episode_reward=1600.00 +/- 220.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=765000, episode_reward=1765.40 +/- 171.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=766000, episode_reward=1828.40 +/- 166.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=767000, episode_reward=1833.60 +/- 165.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=768000, episode_reward=1724.60 +/- 165.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=769000, episode_reward=1685.80 +/- 155.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=770000, episode_reward=1746.00 +/- 390.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=771000, episode_reward=1545.80 +/- 195.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=772000, episode_reward=1730.40 +/- 183.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=773000, episode_reward=1817.40 +/- 196.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=774000, episode_reward=1468.40 +/- 204.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=775000, episode_reward=1626.40 +/- 129.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=776000, episode_reward=1483.40 +/- 194.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=777000, episode_reward=1658.00 +/- 221.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=778000, episode_reward=1633.00 +/- 155.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=779000, episode_reward=1563.20 +/- 255.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=780000, episode_reward=1549.80 +/- 192.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=781000, episode_reward=1695.80 +/- 103.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=782000, episode_reward=1667.00 +/- 206.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=783000, episode_reward=1684.60 +/- 211.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=784000, episode_reward=1552.20 +/- 211.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=785000, episode_reward=1639.80 +/- 190.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=786000, episode_reward=1556.00 +/- 266.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=787000, episode_reward=1551.00 +/- 201.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=788000, episode_reward=1650.80 +/- 356.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=789000, episode_reward=1790.80 +/- 197.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=790000, episode_reward=1683.00 +/- 215.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=791000, episode_reward=1566.40 +/- 215.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=792000, episode_reward=1503.00 +/- 285.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=793000, episode_reward=1600.20 +/- 240.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=794000, episode_reward=1786.00 +/- 221.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=795000, episode_reward=1759.40 +/- 106.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=796000, episode_reward=1730.60 +/- 196.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=797000, episode_reward=1552.20 +/- 158.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=798000, episode_reward=1828.40 +/- 166.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=799000, episode_reward=1574.80 +/- 305.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=800000, episode_reward=1579.40 +/- 114.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=801000, episode_reward=1658.60 +/- 116.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=802000, episode_reward=1514.20 +/- 379.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=803000, episode_reward=1729.60 +/- 123.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=804000, episode_reward=1610.80 +/- 234.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=805000, episode_reward=1674.20 +/- 204.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=806000, episode_reward=1557.80 +/- 191.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=807000, episode_reward=1548.40 +/- 237.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=808000, episode_reward=1604.00 +/- 212.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=809000, episode_reward=1835.40 +/- 254.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=810000, episode_reward=1460.80 +/- 288.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=811000, episode_reward=1605.20 +/- 200.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=812000, episode_reward=1784.60 +/- 162.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=813000, episode_reward=1688.20 +/- 68.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=814000, episode_reward=1691.60 +/- 217.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=815000, episode_reward=1534.60 +/- 328.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=816000, episode_reward=1800.00 +/- 178.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=817000, episode_reward=1712.60 +/- 146.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=818000, episode_reward=1596.40 +/- 150.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=819000, episode_reward=1615.60 +/- 321.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=820000, episode_reward=1707.20 +/- 162.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=821000, episode_reward=1695.40 +/- 131.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=822000, episode_reward=1520.20 +/- 100.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=823000, episode_reward=1536.60 +/- 174.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=824000, episode_reward=1609.80 +/- 370.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=825000, episode_reward=1502.80 +/- 246.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=826000, episode_reward=1714.40 +/- 296.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=827000, episode_reward=1388.20 +/- 221.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=828000, episode_reward=1681.40 +/- 187.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=829000, episode_reward=1627.60 +/- 171.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=830000, episode_reward=1732.40 +/- 237.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=831000, episode_reward=1597.40 +/- 122.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=832000, episode_reward=1606.40 +/- 219.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=833000, episode_reward=1560.20 +/- 129.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=834000, episode_reward=1516.80 +/- 263.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=835000, episode_reward=1788.40 +/- 98.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=836000, episode_reward=1500.20 +/- 179.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=837000, episode_reward=1352.40 +/- 299.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=838000, episode_reward=1781.60 +/- 93.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=839000, episode_reward=1622.20 +/- 150.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=840000, episode_reward=1794.60 +/- 278.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=841000, episode_reward=1738.20 +/- 113.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=842000, episode_reward=1758.80 +/- 149.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=843000, episode_reward=1660.00 +/- 151.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=844000, episode_reward=1823.40 +/- 66.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=845000, episode_reward=1707.60 +/- 170.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=846000, episode_reward=1503.60 +/- 182.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=847000, episode_reward=1538.00 +/- 289.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=848000, episode_reward=1599.00 +/- 134.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=849000, episode_reward=1791.80 +/- 110.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=850000, episode_reward=1601.00 +/- 182.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=851000, episode_reward=1553.60 +/- 275.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=852000, episode_reward=1605.40 +/- 194.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=853000, episode_reward=1675.00 +/- 311.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=854000, episode_reward=1597.00 +/- 345.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=855000, episode_reward=1647.00 +/- 149.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=856000, episode_reward=1643.00 +/- 220.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=857000, episode_reward=1748.00 +/- 167.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=858000, episode_reward=1606.80 +/- 230.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=859000, episode_reward=1594.40 +/- 108.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=860000, episode_reward=1801.80 +/- 167.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=861000, episode_reward=1731.00 +/- 150.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=862000, episode_reward=1566.00 +/- 213.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=863000, episode_reward=1565.60 +/- 112.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=864000, episode_reward=1626.60 +/- 269.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=865000, episode_reward=1512.40 +/- 192.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=866000, episode_reward=1741.60 +/- 103.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=867000, episode_reward=1697.00 +/- 162.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=868000, episode_reward=1708.80 +/- 115.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=869000, episode_reward=1604.60 +/- 108.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=870000, episode_reward=1788.40 +/- 57.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=871000, episode_reward=1695.40 +/- 125.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=872000, episode_reward=1603.60 +/- 374.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=873000, episode_reward=1773.40 +/- 178.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=874000, episode_reward=1405.80 +/- 229.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=875000, episode_reward=1780.00 +/- 149.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=876000, episode_reward=1596.40 +/- 179.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=877000, episode_reward=1682.80 +/- 245.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=878000, episode_reward=1637.40 +/- 195.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=879000, episode_reward=1629.00 +/- 172.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=880000, episode_reward=1621.40 +/- 189.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=881000, episode_reward=1657.40 +/- 166.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=882000, episode_reward=1667.80 +/- 81.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=883000, episode_reward=1624.60 +/- 241.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=884000, episode_reward=1674.60 +/- 264.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=885000, episode_reward=1809.00 +/- 224.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=886000, episode_reward=1570.80 +/- 226.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=887000, episode_reward=1599.20 +/- 180.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=888000, episode_reward=1754.80 +/- 140.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=889000, episode_reward=1819.00 +/- 186.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=890000, episode_reward=1736.80 +/- 131.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=891000, episode_reward=1797.80 +/- 79.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=892000, episode_reward=1667.20 +/- 226.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=893000, episode_reward=1664.00 +/- 244.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=894000, episode_reward=1525.00 +/- 341.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=895000, episode_reward=1734.60 +/- 218.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=896000, episode_reward=1710.20 +/- 202.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=897000, episode_reward=1658.00 +/- 214.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=898000, episode_reward=1769.60 +/- 177.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=899000, episode_reward=1406.20 +/- 245.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=900000, episode_reward=1676.20 +/- 300.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=901000, episode_reward=1801.60 +/- 163.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=902000, episode_reward=1665.60 +/- 104.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=903000, episode_reward=1607.40 +/- 305.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=904000, episode_reward=1753.00 +/- 239.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=905000, episode_reward=1750.60 +/- 352.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=906000, episode_reward=1627.40 +/- 149.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=907000, episode_reward=1702.20 +/- 298.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=908000, episode_reward=1646.20 +/- 198.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=909000, episode_reward=1705.00 +/- 198.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=910000, episode_reward=1771.40 +/- 306.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=911000, episode_reward=1669.00 +/- 249.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=912000, episode_reward=1592.60 +/- 188.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=913000, episode_reward=1702.00 +/- 166.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=914000, episode_reward=1469.60 +/- 203.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=915000, episode_reward=1643.60 +/- 176.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=916000, episode_reward=1534.00 +/- 192.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=917000, episode_reward=1652.60 +/- 186.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=918000, episode_reward=1743.20 +/- 152.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=919000, episode_reward=1552.00 +/- 71.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=920000, episode_reward=1672.60 +/- 228.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=921000, episode_reward=1731.40 +/- 158.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=922000, episode_reward=1606.60 +/- 276.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=923000, episode_reward=1730.80 +/- 138.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=924000, episode_reward=1706.60 +/- 161.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=925000, episode_reward=1580.80 +/- 163.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=926000, episode_reward=1561.00 +/- 150.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=927000, episode_reward=1723.20 +/- 150.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=928000, episode_reward=1468.40 +/- 334.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=929000, episode_reward=1604.40 +/- 329.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=930000, episode_reward=1651.60 +/- 168.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=931000, episode_reward=1567.20 +/- 241.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=932000, episode_reward=1812.40 +/- 107.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=933000, episode_reward=1680.80 +/- 288.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=934000, episode_reward=1707.80 +/- 134.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=935000, episode_reward=1703.00 +/- 90.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=936000, episode_reward=1844.40 +/- 172.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=937000, episode_reward=1761.80 +/- 168.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=938000, episode_reward=1480.00 +/- 243.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=939000, episode_reward=1842.20 +/- 256.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=940000, episode_reward=1494.20 +/- 101.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=941000, episode_reward=1693.00 +/- 247.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=942000, episode_reward=1554.60 +/- 132.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=943000, episode_reward=1776.60 +/- 102.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=944000, episode_reward=1808.20 +/- 152.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=945000, episode_reward=1694.80 +/- 119.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=946000, episode_reward=1707.60 +/- 156.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=947000, episode_reward=1576.20 +/- 267.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=948000, episode_reward=1742.00 +/- 244.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=949000, episode_reward=1682.20 +/- 220.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=950000, episode_reward=1604.20 +/- 138.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=951000, episode_reward=1572.00 +/- 276.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=952000, episode_reward=1623.00 +/- 142.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=953000, episode_reward=1658.00 +/- 223.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=954000, episode_reward=1478.40 +/- 236.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=955000, episode_reward=1450.80 +/- 305.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=956000, episode_reward=1495.20 +/- 103.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=957000, episode_reward=1736.20 +/- 160.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=958000, episode_reward=1614.20 +/- 355.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=959000, episode_reward=1693.20 +/- 264.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=960000, episode_reward=1808.40 +/- 110.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=961000, episode_reward=1719.20 +/- 228.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=962000, episode_reward=1656.80 +/- 233.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=963000, episode_reward=1703.80 +/- 162.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=964000, episode_reward=1792.40 +/- 77.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=965000, episode_reward=1498.00 +/- 79.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=966000, episode_reward=1775.60 +/- 123.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=967000, episode_reward=1665.20 +/- 104.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=968000, episode_reward=1599.80 +/- 135.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=969000, episode_reward=1479.40 +/- 231.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=970000, episode_reward=1787.60 +/- 178.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=971000, episode_reward=1760.60 +/- 217.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=972000, episode_reward=1741.20 +/- 182.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=973000, episode_reward=1743.20 +/- 77.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=974000, episode_reward=1609.00 +/- 194.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=975000, episode_reward=1625.40 +/- 314.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=976000, episode_reward=1625.80 +/- 268.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=977000, episode_reward=1431.00 +/- 268.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=978000, episode_reward=1536.40 +/- 181.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=979000, episode_reward=1744.00 +/- 149.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=980000, episode_reward=1704.60 +/- 101.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=981000, episode_reward=1760.80 +/- 50.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=982000, episode_reward=1899.80 +/- 155.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=983000, episode_reward=1696.20 +/- 341.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=984000, episode_reward=1587.60 +/- 185.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=985000, episode_reward=1633.00 +/- 255.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=986000, episode_reward=1711.80 +/- 112.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=987000, episode_reward=1558.80 +/- 150.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=988000, episode_reward=1747.40 +/- 210.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=989000, episode_reward=1590.60 +/- 196.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=990000, episode_reward=1748.60 +/- 201.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=991000, episode_reward=1666.00 +/- 169.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=992000, episode_reward=1635.00 +/- 220.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=993000, episode_reward=1655.00 +/- 300.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=994000, episode_reward=1825.20 +/- 230.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=995000, episode_reward=1730.80 +/- 192.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=996000, episode_reward=1560.40 +/- 268.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=997000, episode_reward=1697.80 +/- 278.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=998000, episode_reward=1718.40 +/- 262.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=999000, episode_reward=1684.00 +/- 182.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1000000, episode_reward=1801.80 +/- 171.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1001000, episode_reward=1600.80 +/- 145.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Gamma = 0.9, Learning Rate = 0.003, Activation Function = ReLU, Neurons = 8, Process noise = 0.1, Measurement noise = 0.0\n",
      "Eval num_timesteps=1000, episode_reward=-53.60 +/- 133.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=132.40 +/- 170.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3000, episode_reward=-241.60 +/- 191.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=4000, episode_reward=-222.80 +/- 149.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=5000, episode_reward=-183.00 +/- 140.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=6000, episode_reward=-360.60 +/- 80.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=7000, episode_reward=225.40 +/- 245.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=8000, episode_reward=-22.60 +/- 131.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=9000, episode_reward=551.60 +/- 371.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=10000, episode_reward=288.20 +/- 370.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=11000, episode_reward=850.20 +/- 372.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=12000, episode_reward=795.00 +/- 482.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=13000, episode_reward=754.00 +/- 176.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=14000, episode_reward=806.60 +/- 277.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=15000, episode_reward=976.80 +/- 213.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=16000, episode_reward=909.00 +/- 248.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=17000, episode_reward=878.60 +/- 312.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=18000, episode_reward=828.20 +/- 246.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=19000, episode_reward=1037.80 +/- 411.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=20000, episode_reward=1110.80 +/- 104.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=21000, episode_reward=1004.00 +/- 319.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=22000, episode_reward=1139.40 +/- 154.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=23000, episode_reward=1241.60 +/- 367.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=24000, episode_reward=1268.40 +/- 301.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=25000, episode_reward=1189.20 +/- 237.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=26000, episode_reward=1143.20 +/- 153.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=27000, episode_reward=1237.80 +/- 173.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=28000, episode_reward=1247.60 +/- 114.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=29000, episode_reward=1200.40 +/- 178.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=30000, episode_reward=1426.40 +/- 89.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=31000, episode_reward=1269.00 +/- 341.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=32000, episode_reward=1231.20 +/- 196.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=33000, episode_reward=1035.80 +/- 96.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=34000, episode_reward=1256.60 +/- 76.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=35000, episode_reward=1200.80 +/- 393.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=36000, episode_reward=-2063.20 +/- 6724.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=37000, episode_reward=1597.80 +/- 114.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=38000, episode_reward=1309.00 +/- 281.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=39000, episode_reward=1410.20 +/- 249.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=40000, episode_reward=1250.80 +/- 246.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=41000, episode_reward=1424.20 +/- 158.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=42000, episode_reward=1454.00 +/- 327.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=43000, episode_reward=1606.80 +/- 363.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=44000, episode_reward=1494.00 +/- 189.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=45000, episode_reward=1268.60 +/- 264.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=46000, episode_reward=1737.00 +/- 193.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=47000, episode_reward=1619.20 +/- 227.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=48000, episode_reward=1647.40 +/- 109.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=49000, episode_reward=1625.40 +/- 113.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=50000, episode_reward=1510.40 +/- 246.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=51000, episode_reward=1493.80 +/- 279.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=52000, episode_reward=1689.80 +/- 142.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=53000, episode_reward=1735.20 +/- 241.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=54000, episode_reward=1516.80 +/- 353.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=55000, episode_reward=1777.80 +/- 116.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=56000, episode_reward=1678.60 +/- 162.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=57000, episode_reward=1612.40 +/- 124.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=58000, episode_reward=1750.20 +/- 84.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=59000, episode_reward=1485.60 +/- 129.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=60000, episode_reward=1625.40 +/- 382.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=61000, episode_reward=1707.40 +/- 129.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=62000, episode_reward=1687.20 +/- 172.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=63000, episode_reward=1490.80 +/- 188.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=64000, episode_reward=1421.00 +/- 54.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=65000, episode_reward=1476.40 +/- 154.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=66000, episode_reward=1430.00 +/- 231.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=67000, episode_reward=1629.60 +/- 177.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=68000, episode_reward=1557.60 +/- 198.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=69000, episode_reward=1519.00 +/- 168.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=70000, episode_reward=1703.80 +/- 160.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=71000, episode_reward=1748.40 +/- 136.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=72000, episode_reward=1803.20 +/- 97.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=73000, episode_reward=1460.80 +/- 226.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=74000, episode_reward=1573.60 +/- 250.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=75000, episode_reward=1530.80 +/- 96.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=76000, episode_reward=1553.80 +/- 208.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=77000, episode_reward=1502.00 +/- 287.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=78000, episode_reward=1384.00 +/- 153.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=79000, episode_reward=1745.80 +/- 102.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=80000, episode_reward=1506.40 +/- 220.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=81000, episode_reward=1387.00 +/- 274.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=82000, episode_reward=1441.00 +/- 184.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=83000, episode_reward=1640.80 +/- 229.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=84000, episode_reward=1756.00 +/- 185.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=85000, episode_reward=1697.40 +/- 175.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=86000, episode_reward=1703.40 +/- 119.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=87000, episode_reward=1854.20 +/- 124.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=88000, episode_reward=1597.20 +/- 71.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=89000, episode_reward=1610.60 +/- 216.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=90000, episode_reward=1850.00 +/- 273.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=91000, episode_reward=1722.20 +/- 164.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=92000, episode_reward=1840.40 +/- 274.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=93000, episode_reward=1510.60 +/- 431.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=94000, episode_reward=1336.40 +/- 322.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=95000, episode_reward=1536.20 +/- 199.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=96000, episode_reward=1475.00 +/- 299.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=97000, episode_reward=1618.60 +/- 153.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=98000, episode_reward=1584.20 +/- 141.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=99000, episode_reward=1666.40 +/- 297.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=100000, episode_reward=1705.40 +/- 111.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=101000, episode_reward=1726.60 +/- 369.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=102000, episode_reward=1416.80 +/- 197.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=103000, episode_reward=1682.00 +/- 256.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=104000, episode_reward=1615.40 +/- 93.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=105000, episode_reward=1555.80 +/- 440.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=106000, episode_reward=1494.40 +/- 197.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=107000, episode_reward=1555.40 +/- 89.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=108000, episode_reward=1720.00 +/- 245.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=109000, episode_reward=1581.00 +/- 168.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=110000, episode_reward=1648.00 +/- 253.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=111000, episode_reward=1645.20 +/- 236.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=112000, episode_reward=1677.80 +/- 187.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=113000, episode_reward=1772.00 +/- 107.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=114000, episode_reward=1651.40 +/- 102.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=115000, episode_reward=1728.80 +/- 202.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=116000, episode_reward=1658.60 +/- 206.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=117000, episode_reward=1439.20 +/- 281.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=118000, episode_reward=1570.20 +/- 209.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=119000, episode_reward=1590.80 +/- 223.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=120000, episode_reward=1750.40 +/- 170.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=121000, episode_reward=1794.40 +/- 145.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=122000, episode_reward=1564.20 +/- 195.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=123000, episode_reward=1550.20 +/- 177.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=124000, episode_reward=1702.20 +/- 192.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=125000, episode_reward=1739.40 +/- 209.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=126000, episode_reward=1736.00 +/- 175.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=127000, episode_reward=1768.20 +/- 132.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=128000, episode_reward=1734.00 +/- 159.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=129000, episode_reward=1665.40 +/- 107.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=130000, episode_reward=1453.60 +/- 329.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=131000, episode_reward=1624.40 +/- 201.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=132000, episode_reward=1336.20 +/- 158.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=133000, episode_reward=1552.20 +/- 66.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=134000, episode_reward=1694.00 +/- 196.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=135000, episode_reward=1569.80 +/- 351.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=136000, episode_reward=1750.80 +/- 162.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=137000, episode_reward=1517.60 +/- 105.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=138000, episode_reward=1816.00 +/- 181.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=139000, episode_reward=1559.00 +/- 280.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=140000, episode_reward=1837.60 +/- 229.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=141000, episode_reward=1584.20 +/- 106.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=142000, episode_reward=1514.20 +/- 176.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=143000, episode_reward=1668.40 +/- 272.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=144000, episode_reward=1540.40 +/- 170.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=145000, episode_reward=1710.40 +/- 217.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=146000, episode_reward=1671.40 +/- 113.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=147000, episode_reward=1740.80 +/- 103.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=148000, episode_reward=1750.00 +/- 108.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=149000, episode_reward=1661.00 +/- 248.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=150000, episode_reward=1714.00 +/- 177.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=151000, episode_reward=1644.20 +/- 340.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=152000, episode_reward=1649.20 +/- 132.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=153000, episode_reward=1637.40 +/- 134.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=154000, episode_reward=1522.80 +/- 307.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=155000, episode_reward=1817.60 +/- 217.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=156000, episode_reward=1756.20 +/- 281.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=157000, episode_reward=1851.60 +/- 195.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=158000, episode_reward=1720.00 +/- 190.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=159000, episode_reward=1543.00 +/- 259.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=160000, episode_reward=1510.40 +/- 240.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=161000, episode_reward=1766.00 +/- 151.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=162000, episode_reward=1432.20 +/- 389.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=163000, episode_reward=1618.20 +/- 111.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=164000, episode_reward=1566.00 +/- 197.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=165000, episode_reward=1630.00 +/- 174.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=166000, episode_reward=1531.60 +/- 189.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=167000, episode_reward=1591.40 +/- 140.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=168000, episode_reward=1599.60 +/- 188.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=169000, episode_reward=1440.20 +/- 274.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=170000, episode_reward=1775.00 +/- 273.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=171000, episode_reward=1510.40 +/- 254.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=172000, episode_reward=1673.20 +/- 217.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=173000, episode_reward=1533.60 +/- 245.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=174000, episode_reward=1500.00 +/- 127.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=175000, episode_reward=1683.40 +/- 122.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=176000, episode_reward=1715.00 +/- 110.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=177000, episode_reward=1857.60 +/- 249.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=178000, episode_reward=1681.40 +/- 251.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=179000, episode_reward=1425.60 +/- 283.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=180000, episode_reward=1689.00 +/- 273.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=181000, episode_reward=1598.60 +/- 285.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=182000, episode_reward=1706.80 +/- 113.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=183000, episode_reward=1591.00 +/- 201.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=184000, episode_reward=1704.20 +/- 127.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=185000, episode_reward=1403.80 +/- 253.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=186000, episode_reward=1671.60 +/- 108.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=187000, episode_reward=1729.20 +/- 193.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=188000, episode_reward=1692.80 +/- 131.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=189000, episode_reward=1651.20 +/- 91.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=190000, episode_reward=1441.40 +/- 234.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=191000, episode_reward=1600.40 +/- 333.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=192000, episode_reward=1475.80 +/- 184.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=193000, episode_reward=1775.40 +/- 202.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=194000, episode_reward=1748.20 +/- 306.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=195000, episode_reward=1847.20 +/- 89.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=196000, episode_reward=1686.60 +/- 173.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=197000, episode_reward=1627.40 +/- 208.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=198000, episode_reward=1618.80 +/- 180.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=199000, episode_reward=1742.60 +/- 149.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=200000, episode_reward=1802.20 +/- 145.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=201000, episode_reward=1643.80 +/- 92.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=202000, episode_reward=1816.20 +/- 212.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=203000, episode_reward=1755.80 +/- 108.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=204000, episode_reward=1703.60 +/- 200.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=205000, episode_reward=1709.00 +/- 216.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=206000, episode_reward=1681.80 +/- 236.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=207000, episode_reward=1746.00 +/- 232.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=208000, episode_reward=1768.40 +/- 221.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=209000, episode_reward=1771.20 +/- 136.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=210000, episode_reward=1684.00 +/- 146.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=211000, episode_reward=1745.20 +/- 199.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=212000, episode_reward=1666.20 +/- 281.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=213000, episode_reward=1732.20 +/- 138.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=214000, episode_reward=1781.40 +/- 227.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=215000, episode_reward=1745.80 +/- 263.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=216000, episode_reward=1642.60 +/- 153.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=217000, episode_reward=1630.60 +/- 118.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=218000, episode_reward=1765.20 +/- 166.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=219000, episode_reward=1799.80 +/- 151.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=220000, episode_reward=1692.00 +/- 239.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=221000, episode_reward=1788.20 +/- 132.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=222000, episode_reward=1646.20 +/- 156.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=223000, episode_reward=1511.80 +/- 306.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=224000, episode_reward=1745.60 +/- 206.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=225000, episode_reward=1599.80 +/- 210.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=226000, episode_reward=1683.00 +/- 105.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=227000, episode_reward=1705.60 +/- 229.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=228000, episode_reward=1732.00 +/- 298.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=229000, episode_reward=1713.60 +/- 224.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=230000, episode_reward=1862.80 +/- 125.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=231000, episode_reward=1764.20 +/- 138.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=232000, episode_reward=1623.60 +/- 238.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=233000, episode_reward=1612.40 +/- 148.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=234000, episode_reward=1769.00 +/- 160.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=235000, episode_reward=1650.40 +/- 119.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=236000, episode_reward=1648.40 +/- 256.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=237000, episode_reward=1664.80 +/- 183.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=238000, episode_reward=1727.80 +/- 163.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=239000, episode_reward=1680.00 +/- 147.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=240000, episode_reward=1800.80 +/- 177.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=241000, episode_reward=1715.20 +/- 306.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=242000, episode_reward=1746.20 +/- 161.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=243000, episode_reward=1591.40 +/- 249.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=244000, episode_reward=1747.60 +/- 199.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=245000, episode_reward=1845.40 +/- 142.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=246000, episode_reward=1670.60 +/- 263.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=247000, episode_reward=1695.40 +/- 180.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=248000, episode_reward=1646.80 +/- 215.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=249000, episode_reward=1750.80 +/- 348.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=250000, episode_reward=1545.40 +/- 239.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=251000, episode_reward=1822.20 +/- 130.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=252000, episode_reward=1519.20 +/- 273.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=253000, episode_reward=1595.20 +/- 159.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=254000, episode_reward=1508.00 +/- 148.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=255000, episode_reward=1865.60 +/- 112.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=256000, episode_reward=1752.40 +/- 148.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=257000, episode_reward=1538.60 +/- 211.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=258000, episode_reward=1853.20 +/- 126.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=259000, episode_reward=1819.40 +/- 155.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=260000, episode_reward=1657.40 +/- 159.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=261000, episode_reward=1614.40 +/- 308.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=262000, episode_reward=1744.00 +/- 103.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=263000, episode_reward=1659.80 +/- 204.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=264000, episode_reward=1672.00 +/- 114.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=265000, episode_reward=1697.40 +/- 240.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=266000, episode_reward=1808.40 +/- 231.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=267000, episode_reward=1851.40 +/- 206.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=268000, episode_reward=1724.00 +/- 145.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=269000, episode_reward=1711.40 +/- 129.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=270000, episode_reward=1668.40 +/- 203.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=271000, episode_reward=1881.80 +/- 148.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=272000, episode_reward=1470.00 +/- 324.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=273000, episode_reward=1639.40 +/- 229.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=274000, episode_reward=1592.80 +/- 188.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=275000, episode_reward=1810.60 +/- 221.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=276000, episode_reward=1702.60 +/- 323.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=277000, episode_reward=1700.80 +/- 113.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=278000, episode_reward=1824.60 +/- 78.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=279000, episode_reward=1567.40 +/- 221.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=280000, episode_reward=1765.80 +/- 191.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=281000, episode_reward=1681.00 +/- 108.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=282000, episode_reward=1520.40 +/- 179.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=283000, episode_reward=1522.60 +/- 124.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=284000, episode_reward=1695.60 +/- 195.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=285000, episode_reward=1444.60 +/- 287.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=286000, episode_reward=1666.80 +/- 310.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=287000, episode_reward=1670.20 +/- 140.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=288000, episode_reward=1794.60 +/- 173.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=289000, episode_reward=1725.40 +/- 207.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=290000, episode_reward=1685.80 +/- 133.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=291000, episode_reward=1723.00 +/- 164.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=292000, episode_reward=1660.60 +/- 181.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=293000, episode_reward=1643.40 +/- 314.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=294000, episode_reward=1637.20 +/- 136.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=295000, episode_reward=1671.80 +/- 265.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=296000, episode_reward=1689.80 +/- 212.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=297000, episode_reward=1621.60 +/- 171.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=298000, episode_reward=1658.00 +/- 133.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=299000, episode_reward=1687.20 +/- 157.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=300000, episode_reward=1458.20 +/- 139.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=301000, episode_reward=1691.40 +/- 62.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=302000, episode_reward=1771.80 +/- 165.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=303000, episode_reward=1815.40 +/- 209.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=304000, episode_reward=1299.20 +/- 383.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=305000, episode_reward=1738.00 +/- 145.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=306000, episode_reward=1601.20 +/- 226.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=307000, episode_reward=1720.80 +/- 133.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=308000, episode_reward=1578.80 +/- 73.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=309000, episode_reward=1584.60 +/- 266.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=310000, episode_reward=1665.80 +/- 270.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=311000, episode_reward=1556.00 +/- 225.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=312000, episode_reward=1721.60 +/- 98.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=313000, episode_reward=1570.40 +/- 209.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=314000, episode_reward=1738.20 +/- 267.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=315000, episode_reward=1687.40 +/- 200.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=316000, episode_reward=1657.00 +/- 189.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=317000, episode_reward=1602.00 +/- 274.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=318000, episode_reward=1619.60 +/- 259.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=319000, episode_reward=1514.60 +/- 60.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=320000, episode_reward=1788.60 +/- 251.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=321000, episode_reward=1667.00 +/- 138.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=322000, episode_reward=1515.20 +/- 184.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=323000, episode_reward=1563.80 +/- 160.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=324000, episode_reward=1602.20 +/- 141.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=325000, episode_reward=1669.20 +/- 228.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=326000, episode_reward=1759.20 +/- 108.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=327000, episode_reward=1811.20 +/- 109.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=328000, episode_reward=1574.40 +/- 128.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=329000, episode_reward=1711.80 +/- 145.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=330000, episode_reward=1739.40 +/- 209.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=331000, episode_reward=1620.80 +/- 267.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=332000, episode_reward=1564.60 +/- 227.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=333000, episode_reward=1579.20 +/- 258.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=334000, episode_reward=1769.60 +/- 93.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=335000, episode_reward=1700.40 +/- 159.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=336000, episode_reward=1642.80 +/- 262.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=337000, episode_reward=1718.00 +/- 185.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=338000, episode_reward=1801.60 +/- 237.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=339000, episode_reward=1695.60 +/- 265.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=340000, episode_reward=1693.00 +/- 196.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=341000, episode_reward=1730.60 +/- 202.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=342000, episode_reward=1726.00 +/- 142.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=343000, episode_reward=1619.00 +/- 191.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=344000, episode_reward=1426.20 +/- 202.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=345000, episode_reward=1539.40 +/- 130.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=346000, episode_reward=1775.00 +/- 105.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=347000, episode_reward=1582.80 +/- 319.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=348000, episode_reward=1585.60 +/- 221.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=349000, episode_reward=1703.00 +/- 249.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=350000, episode_reward=1650.80 +/- 206.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=351000, episode_reward=1696.40 +/- 221.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=352000, episode_reward=1565.80 +/- 213.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=353000, episode_reward=1690.40 +/- 231.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=354000, episode_reward=1701.60 +/- 89.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=355000, episode_reward=1632.20 +/- 407.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=356000, episode_reward=1734.00 +/- 181.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=357000, episode_reward=1661.20 +/- 279.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=358000, episode_reward=1628.80 +/- 166.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=359000, episode_reward=1697.40 +/- 204.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=360000, episode_reward=1673.40 +/- 172.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=361000, episode_reward=1619.60 +/- 225.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=362000, episode_reward=1828.40 +/- 78.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=363000, episode_reward=1731.60 +/- 201.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=364000, episode_reward=1665.20 +/- 218.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=365000, episode_reward=1557.60 +/- 262.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=366000, episode_reward=1760.40 +/- 109.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=367000, episode_reward=1770.00 +/- 312.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=368000, episode_reward=1787.20 +/- 236.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=369000, episode_reward=1822.80 +/- 156.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=370000, episode_reward=1617.00 +/- 180.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=371000, episode_reward=1425.20 +/- 195.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=372000, episode_reward=1606.60 +/- 243.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=373000, episode_reward=1397.60 +/- 197.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=374000, episode_reward=1601.40 +/- 153.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=375000, episode_reward=1769.20 +/- 147.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=376000, episode_reward=1716.20 +/- 128.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=377000, episode_reward=1438.60 +/- 247.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=378000, episode_reward=1689.40 +/- 171.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=379000, episode_reward=1797.60 +/- 228.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=380000, episode_reward=1547.40 +/- 346.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=381000, episode_reward=1890.60 +/- 208.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=382000, episode_reward=1850.00 +/- 56.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=383000, episode_reward=1784.40 +/- 104.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=384000, episode_reward=1630.80 +/- 127.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=385000, episode_reward=1798.60 +/- 156.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=386000, episode_reward=1729.00 +/- 204.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=387000, episode_reward=1703.80 +/- 97.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=388000, episode_reward=1612.20 +/- 405.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=389000, episode_reward=1728.20 +/- 117.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=390000, episode_reward=1601.20 +/- 263.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=391000, episode_reward=1782.80 +/- 211.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=392000, episode_reward=1645.00 +/- 132.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=393000, episode_reward=1825.40 +/- 257.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=394000, episode_reward=1632.60 +/- 268.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=395000, episode_reward=1659.20 +/- 192.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=396000, episode_reward=1692.00 +/- 109.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=397000, episode_reward=1650.20 +/- 304.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=398000, episode_reward=1629.00 +/- 111.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=399000, episode_reward=1575.20 +/- 143.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=400000, episode_reward=1813.00 +/- 176.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=401000, episode_reward=1781.00 +/- 197.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=402000, episode_reward=1729.00 +/- 124.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=403000, episode_reward=1566.00 +/- 144.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=404000, episode_reward=1819.60 +/- 233.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=405000, episode_reward=1633.60 +/- 305.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=406000, episode_reward=1628.40 +/- 261.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=407000, episode_reward=1729.60 +/- 128.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=408000, episode_reward=1649.40 +/- 147.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=409000, episode_reward=1726.60 +/- 275.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=410000, episode_reward=1770.20 +/- 171.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=411000, episode_reward=1691.80 +/- 160.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=412000, episode_reward=1637.20 +/- 173.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=413000, episode_reward=1696.00 +/- 64.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=414000, episode_reward=1650.00 +/- 329.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=415000, episode_reward=1801.80 +/- 132.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=416000, episode_reward=1602.40 +/- 229.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=417000, episode_reward=1617.40 +/- 214.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=418000, episode_reward=1424.60 +/- 234.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=419000, episode_reward=1644.20 +/- 432.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=420000, episode_reward=1620.80 +/- 68.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=421000, episode_reward=1612.20 +/- 110.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=422000, episode_reward=1701.60 +/- 139.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=423000, episode_reward=1881.20 +/- 157.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=424000, episode_reward=1735.00 +/- 115.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=425000, episode_reward=1511.80 +/- 225.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=426000, episode_reward=1683.20 +/- 173.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=427000, episode_reward=1754.40 +/- 193.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=428000, episode_reward=1770.00 +/- 218.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=429000, episode_reward=1599.00 +/- 197.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=430000, episode_reward=1682.20 +/- 131.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=431000, episode_reward=1549.80 +/- 113.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=432000, episode_reward=1909.00 +/- 117.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=433000, episode_reward=1921.80 +/- 119.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=434000, episode_reward=1765.20 +/- 83.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=435000, episode_reward=1706.80 +/- 186.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=436000, episode_reward=1520.80 +/- 171.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=437000, episode_reward=1808.40 +/- 73.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=438000, episode_reward=1706.00 +/- 131.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=439000, episode_reward=1571.80 +/- 217.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=440000, episode_reward=1653.80 +/- 209.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=441000, episode_reward=1537.60 +/- 215.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=442000, episode_reward=1688.60 +/- 192.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=443000, episode_reward=1528.00 +/- 322.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=444000, episode_reward=1564.60 +/- 131.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=445000, episode_reward=1511.00 +/- 289.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=446000, episode_reward=1632.00 +/- 199.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=447000, episode_reward=1701.40 +/- 88.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=448000, episode_reward=1794.00 +/- 239.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=449000, episode_reward=1602.80 +/- 358.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=450000, episode_reward=1669.20 +/- 343.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=451000, episode_reward=1689.00 +/- 392.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=452000, episode_reward=1754.60 +/- 138.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=453000, episode_reward=1667.00 +/- 130.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=454000, episode_reward=1689.60 +/- 218.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=455000, episode_reward=1750.60 +/- 166.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=456000, episode_reward=1557.20 +/- 277.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=457000, episode_reward=1484.80 +/- 205.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=458000, episode_reward=1733.20 +/- 122.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=459000, episode_reward=1719.60 +/- 257.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=460000, episode_reward=1691.60 +/- 216.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=461000, episode_reward=1508.80 +/- 142.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=462000, episode_reward=1764.00 +/- 135.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=463000, episode_reward=1691.60 +/- 221.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=464000, episode_reward=1604.80 +/- 174.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=465000, episode_reward=1793.60 +/- 155.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=466000, episode_reward=1708.60 +/- 134.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=467000, episode_reward=1473.20 +/- 247.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=468000, episode_reward=1543.80 +/- 169.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=469000, episode_reward=1809.20 +/- 150.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=470000, episode_reward=1498.80 +/- 167.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=471000, episode_reward=1787.20 +/- 147.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=472000, episode_reward=1867.00 +/- 144.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=473000, episode_reward=1708.80 +/- 250.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=474000, episode_reward=1779.20 +/- 52.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=475000, episode_reward=1654.20 +/- 294.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=476000, episode_reward=1431.80 +/- 168.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=477000, episode_reward=1521.60 +/- 216.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=478000, episode_reward=1487.80 +/- 249.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=479000, episode_reward=1693.60 +/- 121.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=480000, episode_reward=1667.80 +/- 54.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=481000, episode_reward=1585.00 +/- 317.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=482000, episode_reward=1708.80 +/- 188.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=483000, episode_reward=1516.60 +/- 193.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=484000, episode_reward=1791.40 +/- 121.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=485000, episode_reward=1681.40 +/- 250.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=486000, episode_reward=1794.80 +/- 138.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=487000, episode_reward=1449.40 +/- 379.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=488000, episode_reward=1536.60 +/- 274.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=489000, episode_reward=1542.20 +/- 188.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=490000, episode_reward=1703.60 +/- 252.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=491000, episode_reward=1689.60 +/- 116.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=492000, episode_reward=1574.80 +/- 262.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=493000, episode_reward=1665.60 +/- 43.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=494000, episode_reward=1539.20 +/- 244.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=495000, episode_reward=1639.00 +/- 258.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=496000, episode_reward=1663.00 +/- 205.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=497000, episode_reward=1751.60 +/- 183.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=498000, episode_reward=1662.80 +/- 195.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=499000, episode_reward=1580.00 +/- 203.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=500000, episode_reward=1735.00 +/- 297.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=501000, episode_reward=1709.20 +/- 207.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=502000, episode_reward=1555.80 +/- 300.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=503000, episode_reward=1716.40 +/- 190.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=504000, episode_reward=1744.60 +/- 220.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=505000, episode_reward=1598.60 +/- 188.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=506000, episode_reward=1654.60 +/- 201.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=507000, episode_reward=1648.60 +/- 262.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=508000, episode_reward=1570.80 +/- 172.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=509000, episode_reward=1741.40 +/- 235.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=510000, episode_reward=1742.40 +/- 102.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=511000, episode_reward=1528.00 +/- 278.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=512000, episode_reward=1666.40 +/- 231.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=513000, episode_reward=1893.80 +/- 154.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=514000, episode_reward=1615.20 +/- 373.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=515000, episode_reward=1581.80 +/- 165.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=516000, episode_reward=1751.20 +/- 50.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=517000, episode_reward=1645.40 +/- 319.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=518000, episode_reward=1798.60 +/- 209.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=519000, episode_reward=1645.60 +/- 222.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=520000, episode_reward=1544.80 +/- 211.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=521000, episode_reward=1650.40 +/- 93.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=522000, episode_reward=1525.00 +/- 118.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=523000, episode_reward=1532.00 +/- 285.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=524000, episode_reward=1634.00 +/- 191.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=525000, episode_reward=1808.80 +/- 110.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=526000, episode_reward=1721.60 +/- 205.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=527000, episode_reward=1635.80 +/- 195.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=528000, episode_reward=1534.40 +/- 161.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=529000, episode_reward=1629.00 +/- 189.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=530000, episode_reward=1565.00 +/- 99.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=531000, episode_reward=1784.80 +/- 219.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=532000, episode_reward=1617.60 +/- 145.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=533000, episode_reward=1712.40 +/- 150.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=534000, episode_reward=1624.40 +/- 120.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=535000, episode_reward=1716.00 +/- 288.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=536000, episode_reward=1625.20 +/- 202.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=537000, episode_reward=1426.80 +/- 178.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=538000, episode_reward=1714.20 +/- 234.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=539000, episode_reward=1628.40 +/- 147.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=540000, episode_reward=1560.40 +/- 126.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=541000, episode_reward=1786.20 +/- 142.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=542000, episode_reward=1643.60 +/- 200.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=543000, episode_reward=1718.20 +/- 227.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=544000, episode_reward=1654.00 +/- 115.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=545000, episode_reward=1479.80 +/- 194.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=546000, episode_reward=1533.20 +/- 218.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=547000, episode_reward=1701.60 +/- 114.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=548000, episode_reward=1801.20 +/- 228.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=549000, episode_reward=1711.80 +/- 192.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=550000, episode_reward=1679.60 +/- 138.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=551000, episode_reward=1848.20 +/- 44.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=552000, episode_reward=1683.20 +/- 107.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=553000, episode_reward=1678.80 +/- 186.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=554000, episode_reward=1642.60 +/- 326.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=555000, episode_reward=1676.60 +/- 229.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=556000, episode_reward=1655.40 +/- 344.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=557000, episode_reward=1799.80 +/- 48.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=558000, episode_reward=1748.80 +/- 245.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=559000, episode_reward=1511.80 +/- 314.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=560000, episode_reward=1554.00 +/- 161.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=561000, episode_reward=1781.00 +/- 117.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=562000, episode_reward=1699.80 +/- 184.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=563000, episode_reward=1757.40 +/- 249.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=564000, episode_reward=1759.40 +/- 142.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=565000, episode_reward=1489.00 +/- 194.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=566000, episode_reward=1707.20 +/- 203.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=567000, episode_reward=1671.00 +/- 368.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=568000, episode_reward=1565.00 +/- 270.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=569000, episode_reward=1515.20 +/- 332.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=570000, episode_reward=1576.40 +/- 257.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=571000, episode_reward=1511.80 +/- 153.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=572000, episode_reward=1702.60 +/- 308.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=573000, episode_reward=1687.40 +/- 257.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=574000, episode_reward=1663.20 +/- 232.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=575000, episode_reward=1655.20 +/- 103.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=576000, episode_reward=1546.60 +/- 258.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=577000, episode_reward=1782.20 +/- 215.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=578000, episode_reward=1608.20 +/- 174.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=579000, episode_reward=1719.80 +/- 184.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=580000, episode_reward=1762.40 +/- 129.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=581000, episode_reward=1879.20 +/- 216.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=582000, episode_reward=1610.00 +/- 158.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=583000, episode_reward=1572.00 +/- 203.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=584000, episode_reward=1707.40 +/- 131.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=585000, episode_reward=1951.80 +/- 272.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=586000, episode_reward=1693.20 +/- 214.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=587000, episode_reward=1540.20 +/- 253.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=588000, episode_reward=1629.00 +/- 166.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=589000, episode_reward=1620.00 +/- 154.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=590000, episode_reward=1756.20 +/- 146.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=591000, episode_reward=1418.80 +/- 300.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=592000, episode_reward=1642.00 +/- 236.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=593000, episode_reward=1553.80 +/- 168.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=594000, episode_reward=1654.00 +/- 195.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=595000, episode_reward=1647.00 +/- 184.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=596000, episode_reward=1577.60 +/- 312.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=597000, episode_reward=1651.80 +/- 126.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=598000, episode_reward=1679.00 +/- 239.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=599000, episode_reward=1853.80 +/- 275.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=600000, episode_reward=1676.60 +/- 54.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=601000, episode_reward=1636.40 +/- 241.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=602000, episode_reward=1529.20 +/- 139.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=603000, episode_reward=1668.80 +/- 100.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=604000, episode_reward=1587.80 +/- 248.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=605000, episode_reward=1581.00 +/- 217.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=606000, episode_reward=1594.60 +/- 187.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=607000, episode_reward=1796.80 +/- 178.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=608000, episode_reward=1537.60 +/- 229.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=609000, episode_reward=1661.80 +/- 221.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=610000, episode_reward=1666.80 +/- 106.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=611000, episode_reward=1688.80 +/- 211.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=612000, episode_reward=1655.60 +/- 212.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=613000, episode_reward=1699.20 +/- 205.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=614000, episode_reward=1764.80 +/- 184.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=615000, episode_reward=1575.00 +/- 248.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=616000, episode_reward=1642.40 +/- 201.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=617000, episode_reward=1577.60 +/- 162.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=618000, episode_reward=1663.20 +/- 307.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=619000, episode_reward=1528.20 +/- 120.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=620000, episode_reward=1630.40 +/- 322.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=621000, episode_reward=1571.80 +/- 274.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=622000, episode_reward=1603.20 +/- 157.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=623000, episode_reward=1648.40 +/- 305.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=624000, episode_reward=1607.60 +/- 250.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=625000, episode_reward=1834.20 +/- 199.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=626000, episode_reward=1544.20 +/- 171.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=627000, episode_reward=1662.40 +/- 175.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=628000, episode_reward=1725.20 +/- 95.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=629000, episode_reward=1628.60 +/- 293.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=630000, episode_reward=1474.20 +/- 335.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=631000, episode_reward=1761.20 +/- 186.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=632000, episode_reward=1736.80 +/- 288.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=633000, episode_reward=1715.00 +/- 172.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=634000, episode_reward=1637.60 +/- 92.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=635000, episode_reward=1711.60 +/- 239.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=636000, episode_reward=1501.80 +/- 337.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=637000, episode_reward=1743.80 +/- 235.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=638000, episode_reward=1993.20 +/- 91.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=639000, episode_reward=1677.80 +/- 275.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=640000, episode_reward=1610.20 +/- 165.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=641000, episode_reward=1628.00 +/- 237.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=642000, episode_reward=1685.00 +/- 160.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=643000, episode_reward=1857.40 +/- 140.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=644000, episode_reward=1622.40 +/- 197.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=645000, episode_reward=1750.40 +/- 251.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=646000, episode_reward=1809.80 +/- 188.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=647000, episode_reward=1735.60 +/- 77.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=648000, episode_reward=1659.60 +/- 222.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=649000, episode_reward=1496.80 +/- 236.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=650000, episode_reward=1741.20 +/- 220.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=651000, episode_reward=1474.40 +/- 277.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=652000, episode_reward=1762.00 +/- 210.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=653000, episode_reward=1603.60 +/- 302.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=654000, episode_reward=1817.20 +/- 196.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=655000, episode_reward=1610.20 +/- 53.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=656000, episode_reward=1690.60 +/- 243.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=657000, episode_reward=1703.80 +/- 178.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=658000, episode_reward=1526.40 +/- 173.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=659000, episode_reward=1888.80 +/- 93.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=660000, episode_reward=1757.00 +/- 213.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=661000, episode_reward=1776.20 +/- 168.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=662000, episode_reward=1725.80 +/- 260.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=663000, episode_reward=1605.40 +/- 129.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=664000, episode_reward=1799.20 +/- 228.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=665000, episode_reward=1694.60 +/- 322.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=666000, episode_reward=1559.00 +/- 160.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=667000, episode_reward=1778.40 +/- 104.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=668000, episode_reward=1651.60 +/- 322.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=669000, episode_reward=1697.80 +/- 125.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=670000, episode_reward=1844.20 +/- 133.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=671000, episode_reward=1672.00 +/- 74.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=672000, episode_reward=1627.80 +/- 172.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=673000, episode_reward=1768.20 +/- 194.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=674000, episode_reward=1787.80 +/- 188.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=675000, episode_reward=1694.60 +/- 223.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=676000, episode_reward=1768.80 +/- 205.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=677000, episode_reward=1708.00 +/- 200.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=678000, episode_reward=1792.00 +/- 186.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=679000, episode_reward=1558.40 +/- 91.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=680000, episode_reward=1784.80 +/- 198.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=681000, episode_reward=1847.20 +/- 178.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=682000, episode_reward=1570.80 +/- 215.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=683000, episode_reward=1616.60 +/- 136.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=684000, episode_reward=1793.80 +/- 161.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=685000, episode_reward=1604.40 +/- 269.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=686000, episode_reward=1456.00 +/- 258.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=687000, episode_reward=1682.80 +/- 178.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=688000, episode_reward=1588.00 +/- 206.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=689000, episode_reward=1698.80 +/- 409.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=690000, episode_reward=1807.60 +/- 248.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=691000, episode_reward=1523.20 +/- 288.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=692000, episode_reward=1789.20 +/- 140.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=693000, episode_reward=1677.40 +/- 228.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=694000, episode_reward=1590.00 +/- 141.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=695000, episode_reward=1772.00 +/- 172.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=696000, episode_reward=1545.60 +/- 289.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=697000, episode_reward=1689.40 +/- 322.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=698000, episode_reward=1594.60 +/- 107.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=699000, episode_reward=1847.00 +/- 234.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=700000, episode_reward=1529.20 +/- 233.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=701000, episode_reward=1806.40 +/- 243.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=702000, episode_reward=1758.20 +/- 176.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=703000, episode_reward=1519.80 +/- 351.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=704000, episode_reward=1573.60 +/- 176.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=705000, episode_reward=1509.80 +/- 243.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=706000, episode_reward=1569.80 +/- 155.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=707000, episode_reward=1834.80 +/- 322.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=708000, episode_reward=1652.60 +/- 137.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=709000, episode_reward=1635.00 +/- 251.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=710000, episode_reward=1497.00 +/- 206.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=711000, episode_reward=1673.20 +/- 306.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=712000, episode_reward=1738.60 +/- 301.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=713000, episode_reward=1655.20 +/- 233.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=714000, episode_reward=1838.40 +/- 167.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=715000, episode_reward=1706.80 +/- 186.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=716000, episode_reward=1688.00 +/- 298.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=717000, episode_reward=1716.60 +/- 307.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=718000, episode_reward=1816.40 +/- 288.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=719000, episode_reward=1564.60 +/- 275.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=720000, episode_reward=1634.60 +/- 185.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=721000, episode_reward=1776.80 +/- 241.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=722000, episode_reward=1631.40 +/- 97.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=723000, episode_reward=1674.40 +/- 283.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=724000, episode_reward=1568.40 +/- 83.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=725000, episode_reward=1747.40 +/- 55.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=726000, episode_reward=1718.00 +/- 244.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=727000, episode_reward=1641.00 +/- 184.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=728000, episode_reward=1431.80 +/- 310.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=729000, episode_reward=1662.60 +/- 253.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=730000, episode_reward=1511.40 +/- 48.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=731000, episode_reward=1782.00 +/- 96.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=732000, episode_reward=1509.20 +/- 219.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=733000, episode_reward=1674.40 +/- 216.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=734000, episode_reward=1720.80 +/- 112.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=735000, episode_reward=1546.20 +/- 253.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=736000, episode_reward=1777.20 +/- 185.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=737000, episode_reward=1756.00 +/- 167.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=738000, episode_reward=1791.40 +/- 105.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=739000, episode_reward=1467.00 +/- 256.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=740000, episode_reward=1451.80 +/- 301.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=741000, episode_reward=1646.80 +/- 94.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=742000, episode_reward=1619.00 +/- 333.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=743000, episode_reward=1599.00 +/- 283.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=744000, episode_reward=1651.20 +/- 217.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=745000, episode_reward=1841.60 +/- 187.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=746000, episode_reward=1814.40 +/- 334.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=747000, episode_reward=1674.60 +/- 156.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=748000, episode_reward=1579.80 +/- 202.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=749000, episode_reward=1660.60 +/- 180.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=750000, episode_reward=1845.40 +/- 192.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=751000, episode_reward=1769.00 +/- 178.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=752000, episode_reward=1620.00 +/- 168.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=753000, episode_reward=1756.00 +/- 171.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=754000, episode_reward=1476.60 +/- 68.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=755000, episode_reward=1545.00 +/- 305.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=756000, episode_reward=1619.60 +/- 133.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=757000, episode_reward=1858.20 +/- 76.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=758000, episode_reward=1615.40 +/- 138.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=759000, episode_reward=1559.00 +/- 203.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=760000, episode_reward=1633.00 +/- 167.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=761000, episode_reward=1787.20 +/- 170.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=762000, episode_reward=1483.40 +/- 274.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=763000, episode_reward=1422.20 +/- 194.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=764000, episode_reward=1606.20 +/- 205.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=765000, episode_reward=1646.40 +/- 271.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=766000, episode_reward=1708.80 +/- 251.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=767000, episode_reward=1674.40 +/- 202.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=768000, episode_reward=1708.00 +/- 353.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=769000, episode_reward=1755.40 +/- 174.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=770000, episode_reward=1897.20 +/- 189.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=771000, episode_reward=1665.40 +/- 301.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=772000, episode_reward=1662.00 +/- 182.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=773000, episode_reward=1666.60 +/- 167.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=774000, episode_reward=1848.60 +/- 153.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=775000, episode_reward=1804.40 +/- 92.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=776000, episode_reward=1702.40 +/- 231.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=777000, episode_reward=1638.20 +/- 188.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=778000, episode_reward=1652.20 +/- 247.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=779000, episode_reward=1503.60 +/- 267.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=780000, episode_reward=1686.40 +/- 214.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=781000, episode_reward=1662.00 +/- 387.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=782000, episode_reward=1763.00 +/- 221.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=783000, episode_reward=1636.20 +/- 237.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=784000, episode_reward=1667.00 +/- 213.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=785000, episode_reward=1757.00 +/- 376.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=786000, episode_reward=1664.20 +/- 137.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=787000, episode_reward=1748.80 +/- 198.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=788000, episode_reward=1611.80 +/- 174.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=789000, episode_reward=1673.00 +/- 114.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=790000, episode_reward=1940.00 +/- 124.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=791000, episode_reward=1818.80 +/- 224.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=792000, episode_reward=1675.00 +/- 268.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=793000, episode_reward=1763.60 +/- 189.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=794000, episode_reward=1586.00 +/- 317.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=795000, episode_reward=1606.60 +/- 247.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=796000, episode_reward=1511.00 +/- 537.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=797000, episode_reward=1809.20 +/- 85.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=798000, episode_reward=1511.00 +/- 173.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=799000, episode_reward=1664.00 +/- 231.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=800000, episode_reward=1620.00 +/- 261.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=801000, episode_reward=1777.40 +/- 76.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=802000, episode_reward=1848.40 +/- 178.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=803000, episode_reward=1947.40 +/- 159.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=804000, episode_reward=1721.20 +/- 58.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=805000, episode_reward=1728.20 +/- 291.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=806000, episode_reward=1701.00 +/- 63.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=807000, episode_reward=1795.00 +/- 153.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=808000, episode_reward=1689.00 +/- 241.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=809000, episode_reward=1624.00 +/- 120.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=810000, episode_reward=1789.00 +/- 118.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=811000, episode_reward=1840.40 +/- 67.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=812000, episode_reward=1740.40 +/- 60.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=813000, episode_reward=1270.20 +/- 234.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=814000, episode_reward=1745.60 +/- 119.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=815000, episode_reward=1365.40 +/- 251.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=816000, episode_reward=1524.80 +/- 256.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=817000, episode_reward=1594.00 +/- 253.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=818000, episode_reward=1691.00 +/- 263.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=819000, episode_reward=1726.40 +/- 153.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=820000, episode_reward=1915.20 +/- 166.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=821000, episode_reward=1576.00 +/- 111.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=822000, episode_reward=1717.20 +/- 123.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=823000, episode_reward=1639.40 +/- 310.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=824000, episode_reward=1620.40 +/- 103.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=825000, episode_reward=1526.00 +/- 281.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=826000, episode_reward=1726.20 +/- 104.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=827000, episode_reward=1580.20 +/- 188.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=828000, episode_reward=1617.60 +/- 206.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=829000, episode_reward=1453.60 +/- 282.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=830000, episode_reward=1598.40 +/- 70.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=831000, episode_reward=1765.20 +/- 142.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=832000, episode_reward=1482.20 +/- 110.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=833000, episode_reward=1556.80 +/- 315.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=834000, episode_reward=1672.80 +/- 225.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=835000, episode_reward=1601.40 +/- 231.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=836000, episode_reward=1545.00 +/- 208.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=837000, episode_reward=1524.80 +/- 91.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=838000, episode_reward=1783.00 +/- 198.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=839000, episode_reward=1536.00 +/- 269.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=840000, episode_reward=1807.00 +/- 188.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=841000, episode_reward=1615.80 +/- 173.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=842000, episode_reward=1477.00 +/- 201.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=843000, episode_reward=1724.60 +/- 108.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=844000, episode_reward=1675.60 +/- 359.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=845000, episode_reward=1625.40 +/- 249.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=846000, episode_reward=1665.60 +/- 141.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=847000, episode_reward=1566.00 +/- 166.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=848000, episode_reward=1700.80 +/- 221.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=849000, episode_reward=1744.20 +/- 84.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=850000, episode_reward=1519.40 +/- 205.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=851000, episode_reward=1799.00 +/- 201.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=852000, episode_reward=1567.60 +/- 402.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=853000, episode_reward=1479.60 +/- 245.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=854000, episode_reward=1642.00 +/- 124.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=855000, episode_reward=1753.00 +/- 159.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=856000, episode_reward=1516.40 +/- 285.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=857000, episode_reward=1562.40 +/- 214.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=858000, episode_reward=1560.80 +/- 234.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=859000, episode_reward=1777.20 +/- 208.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=860000, episode_reward=1404.40 +/- 339.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=861000, episode_reward=1762.20 +/- 133.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=862000, episode_reward=1511.00 +/- 192.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=863000, episode_reward=1621.80 +/- 196.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=864000, episode_reward=1771.20 +/- 185.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=865000, episode_reward=1703.80 +/- 174.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=866000, episode_reward=1790.80 +/- 132.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=867000, episode_reward=1745.80 +/- 181.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=868000, episode_reward=1787.60 +/- 222.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=869000, episode_reward=1848.60 +/- 204.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=870000, episode_reward=1794.00 +/- 320.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=871000, episode_reward=1575.40 +/- 274.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=872000, episode_reward=1605.00 +/- 282.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=873000, episode_reward=1689.60 +/- 102.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=874000, episode_reward=1850.00 +/- 214.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=875000, episode_reward=1694.00 +/- 222.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=876000, episode_reward=1652.40 +/- 79.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=877000, episode_reward=1475.60 +/- 187.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=878000, episode_reward=1470.40 +/- 263.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=879000, episode_reward=1859.40 +/- 246.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=880000, episode_reward=1430.40 +/- 361.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=881000, episode_reward=1464.20 +/- 357.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=882000, episode_reward=1769.60 +/- 229.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=883000, episode_reward=1760.00 +/- 258.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=884000, episode_reward=1698.80 +/- 103.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=885000, episode_reward=1639.20 +/- 233.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=886000, episode_reward=1639.20 +/- 287.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=887000, episode_reward=1798.20 +/- 180.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=888000, episode_reward=1526.60 +/- 282.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=889000, episode_reward=1706.20 +/- 232.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=890000, episode_reward=1787.60 +/- 204.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=891000, episode_reward=1573.40 +/- 168.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=892000, episode_reward=1803.00 +/- 96.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=893000, episode_reward=1638.20 +/- 300.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=894000, episode_reward=1616.80 +/- 253.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=895000, episode_reward=1538.60 +/- 268.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=896000, episode_reward=1528.20 +/- 267.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=897000, episode_reward=1651.00 +/- 272.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=898000, episode_reward=1687.40 +/- 272.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=899000, episode_reward=1685.20 +/- 199.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=900000, episode_reward=1409.60 +/- 199.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=901000, episode_reward=1651.60 +/- 294.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=902000, episode_reward=1513.20 +/- 135.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=903000, episode_reward=1748.80 +/- 142.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=904000, episode_reward=1462.80 +/- 206.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=905000, episode_reward=1570.00 +/- 176.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=906000, episode_reward=1827.00 +/- 89.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=907000, episode_reward=1590.60 +/- 149.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=908000, episode_reward=1888.20 +/- 129.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=909000, episode_reward=1655.20 +/- 345.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=910000, episode_reward=1763.60 +/- 180.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=911000, episode_reward=1568.60 +/- 207.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=912000, episode_reward=1742.60 +/- 141.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=913000, episode_reward=1671.60 +/- 304.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=914000, episode_reward=1815.20 +/- 298.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=915000, episode_reward=1534.00 +/- 189.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=916000, episode_reward=1587.00 +/- 259.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=917000, episode_reward=1712.60 +/- 132.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=918000, episode_reward=1484.80 +/- 206.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=919000, episode_reward=1599.20 +/- 241.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=920000, episode_reward=1684.40 +/- 235.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=921000, episode_reward=1674.40 +/- 247.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=922000, episode_reward=1602.00 +/- 108.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=923000, episode_reward=1794.20 +/- 104.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=924000, episode_reward=1513.40 +/- 301.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=925000, episode_reward=1845.80 +/- 212.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=926000, episode_reward=1572.80 +/- 158.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=927000, episode_reward=1485.00 +/- 264.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=928000, episode_reward=1620.60 +/- 149.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=929000, episode_reward=1723.40 +/- 329.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=930000, episode_reward=1667.60 +/- 138.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=931000, episode_reward=1846.60 +/- 174.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=932000, episode_reward=1807.60 +/- 122.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=933000, episode_reward=1496.20 +/- 155.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=934000, episode_reward=1680.40 +/- 299.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=935000, episode_reward=1671.40 +/- 198.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=936000, episode_reward=1576.80 +/- 309.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=937000, episode_reward=1643.80 +/- 239.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=938000, episode_reward=1590.60 +/- 207.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=939000, episode_reward=1689.00 +/- 189.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=940000, episode_reward=1662.80 +/- 312.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=941000, episode_reward=1575.00 +/- 259.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=942000, episode_reward=1611.40 +/- 179.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=943000, episode_reward=1581.20 +/- 372.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=944000, episode_reward=1534.20 +/- 154.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=945000, episode_reward=1479.20 +/- 237.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=946000, episode_reward=1686.20 +/- 210.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=947000, episode_reward=1557.40 +/- 221.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=948000, episode_reward=1512.00 +/- 354.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=949000, episode_reward=1697.40 +/- 162.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=950000, episode_reward=1862.60 +/- 135.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=951000, episode_reward=1735.80 +/- 177.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=952000, episode_reward=1629.20 +/- 247.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=953000, episode_reward=1777.80 +/- 188.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=954000, episode_reward=1479.20 +/- 164.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=955000, episode_reward=1825.80 +/- 208.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=956000, episode_reward=1741.00 +/- 244.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=957000, episode_reward=1846.00 +/- 150.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=958000, episode_reward=1716.40 +/- 108.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=959000, episode_reward=1641.20 +/- 149.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=960000, episode_reward=1706.40 +/- 208.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=961000, episode_reward=1631.20 +/- 147.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=962000, episode_reward=1701.40 +/- 234.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=963000, episode_reward=1864.00 +/- 111.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=964000, episode_reward=1764.00 +/- 162.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=965000, episode_reward=1667.00 +/- 206.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=966000, episode_reward=1819.00 +/- 203.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=967000, episode_reward=1565.00 +/- 196.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=968000, episode_reward=1555.60 +/- 221.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=969000, episode_reward=1568.00 +/- 236.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=970000, episode_reward=1668.60 +/- 160.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=971000, episode_reward=1568.00 +/- 253.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=972000, episode_reward=1696.60 +/- 248.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=973000, episode_reward=1603.40 +/- 113.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=974000, episode_reward=1689.00 +/- 273.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=975000, episode_reward=1633.80 +/- 194.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=976000, episode_reward=1715.20 +/- 60.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=977000, episode_reward=1752.40 +/- 199.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=978000, episode_reward=1643.60 +/- 168.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=979000, episode_reward=1566.00 +/- 316.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=980000, episode_reward=1685.80 +/- 298.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=981000, episode_reward=1618.00 +/- 214.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=982000, episode_reward=1844.00 +/- 164.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=983000, episode_reward=1757.00 +/- 191.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=984000, episode_reward=1653.40 +/- 88.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=985000, episode_reward=1538.80 +/- 195.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=986000, episode_reward=1626.80 +/- 132.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=987000, episode_reward=1617.00 +/- 222.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=988000, episode_reward=1574.20 +/- 268.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=989000, episode_reward=1759.20 +/- 99.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=990000, episode_reward=1600.40 +/- 173.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=991000, episode_reward=1680.60 +/- 53.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=992000, episode_reward=1702.40 +/- 52.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=993000, episode_reward=1685.80 +/- 128.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=994000, episode_reward=1818.40 +/- 120.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=995000, episode_reward=1863.20 +/- 188.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=996000, episode_reward=1726.00 +/- 246.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=997000, episode_reward=1704.80 +/- 114.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=998000, episode_reward=1640.80 +/- 289.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=999000, episode_reward=1745.40 +/- 230.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1000000, episode_reward=1560.80 +/- 262.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1001000, episode_reward=1526.20 +/- 219.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Gamma = 0.9, Learning Rate = 0.003, Activation Function = ReLU, Neurons = 8, Process noise = 0.1, Measurement noise = 0.0\n",
      "Eval num_timesteps=1000, episode_reward=-1643.60 +/- 234.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=-1523.60 +/- 248.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3000, episode_reward=-462.00 +/- 148.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4000, episode_reward=-419.80 +/- 136.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=5000, episode_reward=159.60 +/- 205.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=6000, episode_reward=93.00 +/- 288.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=7000, episode_reward=158.20 +/- 226.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=8000, episode_reward=238.60 +/- 118.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=9000, episode_reward=178.20 +/- 110.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=10000, episode_reward=309.80 +/- 107.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=11000, episode_reward=684.20 +/- 175.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=12000, episode_reward=170.00 +/- 186.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=13000, episode_reward=630.40 +/- 147.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=14000, episode_reward=447.80 +/- 159.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=15000, episode_reward=564.80 +/- 85.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=16000, episode_reward=601.00 +/- 206.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=17000, episode_reward=581.60 +/- 404.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=18000, episode_reward=584.60 +/- 149.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=19000, episode_reward=895.80 +/- 299.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=20000, episode_reward=860.00 +/- 115.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=21000, episode_reward=1138.40 +/- 279.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=22000, episode_reward=842.00 +/- 291.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=23000, episode_reward=974.60 +/- 311.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=24000, episode_reward=1124.00 +/- 500.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=25000, episode_reward=1423.80 +/- 214.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=26000, episode_reward=1071.00 +/- 400.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=27000, episode_reward=755.00 +/- 291.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=28000, episode_reward=1009.60 +/- 145.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=29000, episode_reward=1421.80 +/- 164.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=30000, episode_reward=1127.80 +/- 337.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=31000, episode_reward=1269.60 +/- 276.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=32000, episode_reward=1165.40 +/- 149.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=33000, episode_reward=1330.60 +/- 300.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=34000, episode_reward=1231.20 +/- 180.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=35000, episode_reward=-857.20 +/- 4189.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=36000, episode_reward=1392.00 +/- 279.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=37000, episode_reward=1482.00 +/- 124.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=38000, episode_reward=1440.40 +/- 104.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=39000, episode_reward=1585.60 +/- 293.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=1318.60 +/- 273.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=41000, episode_reward=1357.40 +/- 209.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=42000, episode_reward=1651.20 +/- 253.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=43000, episode_reward=1239.60 +/- 186.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=44000, episode_reward=1753.80 +/- 237.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=45000, episode_reward=1536.40 +/- 143.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=46000, episode_reward=1356.20 +/- 246.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=47000, episode_reward=1738.80 +/- 220.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=48000, episode_reward=1572.60 +/- 214.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=49000, episode_reward=1608.00 +/- 286.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=50000, episode_reward=1653.20 +/- 193.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=51000, episode_reward=1539.20 +/- 232.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=52000, episode_reward=1443.60 +/- 214.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=53000, episode_reward=1412.80 +/- 173.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=54000, episode_reward=1670.60 +/- 59.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=55000, episode_reward=1592.40 +/- 143.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=56000, episode_reward=1545.80 +/- 312.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=57000, episode_reward=1517.00 +/- 355.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=58000, episode_reward=1411.80 +/- 270.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=59000, episode_reward=1524.40 +/- 217.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=60000, episode_reward=1665.60 +/- 215.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=61000, episode_reward=1597.20 +/- 281.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=62000, episode_reward=1689.80 +/- 321.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=63000, episode_reward=1449.60 +/- 121.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=64000, episode_reward=1431.40 +/- 325.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=65000, episode_reward=1552.40 +/- 258.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=66000, episode_reward=1585.00 +/- 185.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=67000, episode_reward=1515.80 +/- 196.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=68000, episode_reward=1493.80 +/- 136.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=69000, episode_reward=1656.40 +/- 165.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=70000, episode_reward=1655.40 +/- 141.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=71000, episode_reward=1595.60 +/- 230.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=72000, episode_reward=1671.00 +/- 153.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=73000, episode_reward=1644.80 +/- 100.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=74000, episode_reward=1720.40 +/- 163.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=75000, episode_reward=1643.00 +/- 215.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=76000, episode_reward=1603.20 +/- 256.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=77000, episode_reward=1472.80 +/- 290.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=78000, episode_reward=1408.00 +/- 118.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=79000, episode_reward=1625.60 +/- 256.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=80000, episode_reward=1686.20 +/- 60.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=81000, episode_reward=1547.20 +/- 112.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=82000, episode_reward=1502.00 +/- 272.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=83000, episode_reward=1564.60 +/- 231.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=84000, episode_reward=1621.20 +/- 259.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=85000, episode_reward=1457.40 +/- 185.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=86000, episode_reward=1740.20 +/- 144.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=87000, episode_reward=1760.20 +/- 175.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=88000, episode_reward=1790.80 +/- 112.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=89000, episode_reward=1531.60 +/- 333.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=90000, episode_reward=1711.40 +/- 59.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=91000, episode_reward=1683.60 +/- 225.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=92000, episode_reward=1458.00 +/- 156.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=93000, episode_reward=1727.20 +/- 213.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=94000, episode_reward=1578.40 +/- 221.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=95000, episode_reward=1608.80 +/- 256.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=96000, episode_reward=1584.60 +/- 201.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=97000, episode_reward=1567.80 +/- 176.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=98000, episode_reward=1543.20 +/- 201.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=99000, episode_reward=1590.80 +/- 213.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=100000, episode_reward=1756.40 +/- 136.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=101000, episode_reward=1674.60 +/- 239.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=102000, episode_reward=1576.40 +/- 204.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=103000, episode_reward=1529.00 +/- 239.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=104000, episode_reward=1631.60 +/- 271.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=105000, episode_reward=1593.20 +/- 195.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=106000, episode_reward=1692.80 +/- 75.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=107000, episode_reward=1528.00 +/- 233.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=108000, episode_reward=1374.40 +/- 49.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=109000, episode_reward=1482.40 +/- 197.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=110000, episode_reward=1660.40 +/- 129.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=111000, episode_reward=1643.00 +/- 188.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=112000, episode_reward=1574.40 +/- 212.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=113000, episode_reward=1484.80 +/- 284.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=114000, episode_reward=1516.60 +/- 451.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=115000, episode_reward=1632.20 +/- 270.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=116000, episode_reward=1714.40 +/- 204.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=117000, episode_reward=1485.20 +/- 181.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=118000, episode_reward=1669.20 +/- 99.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=119000, episode_reward=1516.00 +/- 129.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=120000, episode_reward=1651.80 +/- 221.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=121000, episode_reward=1697.60 +/- 80.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=122000, episode_reward=1544.80 +/- 147.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=123000, episode_reward=1769.20 +/- 226.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=124000, episode_reward=1394.60 +/- 146.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=125000, episode_reward=1577.20 +/- 195.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=126000, episode_reward=1533.00 +/- 178.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=127000, episode_reward=1592.60 +/- 226.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=128000, episode_reward=1844.40 +/- 142.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=129000, episode_reward=1650.60 +/- 142.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=130000, episode_reward=1632.20 +/- 168.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=131000, episode_reward=1758.80 +/- 115.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=132000, episode_reward=1757.20 +/- 30.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=133000, episode_reward=1296.80 +/- 263.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=134000, episode_reward=1534.40 +/- 103.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=135000, episode_reward=1582.40 +/- 288.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=136000, episode_reward=1446.60 +/- 245.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=137000, episode_reward=1554.20 +/- 263.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=138000, episode_reward=1618.80 +/- 232.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=139000, episode_reward=1561.20 +/- 119.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=140000, episode_reward=1535.40 +/- 297.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=141000, episode_reward=1507.80 +/- 94.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=142000, episode_reward=1672.40 +/- 165.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=143000, episode_reward=1614.00 +/- 185.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=144000, episode_reward=1584.20 +/- 95.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=145000, episode_reward=1605.00 +/- 111.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=146000, episode_reward=1535.00 +/- 205.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=147000, episode_reward=1728.80 +/- 240.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=148000, episode_reward=1503.00 +/- 179.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=149000, episode_reward=1738.20 +/- 144.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=150000, episode_reward=1620.20 +/- 33.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=151000, episode_reward=1647.20 +/- 207.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=152000, episode_reward=1525.60 +/- 35.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=153000, episode_reward=1618.00 +/- 169.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=154000, episode_reward=1624.80 +/- 102.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=155000, episode_reward=1684.20 +/- 228.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=156000, episode_reward=1686.20 +/- 207.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=157000, episode_reward=1721.60 +/- 163.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=158000, episode_reward=1626.60 +/- 392.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=159000, episode_reward=1637.60 +/- 115.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=160000, episode_reward=1714.20 +/- 195.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=161000, episode_reward=1566.40 +/- 329.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=162000, episode_reward=1615.80 +/- 282.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=163000, episode_reward=1747.60 +/- 287.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=164000, episode_reward=1622.20 +/- 249.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=165000, episode_reward=1648.40 +/- 130.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=166000, episode_reward=1686.80 +/- 128.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=167000, episode_reward=1611.40 +/- 70.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=168000, episode_reward=1708.20 +/- 238.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=169000, episode_reward=1515.40 +/- 249.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=170000, episode_reward=1597.40 +/- 128.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=171000, episode_reward=1590.00 +/- 411.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=172000, episode_reward=1613.60 +/- 277.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=173000, episode_reward=1740.20 +/- 186.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=174000, episode_reward=1684.80 +/- 240.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=175000, episode_reward=1483.60 +/- 236.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=176000, episode_reward=1727.00 +/- 250.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=177000, episode_reward=1609.60 +/- 245.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=178000, episode_reward=1607.00 +/- 340.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=179000, episode_reward=1562.00 +/- 394.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=180000, episode_reward=1562.40 +/- 252.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=181000, episode_reward=1601.80 +/- 102.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=182000, episode_reward=1641.40 +/- 211.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=183000, episode_reward=1574.60 +/- 254.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=184000, episode_reward=1686.40 +/- 272.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=185000, episode_reward=1605.00 +/- 205.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=186000, episode_reward=1779.20 +/- 139.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=187000, episode_reward=1468.60 +/- 188.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=188000, episode_reward=1568.40 +/- 238.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=189000, episode_reward=1754.80 +/- 322.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=190000, episode_reward=1769.40 +/- 102.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=191000, episode_reward=1590.60 +/- 223.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=192000, episode_reward=1616.00 +/- 203.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=193000, episode_reward=1653.60 +/- 166.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=194000, episode_reward=1737.00 +/- 251.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=195000, episode_reward=1650.80 +/- 71.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=196000, episode_reward=1821.20 +/- 96.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=197000, episode_reward=1679.60 +/- 254.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=198000, episode_reward=1496.00 +/- 296.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=199000, episode_reward=1505.60 +/- 173.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=200000, episode_reward=1682.20 +/- 308.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=201000, episode_reward=1656.40 +/- 183.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=202000, episode_reward=1543.80 +/- 187.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=203000, episode_reward=1922.20 +/- 201.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=204000, episode_reward=1619.60 +/- 308.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=205000, episode_reward=1724.80 +/- 167.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=206000, episode_reward=1777.20 +/- 235.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=207000, episode_reward=1841.40 +/- 249.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=208000, episode_reward=1834.20 +/- 125.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=209000, episode_reward=1691.40 +/- 263.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=210000, episode_reward=1616.40 +/- 284.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=211000, episode_reward=1685.20 +/- 223.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=212000, episode_reward=1610.80 +/- 189.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=213000, episode_reward=1490.40 +/- 188.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=214000, episode_reward=1755.00 +/- 164.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=215000, episode_reward=1767.40 +/- 146.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=216000, episode_reward=1634.40 +/- 220.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=217000, episode_reward=1596.60 +/- 231.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=218000, episode_reward=1709.40 +/- 388.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=219000, episode_reward=1377.80 +/- 125.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=220000, episode_reward=1413.40 +/- 273.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=221000, episode_reward=1666.20 +/- 126.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=222000, episode_reward=1699.00 +/- 105.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=223000, episode_reward=1515.00 +/- 285.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=224000, episode_reward=1584.60 +/- 155.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=225000, episode_reward=1777.80 +/- 117.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=226000, episode_reward=1594.20 +/- 307.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=227000, episode_reward=1638.00 +/- 82.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=228000, episode_reward=1643.40 +/- 99.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=229000, episode_reward=1635.80 +/- 105.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=230000, episode_reward=1786.00 +/- 190.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=231000, episode_reward=1711.60 +/- 267.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=232000, episode_reward=1642.60 +/- 279.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=233000, episode_reward=1619.20 +/- 220.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=234000, episode_reward=1662.00 +/- 161.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=235000, episode_reward=1622.60 +/- 165.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=236000, episode_reward=1551.60 +/- 177.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=237000, episode_reward=1640.80 +/- 113.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=238000, episode_reward=1575.40 +/- 192.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=239000, episode_reward=1599.40 +/- 252.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=240000, episode_reward=1610.00 +/- 340.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=241000, episode_reward=1628.40 +/- 134.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=242000, episode_reward=1659.40 +/- 155.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=243000, episode_reward=1485.60 +/- 157.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=244000, episode_reward=1814.60 +/- 138.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=245000, episode_reward=1695.00 +/- 281.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=246000, episode_reward=1559.80 +/- 294.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=247000, episode_reward=1752.00 +/- 53.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=248000, episode_reward=1650.00 +/- 279.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=249000, episode_reward=1669.40 +/- 95.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=250000, episode_reward=1468.80 +/- 158.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=251000, episode_reward=1771.60 +/- 132.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=252000, episode_reward=1730.00 +/- 327.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=253000, episode_reward=1677.00 +/- 180.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=254000, episode_reward=1756.20 +/- 241.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=255000, episode_reward=1687.00 +/- 220.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=256000, episode_reward=1532.20 +/- 207.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=257000, episode_reward=1660.40 +/- 156.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=258000, episode_reward=1709.20 +/- 84.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=259000, episode_reward=1808.60 +/- 184.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=260000, episode_reward=1562.60 +/- 172.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=261000, episode_reward=1540.80 +/- 117.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=262000, episode_reward=1675.20 +/- 236.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=263000, episode_reward=1701.40 +/- 249.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=264000, episode_reward=1469.00 +/- 301.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=265000, episode_reward=1684.60 +/- 186.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=266000, episode_reward=1585.40 +/- 295.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=267000, episode_reward=1482.80 +/- 334.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=268000, episode_reward=1499.20 +/- 233.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=269000, episode_reward=1720.00 +/- 136.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=270000, episode_reward=1828.00 +/- 225.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=271000, episode_reward=1674.00 +/- 127.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=272000, episode_reward=1450.00 +/- 176.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=273000, episode_reward=1724.20 +/- 214.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=274000, episode_reward=1640.20 +/- 325.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=275000, episode_reward=1848.80 +/- 174.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=276000, episode_reward=1694.00 +/- 187.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=277000, episode_reward=1765.80 +/- 164.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=278000, episode_reward=1630.60 +/- 383.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=279000, episode_reward=1575.60 +/- 157.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=280000, episode_reward=1637.80 +/- 345.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=281000, episode_reward=1534.60 +/- 280.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=282000, episode_reward=1697.60 +/- 134.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=283000, episode_reward=1588.60 +/- 101.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=284000, episode_reward=1615.20 +/- 210.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=285000, episode_reward=1757.80 +/- 126.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=286000, episode_reward=1570.80 +/- 201.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=287000, episode_reward=1600.40 +/- 291.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=288000, episode_reward=1526.60 +/- 290.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=289000, episode_reward=1689.00 +/- 257.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=290000, episode_reward=1586.80 +/- 169.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=291000, episode_reward=1426.20 +/- 221.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=292000, episode_reward=1586.80 +/- 301.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=293000, episode_reward=1610.40 +/- 378.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=294000, episode_reward=1595.40 +/- 256.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=295000, episode_reward=1644.20 +/- 207.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=296000, episode_reward=1728.20 +/- 156.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=297000, episode_reward=1766.60 +/- 118.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=298000, episode_reward=1702.20 +/- 187.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=299000, episode_reward=1618.20 +/- 222.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=300000, episode_reward=1595.00 +/- 194.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=301000, episode_reward=1672.00 +/- 81.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=302000, episode_reward=1579.40 +/- 195.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=303000, episode_reward=1668.20 +/- 110.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=304000, episode_reward=1724.80 +/- 186.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=305000, episode_reward=1765.80 +/- 214.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=306000, episode_reward=1458.20 +/- 286.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=307000, episode_reward=1728.40 +/- 168.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=308000, episode_reward=1643.40 +/- 210.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=309000, episode_reward=1565.00 +/- 316.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=310000, episode_reward=1439.60 +/- 285.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=311000, episode_reward=1532.20 +/- 243.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=312000, episode_reward=1655.00 +/- 137.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=313000, episode_reward=1419.00 +/- 174.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=314000, episode_reward=1379.80 +/- 271.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=315000, episode_reward=1605.00 +/- 106.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=316000, episode_reward=1657.60 +/- 156.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=317000, episode_reward=1701.00 +/- 216.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=318000, episode_reward=1784.40 +/- 187.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=319000, episode_reward=1608.40 +/- 271.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=320000, episode_reward=1757.40 +/- 220.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=321000, episode_reward=1627.20 +/- 93.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=322000, episode_reward=1694.40 +/- 144.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=323000, episode_reward=1789.40 +/- 112.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=324000, episode_reward=1767.20 +/- 224.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=325000, episode_reward=1736.20 +/- 156.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=326000, episode_reward=1702.20 +/- 277.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=327000, episode_reward=1590.00 +/- 195.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=328000, episode_reward=1857.80 +/- 107.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=329000, episode_reward=1708.80 +/- 118.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=330000, episode_reward=1529.60 +/- 294.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=331000, episode_reward=1695.80 +/- 228.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=332000, episode_reward=1762.60 +/- 229.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=333000, episode_reward=1570.00 +/- 206.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=334000, episode_reward=1589.60 +/- 271.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=335000, episode_reward=1515.00 +/- 217.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=336000, episode_reward=1585.60 +/- 171.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=337000, episode_reward=1397.60 +/- 202.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=338000, episode_reward=1646.20 +/- 248.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=339000, episode_reward=1813.00 +/- 135.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=340000, episode_reward=1674.00 +/- 225.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=341000, episode_reward=1805.60 +/- 157.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=342000, episode_reward=1694.60 +/- 187.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=343000, episode_reward=1536.60 +/- 272.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=344000, episode_reward=1747.00 +/- 260.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=345000, episode_reward=1654.60 +/- 153.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=346000, episode_reward=1555.60 +/- 181.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=347000, episode_reward=1591.00 +/- 302.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=348000, episode_reward=1431.80 +/- 445.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=349000, episode_reward=1612.00 +/- 178.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=350000, episode_reward=1533.20 +/- 285.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=351000, episode_reward=1701.20 +/- 234.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=352000, episode_reward=1561.00 +/- 252.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=353000, episode_reward=1684.20 +/- 138.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=354000, episode_reward=1721.20 +/- 169.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=355000, episode_reward=1767.40 +/- 94.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=356000, episode_reward=1559.80 +/- 275.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=357000, episode_reward=1546.00 +/- 302.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=358000, episode_reward=1829.40 +/- 215.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=359000, episode_reward=1780.80 +/- 139.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=360000, episode_reward=1547.00 +/- 397.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=361000, episode_reward=1490.40 +/- 195.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=362000, episode_reward=1705.40 +/- 214.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=363000, episode_reward=1359.80 +/- 263.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=364000, episode_reward=1534.60 +/- 322.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=365000, episode_reward=1619.60 +/- 373.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=366000, episode_reward=1568.20 +/- 321.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=367000, episode_reward=1541.40 +/- 310.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=368000, episode_reward=1540.80 +/- 260.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=369000, episode_reward=1621.20 +/- 139.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=370000, episode_reward=1494.80 +/- 331.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=371000, episode_reward=1536.60 +/- 169.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=372000, episode_reward=1682.40 +/- 139.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=373000, episode_reward=1613.20 +/- 185.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=374000, episode_reward=1821.40 +/- 235.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=375000, episode_reward=1505.80 +/- 273.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=376000, episode_reward=1540.60 +/- 332.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=377000, episode_reward=1401.20 +/- 244.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=378000, episode_reward=1529.20 +/- 203.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=379000, episode_reward=1419.20 +/- 403.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=380000, episode_reward=1768.60 +/- 196.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=381000, episode_reward=1563.20 +/- 239.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=382000, episode_reward=1779.00 +/- 130.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=383000, episode_reward=1799.60 +/- 190.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=384000, episode_reward=1714.40 +/- 181.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=385000, episode_reward=1406.00 +/- 183.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=386000, episode_reward=1370.40 +/- 71.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=387000, episode_reward=1762.00 +/- 112.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=388000, episode_reward=1259.80 +/- 480.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=389000, episode_reward=1756.00 +/- 182.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=390000, episode_reward=1644.20 +/- 183.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=391000, episode_reward=1485.00 +/- 411.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=392000, episode_reward=1515.20 +/- 247.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=393000, episode_reward=1663.20 +/- 233.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=394000, episode_reward=1536.20 +/- 202.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=395000, episode_reward=1550.40 +/- 260.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=396000, episode_reward=1419.00 +/- 148.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=397000, episode_reward=1581.00 +/- 198.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=398000, episode_reward=1597.40 +/- 194.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=399000, episode_reward=1518.40 +/- 328.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=400000, episode_reward=1691.80 +/- 145.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=401000, episode_reward=1687.20 +/- 106.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=402000, episode_reward=1508.00 +/- 212.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=403000, episode_reward=1453.60 +/- 83.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=404000, episode_reward=1730.80 +/- 250.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=405000, episode_reward=1720.00 +/- 168.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=406000, episode_reward=1554.80 +/- 266.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=407000, episode_reward=1816.80 +/- 302.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=408000, episode_reward=1518.40 +/- 215.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=409000, episode_reward=1552.00 +/- 298.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=410000, episode_reward=1694.20 +/- 315.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=411000, episode_reward=1722.80 +/- 175.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=412000, episode_reward=1616.20 +/- 252.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=413000, episode_reward=1633.00 +/- 272.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=414000, episode_reward=1501.20 +/- 128.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=415000, episode_reward=1657.80 +/- 120.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=416000, episode_reward=1732.00 +/- 236.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=417000, episode_reward=1646.40 +/- 252.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=418000, episode_reward=1530.60 +/- 186.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=419000, episode_reward=1775.20 +/- 271.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=420000, episode_reward=1710.80 +/- 131.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=421000, episode_reward=1813.60 +/- 162.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=422000, episode_reward=1640.80 +/- 315.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=423000, episode_reward=1757.20 +/- 253.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=424000, episode_reward=1727.40 +/- 189.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=425000, episode_reward=1650.60 +/- 209.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=426000, episode_reward=1611.60 +/- 250.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=427000, episode_reward=1515.80 +/- 171.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=428000, episode_reward=1607.20 +/- 213.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=429000, episode_reward=1329.00 +/- 270.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=430000, episode_reward=1426.00 +/- 256.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=431000, episode_reward=1767.20 +/- 283.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=432000, episode_reward=1608.80 +/- 321.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=433000, episode_reward=1595.20 +/- 223.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=434000, episode_reward=1557.40 +/- 264.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=435000, episode_reward=1581.20 +/- 250.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=436000, episode_reward=1668.60 +/- 228.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=437000, episode_reward=1579.80 +/- 289.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=438000, episode_reward=1671.00 +/- 202.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=439000, episode_reward=1592.60 +/- 349.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=440000, episode_reward=1580.80 +/- 189.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=441000, episode_reward=1870.80 +/- 291.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=442000, episode_reward=1706.20 +/- 247.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=443000, episode_reward=1579.80 +/- 197.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=444000, episode_reward=1328.60 +/- 278.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=445000, episode_reward=1711.60 +/- 201.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=446000, episode_reward=1444.40 +/- 182.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=447000, episode_reward=1424.40 +/- 363.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=448000, episode_reward=1525.60 +/- 185.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=449000, episode_reward=1631.40 +/- 277.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=450000, episode_reward=1543.20 +/- 248.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=451000, episode_reward=1639.40 +/- 196.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=452000, episode_reward=1873.80 +/- 205.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=453000, episode_reward=1774.20 +/- 74.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=454000, episode_reward=1432.40 +/- 299.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=455000, episode_reward=1769.60 +/- 255.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=456000, episode_reward=1651.20 +/- 185.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=457000, episode_reward=1695.20 +/- 197.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=458000, episode_reward=1658.60 +/- 204.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=459000, episode_reward=1851.80 +/- 102.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=460000, episode_reward=1706.20 +/- 211.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=461000, episode_reward=1649.40 +/- 67.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=462000, episode_reward=1683.40 +/- 189.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=463000, episode_reward=1362.00 +/- 387.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=464000, episode_reward=1477.80 +/- 233.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=465000, episode_reward=1504.40 +/- 207.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=466000, episode_reward=1754.20 +/- 217.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=467000, episode_reward=1320.20 +/- 200.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=468000, episode_reward=1632.40 +/- 109.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=469000, episode_reward=1625.80 +/- 284.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=470000, episode_reward=1669.40 +/- 167.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=471000, episode_reward=1576.80 +/- 195.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=472000, episode_reward=1363.80 +/- 139.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=473000, episode_reward=1694.40 +/- 309.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=474000, episode_reward=1836.80 +/- 81.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=475000, episode_reward=1660.80 +/- 175.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=476000, episode_reward=1750.60 +/- 299.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=477000, episode_reward=1670.80 +/- 147.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=478000, episode_reward=1713.00 +/- 214.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=479000, episode_reward=1395.60 +/- 199.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=480000, episode_reward=1582.20 +/- 246.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=481000, episode_reward=1410.20 +/- 247.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=482000, episode_reward=1361.20 +/- 164.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=483000, episode_reward=1321.80 +/- 318.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=484000, episode_reward=1527.20 +/- 258.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=485000, episode_reward=1556.60 +/- 140.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=486000, episode_reward=1589.60 +/- 496.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=487000, episode_reward=1436.00 +/- 167.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=488000, episode_reward=1810.80 +/- 115.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=489000, episode_reward=1673.80 +/- 306.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=490000, episode_reward=1668.20 +/- 202.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=491000, episode_reward=1685.00 +/- 228.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=492000, episode_reward=1364.40 +/- 291.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=493000, episode_reward=1552.00 +/- 257.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=494000, episode_reward=1389.80 +/- 256.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=495000, episode_reward=1507.20 +/- 407.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=496000, episode_reward=1619.60 +/- 209.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=497000, episode_reward=1436.20 +/- 129.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=498000, episode_reward=1568.80 +/- 171.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=499000, episode_reward=1434.80 +/- 213.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=500000, episode_reward=1615.60 +/- 198.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=501000, episode_reward=1534.00 +/- 253.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=502000, episode_reward=1770.80 +/- 180.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=503000, episode_reward=1448.80 +/- 281.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=504000, episode_reward=1463.20 +/- 351.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=505000, episode_reward=1602.20 +/- 269.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=506000, episode_reward=1577.80 +/- 208.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=507000, episode_reward=1493.20 +/- 176.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=508000, episode_reward=1677.20 +/- 227.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=509000, episode_reward=1655.80 +/- 289.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=510000, episode_reward=1628.40 +/- 129.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=511000, episode_reward=1693.00 +/- 237.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=512000, episode_reward=1471.20 +/- 243.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=513000, episode_reward=1617.60 +/- 93.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=514000, episode_reward=1636.60 +/- 180.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=515000, episode_reward=1569.20 +/- 162.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=516000, episode_reward=1591.60 +/- 353.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=517000, episode_reward=1453.20 +/- 208.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=518000, episode_reward=1376.40 +/- 246.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=519000, episode_reward=1635.40 +/- 99.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=520000, episode_reward=1516.20 +/- 331.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=521000, episode_reward=1570.60 +/- 386.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=522000, episode_reward=1739.40 +/- 70.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=523000, episode_reward=1579.40 +/- 155.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=524000, episode_reward=1410.00 +/- 167.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=525000, episode_reward=1592.20 +/- 242.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=526000, episode_reward=1697.20 +/- 210.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=527000, episode_reward=1540.80 +/- 337.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=528000, episode_reward=1444.80 +/- 119.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=529000, episode_reward=1636.20 +/- 216.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=530000, episode_reward=1396.20 +/- 230.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=531000, episode_reward=1559.80 +/- 310.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=532000, episode_reward=1797.20 +/- 191.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=533000, episode_reward=1706.40 +/- 172.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=534000, episode_reward=1584.00 +/- 187.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=535000, episode_reward=1550.00 +/- 243.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=536000, episode_reward=1409.00 +/- 238.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=537000, episode_reward=1480.40 +/- 363.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=538000, episode_reward=1643.00 +/- 161.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=539000, episode_reward=1343.00 +/- 171.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=540000, episode_reward=1400.40 +/- 237.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=541000, episode_reward=1448.00 +/- 279.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=542000, episode_reward=1486.20 +/- 187.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=543000, episode_reward=1607.80 +/- 272.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=544000, episode_reward=1579.60 +/- 123.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=545000, episode_reward=1307.00 +/- 184.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=546000, episode_reward=1723.20 +/- 125.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=547000, episode_reward=1791.00 +/- 184.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=548000, episode_reward=1616.40 +/- 252.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=549000, episode_reward=1599.40 +/- 148.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=550000, episode_reward=1593.40 +/- 215.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=551000, episode_reward=1559.20 +/- 245.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=552000, episode_reward=1621.40 +/- 349.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=553000, episode_reward=1639.60 +/- 199.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=554000, episode_reward=1466.00 +/- 263.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=555000, episode_reward=1615.20 +/- 194.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=556000, episode_reward=1619.60 +/- 196.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=557000, episode_reward=1585.00 +/- 151.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=558000, episode_reward=1666.60 +/- 318.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=559000, episode_reward=1393.80 +/- 231.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=560000, episode_reward=1502.40 +/- 393.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=561000, episode_reward=1675.20 +/- 239.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=562000, episode_reward=1553.40 +/- 222.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=563000, episode_reward=1792.60 +/- 50.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=564000, episode_reward=1571.60 +/- 305.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=565000, episode_reward=1665.60 +/- 186.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=566000, episode_reward=1618.00 +/- 231.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=567000, episode_reward=1564.40 +/- 206.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=568000, episode_reward=1546.80 +/- 102.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=569000, episode_reward=1750.60 +/- 123.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=570000, episode_reward=1691.20 +/- 170.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=571000, episode_reward=1457.40 +/- 290.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=572000, episode_reward=1638.20 +/- 129.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=573000, episode_reward=1527.60 +/- 350.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=574000, episode_reward=1724.80 +/- 149.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=575000, episode_reward=1546.20 +/- 100.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=576000, episode_reward=1712.20 +/- 169.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=577000, episode_reward=1544.00 +/- 195.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=578000, episode_reward=1790.80 +/- 144.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=579000, episode_reward=1710.20 +/- 103.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=580000, episode_reward=1565.40 +/- 316.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=581000, episode_reward=1593.40 +/- 160.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=582000, episode_reward=1542.20 +/- 221.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=583000, episode_reward=1620.20 +/- 82.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=584000, episode_reward=1536.00 +/- 253.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=585000, episode_reward=1544.80 +/- 356.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=586000, episode_reward=1685.80 +/- 308.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=587000, episode_reward=1622.40 +/- 258.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=588000, episode_reward=1706.80 +/- 171.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=589000, episode_reward=1637.20 +/- 186.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=590000, episode_reward=1627.60 +/- 145.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=591000, episode_reward=1759.80 +/- 167.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=592000, episode_reward=1571.00 +/- 270.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=593000, episode_reward=1493.60 +/- 156.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=594000, episode_reward=1631.40 +/- 333.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=595000, episode_reward=1883.00 +/- 158.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=596000, episode_reward=1578.60 +/- 170.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=597000, episode_reward=1590.20 +/- 174.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=598000, episode_reward=1647.80 +/- 303.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=599000, episode_reward=1415.60 +/- 317.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=600000, episode_reward=1610.80 +/- 283.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=601000, episode_reward=1517.40 +/- 221.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=602000, episode_reward=1794.20 +/- 161.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=603000, episode_reward=1486.60 +/- 221.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=604000, episode_reward=1610.40 +/- 342.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=605000, episode_reward=1741.00 +/- 110.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=606000, episode_reward=1799.80 +/- 366.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=607000, episode_reward=1632.00 +/- 251.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=608000, episode_reward=1672.80 +/- 219.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=609000, episode_reward=1852.00 +/- 168.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=610000, episode_reward=1752.20 +/- 114.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=611000, episode_reward=1411.60 +/- 195.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=612000, episode_reward=1606.40 +/- 343.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=613000, episode_reward=1605.00 +/- 202.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=614000, episode_reward=1462.40 +/- 270.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=615000, episode_reward=1594.40 +/- 226.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=616000, episode_reward=1565.80 +/- 303.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=617000, episode_reward=1672.80 +/- 172.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=618000, episode_reward=1613.20 +/- 166.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=619000, episode_reward=1613.80 +/- 123.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=620000, episode_reward=1727.80 +/- 181.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=621000, episode_reward=1472.00 +/- 229.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=622000, episode_reward=1639.60 +/- 140.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=623000, episode_reward=1678.80 +/- 113.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=624000, episode_reward=1545.80 +/- 291.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=625000, episode_reward=1550.60 +/- 366.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=626000, episode_reward=1634.40 +/- 234.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=627000, episode_reward=1760.60 +/- 168.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=628000, episode_reward=1637.60 +/- 245.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=629000, episode_reward=1628.00 +/- 327.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=630000, episode_reward=1503.60 +/- 130.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=631000, episode_reward=1716.60 +/- 204.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=632000, episode_reward=1570.20 +/- 175.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=633000, episode_reward=1528.40 +/- 189.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=634000, episode_reward=1587.40 +/- 343.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=635000, episode_reward=1376.20 +/- 257.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=636000, episode_reward=1710.80 +/- 110.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=637000, episode_reward=1578.00 +/- 206.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=638000, episode_reward=1786.00 +/- 84.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=639000, episode_reward=1570.80 +/- 270.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=640000, episode_reward=1764.20 +/- 201.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=641000, episode_reward=1492.40 +/- 95.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=642000, episode_reward=1690.80 +/- 272.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=643000, episode_reward=1513.20 +/- 250.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=644000, episode_reward=1479.00 +/- 276.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=645000, episode_reward=1436.60 +/- 393.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=646000, episode_reward=1604.40 +/- 205.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=647000, episode_reward=1595.60 +/- 354.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=648000, episode_reward=1673.40 +/- 178.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=649000, episode_reward=1501.80 +/- 249.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=650000, episode_reward=1749.20 +/- 214.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=651000, episode_reward=1739.80 +/- 199.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=652000, episode_reward=1405.40 +/- 239.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=653000, episode_reward=1430.80 +/- 171.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=654000, episode_reward=1633.80 +/- 183.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=655000, episode_reward=1609.80 +/- 191.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=656000, episode_reward=1473.20 +/- 262.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=657000, episode_reward=1699.60 +/- 237.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=658000, episode_reward=1613.00 +/- 186.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=659000, episode_reward=1864.40 +/- 125.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=660000, episode_reward=1730.20 +/- 139.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=661000, episode_reward=1636.20 +/- 128.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=662000, episode_reward=1524.80 +/- 144.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=663000, episode_reward=1614.40 +/- 109.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=664000, episode_reward=1674.40 +/- 170.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=665000, episode_reward=1523.20 +/- 209.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=666000, episode_reward=1643.40 +/- 142.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=667000, episode_reward=1693.00 +/- 184.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=668000, episode_reward=1638.60 +/- 165.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=669000, episode_reward=1821.40 +/- 211.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=670000, episode_reward=1640.00 +/- 215.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=671000, episode_reward=1833.00 +/- 266.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=672000, episode_reward=1753.00 +/- 168.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=673000, episode_reward=1841.60 +/- 78.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=674000, episode_reward=1482.60 +/- 154.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=675000, episode_reward=1663.80 +/- 278.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=676000, episode_reward=1596.80 +/- 186.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=677000, episode_reward=1853.40 +/- 223.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=678000, episode_reward=1752.00 +/- 294.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=679000, episode_reward=1833.80 +/- 114.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=680000, episode_reward=1700.00 +/- 210.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=681000, episode_reward=1824.00 +/- 201.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=682000, episode_reward=1787.60 +/- 168.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=683000, episode_reward=1682.40 +/- 189.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=684000, episode_reward=1493.60 +/- 267.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=685000, episode_reward=1569.20 +/- 132.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=686000, episode_reward=1595.00 +/- 213.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=687000, episode_reward=1711.40 +/- 238.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=688000, episode_reward=1715.40 +/- 274.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=689000, episode_reward=1624.60 +/- 383.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=690000, episode_reward=1602.40 +/- 175.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=691000, episode_reward=1722.80 +/- 277.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=692000, episode_reward=1492.20 +/- 113.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=693000, episode_reward=1530.00 +/- 90.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=694000, episode_reward=1591.00 +/- 208.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=695000, episode_reward=1631.80 +/- 130.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=696000, episode_reward=1565.00 +/- 150.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=697000, episode_reward=1785.20 +/- 242.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=698000, episode_reward=1645.00 +/- 352.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=699000, episode_reward=1571.60 +/- 239.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=700000, episode_reward=1646.40 +/- 333.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=701000, episode_reward=1698.00 +/- 118.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=702000, episode_reward=1739.60 +/- 240.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=703000, episode_reward=1482.60 +/- 177.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=704000, episode_reward=1569.60 +/- 270.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=705000, episode_reward=1517.80 +/- 134.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=706000, episode_reward=1640.40 +/- 148.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=707000, episode_reward=1608.20 +/- 305.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=708000, episode_reward=1688.80 +/- 263.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=709000, episode_reward=1692.20 +/- 196.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=710000, episode_reward=1734.20 +/- 227.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=711000, episode_reward=1611.20 +/- 106.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=712000, episode_reward=1829.00 +/- 188.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=713000, episode_reward=1686.80 +/- 102.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=714000, episode_reward=1556.60 +/- 101.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=715000, episode_reward=1602.20 +/- 166.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=716000, episode_reward=1704.60 +/- 255.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=717000, episode_reward=1566.00 +/- 277.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=718000, episode_reward=1775.60 +/- 139.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=719000, episode_reward=1698.40 +/- 186.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=720000, episode_reward=1533.00 +/- 133.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=721000, episode_reward=1718.00 +/- 183.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=722000, episode_reward=1599.40 +/- 141.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=723000, episode_reward=1587.60 +/- 195.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=724000, episode_reward=1618.00 +/- 238.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=725000, episode_reward=1788.00 +/- 210.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=726000, episode_reward=1508.80 +/- 340.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=727000, episode_reward=1676.60 +/- 267.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=728000, episode_reward=1679.60 +/- 231.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=729000, episode_reward=1783.60 +/- 88.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=730000, episode_reward=1604.20 +/- 182.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=731000, episode_reward=1904.80 +/- 118.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=732000, episode_reward=1734.40 +/- 224.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=733000, episode_reward=1848.60 +/- 237.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=734000, episode_reward=1513.00 +/- 388.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=735000, episode_reward=1816.40 +/- 155.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=736000, episode_reward=1607.60 +/- 393.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=737000, episode_reward=1644.60 +/- 171.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=738000, episode_reward=1843.60 +/- 246.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=739000, episode_reward=1757.40 +/- 301.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=740000, episode_reward=1719.00 +/- 386.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=741000, episode_reward=1644.00 +/- 260.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=742000, episode_reward=1786.60 +/- 113.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=743000, episode_reward=1351.00 +/- 457.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=744000, episode_reward=1780.60 +/- 323.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=745000, episode_reward=1557.60 +/- 310.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=746000, episode_reward=1371.20 +/- 315.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=747000, episode_reward=1547.40 +/- 167.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=748000, episode_reward=1689.20 +/- 145.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=749000, episode_reward=1681.20 +/- 250.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=750000, episode_reward=1675.40 +/- 241.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=751000, episode_reward=1828.00 +/- 245.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=752000, episode_reward=1770.40 +/- 175.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=753000, episode_reward=1632.40 +/- 269.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=754000, episode_reward=1672.40 +/- 113.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=755000, episode_reward=1614.00 +/- 134.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=756000, episode_reward=1735.60 +/- 170.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=757000, episode_reward=1674.20 +/- 120.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=758000, episode_reward=1662.60 +/- 122.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=759000, episode_reward=1670.00 +/- 187.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=760000, episode_reward=1504.80 +/- 241.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=761000, episode_reward=1659.60 +/- 212.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=762000, episode_reward=2007.60 +/- 118.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=763000, episode_reward=1712.20 +/- 263.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=764000, episode_reward=1414.80 +/- 303.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=765000, episode_reward=1498.20 +/- 376.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=766000, episode_reward=1574.40 +/- 150.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=767000, episode_reward=1737.40 +/- 177.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=768000, episode_reward=1737.20 +/- 184.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=769000, episode_reward=1716.40 +/- 111.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=770000, episode_reward=1508.40 +/- 277.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=771000, episode_reward=1643.20 +/- 268.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=772000, episode_reward=1437.80 +/- 414.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=773000, episode_reward=1495.40 +/- 203.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=774000, episode_reward=1710.00 +/- 303.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=775000, episode_reward=1790.20 +/- 229.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=776000, episode_reward=1550.00 +/- 274.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=777000, episode_reward=1586.20 +/- 208.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=778000, episode_reward=1777.40 +/- 100.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=779000, episode_reward=1736.80 +/- 204.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=780000, episode_reward=1539.20 +/- 285.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=781000, episode_reward=1574.40 +/- 88.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=782000, episode_reward=1431.80 +/- 311.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=783000, episode_reward=1592.00 +/- 330.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=784000, episode_reward=1749.20 +/- 216.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=785000, episode_reward=1678.80 +/- 195.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=786000, episode_reward=1461.60 +/- 265.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=787000, episode_reward=1626.40 +/- 447.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=788000, episode_reward=1569.60 +/- 239.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=789000, episode_reward=1677.60 +/- 196.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=790000, episode_reward=1589.20 +/- 296.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=791000, episode_reward=1613.00 +/- 292.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=792000, episode_reward=1700.40 +/- 200.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=793000, episode_reward=1796.40 +/- 158.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=794000, episode_reward=1597.80 +/- 138.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=795000, episode_reward=1711.20 +/- 229.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=796000, episode_reward=1551.80 +/- 199.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=797000, episode_reward=1727.60 +/- 365.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=798000, episode_reward=1496.60 +/- 281.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=799000, episode_reward=1557.60 +/- 272.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=800000, episode_reward=1683.00 +/- 215.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=801000, episode_reward=1530.80 +/- 147.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=802000, episode_reward=1651.60 +/- 185.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=803000, episode_reward=1437.00 +/- 283.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=804000, episode_reward=1663.20 +/- 221.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=805000, episode_reward=1664.00 +/- 249.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=806000, episode_reward=1661.80 +/- 144.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=807000, episode_reward=1713.20 +/- 224.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=808000, episode_reward=1681.60 +/- 353.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=809000, episode_reward=1558.60 +/- 141.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=810000, episode_reward=1678.40 +/- 156.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=811000, episode_reward=1685.40 +/- 226.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=812000, episode_reward=1601.80 +/- 188.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=813000, episode_reward=1615.80 +/- 126.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=814000, episode_reward=1499.20 +/- 156.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=815000, episode_reward=1753.00 +/- 113.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=816000, episode_reward=1596.80 +/- 277.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=817000, episode_reward=1569.80 +/- 157.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=818000, episode_reward=1529.20 +/- 265.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=819000, episode_reward=1588.20 +/- 185.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=820000, episode_reward=1714.00 +/- 193.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=821000, episode_reward=1619.40 +/- 175.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=822000, episode_reward=1478.00 +/- 246.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=823000, episode_reward=1603.20 +/- 119.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=824000, episode_reward=1760.20 +/- 237.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=825000, episode_reward=1670.20 +/- 62.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=826000, episode_reward=1598.80 +/- 167.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=827000, episode_reward=1591.60 +/- 186.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=828000, episode_reward=1528.40 +/- 287.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=829000, episode_reward=1775.80 +/- 101.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=830000, episode_reward=1531.40 +/- 122.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=831000, episode_reward=1628.20 +/- 239.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=832000, episode_reward=1456.60 +/- 138.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=833000, episode_reward=1663.20 +/- 242.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=834000, episode_reward=1563.20 +/- 232.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=835000, episode_reward=1625.80 +/- 233.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=836000, episode_reward=1658.80 +/- 41.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=837000, episode_reward=1545.20 +/- 180.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=838000, episode_reward=1688.40 +/- 374.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=839000, episode_reward=1620.40 +/- 127.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=840000, episode_reward=1725.60 +/- 110.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=841000, episode_reward=1812.00 +/- 325.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=842000, episode_reward=1687.60 +/- 103.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=843000, episode_reward=1569.20 +/- 206.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=844000, episode_reward=1650.00 +/- 289.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=845000, episode_reward=1631.00 +/- 111.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=846000, episode_reward=1510.80 +/- 333.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=847000, episode_reward=1685.80 +/- 190.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=848000, episode_reward=1549.20 +/- 260.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=849000, episode_reward=1464.40 +/- 336.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=850000, episode_reward=1543.40 +/- 263.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=851000, episode_reward=1359.80 +/- 159.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=852000, episode_reward=1534.80 +/- 220.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=853000, episode_reward=1609.40 +/- 99.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=854000, episode_reward=1643.80 +/- 220.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=855000, episode_reward=1743.60 +/- 179.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=856000, episode_reward=1635.60 +/- 229.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=857000, episode_reward=1756.60 +/- 203.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=858000, episode_reward=1629.00 +/- 220.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=859000, episode_reward=1620.60 +/- 141.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=860000, episode_reward=1484.60 +/- 274.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=861000, episode_reward=1616.60 +/- 211.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=862000, episode_reward=1640.60 +/- 157.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=863000, episode_reward=1824.60 +/- 158.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=864000, episode_reward=1488.60 +/- 233.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=865000, episode_reward=1720.00 +/- 262.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=866000, episode_reward=1534.80 +/- 180.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=867000, episode_reward=1519.60 +/- 278.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=868000, episode_reward=1672.20 +/- 185.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=869000, episode_reward=1787.60 +/- 211.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=870000, episode_reward=1754.40 +/- 183.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=871000, episode_reward=1574.80 +/- 272.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=872000, episode_reward=1619.00 +/- 454.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=873000, episode_reward=1729.40 +/- 243.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=874000, episode_reward=1652.20 +/- 231.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=875000, episode_reward=1643.00 +/- 181.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=876000, episode_reward=1546.80 +/- 312.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=877000, episode_reward=1631.40 +/- 292.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=878000, episode_reward=1581.40 +/- 177.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=879000, episode_reward=1713.20 +/- 186.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=880000, episode_reward=1795.40 +/- 178.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=881000, episode_reward=1507.60 +/- 283.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=882000, episode_reward=1510.80 +/- 329.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=883000, episode_reward=1683.40 +/- 150.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=884000, episode_reward=1504.40 +/- 154.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=885000, episode_reward=1717.00 +/- 157.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=886000, episode_reward=1786.20 +/- 127.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=887000, episode_reward=1655.00 +/- 150.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=888000, episode_reward=1763.80 +/- 84.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=889000, episode_reward=1730.20 +/- 238.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=890000, episode_reward=1533.20 +/- 260.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=891000, episode_reward=1729.00 +/- 132.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=892000, episode_reward=1784.80 +/- 115.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=893000, episode_reward=1576.40 +/- 304.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=894000, episode_reward=1646.40 +/- 193.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=895000, episode_reward=1742.20 +/- 194.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=896000, episode_reward=1595.20 +/- 329.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=897000, episode_reward=1406.80 +/- 143.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=898000, episode_reward=1488.40 +/- 213.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=899000, episode_reward=1362.60 +/- 332.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=900000, episode_reward=1687.00 +/- 250.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=901000, episode_reward=1722.80 +/- 194.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=902000, episode_reward=1719.20 +/- 186.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=903000, episode_reward=1863.20 +/- 212.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=904000, episode_reward=1702.80 +/- 357.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=905000, episode_reward=1729.20 +/- 215.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=906000, episode_reward=1472.40 +/- 201.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=907000, episode_reward=1596.00 +/- 346.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=908000, episode_reward=1618.60 +/- 316.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=909000, episode_reward=1702.80 +/- 270.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=910000, episode_reward=1653.60 +/- 372.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=911000, episode_reward=1672.60 +/- 196.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=912000, episode_reward=1587.20 +/- 159.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=913000, episode_reward=1689.00 +/- 297.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=914000, episode_reward=1555.20 +/- 315.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=915000, episode_reward=1605.80 +/- 165.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=916000, episode_reward=1655.80 +/- 244.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=917000, episode_reward=1455.00 +/- 227.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=918000, episode_reward=1569.20 +/- 112.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=919000, episode_reward=1369.40 +/- 247.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=920000, episode_reward=1700.60 +/- 289.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=921000, episode_reward=1735.20 +/- 243.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=922000, episode_reward=1974.40 +/- 126.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=923000, episode_reward=1665.40 +/- 268.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=924000, episode_reward=1794.40 +/- 140.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=925000, episode_reward=1549.40 +/- 207.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=926000, episode_reward=1541.40 +/- 309.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=927000, episode_reward=1571.40 +/- 260.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=928000, episode_reward=1657.00 +/- 149.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=929000, episode_reward=1728.00 +/- 136.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=930000, episode_reward=1784.80 +/- 74.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=931000, episode_reward=1640.40 +/- 249.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=932000, episode_reward=1696.60 +/- 176.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=933000, episode_reward=1580.40 +/- 229.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=934000, episode_reward=1662.60 +/- 97.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=935000, episode_reward=1507.80 +/- 135.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=936000, episode_reward=1803.60 +/- 153.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=937000, episode_reward=1750.00 +/- 202.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=938000, episode_reward=1557.20 +/- 189.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=939000, episode_reward=1654.40 +/- 321.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=940000, episode_reward=1725.00 +/- 237.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=941000, episode_reward=1579.20 +/- 218.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=942000, episode_reward=1912.20 +/- 177.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=943000, episode_reward=1607.20 +/- 162.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=944000, episode_reward=1675.80 +/- 216.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=945000, episode_reward=1779.80 +/- 40.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=946000, episode_reward=1418.00 +/- 339.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=947000, episode_reward=1658.20 +/- 316.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=948000, episode_reward=1819.40 +/- 114.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=949000, episode_reward=1694.40 +/- 31.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=950000, episode_reward=1624.60 +/- 180.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=951000, episode_reward=1656.20 +/- 206.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=952000, episode_reward=1857.60 +/- 195.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=953000, episode_reward=1774.40 +/- 222.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=954000, episode_reward=1622.00 +/- 197.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=955000, episode_reward=1652.20 +/- 254.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=956000, episode_reward=1630.20 +/- 215.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=957000, episode_reward=1599.40 +/- 177.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=958000, episode_reward=1666.60 +/- 243.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=959000, episode_reward=1785.60 +/- 179.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=960000, episode_reward=1687.60 +/- 244.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=961000, episode_reward=1703.60 +/- 275.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=962000, episode_reward=1594.60 +/- 221.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=963000, episode_reward=1871.00 +/- 242.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=964000, episode_reward=1754.20 +/- 164.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=965000, episode_reward=1595.00 +/- 286.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=966000, episode_reward=1777.20 +/- 257.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=967000, episode_reward=1730.20 +/- 111.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=968000, episode_reward=1688.80 +/- 278.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=969000, episode_reward=1634.60 +/- 276.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=970000, episode_reward=1576.40 +/- 421.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=971000, episode_reward=1831.80 +/- 193.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=972000, episode_reward=1628.80 +/- 88.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=973000, episode_reward=1769.40 +/- 159.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=974000, episode_reward=1690.40 +/- 370.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=975000, episode_reward=1701.00 +/- 196.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=976000, episode_reward=1765.80 +/- 126.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=977000, episode_reward=1658.20 +/- 224.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=978000, episode_reward=1618.00 +/- 273.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=979000, episode_reward=1462.20 +/- 111.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=980000, episode_reward=1763.00 +/- 277.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=981000, episode_reward=1669.40 +/- 383.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=982000, episode_reward=1782.60 +/- 242.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=983000, episode_reward=1815.40 +/- 138.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=984000, episode_reward=1535.00 +/- 238.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=985000, episode_reward=1638.20 +/- 163.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=986000, episode_reward=1724.20 +/- 111.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=987000, episode_reward=1767.80 +/- 249.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=988000, episode_reward=1813.60 +/- 139.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=989000, episode_reward=1649.40 +/- 196.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=990000, episode_reward=1605.20 +/- 168.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=991000, episode_reward=1716.60 +/- 109.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=992000, episode_reward=1686.80 +/- 191.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=993000, episode_reward=1816.00 +/- 119.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=994000, episode_reward=1697.20 +/- 219.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=995000, episode_reward=1438.60 +/- 213.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=996000, episode_reward=1584.20 +/- 268.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=997000, episode_reward=1710.80 +/- 177.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=998000, episode_reward=1665.60 +/- 168.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=999000, episode_reward=1384.80 +/- 311.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1000000, episode_reward=1543.00 +/- 121.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1001000, episode_reward=1561.80 +/- 318.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Gamma = 0.9, Learning Rate = 0.003, Activation Function = ReLU, Neurons = 8, Process noise = 0.1, Measurement noise = 0.0\n",
      "Eval num_timesteps=1000, episode_reward=-2312.00 +/- 58.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=-3147.00 +/- 1166.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=3000, episode_reward=-318.80 +/- 286.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4000, episode_reward=-382.20 +/- 605.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=5000, episode_reward=20.40 +/- 146.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=6000, episode_reward=243.80 +/- 220.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=7000, episode_reward=42.40 +/- 187.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=8000, episode_reward=88.20 +/- 253.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=9000, episode_reward=272.20 +/- 116.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=10000, episode_reward=477.40 +/- 52.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=11000, episode_reward=290.80 +/- 269.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=12000, episode_reward=372.00 +/- 196.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=13000, episode_reward=205.40 +/- 35.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=14000, episode_reward=276.80 +/- 110.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=15000, episode_reward=131.80 +/- 50.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=16000, episode_reward=232.00 +/- 154.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=17000, episode_reward=144.80 +/- 184.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=18000, episode_reward=317.40 +/- 168.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=19000, episode_reward=684.20 +/- 175.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=20000, episode_reward=767.00 +/- 298.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=21000, episode_reward=1173.80 +/- 222.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=22000, episode_reward=1382.20 +/- 298.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=23000, episode_reward=1287.40 +/- 233.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=24000, episode_reward=-1715.20 +/- 6228.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=25000, episode_reward=1217.00 +/- 240.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=26000, episode_reward=1105.80 +/- 393.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=27000, episode_reward=1538.00 +/- 84.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=28000, episode_reward=1666.40 +/- 84.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=29000, episode_reward=1560.40 +/- 207.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=30000, episode_reward=1411.20 +/- 176.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=31000, episode_reward=1719.00 +/- 180.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=32000, episode_reward=1512.40 +/- 115.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=33000, episode_reward=1336.20 +/- 349.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=34000, episode_reward=1582.00 +/- 290.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=35000, episode_reward=1525.40 +/- 225.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=36000, episode_reward=1610.60 +/- 235.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=37000, episode_reward=1584.60 +/- 134.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=38000, episode_reward=1475.80 +/- 278.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=39000, episode_reward=1186.80 +/- 272.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=40000, episode_reward=1615.40 +/- 101.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=41000, episode_reward=1619.40 +/- 267.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=42000, episode_reward=1593.40 +/- 232.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=43000, episode_reward=1485.80 +/- 346.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=44000, episode_reward=1805.40 +/- 150.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=45000, episode_reward=1647.40 +/- 163.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=46000, episode_reward=1649.80 +/- 133.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=47000, episode_reward=1556.40 +/- 104.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=48000, episode_reward=1656.20 +/- 131.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=49000, episode_reward=1532.20 +/- 307.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=50000, episode_reward=1693.80 +/- 187.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=51000, episode_reward=1322.00 +/- 285.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=52000, episode_reward=1704.00 +/- 200.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=53000, episode_reward=1473.40 +/- 149.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=54000, episode_reward=1689.80 +/- 207.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=55000, episode_reward=1599.20 +/- 197.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=56000, episode_reward=1762.60 +/- 30.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=57000, episode_reward=1549.00 +/- 219.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=58000, episode_reward=1411.00 +/- 285.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=59000, episode_reward=1453.40 +/- 220.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=60000, episode_reward=1491.20 +/- 126.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=61000, episode_reward=1499.60 +/- 302.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=62000, episode_reward=1581.00 +/- 178.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=63000, episode_reward=1761.60 +/- 198.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=64000, episode_reward=1643.80 +/- 349.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=65000, episode_reward=1429.40 +/- 128.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=66000, episode_reward=1556.00 +/- 197.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=67000, episode_reward=1418.40 +/- 158.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=68000, episode_reward=1469.20 +/- 259.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=69000, episode_reward=1647.40 +/- 195.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=70000, episode_reward=1490.00 +/- 80.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=71000, episode_reward=1520.00 +/- 346.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=72000, episode_reward=1532.40 +/- 81.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=73000, episode_reward=1545.40 +/- 208.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=74000, episode_reward=1625.40 +/- 105.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=75000, episode_reward=1539.40 +/- 226.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=76000, episode_reward=1461.60 +/- 322.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=77000, episode_reward=1527.20 +/- 239.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=78000, episode_reward=1434.40 +/- 315.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=79000, episode_reward=1626.40 +/- 222.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=80000, episode_reward=1532.60 +/- 344.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=81000, episode_reward=1505.20 +/- 115.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=82000, episode_reward=1687.60 +/- 167.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=83000, episode_reward=1657.80 +/- 234.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=84000, episode_reward=1504.60 +/- 175.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=85000, episode_reward=1611.20 +/- 291.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=86000, episode_reward=1680.80 +/- 234.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=87000, episode_reward=1661.40 +/- 167.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=88000, episode_reward=1380.40 +/- 334.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=89000, episode_reward=1531.80 +/- 221.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=90000, episode_reward=1699.80 +/- 210.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=91000, episode_reward=1523.80 +/- 148.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=92000, episode_reward=1475.80 +/- 245.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=93000, episode_reward=1392.40 +/- 250.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=94000, episode_reward=1320.60 +/- 391.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=95000, episode_reward=1629.80 +/- 272.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=96000, episode_reward=1581.40 +/- 117.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=97000, episode_reward=1607.00 +/- 278.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=98000, episode_reward=1570.20 +/- 214.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=99000, episode_reward=1523.20 +/- 253.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=100000, episode_reward=1657.80 +/- 143.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=101000, episode_reward=1583.60 +/- 220.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=102000, episode_reward=1585.40 +/- 164.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=103000, episode_reward=1637.80 +/- 258.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=104000, episode_reward=1460.20 +/- 188.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=105000, episode_reward=1515.00 +/- 243.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=106000, episode_reward=1868.40 +/- 160.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=107000, episode_reward=1539.40 +/- 114.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=108000, episode_reward=1499.40 +/- 129.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=109000, episode_reward=1585.60 +/- 241.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=110000, episode_reward=1447.20 +/- 370.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=111000, episode_reward=1752.80 +/- 233.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=112000, episode_reward=1534.80 +/- 112.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=113000, episode_reward=1620.00 +/- 151.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=114000, episode_reward=1548.00 +/- 283.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=115000, episode_reward=1533.60 +/- 155.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=116000, episode_reward=241.60 +/- 2794.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=117000, episode_reward=-153.60 +/- 3683.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=118000, episode_reward=1689.80 +/- 171.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=119000, episode_reward=1536.00 +/- 171.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=120000, episode_reward=1577.00 +/- 217.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=121000, episode_reward=1656.20 +/- 127.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=122000, episode_reward=1508.40 +/- 421.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=123000, episode_reward=1557.40 +/- 122.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=124000, episode_reward=1630.80 +/- 120.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=125000, episode_reward=1481.40 +/- 99.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=126000, episode_reward=1711.20 +/- 175.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=127000, episode_reward=1595.80 +/- 240.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=128000, episode_reward=1636.40 +/- 252.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=129000, episode_reward=1626.40 +/- 308.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=130000, episode_reward=1394.40 +/- 147.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=131000, episode_reward=1480.60 +/- 168.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=132000, episode_reward=1421.80 +/- 393.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=133000, episode_reward=1570.60 +/- 92.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=134000, episode_reward=1531.20 +/- 147.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=135000, episode_reward=1329.40 +/- 216.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=136000, episode_reward=1547.20 +/- 193.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=137000, episode_reward=1340.80 +/- 205.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=138000, episode_reward=1612.60 +/- 187.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=139000, episode_reward=1442.40 +/- 160.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=140000, episode_reward=1829.80 +/- 167.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=141000, episode_reward=1729.20 +/- 146.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=142000, episode_reward=1514.20 +/- 313.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=143000, episode_reward=1312.60 +/- 224.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=144000, episode_reward=1342.00 +/- 208.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=145000, episode_reward=1540.20 +/- 170.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=146000, episode_reward=1434.80 +/- 245.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=147000, episode_reward=1755.40 +/- 137.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=148000, episode_reward=1470.60 +/- 390.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=149000, episode_reward=1509.20 +/- 242.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=150000, episode_reward=1312.40 +/- 392.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=151000, episode_reward=1515.80 +/- 110.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=152000, episode_reward=1728.60 +/- 98.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=153000, episode_reward=1558.00 +/- 300.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=154000, episode_reward=1345.40 +/- 391.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=155000, episode_reward=1498.60 +/- 208.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=156000, episode_reward=1647.40 +/- 262.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=157000, episode_reward=1486.60 +/- 62.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=158000, episode_reward=1590.40 +/- 213.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=159000, episode_reward=1303.80 +/- 380.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=160000, episode_reward=1670.40 +/- 241.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=161000, episode_reward=1611.00 +/- 131.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=162000, episode_reward=1700.00 +/- 123.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=163000, episode_reward=1685.80 +/- 96.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=164000, episode_reward=1671.80 +/- 311.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=165000, episode_reward=1563.00 +/- 151.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=166000, episode_reward=1380.80 +/- 235.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=167000, episode_reward=1762.20 +/- 269.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=168000, episode_reward=1327.60 +/- 333.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=169000, episode_reward=1503.60 +/- 278.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=170000, episode_reward=1537.40 +/- 211.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=171000, episode_reward=1776.40 +/- 121.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=172000, episode_reward=1534.80 +/- 260.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=173000, episode_reward=1625.20 +/- 195.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=174000, episode_reward=1470.40 +/- 295.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=175000, episode_reward=1470.40 +/- 361.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=176000, episode_reward=1300.80 +/- 152.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=177000, episode_reward=1531.40 +/- 294.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=178000, episode_reward=1671.80 +/- 159.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=179000, episode_reward=1622.40 +/- 315.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=180000, episode_reward=1443.60 +/- 153.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=181000, episode_reward=1686.60 +/- 98.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=182000, episode_reward=1733.60 +/- 193.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=183000, episode_reward=1620.80 +/- 195.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=184000, episode_reward=1506.40 +/- 195.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=185000, episode_reward=1608.40 +/- 365.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=186000, episode_reward=1822.60 +/- 47.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=187000, episode_reward=1554.40 +/- 225.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=188000, episode_reward=1442.80 +/- 228.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=189000, episode_reward=1606.80 +/- 158.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=190000, episode_reward=1730.40 +/- 203.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=191000, episode_reward=1462.40 +/- 220.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=192000, episode_reward=1506.80 +/- 303.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=193000, episode_reward=1636.80 +/- 204.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=194000, episode_reward=1379.00 +/- 275.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=195000, episode_reward=1743.20 +/- 206.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=196000, episode_reward=1371.20 +/- 320.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=197000, episode_reward=1533.80 +/- 276.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=198000, episode_reward=1506.60 +/- 208.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=199000, episode_reward=1340.80 +/- 398.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=200000, episode_reward=1764.80 +/- 155.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=201000, episode_reward=1577.40 +/- 268.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=202000, episode_reward=1547.60 +/- 232.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=203000, episode_reward=1584.20 +/- 225.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=204000, episode_reward=1458.00 +/- 406.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=205000, episode_reward=1718.80 +/- 201.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=206000, episode_reward=1688.00 +/- 281.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=207000, episode_reward=1636.20 +/- 221.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=208000, episode_reward=1595.20 +/- 273.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=209000, episode_reward=1710.80 +/- 239.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=210000, episode_reward=1556.40 +/- 227.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=211000, episode_reward=1461.00 +/- 150.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=212000, episode_reward=1527.20 +/- 109.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=213000, episode_reward=1527.00 +/- 105.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=214000, episode_reward=1382.60 +/- 231.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=215000, episode_reward=1766.80 +/- 255.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=216000, episode_reward=1440.20 +/- 286.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=217000, episode_reward=1536.00 +/- 247.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=218000, episode_reward=1318.40 +/- 291.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=219000, episode_reward=1556.20 +/- 251.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=220000, episode_reward=1479.40 +/- 144.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=221000, episode_reward=1348.20 +/- 229.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=222000, episode_reward=1436.40 +/- 209.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=223000, episode_reward=1746.80 +/- 235.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=224000, episode_reward=1752.80 +/- 199.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=225000, episode_reward=1374.20 +/- 147.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=226000, episode_reward=1450.80 +/- 70.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=227000, episode_reward=1428.20 +/- 214.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=228000, episode_reward=1671.20 +/- 205.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=229000, episode_reward=1587.40 +/- 193.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=230000, episode_reward=1527.60 +/- 150.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=231000, episode_reward=1614.20 +/- 359.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=232000, episode_reward=1510.60 +/- 100.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=233000, episode_reward=1787.40 +/- 159.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=234000, episode_reward=1588.80 +/- 110.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=235000, episode_reward=1691.00 +/- 181.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=236000, episode_reward=1353.60 +/- 366.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=237000, episode_reward=1560.00 +/- 262.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=238000, episode_reward=1490.00 +/- 232.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=239000, episode_reward=1662.60 +/- 205.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=240000, episode_reward=1658.00 +/- 144.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=241000, episode_reward=1798.00 +/- 162.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=242000, episode_reward=1576.40 +/- 123.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=243000, episode_reward=1544.60 +/- 163.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=244000, episode_reward=1602.00 +/- 176.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=245000, episode_reward=1594.20 +/- 161.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=246000, episode_reward=1590.60 +/- 193.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=247000, episode_reward=1580.80 +/- 157.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=248000, episode_reward=1708.60 +/- 229.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=249000, episode_reward=1522.60 +/- 267.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=250000, episode_reward=1499.60 +/- 106.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=251000, episode_reward=1603.80 +/- 231.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=252000, episode_reward=1689.80 +/- 302.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=253000, episode_reward=1526.40 +/- 321.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=254000, episode_reward=1707.20 +/- 238.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=255000, episode_reward=1592.40 +/- 173.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=256000, episode_reward=1333.80 +/- 238.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=257000, episode_reward=1569.40 +/- 310.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=258000, episode_reward=1724.00 +/- 244.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=259000, episode_reward=1407.40 +/- 260.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=260000, episode_reward=1533.60 +/- 540.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=261000, episode_reward=1573.00 +/- 111.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=262000, episode_reward=1486.60 +/- 111.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=263000, episode_reward=1593.20 +/- 233.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=264000, episode_reward=1665.00 +/- 177.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=265000, episode_reward=1408.60 +/- 305.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=266000, episode_reward=1527.60 +/- 159.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=267000, episode_reward=1529.00 +/- 245.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=268000, episode_reward=1402.00 +/- 201.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=269000, episode_reward=1661.60 +/- 284.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=270000, episode_reward=1650.00 +/- 189.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=271000, episode_reward=1536.20 +/- 176.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=272000, episode_reward=1508.80 +/- 242.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=273000, episode_reward=1591.40 +/- 223.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=274000, episode_reward=1653.20 +/- 280.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=275000, episode_reward=1621.60 +/- 157.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=276000, episode_reward=1431.20 +/- 322.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=277000, episode_reward=1348.40 +/- 349.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=278000, episode_reward=1617.80 +/- 273.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=279000, episode_reward=1673.60 +/- 216.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=280000, episode_reward=1629.20 +/- 142.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=281000, episode_reward=1582.40 +/- 101.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=282000, episode_reward=1599.40 +/- 440.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=283000, episode_reward=1541.80 +/- 87.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=284000, episode_reward=1669.00 +/- 218.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=285000, episode_reward=1597.20 +/- 173.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=286000, episode_reward=1632.40 +/- 233.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=287000, episode_reward=1417.80 +/- 200.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=288000, episode_reward=1589.40 +/- 314.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=289000, episode_reward=1914.20 +/- 69.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=290000, episode_reward=1471.60 +/- 225.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=291000, episode_reward=1378.60 +/- 261.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=292000, episode_reward=1741.60 +/- 264.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=293000, episode_reward=1522.60 +/- 383.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=294000, episode_reward=1510.20 +/- 101.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=295000, episode_reward=1428.40 +/- 365.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=296000, episode_reward=1614.20 +/- 206.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=297000, episode_reward=1578.40 +/- 129.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=298000, episode_reward=1501.60 +/- 348.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=299000, episode_reward=1543.40 +/- 182.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=300000, episode_reward=1652.80 +/- 311.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=301000, episode_reward=1409.20 +/- 203.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=302000, episode_reward=1625.80 +/- 152.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=303000, episode_reward=1503.80 +/- 522.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=304000, episode_reward=1754.80 +/- 285.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=305000, episode_reward=1546.60 +/- 274.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=306000, episode_reward=1732.00 +/- 243.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=307000, episode_reward=1560.00 +/- 225.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=308000, episode_reward=1711.20 +/- 232.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=309000, episode_reward=1496.60 +/- 289.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=310000, episode_reward=1514.60 +/- 276.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=311000, episode_reward=1637.60 +/- 125.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=312000, episode_reward=1723.40 +/- 208.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=313000, episode_reward=1581.20 +/- 183.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=314000, episode_reward=1734.80 +/- 119.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=315000, episode_reward=1756.40 +/- 94.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=316000, episode_reward=1225.00 +/- 173.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=317000, episode_reward=1564.40 +/- 223.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=318000, episode_reward=1503.00 +/- 95.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=319000, episode_reward=1607.80 +/- 288.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=320000, episode_reward=1657.60 +/- 127.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=321000, episode_reward=1751.20 +/- 274.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=322000, episode_reward=1705.40 +/- 162.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=323000, episode_reward=1563.00 +/- 268.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=324000, episode_reward=1456.60 +/- 218.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=325000, episode_reward=1701.80 +/- 121.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=326000, episode_reward=1593.40 +/- 242.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=327000, episode_reward=1744.80 +/- 153.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=328000, episode_reward=1828.60 +/- 141.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=329000, episode_reward=1333.60 +/- 309.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=330000, episode_reward=1552.60 +/- 187.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=331000, episode_reward=1630.00 +/- 147.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=332000, episode_reward=1536.80 +/- 239.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=333000, episode_reward=1477.80 +/- 295.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=334000, episode_reward=1577.80 +/- 256.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=335000, episode_reward=1429.80 +/- 164.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=336000, episode_reward=1504.60 +/- 251.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=337000, episode_reward=1734.00 +/- 246.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=338000, episode_reward=1714.80 +/- 181.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=339000, episode_reward=1534.00 +/- 220.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=340000, episode_reward=1551.20 +/- 249.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=341000, episode_reward=1515.40 +/- 256.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=342000, episode_reward=1662.00 +/- 205.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=343000, episode_reward=1576.00 +/- 245.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=344000, episode_reward=1755.80 +/- 190.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=345000, episode_reward=1747.20 +/- 65.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=346000, episode_reward=1659.60 +/- 215.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=347000, episode_reward=1740.00 +/- 270.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=348000, episode_reward=1594.20 +/- 143.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=349000, episode_reward=1503.00 +/- 108.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=350000, episode_reward=1626.40 +/- 264.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=351000, episode_reward=1576.00 +/- 100.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=352000, episode_reward=1499.80 +/- 254.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=353000, episode_reward=1551.80 +/- 245.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=354000, episode_reward=1671.60 +/- 188.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=355000, episode_reward=1775.40 +/- 252.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=356000, episode_reward=1516.80 +/- 277.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=357000, episode_reward=1455.80 +/- 246.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=358000, episode_reward=1484.40 +/- 217.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=359000, episode_reward=1595.00 +/- 293.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=360000, episode_reward=1463.60 +/- 123.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=361000, episode_reward=1505.20 +/- 274.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=362000, episode_reward=1700.60 +/- 163.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=363000, episode_reward=1463.00 +/- 167.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=364000, episode_reward=1394.80 +/- 258.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=365000, episode_reward=1540.40 +/- 218.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=366000, episode_reward=1426.00 +/- 242.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=367000, episode_reward=1460.20 +/- 259.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=368000, episode_reward=1679.80 +/- 158.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=369000, episode_reward=1687.40 +/- 302.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=370000, episode_reward=1593.40 +/- 193.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=371000, episode_reward=1414.20 +/- 318.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=372000, episode_reward=1571.40 +/- 213.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=373000, episode_reward=1521.40 +/- 281.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=374000, episode_reward=1469.00 +/- 283.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=375000, episode_reward=1615.20 +/- 187.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=376000, episode_reward=1822.20 +/- 131.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=377000, episode_reward=1664.60 +/- 249.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=378000, episode_reward=1721.80 +/- 68.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=379000, episode_reward=1727.40 +/- 220.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=380000, episode_reward=1608.80 +/- 220.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=381000, episode_reward=1674.80 +/- 219.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=382000, episode_reward=1634.00 +/- 199.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=383000, episode_reward=1282.80 +/- 190.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=384000, episode_reward=1634.60 +/- 302.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=385000, episode_reward=1595.80 +/- 215.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=386000, episode_reward=1681.40 +/- 223.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=387000, episode_reward=1560.60 +/- 339.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=388000, episode_reward=1618.40 +/- 74.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=389000, episode_reward=1722.40 +/- 233.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=390000, episode_reward=1612.40 +/- 144.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=391000, episode_reward=1453.20 +/- 210.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=392000, episode_reward=1634.60 +/- 266.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=393000, episode_reward=1658.80 +/- 192.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=394000, episode_reward=1483.20 +/- 378.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=395000, episode_reward=1772.20 +/- 119.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=396000, episode_reward=1673.80 +/- 154.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=397000, episode_reward=1498.20 +/- 252.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=398000, episode_reward=1501.20 +/- 168.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=399000, episode_reward=1838.60 +/- 217.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=400000, episode_reward=1531.20 +/- 261.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=401000, episode_reward=1559.60 +/- 152.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=402000, episode_reward=1553.40 +/- 197.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=403000, episode_reward=1568.00 +/- 189.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=404000, episode_reward=1368.40 +/- 217.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=405000, episode_reward=1542.40 +/- 318.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=406000, episode_reward=1687.80 +/- 197.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=407000, episode_reward=1668.80 +/- 235.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=408000, episode_reward=1316.20 +/- 283.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=409000, episode_reward=1767.40 +/- 274.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=410000, episode_reward=1500.60 +/- 267.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=411000, episode_reward=1635.80 +/- 392.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=412000, episode_reward=1604.00 +/- 115.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=413000, episode_reward=1607.60 +/- 157.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=414000, episode_reward=1560.40 +/- 344.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=415000, episode_reward=1402.80 +/- 270.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=416000, episode_reward=1656.00 +/- 185.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=417000, episode_reward=1543.20 +/- 384.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=418000, episode_reward=1488.80 +/- 250.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=419000, episode_reward=1655.80 +/- 237.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=420000, episode_reward=1370.20 +/- 182.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=421000, episode_reward=1632.60 +/- 178.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=422000, episode_reward=1536.60 +/- 104.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=423000, episode_reward=1726.80 +/- 185.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=424000, episode_reward=1649.00 +/- 158.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=425000, episode_reward=1701.20 +/- 187.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=426000, episode_reward=1464.20 +/- 309.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=427000, episode_reward=1741.60 +/- 133.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=428000, episode_reward=1392.00 +/- 294.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=429000, episode_reward=1557.20 +/- 206.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=430000, episode_reward=1625.80 +/- 363.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=431000, episode_reward=1658.20 +/- 264.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=432000, episode_reward=1520.60 +/- 190.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=433000, episode_reward=1636.80 +/- 352.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=434000, episode_reward=1601.40 +/- 223.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=435000, episode_reward=1644.00 +/- 118.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=436000, episode_reward=1568.00 +/- 252.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=437000, episode_reward=1486.20 +/- 298.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=438000, episode_reward=1686.40 +/- 264.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=439000, episode_reward=1555.20 +/- 285.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=440000, episode_reward=1656.20 +/- 135.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=441000, episode_reward=1608.80 +/- 243.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=442000, episode_reward=1471.20 +/- 151.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=443000, episode_reward=1646.40 +/- 234.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=444000, episode_reward=1558.40 +/- 258.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=445000, episode_reward=1698.20 +/- 338.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=446000, episode_reward=1552.60 +/- 265.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=447000, episode_reward=1506.00 +/- 233.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=448000, episode_reward=1729.20 +/- 166.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=449000, episode_reward=1733.20 +/- 249.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=450000, episode_reward=1662.60 +/- 214.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=451000, episode_reward=1611.20 +/- 181.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=452000, episode_reward=1640.40 +/- 264.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=453000, episode_reward=1816.20 +/- 165.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=454000, episode_reward=1480.40 +/- 177.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=455000, episode_reward=1621.20 +/- 114.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=456000, episode_reward=1631.60 +/- 117.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=457000, episode_reward=1468.60 +/- 240.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=458000, episode_reward=1768.60 +/- 230.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=459000, episode_reward=1864.60 +/- 123.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=460000, episode_reward=1588.20 +/- 274.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=461000, episode_reward=1505.40 +/- 359.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=462000, episode_reward=1626.40 +/- 351.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=463000, episode_reward=1669.20 +/- 245.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=464000, episode_reward=1536.20 +/- 104.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=465000, episode_reward=1433.60 +/- 342.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=466000, episode_reward=1503.20 +/- 168.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=467000, episode_reward=1458.80 +/- 160.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=468000, episode_reward=1664.20 +/- 201.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=469000, episode_reward=1781.80 +/- 293.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=470000, episode_reward=1416.80 +/- 305.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=471000, episode_reward=1667.00 +/- 117.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=472000, episode_reward=1690.80 +/- 179.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=473000, episode_reward=1647.80 +/- 257.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=474000, episode_reward=1658.20 +/- 242.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=475000, episode_reward=1597.60 +/- 94.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=476000, episode_reward=1761.40 +/- 241.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=477000, episode_reward=1729.60 +/- 288.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=478000, episode_reward=1669.80 +/- 202.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=479000, episode_reward=1802.40 +/- 162.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=480000, episode_reward=1787.60 +/- 186.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=481000, episode_reward=1606.20 +/- 333.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=482000, episode_reward=1520.60 +/- 248.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=483000, episode_reward=1682.00 +/- 201.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=484000, episode_reward=1643.00 +/- 169.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=485000, episode_reward=1610.80 +/- 269.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=486000, episode_reward=1824.20 +/- 257.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=487000, episode_reward=1660.20 +/- 203.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=488000, episode_reward=1657.20 +/- 314.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=489000, episode_reward=1542.40 +/- 264.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=490000, episode_reward=1449.20 +/- 267.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=491000, episode_reward=1305.20 +/- 230.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=492000, episode_reward=1707.60 +/- 192.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=493000, episode_reward=1685.00 +/- 215.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=494000, episode_reward=1817.60 +/- 90.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=495000, episode_reward=1700.00 +/- 120.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=496000, episode_reward=1372.40 +/- 469.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=497000, episode_reward=1580.60 +/- 241.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=498000, episode_reward=1751.00 +/- 259.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=499000, episode_reward=1836.60 +/- 269.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=500000, episode_reward=1589.20 +/- 247.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=501000, episode_reward=1799.00 +/- 222.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=502000, episode_reward=1657.60 +/- 157.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=503000, episode_reward=1635.20 +/- 280.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=504000, episode_reward=1746.00 +/- 186.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=505000, episode_reward=1750.60 +/- 243.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=506000, episode_reward=1574.40 +/- 215.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=507000, episode_reward=1633.20 +/- 289.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=508000, episode_reward=1641.60 +/- 239.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=509000, episode_reward=1663.00 +/- 201.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=510000, episode_reward=1439.40 +/- 330.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=511000, episode_reward=1627.40 +/- 226.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=512000, episode_reward=1568.00 +/- 357.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=513000, episode_reward=1719.00 +/- 163.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=514000, episode_reward=1808.00 +/- 190.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=515000, episode_reward=1604.40 +/- 305.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=516000, episode_reward=1633.20 +/- 246.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=517000, episode_reward=1485.40 +/- 230.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=518000, episode_reward=1577.20 +/- 223.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=519000, episode_reward=1665.40 +/- 231.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=520000, episode_reward=1651.60 +/- 123.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=521000, episode_reward=1651.20 +/- 189.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=522000, episode_reward=1741.80 +/- 93.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=523000, episode_reward=1594.80 +/- 267.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=524000, episode_reward=1735.20 +/- 173.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=525000, episode_reward=1693.80 +/- 277.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=526000, episode_reward=1653.40 +/- 163.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=527000, episode_reward=1757.40 +/- 255.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=528000, episode_reward=1664.60 +/- 233.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=529000, episode_reward=1695.60 +/- 258.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=530000, episode_reward=1585.40 +/- 322.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=531000, episode_reward=1598.80 +/- 234.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=532000, episode_reward=1687.80 +/- 127.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=533000, episode_reward=1660.40 +/- 255.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=534000, episode_reward=1633.00 +/- 268.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=535000, episode_reward=1733.80 +/- 157.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=536000, episode_reward=1727.80 +/- 196.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=537000, episode_reward=1602.80 +/- 209.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=538000, episode_reward=1837.40 +/- 190.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=539000, episode_reward=1860.00 +/- 236.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=540000, episode_reward=1778.20 +/- 200.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=541000, episode_reward=1624.00 +/- 247.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=542000, episode_reward=1799.60 +/- 290.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=543000, episode_reward=1512.80 +/- 157.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=544000, episode_reward=1519.00 +/- 232.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=545000, episode_reward=1816.80 +/- 126.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=546000, episode_reward=1317.80 +/- 204.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=547000, episode_reward=1643.60 +/- 171.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=548000, episode_reward=1657.00 +/- 168.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=549000, episode_reward=1743.40 +/- 234.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=550000, episode_reward=1679.20 +/- 250.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=551000, episode_reward=1649.40 +/- 282.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=552000, episode_reward=1395.60 +/- 239.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=553000, episode_reward=1615.80 +/- 201.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=554000, episode_reward=1687.80 +/- 281.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=555000, episode_reward=1752.00 +/- 140.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=556000, episode_reward=1830.60 +/- 231.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=557000, episode_reward=1666.20 +/- 297.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=558000, episode_reward=1508.20 +/- 295.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=559000, episode_reward=1666.00 +/- 186.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=560000, episode_reward=1682.40 +/- 212.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=561000, episode_reward=1598.40 +/- 284.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=562000, episode_reward=1620.80 +/- 372.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=563000, episode_reward=1655.20 +/- 300.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=564000, episode_reward=1602.60 +/- 154.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=565000, episode_reward=1683.40 +/- 244.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=566000, episode_reward=1639.60 +/- 231.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=567000, episode_reward=1463.40 +/- 169.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=568000, episode_reward=1717.40 +/- 163.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=569000, episode_reward=1568.00 +/- 283.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=570000, episode_reward=1495.80 +/- 137.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=571000, episode_reward=1830.00 +/- 137.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=572000, episode_reward=1563.60 +/- 357.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=573000, episode_reward=1463.60 +/- 169.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=574000, episode_reward=1604.40 +/- 273.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=575000, episode_reward=1791.80 +/- 221.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=576000, episode_reward=1682.40 +/- 191.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=577000, episode_reward=1748.00 +/- 121.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=578000, episode_reward=1631.20 +/- 165.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=579000, episode_reward=1595.20 +/- 249.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=580000, episode_reward=1782.60 +/- 148.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=581000, episode_reward=1733.80 +/- 325.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=582000, episode_reward=1655.40 +/- 322.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=583000, episode_reward=1592.60 +/- 317.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=584000, episode_reward=1358.20 +/- 167.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=585000, episode_reward=1742.20 +/- 188.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=586000, episode_reward=1777.80 +/- 185.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=587000, episode_reward=1425.60 +/- 280.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=588000, episode_reward=1659.60 +/- 128.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=589000, episode_reward=1836.00 +/- 138.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=590000, episode_reward=1661.00 +/- 262.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=591000, episode_reward=1499.80 +/- 237.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=592000, episode_reward=1527.40 +/- 299.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=593000, episode_reward=1668.60 +/- 169.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=594000, episode_reward=1592.40 +/- 271.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=595000, episode_reward=1623.00 +/- 102.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=596000, episode_reward=1736.40 +/- 230.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=597000, episode_reward=1629.40 +/- 257.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=598000, episode_reward=1709.20 +/- 215.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=599000, episode_reward=1640.60 +/- 114.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=600000, episode_reward=1614.40 +/- 242.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=601000, episode_reward=1797.60 +/- 164.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=602000, episode_reward=1706.20 +/- 210.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=603000, episode_reward=1696.20 +/- 194.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=604000, episode_reward=1777.60 +/- 133.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=605000, episode_reward=1714.60 +/- 72.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=606000, episode_reward=1655.20 +/- 119.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=607000, episode_reward=1621.80 +/- 280.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=608000, episode_reward=1714.20 +/- 266.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=609000, episode_reward=1682.80 +/- 199.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=610000, episode_reward=1455.00 +/- 355.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=611000, episode_reward=1656.20 +/- 240.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=612000, episode_reward=1748.20 +/- 138.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=613000, episode_reward=1654.20 +/- 207.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=614000, episode_reward=1513.20 +/- 335.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=615000, episode_reward=1358.00 +/- 367.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=616000, episode_reward=1621.80 +/- 115.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=617000, episode_reward=1711.60 +/- 148.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=618000, episode_reward=1639.80 +/- 429.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=619000, episode_reward=1713.20 +/- 358.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=620000, episode_reward=1839.20 +/- 236.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=621000, episode_reward=1589.80 +/- 291.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=622000, episode_reward=1706.60 +/- 162.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=623000, episode_reward=1800.40 +/- 176.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=624000, episode_reward=1670.20 +/- 119.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=625000, episode_reward=1578.20 +/- 398.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=626000, episode_reward=1659.40 +/- 287.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=627000, episode_reward=1499.40 +/- 244.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=628000, episode_reward=1398.80 +/- 174.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=629000, episode_reward=1793.00 +/- 134.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=630000, episode_reward=1739.20 +/- 95.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=631000, episode_reward=1774.80 +/- 271.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=632000, episode_reward=1744.60 +/- 157.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=633000, episode_reward=1612.20 +/- 279.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=634000, episode_reward=1437.60 +/- 283.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=635000, episode_reward=1744.60 +/- 106.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=636000, episode_reward=1672.20 +/- 85.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=637000, episode_reward=1840.20 +/- 78.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=638000, episode_reward=1631.80 +/- 286.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=639000, episode_reward=1847.40 +/- 191.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=640000, episode_reward=1847.80 +/- 85.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=641000, episode_reward=1531.60 +/- 180.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=642000, episode_reward=1790.20 +/- 257.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=643000, episode_reward=1704.00 +/- 197.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=644000, episode_reward=1753.80 +/- 144.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=645000, episode_reward=1523.60 +/- 251.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=646000, episode_reward=1650.20 +/- 421.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=647000, episode_reward=1705.40 +/- 310.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=648000, episode_reward=1907.40 +/- 131.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=649000, episode_reward=1693.80 +/- 103.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=650000, episode_reward=1596.80 +/- 188.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=651000, episode_reward=1617.80 +/- 235.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=652000, episode_reward=1604.80 +/- 257.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=653000, episode_reward=1490.80 +/- 170.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=654000, episode_reward=1605.00 +/- 201.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=655000, episode_reward=1368.60 +/- 372.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=656000, episode_reward=1731.00 +/- 216.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=657000, episode_reward=1537.60 +/- 95.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=658000, episode_reward=1652.80 +/- 231.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=659000, episode_reward=1643.20 +/- 246.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=660000, episode_reward=1665.60 +/- 233.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=661000, episode_reward=1640.00 +/- 181.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=662000, episode_reward=1575.40 +/- 275.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=663000, episode_reward=1639.40 +/- 103.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=664000, episode_reward=1610.80 +/- 326.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=665000, episode_reward=1520.80 +/- 246.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=666000, episode_reward=1693.80 +/- 217.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=667000, episode_reward=1753.60 +/- 73.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=668000, episode_reward=1693.00 +/- 225.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=669000, episode_reward=1647.40 +/- 224.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=670000, episode_reward=1856.20 +/- 129.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=671000, episode_reward=1606.20 +/- 184.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=672000, episode_reward=1603.60 +/- 221.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=673000, episode_reward=1797.60 +/- 317.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=674000, episode_reward=1655.20 +/- 181.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=675000, episode_reward=1737.60 +/- 141.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=676000, episode_reward=1571.60 +/- 267.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=677000, episode_reward=1673.80 +/- 188.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=678000, episode_reward=1671.60 +/- 276.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=679000, episode_reward=1608.80 +/- 173.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=680000, episode_reward=1705.80 +/- 128.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=681000, episode_reward=1768.60 +/- 153.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=682000, episode_reward=1526.40 +/- 242.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=683000, episode_reward=1727.20 +/- 170.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=684000, episode_reward=1833.20 +/- 228.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=685000, episode_reward=1791.80 +/- 220.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=686000, episode_reward=1706.80 +/- 112.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=687000, episode_reward=1609.20 +/- 208.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=688000, episode_reward=1650.80 +/- 195.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=689000, episode_reward=1402.80 +/- 121.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=690000, episode_reward=1760.00 +/- 203.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=691000, episode_reward=1765.60 +/- 214.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=692000, episode_reward=1611.20 +/- 183.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=693000, episode_reward=1726.00 +/- 258.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=694000, episode_reward=1676.00 +/- 201.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=695000, episode_reward=1722.80 +/- 183.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=696000, episode_reward=1625.00 +/- 252.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=697000, episode_reward=1728.20 +/- 124.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=698000, episode_reward=1673.60 +/- 272.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=699000, episode_reward=1749.60 +/- 81.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=700000, episode_reward=1717.20 +/- 349.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=701000, episode_reward=1592.60 +/- 155.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=702000, episode_reward=1695.60 +/- 162.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=703000, episode_reward=1734.60 +/- 163.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=704000, episode_reward=1604.80 +/- 187.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=705000, episode_reward=1709.40 +/- 213.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=706000, episode_reward=1643.60 +/- 150.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=707000, episode_reward=1928.20 +/- 125.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=708000, episode_reward=1725.80 +/- 115.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=709000, episode_reward=1578.40 +/- 279.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=710000, episode_reward=1693.40 +/- 210.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=711000, episode_reward=1608.40 +/- 182.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=712000, episode_reward=1697.20 +/- 144.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=713000, episode_reward=1721.60 +/- 208.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=714000, episode_reward=1715.40 +/- 177.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=715000, episode_reward=1713.60 +/- 261.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=716000, episode_reward=1499.80 +/- 213.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=717000, episode_reward=1609.60 +/- 247.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=718000, episode_reward=1824.60 +/- 200.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=719000, episode_reward=1706.00 +/- 287.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=720000, episode_reward=1680.40 +/- 235.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=721000, episode_reward=1708.00 +/- 77.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=722000, episode_reward=1757.00 +/- 118.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=723000, episode_reward=1691.40 +/- 161.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=724000, episode_reward=1735.80 +/- 242.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=725000, episode_reward=1839.20 +/- 127.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=726000, episode_reward=1646.00 +/- 200.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=727000, episode_reward=1802.00 +/- 77.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=728000, episode_reward=1709.20 +/- 92.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=729000, episode_reward=1658.60 +/- 288.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=730000, episode_reward=1818.40 +/- 81.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=731000, episode_reward=1606.80 +/- 186.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=732000, episode_reward=1679.60 +/- 233.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=733000, episode_reward=1582.40 +/- 164.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=734000, episode_reward=1474.00 +/- 137.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=735000, episode_reward=1664.40 +/- 200.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=736000, episode_reward=1604.40 +/- 262.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=737000, episode_reward=1850.40 +/- 249.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=738000, episode_reward=1629.40 +/- 225.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=739000, episode_reward=1941.60 +/- 264.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=740000, episode_reward=1723.80 +/- 165.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=741000, episode_reward=1428.80 +/- 120.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=742000, episode_reward=1542.80 +/- 171.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=743000, episode_reward=1571.80 +/- 220.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=744000, episode_reward=1575.80 +/- 279.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=745000, episode_reward=1421.40 +/- 407.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=746000, episode_reward=1412.40 +/- 385.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=747000, episode_reward=1690.40 +/- 297.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=748000, episode_reward=1782.40 +/- 169.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=749000, episode_reward=1614.80 +/- 192.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=750000, episode_reward=1646.60 +/- 227.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=751000, episode_reward=1720.00 +/- 117.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=752000, episode_reward=1704.40 +/- 201.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=753000, episode_reward=1537.20 +/- 132.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=754000, episode_reward=1646.00 +/- 124.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=755000, episode_reward=1638.80 +/- 162.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=756000, episode_reward=1560.20 +/- 160.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=757000, episode_reward=1548.00 +/- 220.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=758000, episode_reward=1681.20 +/- 205.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=759000, episode_reward=1647.20 +/- 321.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=760000, episode_reward=1678.20 +/- 197.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=761000, episode_reward=1633.40 +/- 264.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=762000, episode_reward=1598.20 +/- 137.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=763000, episode_reward=1682.20 +/- 154.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=764000, episode_reward=1491.20 +/- 154.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=765000, episode_reward=1541.40 +/- 211.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=766000, episode_reward=1731.40 +/- 337.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=767000, episode_reward=1586.60 +/- 298.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=768000, episode_reward=1841.40 +/- 242.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=769000, episode_reward=1997.80 +/- 97.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=770000, episode_reward=1874.40 +/- 148.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=771000, episode_reward=1715.80 +/- 137.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=772000, episode_reward=1645.00 +/- 97.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=773000, episode_reward=1482.00 +/- 204.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=774000, episode_reward=1663.20 +/- 257.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=775000, episode_reward=1760.00 +/- 218.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=776000, episode_reward=1613.60 +/- 42.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=777000, episode_reward=1626.40 +/- 285.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=778000, episode_reward=1655.40 +/- 147.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=779000, episode_reward=1536.40 +/- 303.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=780000, episode_reward=1667.40 +/- 227.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=781000, episode_reward=1787.80 +/- 164.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=782000, episode_reward=1664.60 +/- 254.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=783000, episode_reward=1845.00 +/- 232.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=784000, episode_reward=1675.40 +/- 90.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=785000, episode_reward=1660.20 +/- 215.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=786000, episode_reward=1666.40 +/- 157.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=787000, episode_reward=1735.80 +/- 255.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=788000, episode_reward=1674.60 +/- 211.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=789000, episode_reward=1638.00 +/- 282.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=790000, episode_reward=1725.40 +/- 206.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=791000, episode_reward=1646.00 +/- 252.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=792000, episode_reward=1610.00 +/- 100.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=793000, episode_reward=1827.80 +/- 187.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=794000, episode_reward=1693.60 +/- 236.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=795000, episode_reward=1656.40 +/- 141.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=796000, episode_reward=1627.20 +/- 212.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=797000, episode_reward=1711.00 +/- 207.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=798000, episode_reward=1717.40 +/- 215.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=799000, episode_reward=1719.20 +/- 157.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=800000, episode_reward=1760.80 +/- 147.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=801000, episode_reward=1754.20 +/- 279.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=802000, episode_reward=1781.00 +/- 120.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=803000, episode_reward=1762.80 +/- 163.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=804000, episode_reward=1899.80 +/- 47.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=805000, episode_reward=1659.00 +/- 372.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=806000, episode_reward=1563.80 +/- 149.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=807000, episode_reward=1707.60 +/- 327.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=808000, episode_reward=1800.00 +/- 123.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=809000, episode_reward=1563.80 +/- 203.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=810000, episode_reward=1683.80 +/- 268.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=811000, episode_reward=1550.80 +/- 242.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=812000, episode_reward=1640.60 +/- 460.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=813000, episode_reward=1519.40 +/- 261.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=814000, episode_reward=1661.60 +/- 201.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=815000, episode_reward=1742.20 +/- 236.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=816000, episode_reward=1692.00 +/- 228.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=817000, episode_reward=1574.00 +/- 175.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=818000, episode_reward=1733.60 +/- 167.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=819000, episode_reward=1721.20 +/- 363.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=820000, episode_reward=1599.40 +/- 310.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=821000, episode_reward=1630.80 +/- 294.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=822000, episode_reward=1827.60 +/- 193.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=823000, episode_reward=1630.60 +/- 283.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=824000, episode_reward=1570.20 +/- 518.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=825000, episode_reward=1642.60 +/- 267.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=826000, episode_reward=1751.00 +/- 238.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=827000, episode_reward=1543.60 +/- 98.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=828000, episode_reward=1834.00 +/- 179.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=829000, episode_reward=1576.80 +/- 274.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=830000, episode_reward=1696.20 +/- 338.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=831000, episode_reward=1667.00 +/- 93.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=832000, episode_reward=1715.20 +/- 145.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=833000, episode_reward=1675.20 +/- 111.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=834000, episode_reward=1597.20 +/- 141.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=835000, episode_reward=1794.60 +/- 143.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=836000, episode_reward=1613.60 +/- 270.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=837000, episode_reward=1614.00 +/- 220.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=838000, episode_reward=1749.80 +/- 166.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=839000, episode_reward=1421.60 +/- 408.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=840000, episode_reward=1743.80 +/- 176.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=841000, episode_reward=1519.20 +/- 225.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=842000, episode_reward=1599.60 +/- 141.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=843000, episode_reward=1719.20 +/- 238.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=844000, episode_reward=1583.80 +/- 231.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=845000, episode_reward=1551.60 +/- 173.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=846000, episode_reward=1547.40 +/- 343.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=847000, episode_reward=1477.20 +/- 159.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=848000, episode_reward=1619.00 +/- 176.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=849000, episode_reward=1806.80 +/- 164.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=850000, episode_reward=1717.80 +/- 205.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=851000, episode_reward=1734.00 +/- 92.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=852000, episode_reward=1722.60 +/- 316.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=853000, episode_reward=1664.40 +/- 188.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=854000, episode_reward=1634.20 +/- 272.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=855000, episode_reward=1602.00 +/- 230.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=856000, episode_reward=1568.80 +/- 325.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=857000, episode_reward=1575.60 +/- 176.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=858000, episode_reward=1500.20 +/- 284.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=859000, episode_reward=1429.80 +/- 319.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=860000, episode_reward=1683.20 +/- 277.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=861000, episode_reward=1625.20 +/- 205.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=862000, episode_reward=1608.60 +/- 169.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=863000, episode_reward=1733.00 +/- 251.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=864000, episode_reward=1604.20 +/- 243.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=865000, episode_reward=1575.80 +/- 246.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=866000, episode_reward=1438.00 +/- 387.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=867000, episode_reward=1563.00 +/- 197.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=868000, episode_reward=1739.00 +/- 152.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=869000, episode_reward=1711.60 +/- 116.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=870000, episode_reward=1771.20 +/- 140.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=871000, episode_reward=1535.20 +/- 354.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=872000, episode_reward=1744.40 +/- 165.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=873000, episode_reward=1654.20 +/- 184.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=874000, episode_reward=1722.00 +/- 148.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=875000, episode_reward=1436.40 +/- 188.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=876000, episode_reward=1564.40 +/- 363.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=877000, episode_reward=1558.60 +/- 237.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=878000, episode_reward=1670.20 +/- 233.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=879000, episode_reward=1631.00 +/- 276.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=880000, episode_reward=1855.80 +/- 189.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=881000, episode_reward=1792.00 +/- 234.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=882000, episode_reward=1657.80 +/- 152.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=883000, episode_reward=1668.00 +/- 55.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=884000, episode_reward=1697.40 +/- 250.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=885000, episode_reward=1701.60 +/- 250.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=886000, episode_reward=1569.40 +/- 214.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=887000, episode_reward=1720.60 +/- 225.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=888000, episode_reward=1742.00 +/- 235.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=889000, episode_reward=1608.00 +/- 232.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=890000, episode_reward=1758.20 +/- 135.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=891000, episode_reward=1336.00 +/- 255.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=892000, episode_reward=1770.00 +/- 155.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=893000, episode_reward=1618.40 +/- 191.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=894000, episode_reward=1732.40 +/- 209.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=895000, episode_reward=1768.40 +/- 97.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=896000, episode_reward=1319.00 +/- 452.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=897000, episode_reward=1695.00 +/- 128.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=898000, episode_reward=1732.60 +/- 226.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=899000, episode_reward=1523.20 +/- 317.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=900000, episode_reward=1596.80 +/- 170.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=901000, episode_reward=1729.60 +/- 136.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=902000, episode_reward=1650.40 +/- 194.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=903000, episode_reward=1767.20 +/- 144.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=904000, episode_reward=1645.80 +/- 94.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=905000, episode_reward=1685.00 +/- 249.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=906000, episode_reward=1751.60 +/- 199.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=907000, episode_reward=1726.20 +/- 280.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=908000, episode_reward=1776.20 +/- 176.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=909000, episode_reward=1524.20 +/- 106.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=910000, episode_reward=1839.80 +/- 228.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=911000, episode_reward=1716.60 +/- 216.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=912000, episode_reward=1656.60 +/- 255.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=913000, episode_reward=1499.40 +/- 318.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=914000, episode_reward=1535.20 +/- 325.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=915000, episode_reward=1700.60 +/- 405.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=916000, episode_reward=1780.80 +/- 76.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=917000, episode_reward=1668.40 +/- 219.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=918000, episode_reward=1521.60 +/- 222.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=919000, episode_reward=1563.60 +/- 348.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=920000, episode_reward=1773.80 +/- 115.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=921000, episode_reward=1510.60 +/- 223.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=922000, episode_reward=1704.80 +/- 135.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=923000, episode_reward=1469.00 +/- 231.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=924000, episode_reward=1685.40 +/- 387.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=925000, episode_reward=1582.60 +/- 135.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=926000, episode_reward=1567.20 +/- 195.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=927000, episode_reward=1683.80 +/- 219.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=928000, episode_reward=1770.40 +/- 212.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=929000, episode_reward=1840.20 +/- 226.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=930000, episode_reward=1668.40 +/- 155.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=931000, episode_reward=1819.00 +/- 235.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=932000, episode_reward=1607.80 +/- 223.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=933000, episode_reward=1779.20 +/- 129.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=934000, episode_reward=1753.20 +/- 197.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=935000, episode_reward=1668.40 +/- 194.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=936000, episode_reward=1620.00 +/- 236.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=937000, episode_reward=1712.40 +/- 308.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=938000, episode_reward=1763.00 +/- 163.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=939000, episode_reward=1632.80 +/- 98.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=940000, episode_reward=1648.20 +/- 112.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=941000, episode_reward=1820.60 +/- 102.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=942000, episode_reward=1567.20 +/- 121.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=943000, episode_reward=1574.20 +/- 108.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=944000, episode_reward=1763.80 +/- 184.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=945000, episode_reward=1590.40 +/- 166.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=946000, episode_reward=1742.00 +/- 112.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=947000, episode_reward=1725.80 +/- 219.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=948000, episode_reward=1629.20 +/- 192.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=949000, episode_reward=1598.80 +/- 296.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=950000, episode_reward=1513.40 +/- 313.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=951000, episode_reward=1666.20 +/- 221.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=952000, episode_reward=1666.20 +/- 195.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=953000, episode_reward=1653.60 +/- 202.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=954000, episode_reward=1833.20 +/- 72.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=955000, episode_reward=1453.20 +/- 284.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=956000, episode_reward=1735.40 +/- 333.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=957000, episode_reward=1785.00 +/- 180.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=958000, episode_reward=1797.60 +/- 186.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=959000, episode_reward=1693.80 +/- 89.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=960000, episode_reward=1651.00 +/- 86.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=961000, episode_reward=1790.20 +/- 176.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=962000, episode_reward=1847.60 +/- 121.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=963000, episode_reward=1663.00 +/- 181.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=964000, episode_reward=1750.20 +/- 173.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=965000, episode_reward=1666.20 +/- 293.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=966000, episode_reward=1588.00 +/- 219.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=967000, episode_reward=1772.60 +/- 123.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=968000, episode_reward=1772.20 +/- 225.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=969000, episode_reward=1875.20 +/- 210.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=970000, episode_reward=1661.00 +/- 282.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=971000, episode_reward=1358.80 +/- 376.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=972000, episode_reward=1487.40 +/- 262.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=973000, episode_reward=1710.00 +/- 204.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=974000, episode_reward=1446.80 +/- 80.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=975000, episode_reward=1713.80 +/- 179.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=976000, episode_reward=1611.40 +/- 229.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=977000, episode_reward=1811.20 +/- 225.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=978000, episode_reward=1903.00 +/- 221.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=979000, episode_reward=1586.20 +/- 195.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=980000, episode_reward=1647.40 +/- 167.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=981000, episode_reward=1563.80 +/- 448.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=982000, episode_reward=1489.80 +/- 166.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=983000, episode_reward=1557.80 +/- 41.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=984000, episode_reward=1659.00 +/- 104.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=985000, episode_reward=1691.60 +/- 277.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=986000, episode_reward=1681.80 +/- 156.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=987000, episode_reward=1665.40 +/- 175.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=988000, episode_reward=1558.80 +/- 193.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=989000, episode_reward=1530.00 +/- 251.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=990000, episode_reward=1629.60 +/- 222.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=991000, episode_reward=1768.00 +/- 232.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=992000, episode_reward=1572.60 +/- 308.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=993000, episode_reward=1765.60 +/- 154.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=994000, episode_reward=1771.40 +/- 289.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=995000, episode_reward=1594.80 +/- 233.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=996000, episode_reward=1663.60 +/- 249.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=997000, episode_reward=1658.80 +/- 217.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=998000, episode_reward=1531.20 +/- 161.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=999000, episode_reward=1473.00 +/- 268.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1000000, episode_reward=1709.60 +/- 241.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1001000, episode_reward=1626.20 +/- 242.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Gamma = 0.9, Learning Rate = 0.003, Activation Function = ReLU, Neurons = 8, Process noise = 0.1, Measurement noise = 0.0\n",
      "Eval num_timesteps=1000, episode_reward=347.60 +/- 107.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=99.20 +/- 133.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=3000, episode_reward=-7727.00 +/- 1953.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=4000, episode_reward=-6634.00 +/- 528.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=5000, episode_reward=-2330.20 +/- 1667.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=6000, episode_reward=-3878.60 +/- 2752.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=7000, episode_reward=-467.00 +/- 434.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=8000, episode_reward=-1329.80 +/- 1054.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=9000, episode_reward=-452.80 +/- 239.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=10000, episode_reward=-703.80 +/- 451.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=11000, episode_reward=312.00 +/- 224.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=12000, episode_reward=253.00 +/- 297.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=13000, episode_reward=322.00 +/- 306.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=14000, episode_reward=294.60 +/- 182.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=15000, episode_reward=392.60 +/- 244.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=16000, episode_reward=414.40 +/- 114.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=17000, episode_reward=466.80 +/- 182.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=18000, episode_reward=464.40 +/- 377.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=19000, episode_reward=403.60 +/- 285.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=20000, episode_reward=805.60 +/- 189.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=21000, episode_reward=1277.60 +/- 221.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=22000, episode_reward=996.60 +/- 258.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=23000, episode_reward=1270.20 +/- 282.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=24000, episode_reward=946.60 +/- 223.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=25000, episode_reward=1079.20 +/- 223.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=26000, episode_reward=1176.20 +/- 244.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=27000, episode_reward=1426.60 +/- 190.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=28000, episode_reward=1282.40 +/- 133.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=29000, episode_reward=1320.20 +/- 237.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=30000, episode_reward=1278.00 +/- 320.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=31000, episode_reward=1126.00 +/- 298.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=32000, episode_reward=1110.00 +/- 305.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=33000, episode_reward=1257.20 +/- 166.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=34000, episode_reward=1339.60 +/- 282.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=35000, episode_reward=1131.80 +/- 109.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=36000, episode_reward=1456.80 +/- 148.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=37000, episode_reward=1195.20 +/- 104.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=38000, episode_reward=1108.80 +/- 257.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=39000, episode_reward=1247.00 +/- 344.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=40000, episode_reward=1463.80 +/- 112.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=41000, episode_reward=1131.00 +/- 51.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=42000, episode_reward=1043.40 +/- 284.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=43000, episode_reward=354.60 +/- 1449.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=44000, episode_reward=1207.80 +/- 164.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=45000, episode_reward=1131.60 +/- 234.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=46000, episode_reward=1288.80 +/- 321.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=47000, episode_reward=1381.60 +/- 134.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=48000, episode_reward=1331.20 +/- 346.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=49000, episode_reward=1394.80 +/- 247.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=50000, episode_reward=1539.80 +/- 118.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=51000, episode_reward=1461.40 +/- 208.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=52000, episode_reward=1491.00 +/- 280.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=53000, episode_reward=1613.80 +/- 89.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=54000, episode_reward=1686.80 +/- 256.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=55000, episode_reward=1224.60 +/- 360.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=56000, episode_reward=1593.20 +/- 140.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=57000, episode_reward=1525.20 +/- 96.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=58000, episode_reward=1640.60 +/- 131.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=59000, episode_reward=1527.60 +/- 256.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=60000, episode_reward=1588.40 +/- 49.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=61000, episode_reward=1556.40 +/- 213.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=62000, episode_reward=1550.80 +/- 280.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=63000, episode_reward=1606.60 +/- 197.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=64000, episode_reward=1633.20 +/- 43.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=65000, episode_reward=1604.80 +/- 325.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=66000, episode_reward=1487.80 +/- 319.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=67000, episode_reward=1535.40 +/- 307.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=68000, episode_reward=1455.80 +/- 228.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=69000, episode_reward=1600.40 +/- 134.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=70000, episode_reward=1863.40 +/- 58.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=71000, episode_reward=1649.60 +/- 238.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=72000, episode_reward=1852.40 +/- 68.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=73000, episode_reward=1666.20 +/- 214.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=74000, episode_reward=1455.20 +/- 314.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=75000, episode_reward=1797.00 +/- 260.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=76000, episode_reward=1627.80 +/- 203.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=77000, episode_reward=1289.40 +/- 408.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=78000, episode_reward=1766.20 +/- 188.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=79000, episode_reward=1586.40 +/- 180.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=80000, episode_reward=1571.20 +/- 389.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=81000, episode_reward=1396.20 +/- 205.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=82000, episode_reward=1377.60 +/- 316.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=83000, episode_reward=1662.20 +/- 197.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=84000, episode_reward=1450.80 +/- 243.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=85000, episode_reward=1705.40 +/- 154.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=86000, episode_reward=1456.80 +/- 359.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=87000, episode_reward=1375.00 +/- 205.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=88000, episode_reward=1680.40 +/- 256.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=89000, episode_reward=1496.60 +/- 210.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=90000, episode_reward=1433.20 +/- 384.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=91000, episode_reward=1491.00 +/- 378.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=92000, episode_reward=1571.80 +/- 292.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=93000, episode_reward=1412.60 +/- 191.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=94000, episode_reward=1647.40 +/- 175.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=95000, episode_reward=1228.60 +/- 403.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=96000, episode_reward=1589.20 +/- 106.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=97000, episode_reward=1511.20 +/- 175.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=98000, episode_reward=1585.40 +/- 193.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=99000, episode_reward=1551.80 +/- 194.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=100000, episode_reward=1498.80 +/- 346.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=101000, episode_reward=1404.80 +/- 226.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=102000, episode_reward=1549.60 +/- 197.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=103000, episode_reward=1563.80 +/- 120.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=104000, episode_reward=1629.80 +/- 189.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=105000, episode_reward=1592.40 +/- 152.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=106000, episode_reward=1679.00 +/- 264.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=107000, episode_reward=1503.40 +/- 407.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=108000, episode_reward=1127.80 +/- 438.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=109000, episode_reward=1753.00 +/- 304.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=110000, episode_reward=1368.20 +/- 127.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=111000, episode_reward=1804.60 +/- 91.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=112000, episode_reward=1516.00 +/- 124.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=113000, episode_reward=1557.60 +/- 249.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=114000, episode_reward=1570.40 +/- 40.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=115000, episode_reward=1609.20 +/- 157.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=116000, episode_reward=1753.80 +/- 280.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=117000, episode_reward=1579.00 +/- 184.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=118000, episode_reward=1335.60 +/- 202.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=119000, episode_reward=1871.20 +/- 242.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=120000, episode_reward=1745.40 +/- 72.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=121000, episode_reward=1836.40 +/- 205.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=122000, episode_reward=1636.60 +/- 140.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=123000, episode_reward=1708.80 +/- 101.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=124000, episode_reward=1671.60 +/- 73.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=125000, episode_reward=1518.80 +/- 194.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=126000, episode_reward=1588.00 +/- 199.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=127000, episode_reward=1396.00 +/- 224.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=128000, episode_reward=1653.60 +/- 140.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=129000, episode_reward=1621.00 +/- 154.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=130000, episode_reward=1643.00 +/- 219.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=131000, episode_reward=1449.40 +/- 227.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=132000, episode_reward=1628.80 +/- 165.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=133000, episode_reward=1538.60 +/- 114.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=134000, episode_reward=1716.20 +/- 238.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=135000, episode_reward=1467.60 +/- 247.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=136000, episode_reward=1494.20 +/- 300.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=137000, episode_reward=1357.00 +/- 315.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=138000, episode_reward=1554.40 +/- 173.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=139000, episode_reward=1647.60 +/- 159.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=140000, episode_reward=1661.00 +/- 226.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=141000, episode_reward=1324.40 +/- 244.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=142000, episode_reward=1817.60 +/- 85.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=143000, episode_reward=1552.00 +/- 183.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=144000, episode_reward=1767.80 +/- 193.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=145000, episode_reward=1654.60 +/- 171.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=146000, episode_reward=1784.40 +/- 216.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=147000, episode_reward=1491.00 +/- 203.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=148000, episode_reward=-1987.20 +/- 7322.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=149000, episode_reward=1644.60 +/- 360.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=150000, episode_reward=1485.60 +/- 83.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=151000, episode_reward=1521.00 +/- 292.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=152000, episode_reward=1415.40 +/- 138.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=153000, episode_reward=1673.60 +/- 212.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=154000, episode_reward=1664.40 +/- 121.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=155000, episode_reward=1801.80 +/- 137.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=156000, episode_reward=1608.40 +/- 65.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=157000, episode_reward=1599.20 +/- 165.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=158000, episode_reward=1665.20 +/- 186.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=159000, episode_reward=1767.80 +/- 114.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=160000, episode_reward=1781.20 +/- 104.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=161000, episode_reward=1672.20 +/- 214.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=162000, episode_reward=1695.80 +/- 135.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=163000, episode_reward=1386.20 +/- 192.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=164000, episode_reward=1668.60 +/- 91.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=165000, episode_reward=1533.00 +/- 181.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=166000, episode_reward=1639.40 +/- 335.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=167000, episode_reward=1579.60 +/- 92.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=168000, episode_reward=1707.00 +/- 196.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=169000, episode_reward=1644.60 +/- 121.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=170000, episode_reward=1643.80 +/- 219.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=171000, episode_reward=1494.40 +/- 70.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=172000, episode_reward=1690.60 +/- 276.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=173000, episode_reward=1638.60 +/- 208.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=174000, episode_reward=1598.00 +/- 198.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=175000, episode_reward=1563.80 +/- 122.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=176000, episode_reward=1577.20 +/- 216.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=177000, episode_reward=1654.80 +/- 227.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=178000, episode_reward=1576.00 +/- 277.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=179000, episode_reward=1649.00 +/- 183.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=180000, episode_reward=1670.00 +/- 138.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=181000, episode_reward=1547.40 +/- 101.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=182000, episode_reward=1695.00 +/- 289.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=183000, episode_reward=1697.80 +/- 153.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=184000, episode_reward=1760.80 +/- 178.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=185000, episode_reward=1649.60 +/- 351.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=186000, episode_reward=1781.20 +/- 222.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=187000, episode_reward=1700.60 +/- 349.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=188000, episode_reward=1484.60 +/- 298.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=189000, episode_reward=1660.20 +/- 305.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=190000, episode_reward=1678.20 +/- 170.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=191000, episode_reward=1853.40 +/- 95.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=192000, episode_reward=1733.80 +/- 218.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=193000, episode_reward=-1739.00 +/- 6804.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=194000, episode_reward=1577.20 +/- 239.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=195000, episode_reward=1582.00 +/- 167.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=196000, episode_reward=1698.00 +/- 145.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=197000, episode_reward=1459.60 +/- 140.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=198000, episode_reward=1684.60 +/- 220.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=199000, episode_reward=1630.60 +/- 214.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=200000, episode_reward=1606.40 +/- 232.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=201000, episode_reward=1658.60 +/- 221.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=202000, episode_reward=1693.80 +/- 125.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=203000, episode_reward=1507.40 +/- 232.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=204000, episode_reward=1496.40 +/- 181.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=205000, episode_reward=1552.40 +/- 96.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=206000, episode_reward=1569.20 +/- 260.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=207000, episode_reward=1677.60 +/- 137.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=208000, episode_reward=1462.00 +/- 302.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=209000, episode_reward=1578.20 +/- 208.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=210000, episode_reward=1623.80 +/- 166.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=211000, episode_reward=1793.40 +/- 237.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=212000, episode_reward=1655.60 +/- 174.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=213000, episode_reward=1638.60 +/- 176.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=214000, episode_reward=1584.80 +/- 212.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=215000, episode_reward=1604.40 +/- 53.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=216000, episode_reward=1552.40 +/- 222.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=217000, episode_reward=1719.20 +/- 208.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=218000, episode_reward=1762.20 +/- 165.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=219000, episode_reward=1570.80 +/- 68.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=220000, episode_reward=1800.00 +/- 139.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=221000, episode_reward=1657.80 +/- 93.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=222000, episode_reward=1809.80 +/- 125.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=223000, episode_reward=1586.00 +/- 273.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=224000, episode_reward=1656.00 +/- 109.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=225000, episode_reward=1650.80 +/- 125.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=226000, episode_reward=1645.60 +/- 119.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=227000, episode_reward=1662.60 +/- 184.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=228000, episode_reward=1738.20 +/- 134.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=229000, episode_reward=1631.80 +/- 140.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=230000, episode_reward=1720.40 +/- 266.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=231000, episode_reward=1580.80 +/- 315.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=232000, episode_reward=1756.00 +/- 133.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=233000, episode_reward=1691.00 +/- 249.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=234000, episode_reward=1661.80 +/- 223.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=235000, episode_reward=1778.60 +/- 302.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=236000, episode_reward=1583.00 +/- 191.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=237000, episode_reward=1575.20 +/- 157.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=238000, episode_reward=1767.80 +/- 156.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=239000, episode_reward=1686.40 +/- 128.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=240000, episode_reward=1737.60 +/- 152.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=241000, episode_reward=1657.80 +/- 195.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=242000, episode_reward=1669.20 +/- 188.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=243000, episode_reward=1606.40 +/- 87.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=244000, episode_reward=1648.00 +/- 77.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=245000, episode_reward=1578.60 +/- 233.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=246000, episode_reward=1704.40 +/- 231.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=247000, episode_reward=1809.20 +/- 224.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=248000, episode_reward=1824.60 +/- 257.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=249000, episode_reward=1646.40 +/- 195.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=250000, episode_reward=1714.40 +/- 95.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=251000, episode_reward=1638.60 +/- 204.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=252000, episode_reward=1780.20 +/- 216.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=253000, episode_reward=1609.20 +/- 85.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=254000, episode_reward=1670.20 +/- 216.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=255000, episode_reward=1408.00 +/- 769.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=256000, episode_reward=1740.80 +/- 196.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=257000, episode_reward=1754.80 +/- 269.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=258000, episode_reward=1629.20 +/- 102.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=259000, episode_reward=1524.20 +/- 300.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=260000, episode_reward=867.20 +/- 1521.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=261000, episode_reward=1762.80 +/- 123.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=262000, episode_reward=1552.20 +/- 159.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=263000, episode_reward=1886.60 +/- 121.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=264000, episode_reward=1824.00 +/- 202.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=265000, episode_reward=1892.40 +/- 193.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=266000, episode_reward=1817.60 +/- 113.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=267000, episode_reward=1643.20 +/- 127.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=268000, episode_reward=1858.20 +/- 324.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=269000, episode_reward=1725.20 +/- 174.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=270000, episode_reward=1722.40 +/- 111.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=271000, episode_reward=1728.80 +/- 164.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=272000, episode_reward=1789.00 +/- 159.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=273000, episode_reward=1772.00 +/- 210.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=274000, episode_reward=1654.00 +/- 132.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=275000, episode_reward=1614.00 +/- 69.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=276000, episode_reward=1596.40 +/- 145.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=277000, episode_reward=1758.20 +/- 178.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=278000, episode_reward=1593.60 +/- 441.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=279000, episode_reward=1679.40 +/- 295.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=280000, episode_reward=1742.80 +/- 226.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=281000, episode_reward=1748.00 +/- 173.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=282000, episode_reward=1488.00 +/- 144.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=283000, episode_reward=1602.60 +/- 41.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=284000, episode_reward=1610.40 +/- 256.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=285000, episode_reward=1718.00 +/- 255.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=286000, episode_reward=1810.40 +/- 173.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=287000, episode_reward=1544.00 +/- 82.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=288000, episode_reward=1766.40 +/- 174.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=289000, episode_reward=1543.80 +/- 165.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=290000, episode_reward=1762.40 +/- 315.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=291000, episode_reward=1638.60 +/- 262.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=292000, episode_reward=1767.40 +/- 154.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=293000, episode_reward=1513.00 +/- 213.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=294000, episode_reward=1669.40 +/- 101.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=295000, episode_reward=1620.60 +/- 203.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=296000, episode_reward=1755.00 +/- 71.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=297000, episode_reward=1724.40 +/- 265.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=298000, episode_reward=1618.60 +/- 269.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=299000, episode_reward=1696.60 +/- 63.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=300000, episode_reward=1781.60 +/- 171.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=301000, episode_reward=1685.40 +/- 120.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=302000, episode_reward=1749.20 +/- 302.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=303000, episode_reward=1645.40 +/- 111.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=304000, episode_reward=1818.40 +/- 119.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=305000, episode_reward=1713.40 +/- 338.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=306000, episode_reward=1784.00 +/- 154.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=307000, episode_reward=1687.00 +/- 86.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=308000, episode_reward=1710.40 +/- 202.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=309000, episode_reward=1741.80 +/- 131.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=310000, episode_reward=1775.80 +/- 97.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=311000, episode_reward=1699.60 +/- 242.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=312000, episode_reward=1738.60 +/- 350.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=313000, episode_reward=1701.20 +/- 275.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=314000, episode_reward=1719.60 +/- 181.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=315000, episode_reward=1688.40 +/- 165.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=316000, episode_reward=1684.80 +/- 293.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=317000, episode_reward=1631.60 +/- 244.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=318000, episode_reward=1690.60 +/- 251.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=319000, episode_reward=1706.00 +/- 270.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=320000, episode_reward=1513.60 +/- 282.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=321000, episode_reward=1559.40 +/- 134.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=322000, episode_reward=1413.60 +/- 233.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=323000, episode_reward=1668.80 +/- 216.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=324000, episode_reward=1705.60 +/- 166.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=325000, episode_reward=1713.40 +/- 161.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=326000, episode_reward=1758.60 +/- 105.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=327000, episode_reward=1575.00 +/- 199.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=328000, episode_reward=1799.40 +/- 92.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=329000, episode_reward=1652.60 +/- 228.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=330000, episode_reward=1803.80 +/- 96.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=331000, episode_reward=1614.80 +/- 352.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=332000, episode_reward=1331.00 +/- 266.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=333000, episode_reward=1864.00 +/- 143.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=334000, episode_reward=1609.00 +/- 221.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=335000, episode_reward=1746.40 +/- 170.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=336000, episode_reward=1703.60 +/- 207.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=337000, episode_reward=1649.20 +/- 369.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=338000, episode_reward=1531.20 +/- 290.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=339000, episode_reward=1687.40 +/- 195.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=340000, episode_reward=1596.80 +/- 205.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=341000, episode_reward=1785.40 +/- 143.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=342000, episode_reward=1650.60 +/- 236.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=343000, episode_reward=1609.60 +/- 389.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=344000, episode_reward=1667.00 +/- 204.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=345000, episode_reward=1760.00 +/- 242.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=346000, episode_reward=1617.60 +/- 130.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=347000, episode_reward=1742.60 +/- 126.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=348000, episode_reward=1585.40 +/- 225.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=349000, episode_reward=1808.80 +/- 121.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=350000, episode_reward=1486.00 +/- 245.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=351000, episode_reward=1611.00 +/- 217.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=352000, episode_reward=1711.80 +/- 341.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=353000, episode_reward=1691.20 +/- 153.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=354000, episode_reward=1680.20 +/- 76.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=355000, episode_reward=1662.40 +/- 171.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=356000, episode_reward=1584.60 +/- 183.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=357000, episode_reward=1880.00 +/- 281.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=358000, episode_reward=1747.40 +/- 134.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=359000, episode_reward=1761.20 +/- 262.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=360000, episode_reward=1524.00 +/- 267.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=361000, episode_reward=1550.80 +/- 273.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=362000, episode_reward=1691.80 +/- 261.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=363000, episode_reward=1771.20 +/- 198.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=364000, episode_reward=1618.00 +/- 193.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=365000, episode_reward=1768.80 +/- 223.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=366000, episode_reward=1699.20 +/- 125.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=367000, episode_reward=1566.00 +/- 261.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=368000, episode_reward=1706.60 +/- 122.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=369000, episode_reward=1688.40 +/- 88.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=370000, episode_reward=1441.80 +/- 323.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=371000, episode_reward=1693.00 +/- 76.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=372000, episode_reward=1799.20 +/- 131.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=373000, episode_reward=1907.20 +/- 197.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=374000, episode_reward=1756.60 +/- 188.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=375000, episode_reward=1789.00 +/- 238.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=376000, episode_reward=1813.20 +/- 204.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=377000, episode_reward=1803.20 +/- 121.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=378000, episode_reward=1632.80 +/- 211.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=379000, episode_reward=1650.40 +/- 173.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=380000, episode_reward=1654.40 +/- 204.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=381000, episode_reward=1648.20 +/- 204.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=382000, episode_reward=1615.20 +/- 237.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=383000, episode_reward=1861.60 +/- 23.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=384000, episode_reward=1891.40 +/- 109.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=385000, episode_reward=1564.40 +/- 111.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=386000, episode_reward=1655.60 +/- 263.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=387000, episode_reward=1766.80 +/- 207.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=388000, episode_reward=1651.00 +/- 196.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=389000, episode_reward=1732.00 +/- 211.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=390000, episode_reward=1730.00 +/- 177.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=391000, episode_reward=1697.40 +/- 243.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=392000, episode_reward=1436.40 +/- 251.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=393000, episode_reward=1673.00 +/- 233.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=394000, episode_reward=1673.80 +/- 241.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=395000, episode_reward=1660.00 +/- 104.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=396000, episode_reward=1561.60 +/- 351.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=397000, episode_reward=1478.80 +/- 247.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=398000, episode_reward=1670.00 +/- 186.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=399000, episode_reward=1750.60 +/- 308.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=400000, episode_reward=1802.60 +/- 142.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=401000, episode_reward=1386.80 +/- 165.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=402000, episode_reward=1565.60 +/- 305.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=403000, episode_reward=1610.60 +/- 355.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=404000, episode_reward=1655.80 +/- 146.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=405000, episode_reward=1490.00 +/- 242.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=406000, episode_reward=1705.80 +/- 210.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=407000, episode_reward=1632.00 +/- 92.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=408000, episode_reward=1745.20 +/- 188.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=409000, episode_reward=1646.20 +/- 134.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=410000, episode_reward=1506.80 +/- 207.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=411000, episode_reward=1807.40 +/- 112.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=412000, episode_reward=1714.40 +/- 174.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=413000, episode_reward=1750.00 +/- 159.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=414000, episode_reward=1552.20 +/- 221.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=415000, episode_reward=1608.40 +/- 235.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=416000, episode_reward=1622.40 +/- 106.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=417000, episode_reward=1664.20 +/- 117.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=418000, episode_reward=1652.40 +/- 196.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=419000, episode_reward=1543.40 +/- 310.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=420000, episode_reward=1727.20 +/- 137.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=421000, episode_reward=1609.40 +/- 241.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=422000, episode_reward=1494.60 +/- 309.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=423000, episode_reward=1585.80 +/- 189.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=424000, episode_reward=1755.80 +/- 412.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=425000, episode_reward=1604.80 +/- 133.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=426000, episode_reward=1686.60 +/- 139.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=427000, episode_reward=1728.40 +/- 234.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=428000, episode_reward=1819.80 +/- 225.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=429000, episode_reward=1829.40 +/- 114.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=430000, episode_reward=1334.00 +/- 360.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=431000, episode_reward=1637.60 +/- 262.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=432000, episode_reward=1626.60 +/- 236.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=433000, episode_reward=1825.60 +/- 323.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=434000, episode_reward=1553.00 +/- 248.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=435000, episode_reward=1883.80 +/- 265.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=436000, episode_reward=1723.80 +/- 257.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=437000, episode_reward=1681.80 +/- 151.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=438000, episode_reward=1554.60 +/- 105.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=439000, episode_reward=1614.00 +/- 200.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=440000, episode_reward=1597.60 +/- 173.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=441000, episode_reward=1558.80 +/- 96.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=442000, episode_reward=1723.00 +/- 268.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=443000, episode_reward=1673.60 +/- 166.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=444000, episode_reward=1685.60 +/- 87.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=445000, episode_reward=1576.40 +/- 200.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=446000, episode_reward=1769.20 +/- 184.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=447000, episode_reward=1558.00 +/- 293.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=448000, episode_reward=1813.60 +/- 140.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=449000, episode_reward=1688.80 +/- 246.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=450000, episode_reward=1650.00 +/- 201.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=451000, episode_reward=1777.80 +/- 148.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=452000, episode_reward=1592.40 +/- 239.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=453000, episode_reward=1546.80 +/- 347.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=454000, episode_reward=1583.80 +/- 124.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=455000, episode_reward=1730.20 +/- 190.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=456000, episode_reward=1770.60 +/- 120.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=457000, episode_reward=1650.40 +/- 210.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=458000, episode_reward=1643.40 +/- 280.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=459000, episode_reward=1650.20 +/- 285.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=460000, episode_reward=1755.00 +/- 201.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=461000, episode_reward=1556.60 +/- 190.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=462000, episode_reward=1694.40 +/- 289.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=463000, episode_reward=1730.40 +/- 243.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=464000, episode_reward=1730.80 +/- 279.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=465000, episode_reward=1717.60 +/- 141.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=466000, episode_reward=1614.40 +/- 182.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=467000, episode_reward=1559.00 +/- 274.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=468000, episode_reward=1553.20 +/- 200.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=469000, episode_reward=1571.80 +/- 249.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=470000, episode_reward=1628.40 +/- 237.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=471000, episode_reward=1533.80 +/- 179.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=472000, episode_reward=1670.40 +/- 284.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=473000, episode_reward=1691.80 +/- 222.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=474000, episode_reward=1653.00 +/- 110.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=475000, episode_reward=1589.40 +/- 217.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=476000, episode_reward=1628.00 +/- 291.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=477000, episode_reward=1758.20 +/- 308.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=478000, episode_reward=1748.60 +/- 274.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=479000, episode_reward=1931.40 +/- 208.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=480000, episode_reward=1330.80 +/- 270.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=481000, episode_reward=1746.60 +/- 380.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=482000, episode_reward=1631.60 +/- 238.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=483000, episode_reward=1505.00 +/- 70.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=484000, episode_reward=1676.60 +/- 166.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=485000, episode_reward=1510.40 +/- 316.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=486000, episode_reward=1462.20 +/- 313.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=487000, episode_reward=1735.60 +/- 93.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=488000, episode_reward=1714.60 +/- 273.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=489000, episode_reward=1737.00 +/- 203.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=490000, episode_reward=1695.80 +/- 135.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=491000, episode_reward=1933.40 +/- 63.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=492000, episode_reward=1456.80 +/- 115.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=493000, episode_reward=1605.60 +/- 297.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=494000, episode_reward=1497.40 +/- 246.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=495000, episode_reward=1748.80 +/- 277.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=496000, episode_reward=1506.00 +/- 212.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=497000, episode_reward=1656.40 +/- 181.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=498000, episode_reward=1470.40 +/- 204.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=499000, episode_reward=1407.00 +/- 413.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=500000, episode_reward=1632.60 +/- 283.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=501000, episode_reward=1826.80 +/- 223.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=502000, episode_reward=1573.00 +/- 168.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=503000, episode_reward=1655.40 +/- 155.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=504000, episode_reward=1523.80 +/- 330.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=505000, episode_reward=1307.60 +/- 632.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=506000, episode_reward=1512.80 +/- 353.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=507000, episode_reward=1528.20 +/- 131.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=508000, episode_reward=1472.20 +/- 200.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=509000, episode_reward=1418.80 +/- 108.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=510000, episode_reward=1540.00 +/- 244.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=511000, episode_reward=1771.60 +/- 195.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=512000, episode_reward=1605.60 +/- 125.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=513000, episode_reward=1554.00 +/- 180.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=514000, episode_reward=1498.20 +/- 348.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=515000, episode_reward=1687.00 +/- 330.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=516000, episode_reward=1491.80 +/- 375.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=517000, episode_reward=1845.20 +/- 246.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=518000, episode_reward=1644.00 +/- 194.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=519000, episode_reward=1687.20 +/- 231.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=520000, episode_reward=1700.20 +/- 124.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=521000, episode_reward=1690.60 +/- 150.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=522000, episode_reward=1367.60 +/- 387.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=523000, episode_reward=1596.60 +/- 309.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=524000, episode_reward=1468.20 +/- 222.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=525000, episode_reward=1735.80 +/- 363.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=526000, episode_reward=1520.00 +/- 455.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=527000, episode_reward=1680.60 +/- 435.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=528000, episode_reward=1507.80 +/- 162.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=529000, episode_reward=1541.80 +/- 116.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=530000, episode_reward=1743.60 +/- 101.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=531000, episode_reward=1720.60 +/- 64.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=532000, episode_reward=1459.60 +/- 259.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=533000, episode_reward=1731.60 +/- 109.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=534000, episode_reward=1707.00 +/- 240.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=535000, episode_reward=1731.40 +/- 187.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=536000, episode_reward=1749.40 +/- 237.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=537000, episode_reward=1684.60 +/- 115.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=538000, episode_reward=1696.40 +/- 229.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=539000, episode_reward=1831.80 +/- 130.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=540000, episode_reward=1640.00 +/- 362.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=541000, episode_reward=1802.20 +/- 114.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=542000, episode_reward=1739.20 +/- 341.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=543000, episode_reward=1731.80 +/- 345.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=544000, episode_reward=1658.80 +/- 301.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=545000, episode_reward=1723.00 +/- 186.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=546000, episode_reward=1696.00 +/- 83.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=547000, episode_reward=1598.00 +/- 199.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=548000, episode_reward=1807.40 +/- 287.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=549000, episode_reward=1616.80 +/- 231.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=550000, episode_reward=1745.60 +/- 192.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=551000, episode_reward=1662.80 +/- 99.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=552000, episode_reward=1616.80 +/- 247.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=553000, episode_reward=1743.00 +/- 135.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=554000, episode_reward=1534.00 +/- 193.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=555000, episode_reward=1507.20 +/- 146.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=556000, episode_reward=1652.40 +/- 114.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=557000, episode_reward=1518.40 +/- 167.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=558000, episode_reward=1807.60 +/- 150.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=559000, episode_reward=1741.60 +/- 131.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=560000, episode_reward=1698.20 +/- 101.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=561000, episode_reward=1968.80 +/- 205.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=562000, episode_reward=1762.40 +/- 184.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=563000, episode_reward=1508.60 +/- 336.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=564000, episode_reward=1498.20 +/- 83.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=565000, episode_reward=1668.00 +/- 256.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=566000, episode_reward=1640.00 +/- 137.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=567000, episode_reward=1845.80 +/- 177.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=568000, episode_reward=1721.60 +/- 173.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=569000, episode_reward=1762.40 +/- 151.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=570000, episode_reward=1814.00 +/- 225.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=571000, episode_reward=1735.20 +/- 83.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=572000, episode_reward=1801.20 +/- 205.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=573000, episode_reward=1727.20 +/- 129.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=574000, episode_reward=1741.40 +/- 186.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=575000, episode_reward=1526.00 +/- 225.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=576000, episode_reward=1619.60 +/- 218.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=577000, episode_reward=1810.60 +/- 136.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=578000, episode_reward=1710.40 +/- 231.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=579000, episode_reward=1632.40 +/- 224.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=580000, episode_reward=1551.40 +/- 243.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=581000, episode_reward=1824.40 +/- 101.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=582000, episode_reward=1718.00 +/- 130.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=583000, episode_reward=1542.40 +/- 273.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=584000, episode_reward=1844.40 +/- 165.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=585000, episode_reward=1609.20 +/- 200.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=586000, episode_reward=1824.20 +/- 65.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=587000, episode_reward=1697.80 +/- 203.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=588000, episode_reward=1560.60 +/- 252.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=589000, episode_reward=1458.80 +/- 259.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=590000, episode_reward=1634.60 +/- 134.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=591000, episode_reward=1763.00 +/- 195.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=592000, episode_reward=1622.80 +/- 193.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=593000, episode_reward=1773.20 +/- 253.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=594000, episode_reward=1687.20 +/- 146.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=595000, episode_reward=1619.40 +/- 168.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=596000, episode_reward=1494.60 +/- 148.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=597000, episode_reward=1728.60 +/- 148.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=598000, episode_reward=1695.40 +/- 207.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=599000, episode_reward=1577.20 +/- 379.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=600000, episode_reward=1511.00 +/- 188.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=601000, episode_reward=1848.80 +/- 157.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=602000, episode_reward=1440.60 +/- 190.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=603000, episode_reward=1817.00 +/- 145.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=604000, episode_reward=1671.40 +/- 217.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=605000, episode_reward=1543.40 +/- 114.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=606000, episode_reward=1678.00 +/- 210.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=607000, episode_reward=1652.20 +/- 349.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=608000, episode_reward=1638.20 +/- 106.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=609000, episode_reward=1662.60 +/- 233.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=610000, episode_reward=1742.40 +/- 298.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=611000, episode_reward=1681.60 +/- 205.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=612000, episode_reward=1852.40 +/- 247.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=613000, episode_reward=1637.00 +/- 262.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=614000, episode_reward=1863.40 +/- 125.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=615000, episode_reward=1656.80 +/- 147.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=616000, episode_reward=1582.60 +/- 156.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=617000, episode_reward=1665.40 +/- 298.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=618000, episode_reward=1730.20 +/- 195.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=619000, episode_reward=1807.80 +/- 126.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=620000, episode_reward=1751.80 +/- 198.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=621000, episode_reward=1682.20 +/- 132.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=622000, episode_reward=1475.60 +/- 300.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=623000, episode_reward=1852.00 +/- 182.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=624000, episode_reward=1376.00 +/- 182.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=625000, episode_reward=1790.20 +/- 136.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=626000, episode_reward=1581.20 +/- 291.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=627000, episode_reward=1644.60 +/- 179.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=628000, episode_reward=1705.60 +/- 270.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=629000, episode_reward=1635.80 +/- 175.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=630000, episode_reward=1566.40 +/- 144.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=631000, episode_reward=1622.80 +/- 208.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=632000, episode_reward=1614.60 +/- 317.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=633000, episode_reward=1648.00 +/- 185.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=634000, episode_reward=1705.80 +/- 68.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=635000, episode_reward=1668.00 +/- 149.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=636000, episode_reward=1871.80 +/- 159.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=637000, episode_reward=1620.60 +/- 283.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=638000, episode_reward=1679.40 +/- 210.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=639000, episode_reward=1702.40 +/- 217.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=640000, episode_reward=1634.20 +/- 171.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=641000, episode_reward=1476.60 +/- 332.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=642000, episode_reward=1522.00 +/- 166.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=643000, episode_reward=1749.80 +/- 232.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=644000, episode_reward=1701.60 +/- 78.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=645000, episode_reward=1636.60 +/- 351.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=646000, episode_reward=1612.20 +/- 181.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=647000, episode_reward=1675.20 +/- 178.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=648000, episode_reward=1621.60 +/- 316.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=649000, episode_reward=1644.80 +/- 124.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=650000, episode_reward=1627.00 +/- 262.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=651000, episode_reward=1592.20 +/- 179.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=652000, episode_reward=1637.00 +/- 199.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=653000, episode_reward=1590.00 +/- 222.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=654000, episode_reward=1665.20 +/- 72.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=655000, episode_reward=1478.80 +/- 231.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=656000, episode_reward=1708.00 +/- 174.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=657000, episode_reward=1872.80 +/- 230.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=658000, episode_reward=1637.80 +/- 135.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=659000, episode_reward=1711.60 +/- 213.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=660000, episode_reward=1586.60 +/- 136.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=661000, episode_reward=1575.60 +/- 257.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=662000, episode_reward=1664.20 +/- 243.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=663000, episode_reward=1799.20 +/- 190.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=664000, episode_reward=1698.80 +/- 228.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=665000, episode_reward=1760.00 +/- 157.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=666000, episode_reward=1728.20 +/- 252.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=667000, episode_reward=1791.40 +/- 115.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=668000, episode_reward=1475.60 +/- 240.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=669000, episode_reward=1747.60 +/- 168.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=670000, episode_reward=1552.20 +/- 302.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=671000, episode_reward=1745.80 +/- 224.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=672000, episode_reward=1734.60 +/- 225.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=673000, episode_reward=1799.20 +/- 128.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=674000, episode_reward=1691.00 +/- 117.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=675000, episode_reward=1487.20 +/- 201.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=676000, episode_reward=1662.80 +/- 213.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=677000, episode_reward=1589.80 +/- 272.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=678000, episode_reward=1662.40 +/- 219.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=679000, episode_reward=1709.20 +/- 87.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=680000, episode_reward=1563.80 +/- 322.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=681000, episode_reward=1775.80 +/- 213.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=682000, episode_reward=1655.80 +/- 156.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=683000, episode_reward=1754.60 +/- 178.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=684000, episode_reward=1671.40 +/- 223.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=685000, episode_reward=1584.40 +/- 227.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=686000, episode_reward=1805.00 +/- 333.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=687000, episode_reward=1655.60 +/- 252.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=688000, episode_reward=1659.00 +/- 343.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=689000, episode_reward=1662.80 +/- 223.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=690000, episode_reward=1519.20 +/- 282.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=691000, episode_reward=1665.40 +/- 184.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=692000, episode_reward=1694.00 +/- 251.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=693000, episode_reward=1672.80 +/- 254.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=694000, episode_reward=1509.40 +/- 190.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=695000, episode_reward=1614.60 +/- 308.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=696000, episode_reward=1527.20 +/- 227.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=697000, episode_reward=1841.20 +/- 186.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=698000, episode_reward=1561.60 +/- 184.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=699000, episode_reward=1683.80 +/- 242.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=700000, episode_reward=1715.40 +/- 169.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=701000, episode_reward=1741.80 +/- 330.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=702000, episode_reward=1598.40 +/- 353.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=703000, episode_reward=1645.80 +/- 308.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=704000, episode_reward=1609.40 +/- 321.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=705000, episode_reward=1912.60 +/- 75.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=706000, episode_reward=1848.20 +/- 184.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=707000, episode_reward=1635.40 +/- 75.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=708000, episode_reward=1549.00 +/- 231.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=709000, episode_reward=1566.60 +/- 328.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=710000, episode_reward=1726.40 +/- 279.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=711000, episode_reward=1520.80 +/- 319.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=712000, episode_reward=1398.20 +/- 213.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=713000, episode_reward=1608.00 +/- 327.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=714000, episode_reward=1631.40 +/- 123.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=715000, episode_reward=1648.20 +/- 205.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=716000, episode_reward=1465.40 +/- 280.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=717000, episode_reward=1843.00 +/- 269.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=718000, episode_reward=1517.20 +/- 106.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=719000, episode_reward=1715.40 +/- 300.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=720000, episode_reward=1501.40 +/- 149.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=721000, episode_reward=1777.60 +/- 279.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=722000, episode_reward=1689.80 +/- 289.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=723000, episode_reward=1592.80 +/- 360.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=724000, episode_reward=1714.20 +/- 191.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=725000, episode_reward=1522.00 +/- 224.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=726000, episode_reward=1656.20 +/- 270.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=727000, episode_reward=1520.40 +/- 324.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=728000, episode_reward=1637.60 +/- 324.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=729000, episode_reward=1857.60 +/- 75.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=730000, episode_reward=1532.40 +/- 203.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=731000, episode_reward=1665.80 +/- 195.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=732000, episode_reward=1633.40 +/- 195.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=733000, episode_reward=1747.40 +/- 69.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=734000, episode_reward=1630.40 +/- 299.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=735000, episode_reward=1809.00 +/- 106.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=736000, episode_reward=1501.40 +/- 312.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=737000, episode_reward=1652.20 +/- 163.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=738000, episode_reward=1575.20 +/- 200.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=739000, episode_reward=1525.60 +/- 228.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=740000, episode_reward=1586.80 +/- 119.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=741000, episode_reward=1605.80 +/- 86.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=742000, episode_reward=1805.40 +/- 141.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=743000, episode_reward=1643.40 +/- 318.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=744000, episode_reward=1745.80 +/- 203.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=745000, episode_reward=1715.80 +/- 183.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=746000, episode_reward=1628.80 +/- 193.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=747000, episode_reward=1620.80 +/- 316.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=748000, episode_reward=1561.60 +/- 202.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=749000, episode_reward=1401.40 +/- 198.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=750000, episode_reward=1620.60 +/- 179.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=751000, episode_reward=1573.80 +/- 203.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=752000, episode_reward=1648.00 +/- 210.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=753000, episode_reward=1627.40 +/- 187.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=754000, episode_reward=1731.60 +/- 91.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=755000, episode_reward=1624.40 +/- 153.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=756000, episode_reward=1642.00 +/- 144.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=757000, episode_reward=1739.80 +/- 350.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=758000, episode_reward=1724.20 +/- 112.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=759000, episode_reward=1440.40 +/- 116.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=760000, episode_reward=1576.60 +/- 194.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=761000, episode_reward=1632.80 +/- 177.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=762000, episode_reward=1521.60 +/- 404.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=763000, episode_reward=1810.80 +/- 73.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=764000, episode_reward=1888.20 +/- 144.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=765000, episode_reward=1705.00 +/- 249.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=766000, episode_reward=1835.00 +/- 159.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=767000, episode_reward=1763.60 +/- 134.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=768000, episode_reward=1596.80 +/- 201.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=769000, episode_reward=1661.00 +/- 155.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=770000, episode_reward=1470.60 +/- 65.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=771000, episode_reward=1578.60 +/- 251.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=772000, episode_reward=1777.40 +/- 267.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=773000, episode_reward=1719.40 +/- 186.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=774000, episode_reward=1633.60 +/- 105.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=775000, episode_reward=1676.60 +/- 79.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=776000, episode_reward=1727.00 +/- 96.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=777000, episode_reward=1855.00 +/- 181.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=778000, episode_reward=1407.00 +/- 300.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=779000, episode_reward=1627.60 +/- 147.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=780000, episode_reward=1635.60 +/- 203.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=781000, episode_reward=1478.00 +/- 164.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=782000, episode_reward=1725.80 +/- 171.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=783000, episode_reward=1840.20 +/- 272.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=784000, episode_reward=1649.80 +/- 223.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=785000, episode_reward=1744.40 +/- 111.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=786000, episode_reward=1748.40 +/- 153.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=787000, episode_reward=1710.60 +/- 220.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=788000, episode_reward=1538.00 +/- 159.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=789000, episode_reward=1636.20 +/- 132.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=790000, episode_reward=1745.80 +/- 201.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=791000, episode_reward=1628.00 +/- 120.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=792000, episode_reward=1863.80 +/- 140.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=793000, episode_reward=1651.00 +/- 93.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=794000, episode_reward=1792.60 +/- 173.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=795000, episode_reward=1604.20 +/- 179.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=796000, episode_reward=1612.80 +/- 281.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=797000, episode_reward=1613.00 +/- 170.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=798000, episode_reward=1574.60 +/- 213.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=799000, episode_reward=1759.60 +/- 197.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=800000, episode_reward=1839.60 +/- 148.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=801000, episode_reward=1671.40 +/- 204.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=802000, episode_reward=1809.80 +/- 236.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=803000, episode_reward=1505.60 +/- 141.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=804000, episode_reward=1587.00 +/- 108.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=805000, episode_reward=1674.20 +/- 141.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=806000, episode_reward=1862.60 +/- 78.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=807000, episode_reward=1688.80 +/- 254.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=808000, episode_reward=1706.00 +/- 139.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=809000, episode_reward=1624.40 +/- 133.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=810000, episode_reward=1707.00 +/- 115.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=811000, episode_reward=1740.40 +/- 163.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=812000, episode_reward=1600.80 +/- 215.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=813000, episode_reward=1651.40 +/- 160.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=814000, episode_reward=1920.40 +/- 98.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=815000, episode_reward=1681.80 +/- 305.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=816000, episode_reward=1657.80 +/- 141.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=817000, episode_reward=1710.00 +/- 96.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=818000, episode_reward=1488.20 +/- 456.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=819000, episode_reward=1635.60 +/- 190.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=820000, episode_reward=1687.60 +/- 250.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=821000, episode_reward=1522.80 +/- 148.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=822000, episode_reward=1564.20 +/- 317.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=823000, episode_reward=1628.20 +/- 165.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=824000, episode_reward=1750.80 +/- 319.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=825000, episode_reward=1493.80 +/- 152.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=826000, episode_reward=1753.00 +/- 270.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=827000, episode_reward=1724.60 +/- 197.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=828000, episode_reward=1514.40 +/- 183.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=829000, episode_reward=1584.60 +/- 279.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=830000, episode_reward=1724.80 +/- 264.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=831000, episode_reward=1708.40 +/- 218.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=832000, episode_reward=1765.80 +/- 140.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=833000, episode_reward=1563.00 +/- 222.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=834000, episode_reward=1613.00 +/- 210.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=835000, episode_reward=1582.60 +/- 318.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=836000, episode_reward=1756.00 +/- 248.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=837000, episode_reward=1700.20 +/- 187.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=838000, episode_reward=1625.40 +/- 260.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=839000, episode_reward=1809.20 +/- 346.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=840000, episode_reward=1785.00 +/- 210.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=841000, episode_reward=1745.40 +/- 194.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=842000, episode_reward=1721.20 +/- 173.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=843000, episode_reward=1612.60 +/- 120.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=844000, episode_reward=1542.20 +/- 235.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=845000, episode_reward=1689.40 +/- 167.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=846000, episode_reward=1541.60 +/- 179.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=847000, episode_reward=1687.20 +/- 298.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=848000, episode_reward=1514.40 +/- 164.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=849000, episode_reward=1812.40 +/- 182.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=850000, episode_reward=1814.20 +/- 149.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=851000, episode_reward=1775.40 +/- 179.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=852000, episode_reward=1682.00 +/- 307.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=853000, episode_reward=1576.80 +/- 271.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=854000, episode_reward=1640.20 +/- 208.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=855000, episode_reward=1572.80 +/- 279.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=856000, episode_reward=1828.20 +/- 167.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=857000, episode_reward=1646.60 +/- 228.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=858000, episode_reward=1747.00 +/- 137.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=859000, episode_reward=1766.00 +/- 244.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=860000, episode_reward=1838.40 +/- 158.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=861000, episode_reward=1649.60 +/- 181.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=862000, episode_reward=1738.80 +/- 231.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=863000, episode_reward=1332.60 +/- 362.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=864000, episode_reward=1602.80 +/- 214.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=865000, episode_reward=1731.80 +/- 97.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=866000, episode_reward=1792.80 +/- 47.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=867000, episode_reward=1697.40 +/- 171.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=868000, episode_reward=1783.00 +/- 267.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=869000, episode_reward=1361.40 +/- 402.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=870000, episode_reward=1732.40 +/- 220.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=871000, episode_reward=1619.80 +/- 141.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=872000, episode_reward=1698.80 +/- 344.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=873000, episode_reward=1642.20 +/- 170.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=874000, episode_reward=1576.00 +/- 171.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=875000, episode_reward=1929.80 +/- 192.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=876000, episode_reward=1524.20 +/- 128.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=877000, episode_reward=1543.40 +/- 199.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=878000, episode_reward=1873.60 +/- 92.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=879000, episode_reward=1567.80 +/- 208.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=880000, episode_reward=1668.40 +/- 309.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=881000, episode_reward=1523.80 +/- 149.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=882000, episode_reward=1649.00 +/- 124.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=883000, episode_reward=1548.20 +/- 71.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=884000, episode_reward=1507.00 +/- 226.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=885000, episode_reward=1550.80 +/- 198.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=886000, episode_reward=1711.80 +/- 302.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=887000, episode_reward=1858.80 +/- 74.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=888000, episode_reward=1525.40 +/- 311.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=889000, episode_reward=1745.80 +/- 202.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=890000, episode_reward=1737.80 +/- 228.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=891000, episode_reward=1635.60 +/- 234.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=892000, episode_reward=1522.40 +/- 267.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=893000, episode_reward=1572.20 +/- 122.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=894000, episode_reward=1783.60 +/- 223.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=895000, episode_reward=1690.80 +/- 315.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=896000, episode_reward=1658.80 +/- 221.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=897000, episode_reward=1578.60 +/- 268.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=898000, episode_reward=1807.60 +/- 214.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=899000, episode_reward=1691.20 +/- 189.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=900000, episode_reward=1675.80 +/- 139.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=901000, episode_reward=1850.20 +/- 236.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=902000, episode_reward=1504.20 +/- 138.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=903000, episode_reward=1578.00 +/- 337.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=904000, episode_reward=1635.80 +/- 268.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=905000, episode_reward=1675.40 +/- 139.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=906000, episode_reward=1561.00 +/- 440.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=907000, episode_reward=1544.60 +/- 78.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=908000, episode_reward=1647.00 +/- 153.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=909000, episode_reward=1655.20 +/- 279.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=910000, episode_reward=1551.00 +/- 325.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=911000, episode_reward=1666.60 +/- 100.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=912000, episode_reward=1835.80 +/- 192.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=913000, episode_reward=1679.80 +/- 165.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=914000, episode_reward=1700.40 +/- 239.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=915000, episode_reward=1794.20 +/- 114.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=916000, episode_reward=1709.20 +/- 282.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=917000, episode_reward=1588.00 +/- 235.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=918000, episode_reward=1637.00 +/- 206.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=919000, episode_reward=1704.40 +/- 182.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=920000, episode_reward=1808.20 +/- 125.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=921000, episode_reward=1717.80 +/- 139.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=922000, episode_reward=1575.60 +/- 274.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=923000, episode_reward=1722.40 +/- 189.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=924000, episode_reward=1451.80 +/- 396.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=925000, episode_reward=1763.80 +/- 196.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=926000, episode_reward=1733.60 +/- 91.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=927000, episode_reward=1693.60 +/- 130.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=928000, episode_reward=1545.20 +/- 215.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=929000, episode_reward=1867.20 +/- 109.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=930000, episode_reward=1487.20 +/- 232.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=931000, episode_reward=1640.60 +/- 106.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=932000, episode_reward=1901.80 +/- 134.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=933000, episode_reward=1637.00 +/- 120.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=934000, episode_reward=1479.40 +/- 368.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=935000, episode_reward=1699.80 +/- 368.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=936000, episode_reward=1590.20 +/- 185.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=937000, episode_reward=1707.40 +/- 62.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=938000, episode_reward=1832.40 +/- 170.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=939000, episode_reward=1631.60 +/- 268.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=940000, episode_reward=1591.20 +/- 149.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=941000, episode_reward=1612.60 +/- 96.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=942000, episode_reward=1845.60 +/- 213.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=943000, episode_reward=1648.00 +/- 141.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=944000, episode_reward=1559.00 +/- 196.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=945000, episode_reward=1690.80 +/- 198.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=946000, episode_reward=1728.20 +/- 101.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=947000, episode_reward=1818.40 +/- 140.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=948000, episode_reward=1731.00 +/- 107.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=949000, episode_reward=1639.80 +/- 281.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=950000, episode_reward=1611.80 +/- 96.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=951000, episode_reward=1668.40 +/- 147.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=952000, episode_reward=1574.40 +/- 123.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=953000, episode_reward=1566.80 +/- 123.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=954000, episode_reward=1637.40 +/- 215.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=955000, episode_reward=1824.20 +/- 125.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=956000, episode_reward=1577.00 +/- 139.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=957000, episode_reward=1656.80 +/- 137.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=958000, episode_reward=1375.80 +/- 283.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=959000, episode_reward=1625.40 +/- 167.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=960000, episode_reward=1633.60 +/- 124.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=961000, episode_reward=1809.20 +/- 24.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=962000, episode_reward=1720.00 +/- 224.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=963000, episode_reward=1665.80 +/- 263.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=964000, episode_reward=1491.80 +/- 309.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=965000, episode_reward=1623.00 +/- 206.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=966000, episode_reward=1765.20 +/- 193.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=967000, episode_reward=1644.20 +/- 370.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=968000, episode_reward=1773.00 +/- 193.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=969000, episode_reward=1725.20 +/- 207.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=970000, episode_reward=1667.40 +/- 272.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=971000, episode_reward=1651.80 +/- 238.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=972000, episode_reward=1563.40 +/- 152.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=973000, episode_reward=1468.60 +/- 419.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=974000, episode_reward=1638.40 +/- 181.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=975000, episode_reward=1733.40 +/- 175.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=976000, episode_reward=1582.20 +/- 224.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=977000, episode_reward=1719.00 +/- 102.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=978000, episode_reward=1711.40 +/- 305.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=979000, episode_reward=1653.80 +/- 183.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=980000, episode_reward=1549.20 +/- 204.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=981000, episode_reward=1686.60 +/- 144.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=982000, episode_reward=1570.40 +/- 278.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=983000, episode_reward=1439.20 +/- 363.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=984000, episode_reward=1782.80 +/- 148.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=985000, episode_reward=1531.60 +/- 82.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=986000, episode_reward=1754.00 +/- 170.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=987000, episode_reward=1543.40 +/- 243.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=988000, episode_reward=1744.00 +/- 258.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=989000, episode_reward=1504.60 +/- 436.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=990000, episode_reward=1639.40 +/- 114.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=991000, episode_reward=1610.00 +/- 310.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=992000, episode_reward=1586.80 +/- 314.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=993000, episode_reward=1678.40 +/- 172.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=994000, episode_reward=1686.40 +/- 244.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=995000, episode_reward=1742.00 +/- 117.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=996000, episode_reward=1533.80 +/- 288.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=997000, episode_reward=1671.80 +/- 179.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=998000, episode_reward=1874.60 +/- 197.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=999000, episode_reward=1512.40 +/- 218.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1000000, episode_reward=1457.80 +/- 259.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1001000, episode_reward=1431.80 +/- 207.77\n",
      "Episode length: 100.00 +/- 0.00\n"
     ]
    }
   ],
   "source": [
    "### REINFORCEMENT LEARNING I ###\n",
    "### TRAIN, SAVE, EVALUATE MODEL - HYPERPARAMETER TUNING, NOISE LEVELS ###\n",
    "\n",
    "import gym\n",
    "import stable_baselines3 as sb\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "import pickle\n",
    "import torch as th\n",
    "\n",
    "total_timesteps = 1e6\n",
    "for gamma in [0.9]: #[0.8, 0.9, 0.99]: # Default PPO = 0.99 / DQN = 0.99 / A2C = 0.99\n",
    "        for learningrate in [0.003]: #[0.003, 0.0003, 0.00003]: # Default PPO = 0.0003 / DQN = 0.0001 / A2C = 0.0007\n",
    "            for actfn in [th.nn.ReLU]: #[th.nn.Tanh, th.nn.ReLU]: # Default PPO = Tanh / DQN = ReLU / A2C = Tanh\n",
    "                # The ``net_arch`` parameter allows to specify the amount and size of the hidden layers and how many\n",
    "                # of them are shared between the policy network and the value network. It is assumed to be a list with the following\n",
    "                # structure:\n",
    "                # 1. An arbitrary length (zero allowed) number of integers each specifying the number of units in a shared layer.\n",
    "                #    If the number of ints is zero, there will be no shared layers.\n",
    "                # 2. An optional dict, to specify the following non-shared layers for the value network and the policy network.\n",
    "                #    It is formatted like ``dict(vf=[<value layer sizes>], pi=[<policy layer sizes>])``.\n",
    "                #    If it is missing any of the keys (pi or vf), no non-shared layers (empty list) is assumed.\n",
    "                # For example to construct a network with one shared layer of size 55 followed by two non-shared layers for the value\n",
    "                # network of size 255 and a single non-shared layer of size 128 for the policy network, the following layers_spec\n",
    "                # would be used: ``[55, dict(vf=[255, 255], pi=[128])]``. A simple shared network topology with two layers of size 128\n",
    "                # would be specified as [128, 128].\n",
    "                for neurons in [8]: #[8, 32, 64]: # Default PPO = net_arch = [dict(vf=[64, 64], pi=[64, 64])] / DQN = net_arch = [64, 64] / A2C = net_arch = [dict(vf=[64, 64], pi=[64, 64])]\n",
    "                    for pn in [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]: #[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]: # Default = 1\n",
    "                        for mn in [0]: #[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]: # Default = 0\n",
    "                            print('Gamma = ' + str(gamma) + ', Learning Rate = ' + str(learningrate) + ', Activation Function = ' + str(actfn.__name__) +\n",
    "                            ', Neurons = ' + str(neurons) + ', Process noise = ' + str(0.1*pn) + ', Measurement noise = ' + str(0.1*mn))\n",
    "                            diag_model = pickle.load(open('diagnostics/model_' + 'pn' + str(pn) + '_mn' + str(mn), 'rb'))\n",
    "                            env = gym.make('Production-v0', diag_model = diag_model, process_noise = 0.1*pn, prod_levels = 5, forecast = 0)\n",
    "                            # Callback for best model\n",
    "                            best_callback = EvalCallback(env, best_model_save_path='./REV1/callback/PPONOISE_fc0_l5_g' + str(gamma) + '_lr' + str(learningrate) + '_act' + str(actfn.__name__) + '_nn' + str(neurons) + '_pn' + str(pn) + '_mn' + str(mn),\n",
    "                                                                        log_path='./REV1/callback/PPONOISE_fc0_l5_g' + str(gamma) + '_lr' + str(learningrate) + '_act' + str(actfn.__name__) + '_nn' + str(neurons) + '_pn' + str(pn) + '_mn' + str(mn),\n",
    "                                                    eval_freq=1000, deterministic=True, render=False)\n",
    "                            model = sb.PPO('MlpPolicy', env, gamma = gamma, learning_rate = learningrate, policy_kwargs = dict(activation_fn=actfn, net_arch=[dict(vf=[neurons, neurons], pi=[neurons, neurons])]), tensorboard_log=\"./REV1/tensorboard/\")\n",
    "                            model.learn(total_timesteps=total_timesteps, tb_log_name='PPONOISE_fc0_l5_g' + str(gamma) + '_lr' + str(learningrate) + '_act' + str(actfn.__name__) + '_nn' + str(neurons) + '_pn' + str(pn) + '_mn' + str(mn),\n",
    "                                        callback = best_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gamma = 0.9, Learning Rate = 0.003, Activation Function = ReLU, Neurons = 8, Process noise = 0.1, Measurement noise = 0.0\n",
      "The average number of reactive maintenance interventions per episode is:  0.91\n",
      "The average number of preventive maintenance interventions per episode is:  6.73\n",
      "The average sum of inventory per episode is:  220.36\n",
      "The average sum of spare parts inventory per episode is:  8.62\n",
      "The average reward per episode is:  1655.18\n",
      "The average upper bound per episode is:  3008.1\n",
      "Gamma = 0.9, Learning Rate = 0.003, Activation Function = ReLU, Neurons = 8, Process noise = 0.1, Measurement noise = 0.0\n",
      "The average number of reactive maintenance interventions per episode is:  0.82\n",
      "The average number of preventive maintenance interventions per episode is:  6.79\n",
      "The average sum of inventory per episode is:  187.98\n",
      "The average sum of spare parts inventory per episode is:  9.92\n",
      "The average reward per episode is:  1640.9\n",
      "The average upper bound per episode is:  2998.4\n",
      "Gamma = 0.9, Learning Rate = 0.003, Activation Function = ReLU, Neurons = 8, Process noise = 0.1, Measurement noise = 0.0\n",
      "The average number of reactive maintenance interventions per episode is:  1.29\n",
      "The average number of preventive maintenance interventions per episode is:  6.18\n",
      "The average sum of inventory per episode is:  185.94\n",
      "The average sum of spare parts inventory per episode is:  8.66\n",
      "The average reward per episode is:  1646.67\n",
      "The average upper bound per episode is:  3016.8\n",
      "Gamma = 0.9, Learning Rate = 0.003, Activation Function = ReLU, Neurons = 8, Process noise = 0.1, Measurement noise = 0.0\n",
      "The average number of reactive maintenance interventions per episode is:  1.3\n",
      "The average number of preventive maintenance interventions per episode is:  6.08\n",
      "The average sum of inventory per episode is:  209.68\n",
      "The average sum of spare parts inventory per episode is:  7.18\n",
      "The average reward per episode is:  1644.28\n",
      "The average upper bound per episode is:  3019.0\n",
      "Gamma = 0.9, Learning Rate = 0.003, Activation Function = ReLU, Neurons = 8, Process noise = 0.1, Measurement noise = 0.0\n",
      "The average number of reactive maintenance interventions per episode is:  1.21\n",
      "The average number of preventive maintenance interventions per episode is:  5.98\n",
      "The average sum of inventory per episode is:  198.38\n",
      "The average sum of spare parts inventory per episode is:  8.93\n",
      "The average reward per episode is:  1633.24\n",
      "The average upper bound per episode is:  2988.4\n",
      "Gamma = 0.9, Learning Rate = 0.003, Activation Function = ReLU, Neurons = 8, Process noise = 0.1, Measurement noise = 0.0\n",
      "The average number of reactive maintenance interventions per episode is:  0.86\n",
      "The average number of preventive maintenance interventions per episode is:  6.72\n",
      "The average sum of inventory per episode is:  183.78\n",
      "The average sum of spare parts inventory per episode is:  9.73\n",
      "The average reward per episode is:  1664.53\n",
      "The average upper bound per episode is:  3014.1\n",
      "Gamma = 0.9, Learning Rate = 0.003, Activation Function = ReLU, Neurons = 8, Process noise = 0.1, Measurement noise = 0.0\n",
      "The average number of reactive maintenance interventions per episode is:  0.87\n",
      "The average number of preventive maintenance interventions per episode is:  6.66\n",
      "The average sum of inventory per episode is:  208.41\n",
      "The average sum of spare parts inventory per episode is:  8.66\n",
      "The average reward per episode is:  1670.96\n",
      "The average upper bound per episode is:  2988.4\n",
      "Gamma = 0.9, Learning Rate = 0.003, Activation Function = ReLU, Neurons = 8, Process noise = 0.1, Measurement noise = 0.0\n",
      "The average number of reactive maintenance interventions per episode is:  1.11\n",
      "The average number of preventive maintenance interventions per episode is:  6.77\n",
      "The average sum of inventory per episode is:  195.18\n",
      "The average sum of spare parts inventory per episode is:  9.96\n",
      "The average reward per episode is:  1611.93\n",
      "The average upper bound per episode is:  3013.7\n",
      "Gamma = 0.9, Learning Rate = 0.003, Activation Function = ReLU, Neurons = 8, Process noise = 0.1, Measurement noise = 0.0\n",
      "The average number of reactive maintenance interventions per episode is:  0.78\n",
      "The average number of preventive maintenance interventions per episode is:  6.84\n",
      "The average sum of inventory per episode is:  223.61\n",
      "The average sum of spare parts inventory per episode is:  8.91\n",
      "The average reward per episode is:  1667.51\n",
      "The average upper bound per episode is:  2968.0\n",
      "Gamma = 0.9, Learning Rate = 0.003, Activation Function = ReLU, Neurons = 8, Process noise = 0.1, Measurement noise = 0.0\n",
      "The average number of reactive maintenance interventions per episode is:  0.96\n",
      "The average number of preventive maintenance interventions per episode is:  6.46\n",
      "The average sum of inventory per episode is:  207.05\n",
      "The average sum of spare parts inventory per episode is:  8.51\n",
      "The average reward per episode is:  1651.86\n",
      "The average upper bound per episode is:  3011.4\n"
     ]
    }
   ],
   "source": [
    "### REINFORCEMENT LEARNING II ###\n",
    "### TRY AND EVALUATE MY MODEL ###\n",
    "import pandas as pd\n",
    "from stable_baselines3 import PPO, DQN\n",
    "import gym\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "import torch as th\n",
    "\n",
    "for ite in [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]:\n",
    "    for gamma in [0.9]: #[0.8, 0.9, 0.99]: # Default = 0.99\n",
    "            for learningrate in [0.003]: #[0.003, 0.0003, 0.00003]: # Default = 3e-4 = 0.0003\n",
    "                for actfn in [th.nn.ReLU]: #[th.nn.Tanh, th.nn.ReLU]: # Default = Tanh\n",
    "                    for neurons in [8]: #[8, 32, 64]: # Default = 64\n",
    "                        for pn in [1]: #[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]: # Default = 1\n",
    "                            for mn in [0]: #[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]: # Default = 0\n",
    "                                print('Gamma = ' + str(gamma) + ', Learning Rate = ' + str(learningrate) + ', Activation Function = ' + str(actfn.__name__) +\n",
    "                                                    ', Neurons = ' + str(neurons) + ', Process noise = ' + str(0.1*pn) + ', Measurement noise = ' + str(0.1*mn))\n",
    "                                diag_model = pickle.load(open('diagnostics/model_' + 'pn' + str(pn) + '_mn' + str(mn), 'rb'))\n",
    "                                env = gym.make('Production-v0', diag_model = diag_model, process_noise = 0.1*pn, prod_levels = 5, forecast = 0)\n",
    "                                model = PPO.load('./REV1/callback/PPOBEST' + str(ite) + '_fc0_l5_g' + str(gamma) + '_lr' + str(learningrate) + '_act' + str(actfn.__name__) + '_nn' + str(neurons) + '_pn' + str(pn) + '_mn' + str(mn) + '/best_model', env = env)\n",
    "                                \n",
    "                                # Set iterations\n",
    "                                iterations = 100\n",
    "                                # Initilaize Reward\n",
    "                                result_df = pd.DataFrame(np.nan, index=range(0,iterations), columns=['RM', 'PM', 'Inventory', 'Spare Parts Inventory', 'Reward', 'Upper'])\n",
    "                                # Calculate reward with no costs and fulfillment of all orders\n",
    "                                \n",
    "                                for i in range(iterations):\n",
    "                                    # Initialize episode\n",
    "                                    store = []\n",
    "                                    obs = env.reset()\n",
    "                                    done = False\n",
    "                                    store.append([0, obs[0], env.breakdown, obs[1], obs[2], 0, done, env.old_order])\n",
    "                                    # Compute one episode\n",
    "                                    while not done:\n",
    "                                        # Get best action for state\n",
    "                                        action, _state = model.predict(obs, deterministic=True)\n",
    "                                        # Compute next state\n",
    "                                        #action = random.choice([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
    "                                        obs, reward, done, info = env.step(action)\n",
    "                                        # Store results of this episode\n",
    "                                        store.append([action, obs[0], env.breakdown, obs[1], obs[2], reward, done, env.old_order])\n",
    "                                    eps_df = pd.DataFrame(store, columns=['action', 'health_rul', 'breakdown', 'inventory', 'sp_inventory', 'reward', 'done', 'old_order'])\n",
    "                                    # Calculate nr. of reactive maintenance interventions by counting health 'resets' and substracting PM actions\n",
    "                                    result_df.iloc[i]['RM'] = sum(eps_df['breakdown']==True)\n",
    "                                    # Calculate nr. of preventive maintenance interventions\n",
    "                                    result_df.iloc[i]['PM'] = sum(eps_df['action']== env.actions-1)\n",
    "                                    # Calculate inventory\n",
    "                                    result_df.iloc[i]['Inventory'] = sum(eps_df['inventory'])\n",
    "                                    # Calculate spare parts inventory per period\n",
    "                                    result_df.iloc[i]['Spare Parts Inventory'] = sum(eps_df['sp_inventory'])\n",
    "                                    # Calculate reward\n",
    "                                    result_df.iloc[i]['Reward'] = sum(eps_df['reward'])\n",
    "                                    # Calculate reward with no costs and fulfillment of all orders\n",
    "                                    result_df.iloc[i]['Upper'] = sum(eps_df['old_order']) * env.order_r\n",
    "\n",
    "                                print(\"The average number of reactive maintenance interventions per episode is: \", result_df['RM'].mean())\n",
    "                                print(\"The average number of preventive maintenance interventions per episode is: \", result_df['PM'].mean())\n",
    "                                print(\"The average sum of inventory per episode is: \", result_df['Inventory'].mean())\n",
    "                                print(\"The average sum of spare parts inventory per episode is: \", result_df['Spare Parts Inventory'].mean())\n",
    "                                print(\"The average reward per episode is: \", result_df['Reward'].mean())\n",
    "                                print(\"The average upper bound per episode is: \", result_df['Upper'].mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x900 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize Exponential Degradation\n",
    "import gym\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import random\n",
    "store = []\n",
    "diag_model = pickle.load(open('diagnostics/model_' + 'pn' + str(0) + '_mn' + str(0), 'rb'))\n",
    "env = gym.make('Production-v0', diag_model = diag_model, prod_levels = 5)\n",
    "obs = env.reset()\n",
    "done = False\n",
    "store.append([1])\n",
    "#while not done:\n",
    "for i in range(100):\n",
    "    # Compute next state\n",
    "    #action = random.choice([0, 1, 2, 3, 4])\n",
    "    obs, reward, done, info = env.step(4)\n",
    "    # Store results of this episode\n",
    "    store.append([env.true_health])\n",
    "eps_df = pd.DataFrame(store, columns=['health'])\n",
    "eps_df.plot(y='health')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x900 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "eps_df.plot(y='health_rul')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average number of reactive maintenance interventions per episode is:  5.92\n",
      "The average number of preventive maintenance interventions per episode is:  0.0\n",
      "The average mean time between failure per episode is:  14.887095238095235\n",
      "The standard deviation of the mean time between failure per episode is:  1.5405320836510183\n",
      "The average sum of inventory per episode is:  114.34\n",
      "The average sum of spare parts inventory per episode is:  0.0\n",
      "The average reward per episode is:  317.37\n",
      "The average upper bound per episode is:  2991.5\n"
     ]
    }
   ],
   "source": [
    "### REINFORCEMENT LEARNING III ###\n",
    "### EVALUATE REACTIVE MODEL ###\n",
    "\n",
    "import pandas as pd\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3 import PPO\n",
    "import numpy as np\n",
    "import math\n",
    "import gym\n",
    "\n",
    "env = gym.make('Production-v0', reactive_mode = True, forecast = 0, prod_levels = 5)\n",
    "# Initialize Reward\n",
    "# Set iterations\n",
    "iterations = 100\n",
    "result_df = pd.DataFrame(np.nan, index=range(0,iterations), columns=['RM', 'PM', 'MTBF', 'Inventory', 'Spare Parts Inventory', 'Reward', 'Upper'])\n",
    "\n",
    "for i in range(iterations):\n",
    "    # Initialize episode\n",
    "    store = []\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    store.append([0, env.true_health, env.breakdown, obs[0], obs[1], 0, done, env.old_order])\n",
    "    # Compute one episode\n",
    "    while not done:\n",
    "        # Get best action for state\n",
    "        action = min(env.prod_levels-1, max(0, round(env.old_order)))\n",
    "        # Compute next state\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        # Store results of this episode\n",
    "        store.append([action, env.true_health, env.breakdown, obs[0], obs[1], reward, done, env.old_order])\n",
    "    eps_df = pd.DataFrame(store, columns=['action', 'health', 'breakdown', 'inventory', 'sp_inventory', 'reward', 'done', 'old_order'])\n",
    "    # Calculate nr. of reactive maintenance interventions by counting health 'resets' and substracting PM actions\n",
    "    result_df.iloc[i]['RM'] = sum(eps_df['breakdown']==True)\n",
    "    # Calculate nr. of preventive maintenance interventions\n",
    "    result_df.iloc[i]['PM'] = sum(eps_df['action']== env.actions-1)\n",
    "    # Calculate mean time between failures\n",
    "    # Cut df after last breakdown\n",
    "    eps_df_trim = eps_df.iloc[:(np.where(eps_df['breakdown'].eq(True), eps_df.index, 0).max()+1)]\n",
    "    # Calculate MTBF by dividing periods where machine is running / breakdowns\n",
    "    result_df.iloc[i]['MTBF'] = (len(eps_df_trim) - sum(eps_df_trim['breakdown'] == True)) / sum(eps_df_trim['breakdown'] == True)\n",
    "    # Calculate inventory\n",
    "    result_df.iloc[i]['Inventory'] = sum(eps_df['inventory'])\n",
    "    # Calculate spare parts inventory per period\n",
    "    result_df.iloc[i]['Spare Parts Inventory'] = sum(eps_df['sp_inventory'])\n",
    "    # Calculate reward\n",
    "    result_df.iloc[i]['Reward'] = sum(eps_df['reward'])\n",
    "    # Calculate reward with no costs and fulfillment of all orders\n",
    "    result_df.iloc[i]['Upper'] = sum(eps_df['old_order']) * env.order_r\n",
    "\n",
    "print(\"The average number of reactive maintenance interventions per episode is: \", result_df['RM'].mean())\n",
    "print(\"The average number of preventive maintenance interventions per episode is: \", result_df['PM'].mean())\n",
    "print(\"The average mean time between failure per episode is: \", result_df['MTBF'].mean())\n",
    "print(\"The standard deviation of the mean time between failure per episode is: \", result_df['MTBF'].std())\n",
    "print(\"The average sum of inventory per episode is: \", result_df['Inventory'].mean())\n",
    "print(\"The average sum of spare parts inventory per episode is: \", result_df['Spare Parts Inventory'].mean())\n",
    "print(\"The average reward per episode is: \", result_df['Reward'].mean())\n",
    "print(\"The average upper bound per episode is: \", result_df['Upper'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Maintenance Interval:  11 Coefficient:  0.55\n",
      "The average number of reactive maintenance interventions per episode is:  1.65\n",
      "The average number of preventive maintenance interventions per episode is:  6.57\n",
      "The average sum of inventory per episode is:  156.97\n",
      "The average sum of spare parts inventory per episode is:  6.63\n",
      "The average reward per episode is:  1221.5\n",
      "The average upper bound per episode is:  3010.1\n"
     ]
    }
   ],
   "source": [
    "### REINFORCEMENT LEARNING IV ###\n",
    "### EVALUATE TIME-BASED PREVENTIVE MODEL ###\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gym\n",
    "import math\n",
    "\n",
    "env = gym.make('Production-v0', forecast = 0, prod_levels = 6)\n",
    "interval = range (11, 12)\n",
    "# Set iterations\n",
    "iterations = 100\n",
    "\n",
    "for k in interval:\n",
    "    # Initilaize Reward\n",
    "    result_df = pd.DataFrame(np.nan, index=range(0,100), columns=['RM', 'PM', 'Inventory', 'Spare Parts Inventory', 'Reward', 'Upper'])\n",
    "\n",
    "    for i in range(iterations):\n",
    "        # Initialize episode\n",
    "        store = []\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        store.append([0, obs[0], env.breakdown, obs[1], obs[2], 0, done, env.old_order])\n",
    "        # Compute one episode\n",
    "        while not done:\n",
    "            # One period before maintenance: action = order + spare part order\n",
    "            if env.scheduled_maintenance_counter == k-1:\n",
    "                action = min(env.prod_levels-1, max(0, round(env.old_order))) + env.prod_levels\n",
    "            # At period of mtbf: maintain\n",
    "            elif env.scheduled_maintenance_counter == k:\n",
    "                action = env.actions-1\n",
    "            # Else: action = order    \n",
    "            else:             \n",
    "                action = min(env.prod_levels-1, max(0, round(env.old_order)))\n",
    "            # Compute next state\n",
    "            obs, reward, done, info = env.step(action)\n",
    "            # Store results of this episode\n",
    "            store.append([action, obs[0], env.breakdown, obs[1], obs[2], reward, done, env.old_order])\n",
    "        eps_df = pd.DataFrame(store, columns=['action', 'health', 'breakdown', 'inventory', 'sp_inventory', 'reward', 'done', 'old_order'])\n",
    "        # Calculate nr. of reactive maintenance interventions by counting health 'resets' and substracting PM actions\n",
    "        result_df.iloc[i]['RM'] = sum(eps_df['breakdown']==True)\n",
    "        # Calculate nr. of preventive maintenance interventions\n",
    "        result_df.iloc[i]['PM'] = sum(eps_df['action']==env.actions-1)\n",
    "        # Calculate inventory\n",
    "        result_df.iloc[i]['Inventory'] = sum(eps_df['inventory'])\n",
    "        # Calculate spare parts inventory per period\n",
    "        result_df.iloc[i]['Spare Parts Inventory'] = sum(eps_df['sp_inventory'])\n",
    "        # Calculate reward\n",
    "        result_df.iloc[i]['Reward'] = sum(eps_df['reward'])\n",
    "        # Calculate reward with no costs and fulfillment of all orders\n",
    "        result_df.iloc[i]['Upper'] = sum(eps_df['old_order']) * env.order_r\n",
    "\n",
    "    print(\"\\n\", \"Maintenance Interval: \", k, \"Coefficient: \", 0+0.05*k)\n",
    "    print(\"The average number of reactive maintenance interventions per episode is: \", result_df['RM'].mean())\n",
    "    print(\"The average number of preventive maintenance interventions per episode is: \", result_df['PM'].mean())\n",
    "    print(\"The average sum of inventory per episode is: \", result_df['Inventory'].mean())\n",
    "    print(\"The average sum of spare parts inventory per episode is: \", result_df['Spare Parts Inventory'].mean())\n",
    "    print(\"The average reward per episode is: \", result_df['Reward'].mean())\n",
    "    print(\"The average upper bound per episode is: \", result_df['Upper'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### REINFORCEMENT LEARNING Va ###\n",
    "### VISUALIZE STATE-ACTION ###\n",
    "import numpy as np\n",
    "state_action = []\n",
    "\n",
    "# Define observation grid\n",
    "grid_health = np.arange(0.0, 1.01, 0.01)\n",
    "#grid_order = range(0, 5)\n",
    "grid_inventory = range(0, 10)\n",
    "grid_sp_inventory = [0, 1]\n",
    "\n",
    "# Loop through grid and store best action for each state\n",
    "for hlt in grid_health:\n",
    "    #for ord in grid_order:\n",
    "    for inv in grid_inventory:\n",
    "        for sin in grid_sp_inventory:\n",
    "            # Predict\n",
    "            #action, _state = model.predict((hlt, ord, inv, sin), deterministic=True)\n",
    "            #state_action.append([hlt, ord, inv, sin, action])\n",
    "            action, _state = model.predict((hlt, inv, sin), deterministic=True)\n",
    "            state_action.append([hlt, inv, sin, action])\n",
    "\n",
    "#state_action_df = pd.DataFrame(state_action, columns=['health', 'order', 'inventory', 'sp_inventory', 'action'])\n",
    "state_action_df = pd.DataFrame(state_action, columns=['health', 'inventory', 'sp_inventory', 'action'])\n",
    "state_action_df.to_excel(\"./Rev1/visuals/state_action.xlsx\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "obs_to_tensor() takes 2 positional arguments but 4 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_17240/2597559145.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m             \u001b[1;31m# Predict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m             \u001b[1;31m#obs, _ = model.policy.obs_to_tensor(hlt, ord, inv, sin)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m             \u001b[0mobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobs_to_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhlt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msin\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m             \u001b[1;31m#state_value.append([hlt, ord, inv, sin, value])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: obs_to_tensor() takes 2 positional arguments but 4 were given"
     ]
    }
   ],
   "source": [
    "### REINFORCEMENT LEARNING Vb ###\n",
    "### VISUALIZE STATE-VALUE ###\n",
    "import numpy as np\n",
    "state_value = []\n",
    "\n",
    "# Define observation grid\n",
    "grid_health = np.arange(0.0, 1.01, 0.01)\n",
    "#grid_order = range(0, 5)\n",
    "grid_inventory = range(0, 10)\n",
    "grid_sp_inventory = [0, 1]\n",
    "\n",
    "# Loop through grid and store best action for each state\n",
    "for hlt in grid_health:\n",
    "    #for ord in grid_order:\n",
    "    for inv in grid_inventory:\n",
    "        for sin in grid_sp_inventory:\n",
    "            # Predict\n",
    "            #obs, _ = model.policy.obs_to_tensor(hlt, ord, inv, sin)\n",
    "            obs, _ = model.policy.obs_to_tensor(hlt, inv, sin)\n",
    "            value = model.policy.predict_values(obs).item()\n",
    "            #state_value.append([hlt, ord, inv, sin, value])\n",
    "            state_value.append([hlt, inv, sin, value])\n",
    "\n",
    "#state_value_df = pd.DataFrame(state_value, columns=['health', 'order', 'inventory', 'sp_inventory', 'value'])\n",
    "state_value_df = pd.DataFrame(state_value, columns=['health', 'inventory', 'sp_inventory', 'value'])\n",
    "state_value_df.to_excel(\"./Rev1/visuals/state_value.xlsx\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps_df.to_excel(\"./Rev1/visuals/eps.xlsx\") "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6f69c5940b32a5cbabe45c9825076a627c6cdb9ede58cf4d0fa74ca6057ffe74"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
