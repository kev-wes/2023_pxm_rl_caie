{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process noise = 0.0, Measurement noise = 0.0\n",
      "Process noise = 0.1, Measurement noise = 0.0\n",
      "Process noise = 0.2, Measurement noise = 0.0\n",
      "Process noise = 0.30000000000000004, Measurement noise = 0.0\n",
      "Process noise = 0.4, Measurement noise = 0.0\n",
      "Process noise = 0.5, Measurement noise = 0.0\n",
      "Process noise = 0.6000000000000001, Measurement noise = 0.0\n",
      "Process noise = 0.7000000000000001, Measurement noise = 0.0\n",
      "Process noise = 0.8, Measurement noise = 0.0\n",
      "Process noise = 0.9, Measurement noise = 0.0\n",
      "Process noise = 1.0, Measurement noise = 0.0\n"
     ]
    }
   ],
   "source": [
    "### DATA-DRIVEN DIAGNOSTICS I ###\n",
    "### GENERATE DATA FOR DATA-DRIVEN MODEL ###\n",
    "\n",
    "import random\n",
    "from prog_models.models import BatteryCircuit\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "# Ignore warnings when machine exceeds its end of life\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\"\"\" Method that uses a physical machine model from the prog_models package and a current (health) state of the model and\n",
    "an action (i.e., intensity), which is performed for 100 time steps\n",
    "    Parameter:\n",
    "        machine             machine model from the prog_models package\n",
    "        state               current (health) state of the model\n",
    "        action              loading of the machine for the next 100 time steps\n",
    "    Return:\n",
    "        health                               \n",
    "    \"\"\"\n",
    "def produce_model(machine, states, action):\n",
    "        \n",
    "        # Define load of battery\n",
    "        def future_loading(t, x=None):\n",
    "            return {'i': action}\n",
    "\n",
    "        # Set current state of machine\n",
    "        machine.parameters['x0'] = states\n",
    "        # Simulate 100 steps\n",
    "        options = {\n",
    "            'save_freq': 100,  # Frequency at which results are saved\n",
    "            'dt': 2  # Timestep\n",
    "        }\n",
    "        (_, _, states, outputs, event_states) = machine.simulate_to(100, future_loading, **options)\n",
    "        health = event_states[-1]['EOD']\n",
    "        return(round(health, 2), states[-1], outputs[-1]['t'], outputs[-1]['v'])\n",
    "def reset_states(machine):\n",
    "    # Returns initial states of machine, e.g., {'tb': 18.95, 'qb': 7856.3254, 'qcp': 0, 'qcs': 0} for Battery\n",
    "    return(machine.default_parameters['x0'])\n",
    "   \n",
    "for pn in [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]: \n",
    "    for mn in [0]:\n",
    "        print('Process noise = ' + str(0.1*pn) + ', Measurement noise = ' + str(0.1*mn))\n",
    "        battery = BatteryCircuit(process_noise = 0.1*pn, measurement_noise = 0.1*mn)\n",
    "        states = reset_states(battery)\n",
    "        reset_counter = 0\n",
    "        dataset = []\n",
    "        for i in range(int(1e4)):\n",
    "            # If asset failed last period, reset all historical values\n",
    "            if reset_counter == 0: t = v = t_1 = v_1 = t_2 = v_2 = t_3 = v_3 = 0 \n",
    "            # Shift history by one time period\n",
    "            v_3 = v_2\n",
    "            t_3 = t_2\n",
    "            v_2 = v_1\n",
    "            t_2 = t_1\n",
    "            v_1 = v\n",
    "            t_1 = t\n",
    "\n",
    "            # Increment reset_counter\n",
    "            reset_counter = reset_counter + 1\n",
    "            # Compute new health, states, t, and v using last battery state and a random new action\n",
    "            health, states, t, v = produce_model(machine=battery, states=states, action=random.sample((0, 1, 2, 3, 4), 1)[0])\n",
    "            \n",
    "            if health <= 0: \n",
    "                # Reset battery states to initialize battery for next produce_model call\n",
    "                states = reset_states(battery)\n",
    "                # Initialize reset_counter\n",
    "                reset_counter = 0\n",
    "                # Sometimes produce_model returns weird or negative values as the end of life is exceeded\n",
    "                # Here, we just simply set it to zero to not confuse a later learner \n",
    "                health = 0\n",
    "\n",
    "            # append to two-dimensional list\n",
    "            dataset.append([t, v, t_1, v_1, t_2, v_2, t_3, v_3, health])\n",
    "\n",
    "        # Transform two-dim list to dataframe\n",
    "        dataset = pd.DataFrame(dataset, columns=['t', 'v', 't_1', 'v_1', 't_2', 'v_2', 't_3', 'v_3', 'health'])\n",
    "        # Denote machine runs to-failure with incrementing id\n",
    "        k = 0\n",
    "        for i in range(dataset.shape[0]):\n",
    "            if (dataset.iloc[i]['t_1'] == 0 and dataset.iloc[i]['v_1'] == 0):\n",
    "                j = 1\n",
    "                k = k + 1\n",
    "            dataset.loc[i, 'time'] = j\n",
    "            j = j + 1\n",
    "            dataset.loc[i, 'ID'] = k\n",
    "        dataset = dataset.sort_values(['ID', 'time'], ascending=[True, False])\n",
    "        # Assign RUL by counting upwards per serial number in the descended data frame\n",
    "        dataset['RUL'] = dataset.groupby((dataset['ID'] != dataset['ID'].shift(1)).cumsum()).cumcount()\n",
    "        dataset = dataset.sort_values(['ID', 'time'], ascending=True)\n",
    "\n",
    "        # Save it as pickle\n",
    "        dataset.to_pickle('diagnostics/data_' + 'pn' + str(pn) + '_mn' + str(mn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model based on RUL (prognostics = True) or health (diagnostics, prognostics = False)\n",
    "prognostics = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process noise = 0.0, Measurement noise = 0.0\n",
      "R2: [0.98346924 0.97869884 0.98271119 0.98084096 0.98029931]\n",
      "mae: [-0.0186088  -0.02097155 -0.01949765 -0.0201391  -0.0208858 ]\n",
      "mape: [-7.86658635e-02 -3.23583633e+11 -9.00719925e+09 -9.20286148e-02\n",
      " -1.71587146e+11]\n",
      "rmse: [-0.03669848 -0.04135376 -0.03768967 -0.03960828 -0.04014499]\n",
      "Process noise = 0.1, Measurement noise = 0.0\n",
      "R2: [0.94511725 0.94338909 0.94308702 0.95269309 0.92781427]\n",
      "mae: [-0.04355035 -0.0430732  -0.0434886  -0.0409953  -0.04892035]\n",
      "mape: [-1.57625987e+09 -6.75539944e+09 -2.31935381e+10 -3.03992975e+10\n",
      " -3.69295169e+11]\n",
      "rmse: [-0.06585241 -0.06812947 -0.06765209 -0.06181947 -0.07649472]\n",
      "Process noise = 0.2, Measurement noise = 0.0\n",
      "R2: [0.89160295 0.90719555 0.90106071 0.89986382 0.8983238 ]\n",
      "mae: [-0.06726765 -0.0624331  -0.0637716  -0.0658622  -0.0643069 ]\n",
      "mape: [-3.90912448e+11 -3.67493730e+11 -1.03605309e+12 -7.60657977e+11\n",
      " -1.41413028e+11]\n",
      "rmse: [-0.09406841 -0.08643758 -0.08901846 -0.09017457 -0.0908437 ]\n",
      "Process noise = 0.30000000000000004, Measurement noise = 0.0\n",
      "R2: [0.87037827 0.84902257 0.84585148 0.85446821 0.85283189]\n",
      "mae: [-0.07610885 -0.0816426  -0.0808236  -0.0798498  -0.0796984 ]\n",
      "mape: [-8.21906932e+10 -2.44320280e+11 -3.61413870e+11 -2.58281439e+11\n",
      " -2.56705179e+10]\n",
      "rmse: [-0.10279177 -0.11007041 -0.11067439 -0.10907084 -0.10878756]\n",
      "Process noise = 0.4, Measurement noise = 0.0\n",
      "R2: [0.83456832 0.83012227 0.80835482 0.83903277 0.81974217]\n",
      "mae: [-0.0846392  -0.0875222  -0.09059015 -0.08526315 -0.090239  ]\n",
      "mape: [-1.02456892e+11 -4.00820367e+11 -2.39816680e+11 -2.95661316e+11\n",
      " -6.73288144e+10]\n",
      "rmse: [-0.11565337 -0.11667704 -0.12263368 -0.11445298 -0.12049927]\n",
      "Process noise = 0.5, Measurement noise = 0.0\n",
      "R2: [0.82839126 0.80480565 0.82500873 0.8179684  0.817267  ]\n",
      "mae: [-0.09095045 -0.096345   -0.0892272  -0.09109585 -0.09089715]\n",
      "mape: [-4.76480841e+11 -2.50850499e+11 -2.85978576e+11 -7.34086739e+10\n",
      " -4.02171447e+11]\n",
      "rmse: [-0.11899326 -0.12706747 -0.11847659 -0.12179301 -0.12175318]\n",
      "Process noise = 0.6000000000000001, Measurement noise = 0.0\n",
      "R2: [0.78807567 0.79536683 0.79253112 0.78549367 0.7681572 ]\n",
      "mae: [-0.09885245 -0.09946715 -0.09709435 -0.1021844  -0.1052449 ]\n",
      "mape: [-3.47452711e+11 -4.54638382e+11 -8.87209127e+11 -2.08516663e+11\n",
      " -3.28762773e+11]\n",
      "rmse: [-0.13075345 -0.13061024 -0.12871161 -0.13304715 -0.13793793]\n",
      "Process noise = 0.7000000000000001, Measurement noise = 0.0\n",
      "R2: [0.77073703 0.76979583 0.75657293 0.74444239 0.7908058 ]\n",
      "mae: [-0.1031984  -0.1025792  -0.10822995 -0.11202985 -0.09902865]\n",
      "mape: [-2.76746197e+11 -4.12529726e+11 -1.64156206e+11 -5.88395291e+11\n",
      " -3.12099454e+11]\n",
      "rmse: [-0.13438963 -0.1351357  -0.14146253 -0.14412065 -0.1310333 ]\n",
      "Process noise = 0.8, Measurement noise = 0.0\n",
      "R2: [0.75441041 0.73836646 0.72956504 0.7436836  0.74272255]\n",
      "mae: [-0.1075912  -0.1102882  -0.11299515 -0.10926115 -0.1102889 ]\n",
      "mape: [-3.29888673e+11 -1.32315757e+12 -3.39571412e+11 -9.10177485e+11\n",
      " -5.11158558e+11]\n",
      "rmse: [-0.14059851 -0.14411492 -0.14701622 -0.1434903  -0.14296051]\n",
      "Process noise = 0.9, Measurement noise = 0.0\n",
      "R2: [0.7459057  0.72225387 0.71386912 0.73079549 0.71229312]\n",
      "mae: [-0.1088923  -0.115251   -0.11461225 -0.1123348  -0.1146464 ]\n",
      "mape: [-2.86203756e+11 -1.43439648e+11 -3.62990130e+11 -8.26410532e+10\n",
      " -6.39285967e+11]\n",
      "rmse: [-0.14232725 -0.14946078 -0.14982241 -0.14679103 -0.14974282]\n",
      "Process noise = 1.0, Measurement noise = 0.0\n",
      "R2: [0.70404433 0.7015158  0.69136757 0.70469365 0.71885872]\n",
      "mae: [-0.1154981  -0.1165566  -0.11575255 -0.11541465 -0.11638255]\n",
      "mape: [-1.49744688e+11 -5.21967197e+11 -5.96051411e+11 -2.15925084e+12\n",
      " -3.37769972e+11]\n",
      "rmse: [-0.15095345 -0.15119491 -0.15424514 -0.15225114 -0.15056938]\n"
     ]
    }
   ],
   "source": [
    "### DATA-DRIVEN DIAGNOSTICS II ###\n",
    "### FIT AND TEST MODEL ###\n",
    "from sklearn import tree, linear_model, kernel_ridge, svm, neighbors, gaussian_process, ensemble, neural_network\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_validate\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "score_data = []\n",
    "for pn in [0, 1, 2, 3, 4, 5, 6, 7 , 8, 9, 10]: \n",
    "    for mn in [0]:\n",
    "        print('Process noise = ' + str(0.1*pn) + ', Measurement noise = ' + str(0.1*mn))\n",
    "        dataset = pd.read_pickle('diagnostics/data_' + 'pn' + str(pn) + '_mn' + str(mn))\n",
    "        X = dataset[['t', 'v', 't_1', 'v_1', 't_2', 'v_2', 't_3', 'v_3']]\n",
    "        if prognostics:\n",
    "            y = dataset['RUL']\n",
    "        else:\n",
    "            y = dataset['health']\n",
    "        learner = ensemble.RandomForestRegressor()\n",
    "        scoring = {'r2': 'r2',\n",
    "                    'mae': 'neg_mean_absolute_error',\n",
    "                    'mape': 'neg_mean_absolute_percentage_error',\n",
    "                    'rmse': 'neg_root_mean_squared_error'}\n",
    "        scores = cross_validate(learner, X, y, scoring=scoring, cv=5) # default scoring R2\n",
    "        print(\"R2: \" + str(scores['test_r2']))\n",
    "        print(\"mae: \" + str(scores['test_mae']))\n",
    "        print(\"mape: \" + str(scores['test_mape']))\n",
    "        print(\"rmse: \" + str(scores['test_rmse']))\n",
    "        # Store measures\n",
    "        score_data.append(['model_' + 'pn' + str(pn) + '_mn' + str(mn), np.mean(scores['test_r2']),\n",
    "                            np.mean(scores['test_mae']), np.mean(scores['test_mape']), np.mean(scores['test_rmse'])])\n",
    "\n",
    "        # Fit on all data\n",
    "        model = ensemble.RandomForestRegressor().fit(X, y)\n",
    "        if prognostics:\n",
    "            pickle.dump(model, open('prognostics/model_' + 'pn' + str(pn) + '_mn' + str(mn), 'wb'))\n",
    "        else:\n",
    "            pickle.dump(model, open('diagnostics/model_' + 'pn' + str(pn) + '_mn' + str(mn), 'wb'))\n",
    "# Transform two-dim list to dataframe\n",
    "score_df = pd.DataFrame(score_data, columns=['model', 'r2', 'mae', 'mape', 'rmse'])\n",
    "if prognostics:\n",
    "    score_df.to_excel(\"prognostics/scores.xlsx\") \n",
    "else:\n",
    "    score_df.to_excel(\"diagnostics/scores.xlsx\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process noise = 0.0, Measurement noise = 0.0\n",
      "Process noise = 0.1, Measurement noise = 0.0\n",
      "Process noise = 0.2, Measurement noise = 0.0\n",
      "Process noise = 0.30000000000000004, Measurement noise = 0.0\n",
      "Process noise = 0.4, Measurement noise = 0.0\n",
      "Process noise = 0.5, Measurement noise = 0.0\n",
      "Process noise = 0.6000000000000001, Measurement noise = 0.0\n",
      "Process noise = 0.7000000000000001, Measurement noise = 0.0\n",
      "Process noise = 0.8, Measurement noise = 0.0\n",
      "Process noise = 0.9, Measurement noise = 0.0\n",
      "Process noise = 1.0, Measurement noise = 0.0\n"
     ]
    }
   ],
   "source": [
    "### DATA-DRIVEN DIAGNOSTICS IIIa ###\n",
    "### FIT, TEST, VISUALIZE MODEL USING TRAIN AND TEST SETS ###\n",
    "\n",
    "for pn in [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]: \n",
    "    for mn in [0]:\n",
    "        print('Process noise = ' + str(0+0.1*pn) + ', Measurement noise = ' + str(0+0.1*mn))\n",
    "        if prognostics:\n",
    "            dataset = pd.read_pickle('diagnostics/data_' + 'pn' + str(pn) + '_mn' + str(mn))\n",
    "            y = dataset['RUL']\n",
    "        else:\n",
    "            dataset = pd.read_pickle('prognostics/data_' + 'pn' + str(pn) + '_mn' + str(mn))\n",
    "            y = dataset['health']\n",
    "        X = dataset[['t', 'v', 't_1', 'v_1', 't_2', 'v_2', 't_3', 'v_3']]\n",
    "        # Find index of healthy machines\n",
    "        index_df = X.index[(X['t_1'] == 0) & (X['v_1'] == 0) & (X['t_2'] == 0) & (X['v_2'] == 0) & (X['t_3'] == 0) & (X['v_3'] == 0)].tolist()\n",
    "        index_test = round(len(index_df)*0.8)\n",
    "\n",
    "        # Create train and test set without disrupting machine runs to-failure\n",
    "        X_train = X.iloc[0:(index_df[index_test])]\n",
    "        y_train = y.iloc[0:(index_df[index_test])]\n",
    "        X_test = X.iloc[index_df[index_test]:(len(X))]\n",
    "        y_test = y.iloc[index_df[index_test]:(len(y))]\n",
    "\n",
    "        ## Train\n",
    "        model = ensemble.RandomForestRegressor().fit(X_train, y_train)\n",
    "        ## Predict\n",
    "        y_pred = pd.DataFrame(model.predict(X_test), columns=['Pred'])\n",
    "        ## Analyze\n",
    "        #reset index of each DataFrame\n",
    "        X_test.reset_index(drop=True, inplace=True)\n",
    "        y_test.reset_index(drop=True, inplace=True)\n",
    "        # Concat dataframes\n",
    "        test_df = pd.concat([X_test, y_test, y_pred], axis=1)\n",
    "        # Print for visualization (e.g., in R)\n",
    "        if prognostics:\n",
    "            test_df.to_excel(\"prognostics/test_results_\" + 'pn' + str(pn) + '_mn' + str(mn) + \".xlsx\") \n",
    "        else:\n",
    "            test_df.to_excel(\"diagnostics/test_results_\" + 'pn' + str(pn) + '_mn' + str(mn) + \".xlsx\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gamma = 0.8, Learning Rate = 0.003, Activation Function = Tanh, Neurons = 8, Process noise = 0.1, Measurement noise = 0.0\n",
      "Eval num_timesteps=1000, episode_reward=-18955.00 +/- 88.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=-18961.00 +/- 90.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=3000, episode_reward=161.60 +/- 224.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4000, episode_reward=-94.00 +/- 252.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=5000, episode_reward=230.20 +/- 145.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=6000, episode_reward=166.60 +/- 189.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=7000, episode_reward=357.00 +/- 166.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=8000, episode_reward=326.20 +/- 83.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=9000, episode_reward=174.60 +/- 152.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=10000, episode_reward=223.60 +/- 184.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=11000, episode_reward=257.20 +/- 233.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=12000, episode_reward=301.40 +/- 224.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=13000, episode_reward=392.60 +/- 192.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=14000, episode_reward=436.80 +/- 231.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=15000, episode_reward=548.80 +/- 294.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=16000, episode_reward=550.40 +/- 296.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=17000, episode_reward=711.40 +/- 362.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=18000, episode_reward=845.60 +/- 298.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=19000, episode_reward=801.40 +/- 177.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=20000, episode_reward=910.20 +/- 214.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=21000, episode_reward=936.80 +/- 157.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=22000, episode_reward=1044.80 +/- 250.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=23000, episode_reward=1035.40 +/- 311.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=24000, episode_reward=1175.00 +/- 248.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=25000, episode_reward=1219.80 +/- 268.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=26000, episode_reward=997.80 +/- 155.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=27000, episode_reward=-2237.80 +/- 7079.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=28000, episode_reward=990.20 +/- 285.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=29000, episode_reward=1249.60 +/- 161.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=30000, episode_reward=1150.20 +/- 81.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=31000, episode_reward=1191.20 +/- 242.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=32000, episode_reward=1302.40 +/- 224.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=33000, episode_reward=-2408.60 +/- 6969.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=34000, episode_reward=1359.60 +/- 223.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=35000, episode_reward=1208.20 +/- 306.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=36000, episode_reward=1064.00 +/- 223.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=37000, episode_reward=1144.60 +/- 217.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=38000, episode_reward=1276.20 +/- 186.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=39000, episode_reward=-2001.00 +/- 6405.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=40000, episode_reward=1360.40 +/- 140.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=41000, episode_reward=915.40 +/- 163.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=42000, episode_reward=1106.80 +/- 321.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=43000, episode_reward=333.40 +/- 2148.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=44000, episode_reward=1394.60 +/- 281.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=45000, episode_reward=-214.00 +/- 2793.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=46000, episode_reward=-1979.00 +/- 6847.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=47000, episode_reward=1349.80 +/- 121.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=48000, episode_reward=1633.40 +/- 135.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=49000, episode_reward=1650.40 +/- 145.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=50000, episode_reward=1545.80 +/- 198.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=51000, episode_reward=1375.00 +/- 309.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=52000, episode_reward=1397.80 +/- 298.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=53000, episode_reward=1452.20 +/- 316.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=54000, episode_reward=1502.00 +/- 227.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=55000, episode_reward=1625.20 +/- 195.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=56000, episode_reward=1475.60 +/- 168.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=57000, episode_reward=1530.00 +/- 172.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=58000, episode_reward=1670.80 +/- 196.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=59000, episode_reward=1408.60 +/- 278.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=60000, episode_reward=1763.20 +/- 126.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=61000, episode_reward=1471.20 +/- 192.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=62000, episode_reward=1667.20 +/- 340.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=63000, episode_reward=1499.20 +/- 106.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=64000, episode_reward=1485.80 +/- 24.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=65000, episode_reward=1362.40 +/- 135.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=66000, episode_reward=1751.80 +/- 249.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=67000, episode_reward=1622.40 +/- 168.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=68000, episode_reward=1630.60 +/- 211.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=69000, episode_reward=1606.80 +/- 176.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=70000, episode_reward=1814.00 +/- 186.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=71000, episode_reward=1585.20 +/- 312.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=72000, episode_reward=1689.40 +/- 163.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=73000, episode_reward=1563.00 +/- 208.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=74000, episode_reward=1412.40 +/- 138.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=75000, episode_reward=1655.40 +/- 231.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=76000, episode_reward=1619.00 +/- 91.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=77000, episode_reward=1411.60 +/- 223.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=78000, episode_reward=1590.80 +/- 183.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=79000, episode_reward=1670.60 +/- 49.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=80000, episode_reward=1572.40 +/- 262.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=81000, episode_reward=1731.00 +/- 186.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=82000, episode_reward=1594.80 +/- 136.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=83000, episode_reward=1639.80 +/- 160.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=84000, episode_reward=1855.20 +/- 143.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=85000, episode_reward=1473.40 +/- 302.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=86000, episode_reward=1624.80 +/- 190.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=87000, episode_reward=1781.20 +/- 189.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=88000, episode_reward=1693.80 +/- 174.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=89000, episode_reward=1702.20 +/- 245.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=90000, episode_reward=1626.80 +/- 212.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=91000, episode_reward=1717.20 +/- 175.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=92000, episode_reward=1455.80 +/- 288.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=93000, episode_reward=1563.80 +/- 167.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=94000, episode_reward=1467.80 +/- 113.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=95000, episode_reward=1591.00 +/- 201.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=96000, episode_reward=1591.20 +/- 205.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=97000, episode_reward=1696.40 +/- 154.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=98000, episode_reward=1699.20 +/- 230.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=99000, episode_reward=1708.20 +/- 85.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=100000, episode_reward=1502.80 +/- 168.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=101000, episode_reward=1523.40 +/- 216.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=102000, episode_reward=1705.00 +/- 183.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=103000, episode_reward=1628.00 +/- 146.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=104000, episode_reward=1625.80 +/- 178.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=105000, episode_reward=1485.80 +/- 203.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=106000, episode_reward=1647.80 +/- 281.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=107000, episode_reward=1614.20 +/- 115.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=108000, episode_reward=1450.80 +/- 246.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=109000, episode_reward=1634.40 +/- 210.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=110000, episode_reward=1493.60 +/- 206.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=111000, episode_reward=1685.20 +/- 208.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=112000, episode_reward=1749.40 +/- 119.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=113000, episode_reward=1497.60 +/- 299.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=114000, episode_reward=1435.40 +/- 210.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=115000, episode_reward=1532.60 +/- 272.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=116000, episode_reward=1521.20 +/- 135.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=117000, episode_reward=1693.20 +/- 187.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=118000, episode_reward=1621.20 +/- 134.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=119000, episode_reward=1456.40 +/- 143.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=120000, episode_reward=1766.20 +/- 180.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=121000, episode_reward=1624.80 +/- 193.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=122000, episode_reward=1531.20 +/- 248.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=123000, episode_reward=1507.60 +/- 248.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=124000, episode_reward=1664.80 +/- 257.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=125000, episode_reward=1530.80 +/- 129.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=126000, episode_reward=1483.20 +/- 125.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=127000, episode_reward=1581.60 +/- 168.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=128000, episode_reward=1669.60 +/- 59.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=129000, episode_reward=1729.60 +/- 181.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=130000, episode_reward=1484.80 +/- 341.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=131000, episode_reward=1730.40 +/- 239.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=132000, episode_reward=1654.60 +/- 166.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=133000, episode_reward=1520.20 +/- 290.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=134000, episode_reward=1583.60 +/- 155.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=135000, episode_reward=1483.40 +/- 120.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=136000, episode_reward=1755.60 +/- 201.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=137000, episode_reward=1633.40 +/- 167.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=138000, episode_reward=1563.00 +/- 304.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=139000, episode_reward=1635.20 +/- 246.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=140000, episode_reward=1725.80 +/- 91.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=141000, episode_reward=1592.00 +/- 176.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=142000, episode_reward=1690.80 +/- 249.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=143000, episode_reward=1500.00 +/- 95.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=144000, episode_reward=1578.00 +/- 195.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=145000, episode_reward=1708.80 +/- 188.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=146000, episode_reward=1719.40 +/- 243.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=147000, episode_reward=1853.80 +/- 98.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=148000, episode_reward=1584.00 +/- 154.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=149000, episode_reward=1523.20 +/- 197.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=150000, episode_reward=1736.40 +/- 209.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=151000, episode_reward=1389.60 +/- 407.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=152000, episode_reward=1614.60 +/- 237.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=153000, episode_reward=1706.40 +/- 186.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=154000, episode_reward=1750.40 +/- 126.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=155000, episode_reward=1767.80 +/- 158.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=156000, episode_reward=1541.00 +/- 195.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=157000, episode_reward=1628.00 +/- 252.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=158000, episode_reward=1624.20 +/- 187.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=159000, episode_reward=1518.80 +/- 188.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=160000, episode_reward=1647.00 +/- 211.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=161000, episode_reward=1530.80 +/- 318.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=162000, episode_reward=1248.60 +/- 264.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=163000, episode_reward=1297.20 +/- 355.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=164000, episode_reward=1617.20 +/- 221.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=165000, episode_reward=1624.80 +/- 157.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=166000, episode_reward=1507.80 +/- 298.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=167000, episode_reward=1600.80 +/- 225.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=168000, episode_reward=1862.00 +/- 174.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=169000, episode_reward=1660.80 +/- 228.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=170000, episode_reward=1502.40 +/- 130.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=171000, episode_reward=1498.00 +/- 196.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=172000, episode_reward=1606.40 +/- 250.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=173000, episode_reward=1494.40 +/- 294.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=174000, episode_reward=1525.20 +/- 259.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=175000, episode_reward=1511.40 +/- 356.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=176000, episode_reward=1521.00 +/- 213.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=177000, episode_reward=1500.20 +/- 210.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=178000, episode_reward=1545.80 +/- 228.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=179000, episode_reward=1421.60 +/- 257.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=180000, episode_reward=1604.40 +/- 235.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=181000, episode_reward=1763.00 +/- 268.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=182000, episode_reward=1544.20 +/- 198.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=183000, episode_reward=1573.20 +/- 202.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=184000, episode_reward=1516.60 +/- 256.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=185000, episode_reward=1707.00 +/- 275.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=186000, episode_reward=1346.60 +/- 265.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=187000, episode_reward=1690.00 +/- 202.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=188000, episode_reward=1727.60 +/- 168.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=189000, episode_reward=1503.80 +/- 358.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=190000, episode_reward=1615.80 +/- 208.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=191000, episode_reward=1579.60 +/- 192.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=192000, episode_reward=1559.40 +/- 298.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=193000, episode_reward=1555.40 +/- 98.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=194000, episode_reward=1728.80 +/- 198.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=195000, episode_reward=1585.80 +/- 154.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=196000, episode_reward=1369.00 +/- 339.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=197000, episode_reward=1540.60 +/- 392.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=198000, episode_reward=1846.60 +/- 106.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=199000, episode_reward=1421.20 +/- 218.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=200000, episode_reward=1710.20 +/- 127.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=201000, episode_reward=1539.20 +/- 232.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=202000, episode_reward=1589.60 +/- 135.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=203000, episode_reward=1532.20 +/- 266.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=204000, episode_reward=1652.40 +/- 211.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=205000, episode_reward=1564.40 +/- 405.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=206000, episode_reward=1609.00 +/- 249.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=207000, episode_reward=1752.00 +/- 93.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=208000, episode_reward=1502.60 +/- 270.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=209000, episode_reward=1551.60 +/- 236.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=210000, episode_reward=1746.40 +/- 208.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=211000, episode_reward=1429.00 +/- 133.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=212000, episode_reward=1622.40 +/- 308.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=213000, episode_reward=1546.80 +/- 201.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=214000, episode_reward=1584.60 +/- 273.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=215000, episode_reward=1503.60 +/- 384.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=216000, episode_reward=1509.60 +/- 178.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=217000, episode_reward=1657.20 +/- 196.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=218000, episode_reward=1621.60 +/- 143.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=219000, episode_reward=1664.60 +/- 240.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=220000, episode_reward=1683.40 +/- 224.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=221000, episode_reward=1512.80 +/- 444.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=222000, episode_reward=1714.40 +/- 157.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=223000, episode_reward=1707.60 +/- 250.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=224000, episode_reward=1349.80 +/- 302.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=225000, episode_reward=1757.00 +/- 95.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=226000, episode_reward=1685.40 +/- 142.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=227000, episode_reward=1757.20 +/- 200.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=228000, episode_reward=1684.20 +/- 212.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=229000, episode_reward=1574.00 +/- 200.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=230000, episode_reward=1606.40 +/- 143.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=231000, episode_reward=1737.80 +/- 145.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=232000, episode_reward=1647.00 +/- 129.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=233000, episode_reward=1671.00 +/- 279.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=234000, episode_reward=1664.60 +/- 198.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=235000, episode_reward=1392.80 +/- 406.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=236000, episode_reward=1685.40 +/- 202.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=237000, episode_reward=1787.60 +/- 261.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=238000, episode_reward=1622.80 +/- 208.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=239000, episode_reward=1733.20 +/- 144.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=240000, episode_reward=1552.00 +/- 108.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=241000, episode_reward=1758.20 +/- 136.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=242000, episode_reward=1694.40 +/- 93.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=243000, episode_reward=1714.00 +/- 256.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=244000, episode_reward=1786.20 +/- 164.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=245000, episode_reward=1533.80 +/- 151.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=246000, episode_reward=1635.00 +/- 89.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=247000, episode_reward=1602.80 +/- 326.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=248000, episode_reward=1502.40 +/- 177.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=249000, episode_reward=1453.80 +/- 167.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=250000, episode_reward=1633.20 +/- 212.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=251000, episode_reward=1667.20 +/- 105.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=252000, episode_reward=1593.00 +/- 208.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=253000, episode_reward=1522.40 +/- 235.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=254000, episode_reward=1653.60 +/- 192.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=255000, episode_reward=1611.80 +/- 168.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=256000, episode_reward=1481.40 +/- 153.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=257000, episode_reward=1727.60 +/- 164.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=258000, episode_reward=1624.80 +/- 349.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=259000, episode_reward=1719.40 +/- 260.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=260000, episode_reward=1373.40 +/- 181.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=261000, episode_reward=1673.60 +/- 176.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=262000, episode_reward=1563.20 +/- 258.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=263000, episode_reward=1625.20 +/- 163.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=264000, episode_reward=1645.80 +/- 208.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=265000, episode_reward=1508.00 +/- 115.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=266000, episode_reward=1684.20 +/- 337.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=267000, episode_reward=1661.60 +/- 297.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=268000, episode_reward=1462.00 +/- 283.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=269000, episode_reward=1766.60 +/- 153.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=270000, episode_reward=1536.60 +/- 302.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=271000, episode_reward=1621.00 +/- 229.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=272000, episode_reward=1638.80 +/- 302.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=273000, episode_reward=1696.20 +/- 350.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=274000, episode_reward=1579.80 +/- 188.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=275000, episode_reward=1611.00 +/- 160.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=276000, episode_reward=1686.60 +/- 164.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=277000, episode_reward=1624.40 +/- 181.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=278000, episode_reward=1620.20 +/- 269.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=279000, episode_reward=1530.00 +/- 313.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=280000, episode_reward=1671.80 +/- 228.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=281000, episode_reward=1690.60 +/- 144.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=282000, episode_reward=1542.00 +/- 275.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=283000, episode_reward=1735.00 +/- 154.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=284000, episode_reward=1353.00 +/- 162.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=285000, episode_reward=1530.00 +/- 49.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=286000, episode_reward=1300.80 +/- 267.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=287000, episode_reward=1522.40 +/- 159.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=288000, episode_reward=1697.40 +/- 86.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=289000, episode_reward=1660.60 +/- 275.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=290000, episode_reward=1356.20 +/- 223.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=291000, episode_reward=1683.20 +/- 228.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=292000, episode_reward=1442.40 +/- 67.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=293000, episode_reward=1533.60 +/- 248.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=294000, episode_reward=1728.40 +/- 184.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=295000, episode_reward=1625.00 +/- 326.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=296000, episode_reward=1716.20 +/- 206.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=297000, episode_reward=1443.20 +/- 278.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=298000, episode_reward=1618.60 +/- 182.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=299000, episode_reward=1731.80 +/- 211.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=300000, episode_reward=1604.80 +/- 288.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=301000, episode_reward=1790.00 +/- 127.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=302000, episode_reward=1708.60 +/- 267.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=303000, episode_reward=1564.60 +/- 399.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=304000, episode_reward=1648.00 +/- 292.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=305000, episode_reward=1614.20 +/- 123.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=306000, episode_reward=1394.60 +/- 263.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=307000, episode_reward=1688.00 +/- 202.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=308000, episode_reward=1723.20 +/- 187.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=309000, episode_reward=1713.80 +/- 292.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=310000, episode_reward=1716.00 +/- 185.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=311000, episode_reward=1584.80 +/- 347.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=312000, episode_reward=1407.00 +/- 347.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=313000, episode_reward=1561.20 +/- 182.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=314000, episode_reward=1608.40 +/- 182.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=315000, episode_reward=1354.40 +/- 299.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=316000, episode_reward=1577.40 +/- 95.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=317000, episode_reward=1727.80 +/- 142.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=318000, episode_reward=1686.80 +/- 193.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=319000, episode_reward=1713.20 +/- 146.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=320000, episode_reward=1635.20 +/- 200.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=321000, episode_reward=1553.80 +/- 218.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=322000, episode_reward=1683.40 +/- 179.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=323000, episode_reward=1722.00 +/- 155.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=324000, episode_reward=1431.20 +/- 256.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=325000, episode_reward=1654.60 +/- 181.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=326000, episode_reward=1585.00 +/- 199.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=327000, episode_reward=1687.20 +/- 230.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=328000, episode_reward=1644.40 +/- 400.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=329000, episode_reward=1647.80 +/- 210.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=330000, episode_reward=1600.60 +/- 169.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=331000, episode_reward=1729.00 +/- 353.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=332000, episode_reward=1670.20 +/- 103.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=333000, episode_reward=1839.60 +/- 132.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=334000, episode_reward=1688.00 +/- 80.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=335000, episode_reward=1569.00 +/- 177.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=336000, episode_reward=1687.80 +/- 194.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=337000, episode_reward=1607.20 +/- 232.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=338000, episode_reward=1636.60 +/- 212.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=339000, episode_reward=1576.00 +/- 304.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=340000, episode_reward=1634.00 +/- 271.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=341000, episode_reward=1758.80 +/- 124.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=342000, episode_reward=1828.80 +/- 195.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=343000, episode_reward=1707.20 +/- 166.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=344000, episode_reward=1633.00 +/- 109.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=345000, episode_reward=1702.40 +/- 159.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=346000, episode_reward=1624.20 +/- 227.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=347000, episode_reward=1678.20 +/- 134.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=348000, episode_reward=1585.80 +/- 200.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=349000, episode_reward=1489.20 +/- 348.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=350000, episode_reward=1691.40 +/- 240.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=351000, episode_reward=1740.80 +/- 253.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=352000, episode_reward=1792.80 +/- 238.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=353000, episode_reward=1656.20 +/- 236.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=354000, episode_reward=1686.20 +/- 203.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=355000, episode_reward=1578.20 +/- 192.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=356000, episode_reward=1779.60 +/- 218.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=357000, episode_reward=1661.20 +/- 282.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=358000, episode_reward=1703.60 +/- 122.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=359000, episode_reward=1736.20 +/- 133.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=360000, episode_reward=1761.20 +/- 84.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=361000, episode_reward=1503.80 +/- 141.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=362000, episode_reward=1698.40 +/- 123.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=363000, episode_reward=1646.20 +/- 249.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=364000, episode_reward=1602.20 +/- 177.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=365000, episode_reward=1616.40 +/- 150.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=366000, episode_reward=1641.60 +/- 283.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=367000, episode_reward=1666.80 +/- 205.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=368000, episode_reward=1774.60 +/- 228.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=369000, episode_reward=1623.60 +/- 175.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=370000, episode_reward=1642.40 +/- 142.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=371000, episode_reward=1644.60 +/- 166.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=372000, episode_reward=1670.80 +/- 252.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=373000, episode_reward=1626.00 +/- 322.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=374000, episode_reward=1588.60 +/- 151.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=375000, episode_reward=1652.80 +/- 68.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=376000, episode_reward=1718.20 +/- 189.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=377000, episode_reward=1387.00 +/- 265.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=378000, episode_reward=1624.00 +/- 243.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=379000, episode_reward=1739.40 +/- 310.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=380000, episode_reward=1379.40 +/- 239.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=381000, episode_reward=1661.80 +/- 166.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=382000, episode_reward=1551.80 +/- 305.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=383000, episode_reward=1603.80 +/- 190.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=384000, episode_reward=1681.80 +/- 197.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=385000, episode_reward=1553.20 +/- 169.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=386000, episode_reward=1681.60 +/- 66.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=387000, episode_reward=1647.60 +/- 209.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=388000, episode_reward=1619.80 +/- 187.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=389000, episode_reward=1629.60 +/- 269.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=390000, episode_reward=1753.00 +/- 165.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=391000, episode_reward=1758.80 +/- 192.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=392000, episode_reward=1573.80 +/- 274.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=393000, episode_reward=1680.80 +/- 169.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=394000, episode_reward=1799.00 +/- 236.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=395000, episode_reward=1651.40 +/- 302.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=396000, episode_reward=1608.00 +/- 87.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=397000, episode_reward=1630.80 +/- 71.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=398000, episode_reward=1387.00 +/- 350.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=399000, episode_reward=1641.60 +/- 138.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=400000, episode_reward=1552.80 +/- 188.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=401000, episode_reward=1691.60 +/- 212.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=402000, episode_reward=1556.00 +/- 229.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=403000, episode_reward=1837.80 +/- 189.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=404000, episode_reward=1664.60 +/- 173.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=405000, episode_reward=1581.60 +/- 220.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=406000, episode_reward=1701.40 +/- 89.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=407000, episode_reward=1575.60 +/- 322.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=408000, episode_reward=1633.20 +/- 323.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=409000, episode_reward=1521.00 +/- 138.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=410000, episode_reward=1598.00 +/- 375.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=411000, episode_reward=1552.80 +/- 287.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=412000, episode_reward=1551.00 +/- 125.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=413000, episode_reward=1702.60 +/- 198.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=414000, episode_reward=1722.00 +/- 281.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=415000, episode_reward=1607.40 +/- 257.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=416000, episode_reward=1722.00 +/- 143.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=417000, episode_reward=1612.20 +/- 170.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=418000, episode_reward=1813.40 +/- 132.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=419000, episode_reward=1471.20 +/- 262.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=420000, episode_reward=1597.40 +/- 185.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=421000, episode_reward=1586.60 +/- 289.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=422000, episode_reward=1643.40 +/- 146.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=423000, episode_reward=1615.60 +/- 277.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=424000, episode_reward=1492.60 +/- 238.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=425000, episode_reward=1333.80 +/- 350.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=426000, episode_reward=1797.20 +/- 240.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=427000, episode_reward=1661.80 +/- 322.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=428000, episode_reward=1596.80 +/- 212.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=429000, episode_reward=1578.40 +/- 261.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=430000, episode_reward=1625.20 +/- 275.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=431000, episode_reward=1470.80 +/- 326.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=432000, episode_reward=1577.40 +/- 239.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=433000, episode_reward=1646.40 +/- 105.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=434000, episode_reward=1500.60 +/- 162.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=435000, episode_reward=1724.40 +/- 297.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=436000, episode_reward=1600.00 +/- 222.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=437000, episode_reward=1706.60 +/- 103.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=438000, episode_reward=1597.60 +/- 244.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=439000, episode_reward=1720.00 +/- 112.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=440000, episode_reward=1542.60 +/- 145.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=441000, episode_reward=1707.20 +/- 76.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=442000, episode_reward=1608.80 +/- 373.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=443000, episode_reward=1649.00 +/- 64.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=444000, episode_reward=1602.00 +/- 250.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=445000, episode_reward=1691.60 +/- 172.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=446000, episode_reward=1565.20 +/- 292.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=447000, episode_reward=1519.80 +/- 238.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=448000, episode_reward=1488.60 +/- 325.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=449000, episode_reward=1709.60 +/- 242.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=450000, episode_reward=1723.80 +/- 189.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=451000, episode_reward=1640.00 +/- 183.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=452000, episode_reward=1757.40 +/- 111.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=453000, episode_reward=1639.20 +/- 109.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=454000, episode_reward=1702.80 +/- 57.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=455000, episode_reward=1584.80 +/- 197.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=456000, episode_reward=1677.20 +/- 187.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=457000, episode_reward=1501.20 +/- 271.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=458000, episode_reward=1569.60 +/- 194.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=459000, episode_reward=1893.40 +/- 188.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=460000, episode_reward=1592.80 +/- 302.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=461000, episode_reward=1699.00 +/- 150.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=462000, episode_reward=1462.20 +/- 285.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=463000, episode_reward=1673.60 +/- 96.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=464000, episode_reward=1702.20 +/- 59.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=465000, episode_reward=1750.20 +/- 292.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=466000, episode_reward=1777.40 +/- 193.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=467000, episode_reward=1668.80 +/- 195.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=468000, episode_reward=1809.20 +/- 248.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=469000, episode_reward=1652.80 +/- 100.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=470000, episode_reward=1485.80 +/- 273.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=471000, episode_reward=1854.00 +/- 216.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=472000, episode_reward=1557.00 +/- 431.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=473000, episode_reward=1703.40 +/- 123.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=474000, episode_reward=1588.60 +/- 132.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=475000, episode_reward=1604.00 +/- 269.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=476000, episode_reward=1565.00 +/- 123.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=477000, episode_reward=1598.20 +/- 183.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=478000, episode_reward=1709.00 +/- 137.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=479000, episode_reward=1662.20 +/- 178.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=480000, episode_reward=1598.20 +/- 364.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=481000, episode_reward=1687.80 +/- 183.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=482000, episode_reward=1689.80 +/- 269.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=483000, episode_reward=1574.20 +/- 277.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=484000, episode_reward=1557.00 +/- 68.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=485000, episode_reward=1446.80 +/- 208.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=486000, episode_reward=1579.80 +/- 327.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=487000, episode_reward=1641.40 +/- 368.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=488000, episode_reward=1608.60 +/- 112.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=489000, episode_reward=1486.40 +/- 206.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=490000, episode_reward=1552.60 +/- 301.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=491000, episode_reward=1848.40 +/- 125.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=492000, episode_reward=1713.80 +/- 203.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=493000, episode_reward=1642.80 +/- 333.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=494000, episode_reward=1681.60 +/- 181.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=495000, episode_reward=1528.20 +/- 261.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=496000, episode_reward=1437.80 +/- 85.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=497000, episode_reward=1492.80 +/- 175.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=498000, episode_reward=1476.20 +/- 175.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=499000, episode_reward=1696.20 +/- 257.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=500000, episode_reward=1412.60 +/- 241.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=501000, episode_reward=1469.80 +/- 313.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Gamma = 0.8, Learning Rate = 0.003, Activation Function = Tanh, Neurons = 32, Process noise = 0.1, Measurement noise = 0.0\n",
      "Eval num_timesteps=1000, episode_reward=-845.20 +/- 303.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=-1006.20 +/- 388.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=3000, episode_reward=-38.20 +/- 131.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4000, episode_reward=-66.40 +/- 154.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=5000, episode_reward=540.80 +/- 105.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=6000, episode_reward=503.60 +/- 415.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=7000, episode_reward=758.20 +/- 185.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=8000, episode_reward=615.40 +/- 150.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=9000, episode_reward=198.60 +/- 172.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=10000, episode_reward=257.40 +/- 176.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=11000, episode_reward=330.60 +/- 109.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=12000, episode_reward=351.20 +/- 228.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=13000, episode_reward=254.40 +/- 251.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=14000, episode_reward=293.60 +/- 122.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=15000, episode_reward=251.00 +/- 193.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=16000, episode_reward=252.40 +/- 202.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=17000, episode_reward=164.00 +/- 269.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=18000, episode_reward=628.40 +/- 263.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=19000, episode_reward=1073.20 +/- 182.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=20000, episode_reward=665.00 +/- 357.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=21000, episode_reward=752.00 +/- 273.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=22000, episode_reward=839.60 +/- 366.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=23000, episode_reward=1038.20 +/- 411.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=24000, episode_reward=853.80 +/- 191.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=25000, episode_reward=1325.00 +/- 195.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=26000, episode_reward=1262.80 +/- 197.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=27000, episode_reward=768.40 +/- 261.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=28000, episode_reward=900.20 +/- 250.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=29000, episode_reward=905.20 +/- 208.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=30000, episode_reward=1379.00 +/- 319.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=31000, episode_reward=1257.20 +/- 179.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=32000, episode_reward=1350.60 +/- 127.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=33000, episode_reward=1306.00 +/- 265.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=34000, episode_reward=1415.80 +/- 191.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=35000, episode_reward=1442.80 +/- 342.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=36000, episode_reward=1347.00 +/- 210.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=37000, episode_reward=1326.20 +/- 304.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=38000, episode_reward=1479.40 +/- 288.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=39000, episode_reward=1359.40 +/- 205.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=40000, episode_reward=1215.20 +/- 327.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=41000, episode_reward=1518.20 +/- 354.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=42000, episode_reward=1683.00 +/- 237.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=43000, episode_reward=1254.80 +/- 285.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=44000, episode_reward=1462.00 +/- 272.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=45000, episode_reward=1369.60 +/- 438.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=46000, episode_reward=1251.40 +/- 393.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=47000, episode_reward=1399.20 +/- 327.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=48000, episode_reward=1514.40 +/- 106.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=49000, episode_reward=1382.60 +/- 257.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=50000, episode_reward=1518.40 +/- 296.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=51000, episode_reward=1378.20 +/- 371.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=52000, episode_reward=1384.80 +/- 233.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=53000, episode_reward=1541.40 +/- 252.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=54000, episode_reward=1506.80 +/- 294.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=55000, episode_reward=1551.20 +/- 262.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=56000, episode_reward=1565.60 +/- 223.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=57000, episode_reward=1358.00 +/- 361.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=58000, episode_reward=1166.80 +/- 439.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=59000, episode_reward=1509.60 +/- 172.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=60000, episode_reward=1505.20 +/- 394.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=61000, episode_reward=1496.00 +/- 321.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=62000, episode_reward=1544.20 +/- 266.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=63000, episode_reward=1409.40 +/- 247.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=64000, episode_reward=1480.40 +/- 260.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=65000, episode_reward=1410.60 +/- 199.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=66000, episode_reward=1344.80 +/- 300.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=67000, episode_reward=1698.00 +/- 388.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=68000, episode_reward=1540.80 +/- 245.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=69000, episode_reward=1515.20 +/- 251.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=70000, episode_reward=1543.00 +/- 212.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=71000, episode_reward=1637.00 +/- 253.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=72000, episode_reward=1737.00 +/- 251.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=73000, episode_reward=1437.20 +/- 100.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=74000, episode_reward=1352.00 +/- 248.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=75000, episode_reward=1488.20 +/- 430.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=76000, episode_reward=1505.80 +/- 141.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=77000, episode_reward=1431.60 +/- 122.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=78000, episode_reward=1532.40 +/- 267.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=79000, episode_reward=1370.40 +/- 299.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=80000, episode_reward=1326.20 +/- 220.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=81000, episode_reward=1557.80 +/- 313.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=82000, episode_reward=1583.80 +/- 193.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=83000, episode_reward=1521.60 +/- 321.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=84000, episode_reward=1382.60 +/- 444.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=85000, episode_reward=1579.80 +/- 148.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=86000, episode_reward=1532.00 +/- 229.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=87000, episode_reward=1438.00 +/- 366.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=88000, episode_reward=1648.80 +/- 94.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=89000, episode_reward=1438.40 +/- 197.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=90000, episode_reward=1532.20 +/- 374.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=91000, episode_reward=1386.80 +/- 342.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=92000, episode_reward=1498.80 +/- 278.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=93000, episode_reward=1629.00 +/- 377.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=94000, episode_reward=1469.40 +/- 254.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=95000, episode_reward=1489.80 +/- 399.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=96000, episode_reward=1655.60 +/- 108.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=97000, episode_reward=1367.80 +/- 87.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=98000, episode_reward=1673.00 +/- 171.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=99000, episode_reward=1501.00 +/- 238.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=100000, episode_reward=1511.80 +/- 270.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=101000, episode_reward=1723.20 +/- 261.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=102000, episode_reward=1385.00 +/- 344.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=103000, episode_reward=1780.40 +/- 190.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=104000, episode_reward=1620.60 +/- 296.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=105000, episode_reward=1514.60 +/- 132.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=106000, episode_reward=1477.80 +/- 71.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=107000, episode_reward=1506.40 +/- 97.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=108000, episode_reward=1676.20 +/- 268.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=109000, episode_reward=1646.40 +/- 186.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=110000, episode_reward=1800.20 +/- 222.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=111000, episode_reward=1679.00 +/- 118.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=112000, episode_reward=1679.20 +/- 67.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=113000, episode_reward=1608.20 +/- 112.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=114000, episode_reward=1516.20 +/- 300.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=115000, episode_reward=1488.20 +/- 268.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=116000, episode_reward=1729.80 +/- 66.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=117000, episode_reward=1824.80 +/- 237.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=118000, episode_reward=1459.60 +/- 325.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=119000, episode_reward=1384.00 +/- 77.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=120000, episode_reward=1666.80 +/- 142.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=121000, episode_reward=1526.60 +/- 173.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=122000, episode_reward=1535.60 +/- 257.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=123000, episode_reward=1379.40 +/- 256.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=124000, episode_reward=1440.00 +/- 212.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=125000, episode_reward=1659.40 +/- 288.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=126000, episode_reward=1632.60 +/- 298.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=127000, episode_reward=1702.80 +/- 267.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=128000, episode_reward=1395.40 +/- 197.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=129000, episode_reward=1302.20 +/- 311.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=130000, episode_reward=1602.40 +/- 271.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=131000, episode_reward=1685.00 +/- 162.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=132000, episode_reward=1372.40 +/- 306.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=133000, episode_reward=1582.20 +/- 192.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=134000, episode_reward=1433.20 +/- 246.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=135000, episode_reward=1673.60 +/- 236.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=136000, episode_reward=1701.60 +/- 290.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=137000, episode_reward=1604.60 +/- 141.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=138000, episode_reward=1365.80 +/- 284.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=139000, episode_reward=1585.00 +/- 218.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=140000, episode_reward=1452.80 +/- 343.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=141000, episode_reward=1435.40 +/- 313.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=142000, episode_reward=1399.00 +/- 214.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=143000, episode_reward=1696.60 +/- 350.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=144000, episode_reward=1641.20 +/- 175.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=145000, episode_reward=1517.00 +/- 146.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=146000, episode_reward=1611.00 +/- 349.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=147000, episode_reward=1665.20 +/- 221.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=148000, episode_reward=1736.80 +/- 206.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=149000, episode_reward=1620.60 +/- 170.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=150000, episode_reward=1449.00 +/- 266.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=151000, episode_reward=1562.60 +/- 224.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=152000, episode_reward=1620.00 +/- 111.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=153000, episode_reward=1311.40 +/- 268.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=154000, episode_reward=1592.00 +/- 230.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=155000, episode_reward=1776.00 +/- 212.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=156000, episode_reward=1757.20 +/- 171.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=157000, episode_reward=1665.20 +/- 167.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=158000, episode_reward=1725.00 +/- 150.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=159000, episode_reward=1541.00 +/- 193.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=160000, episode_reward=1534.40 +/- 262.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=161000, episode_reward=1757.80 +/- 68.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=162000, episode_reward=1522.40 +/- 198.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=163000, episode_reward=1678.00 +/- 271.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=164000, episode_reward=1885.80 +/- 279.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=165000, episode_reward=1827.20 +/- 171.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=166000, episode_reward=1721.00 +/- 202.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=167000, episode_reward=1647.20 +/- 212.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=168000, episode_reward=1537.60 +/- 189.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=169000, episode_reward=1802.60 +/- 187.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=170000, episode_reward=1556.80 +/- 272.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=171000, episode_reward=1509.80 +/- 200.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=172000, episode_reward=1632.40 +/- 91.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=173000, episode_reward=1382.20 +/- 369.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=174000, episode_reward=1614.80 +/- 235.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=175000, episode_reward=1316.20 +/- 323.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=176000, episode_reward=1397.60 +/- 359.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=177000, episode_reward=1405.40 +/- 215.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=178000, episode_reward=1506.60 +/- 378.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=179000, episode_reward=1683.60 +/- 225.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=180000, episode_reward=1705.00 +/- 157.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=181000, episode_reward=1423.20 +/- 259.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=182000, episode_reward=1584.80 +/- 298.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=183000, episode_reward=1230.40 +/- 256.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=184000, episode_reward=1380.00 +/- 436.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=185000, episode_reward=1580.00 +/- 173.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=186000, episode_reward=1705.60 +/- 187.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=187000, episode_reward=1616.20 +/- 265.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=188000, episode_reward=1494.80 +/- 431.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=189000, episode_reward=1631.20 +/- 301.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=190000, episode_reward=1669.20 +/- 148.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=191000, episode_reward=1451.00 +/- 325.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=192000, episode_reward=1556.80 +/- 136.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=193000, episode_reward=1738.60 +/- 297.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=194000, episode_reward=1572.20 +/- 285.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=195000, episode_reward=1668.00 +/- 189.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=196000, episode_reward=1694.00 +/- 159.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=197000, episode_reward=1654.00 +/- 274.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=198000, episode_reward=1664.80 +/- 263.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=199000, episode_reward=1555.40 +/- 193.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=200000, episode_reward=1541.60 +/- 282.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=201000, episode_reward=1497.60 +/- 298.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=202000, episode_reward=1403.80 +/- 200.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=203000, episode_reward=1500.40 +/- 296.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=204000, episode_reward=1568.60 +/- 391.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=205000, episode_reward=1554.60 +/- 184.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=206000, episode_reward=1675.80 +/- 194.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=207000, episode_reward=1407.80 +/- 204.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=208000, episode_reward=1684.60 +/- 181.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=209000, episode_reward=1607.60 +/- 248.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=210000, episode_reward=1641.80 +/- 198.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=211000, episode_reward=1613.00 +/- 154.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=212000, episode_reward=1744.20 +/- 329.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=213000, episode_reward=1665.80 +/- 251.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=214000, episode_reward=1552.00 +/- 219.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=215000, episode_reward=1532.60 +/- 235.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=216000, episode_reward=1395.80 +/- 103.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=217000, episode_reward=1552.40 +/- 263.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=218000, episode_reward=1438.20 +/- 266.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=219000, episode_reward=1559.80 +/- 401.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=220000, episode_reward=1544.00 +/- 420.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=221000, episode_reward=1642.80 +/- 332.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=222000, episode_reward=1450.60 +/- 303.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=223000, episode_reward=1593.80 +/- 139.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=224000, episode_reward=1540.20 +/- 364.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=225000, episode_reward=1419.00 +/- 246.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=226000, episode_reward=1301.00 +/- 199.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=227000, episode_reward=1527.60 +/- 256.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=228000, episode_reward=1685.40 +/- 317.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=229000, episode_reward=1503.00 +/- 372.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=230000, episode_reward=1524.00 +/- 274.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=231000, episode_reward=1503.40 +/- 224.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=232000, episode_reward=1663.80 +/- 79.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=233000, episode_reward=1802.80 +/- 233.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=234000, episode_reward=1689.60 +/- 208.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=235000, episode_reward=1685.00 +/- 164.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=236000, episode_reward=1441.00 +/- 375.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=237000, episode_reward=1725.80 +/- 132.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=238000, episode_reward=1621.00 +/- 294.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=239000, episode_reward=1878.00 +/- 234.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=240000, episode_reward=1218.60 +/- 341.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=241000, episode_reward=1523.40 +/- 225.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=242000, episode_reward=1396.80 +/- 327.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=243000, episode_reward=1472.80 +/- 163.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=244000, episode_reward=1298.60 +/- 330.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=245000, episode_reward=1594.20 +/- 342.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=246000, episode_reward=1772.60 +/- 170.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=247000, episode_reward=1591.40 +/- 125.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=248000, episode_reward=1554.40 +/- 171.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=249000, episode_reward=1755.40 +/- 268.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=250000, episode_reward=1609.00 +/- 63.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=251000, episode_reward=1537.00 +/- 262.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=252000, episode_reward=1263.60 +/- 266.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=253000, episode_reward=1643.60 +/- 232.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=254000, episode_reward=1592.00 +/- 308.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=255000, episode_reward=1494.20 +/- 227.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=256000, episode_reward=1607.40 +/- 211.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=257000, episode_reward=1628.80 +/- 108.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=258000, episode_reward=1663.40 +/- 210.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=259000, episode_reward=1658.80 +/- 287.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=260000, episode_reward=1550.00 +/- 145.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=261000, episode_reward=1499.00 +/- 289.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=262000, episode_reward=1621.60 +/- 360.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=263000, episode_reward=1553.60 +/- 197.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=264000, episode_reward=1654.80 +/- 147.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=265000, episode_reward=1484.40 +/- 272.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=266000, episode_reward=1668.60 +/- 167.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=267000, episode_reward=1654.40 +/- 110.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=268000, episode_reward=1693.00 +/- 237.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=269000, episode_reward=1615.80 +/- 450.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=270000, episode_reward=1651.80 +/- 206.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=271000, episode_reward=1762.80 +/- 246.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=272000, episode_reward=1626.40 +/- 72.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=273000, episode_reward=1715.80 +/- 152.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=274000, episode_reward=1699.20 +/- 280.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=275000, episode_reward=1496.00 +/- 224.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=276000, episode_reward=1440.80 +/- 211.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=277000, episode_reward=1661.20 +/- 213.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=278000, episode_reward=1608.60 +/- 127.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=279000, episode_reward=1835.60 +/- 170.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=280000, episode_reward=1787.60 +/- 23.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=281000, episode_reward=1519.00 +/- 320.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=282000, episode_reward=1564.60 +/- 173.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=283000, episode_reward=1479.60 +/- 263.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=284000, episode_reward=1790.00 +/- 163.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=285000, episode_reward=1469.60 +/- 477.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=286000, episode_reward=1329.20 +/- 181.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=287000, episode_reward=1262.80 +/- 301.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=288000, episode_reward=1474.00 +/- 149.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=289000, episode_reward=1496.20 +/- 344.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=290000, episode_reward=1455.00 +/- 231.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=291000, episode_reward=1511.00 +/- 269.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=292000, episode_reward=1436.40 +/- 232.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=293000, episode_reward=1565.40 +/- 295.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=294000, episode_reward=1620.80 +/- 288.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=295000, episode_reward=1629.20 +/- 184.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=296000, episode_reward=1701.60 +/- 120.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=297000, episode_reward=1695.80 +/- 319.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=298000, episode_reward=1588.60 +/- 277.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=299000, episode_reward=1561.20 +/- 237.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=300000, episode_reward=1549.60 +/- 412.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=301000, episode_reward=1409.60 +/- 284.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=302000, episode_reward=1511.60 +/- 262.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=303000, episode_reward=1739.80 +/- 301.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=304000, episode_reward=1425.40 +/- 313.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=305000, episode_reward=1772.60 +/- 170.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=306000, episode_reward=1597.00 +/- 191.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=307000, episode_reward=1753.80 +/- 129.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=308000, episode_reward=1830.60 +/- 113.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=309000, episode_reward=1782.60 +/- 99.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=310000, episode_reward=1376.60 +/- 378.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=311000, episode_reward=1812.40 +/- 184.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=312000, episode_reward=1650.40 +/- 295.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=313000, episode_reward=1566.20 +/- 253.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=314000, episode_reward=1586.00 +/- 228.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=315000, episode_reward=1706.60 +/- 243.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=316000, episode_reward=1836.40 +/- 269.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=317000, episode_reward=1424.40 +/- 292.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=318000, episode_reward=1677.60 +/- 122.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=319000, episode_reward=1692.40 +/- 269.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=320000, episode_reward=1640.20 +/- 353.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=321000, episode_reward=1544.00 +/- 183.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=322000, episode_reward=1563.00 +/- 68.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=323000, episode_reward=1574.60 +/- 197.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=324000, episode_reward=1706.40 +/- 203.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=325000, episode_reward=1586.00 +/- 195.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=326000, episode_reward=1632.40 +/- 206.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=327000, episode_reward=1703.40 +/- 133.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=328000, episode_reward=1559.20 +/- 217.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=329000, episode_reward=1685.20 +/- 235.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=330000, episode_reward=1642.00 +/- 176.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=331000, episode_reward=1755.20 +/- 279.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=332000, episode_reward=1549.00 +/- 150.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=333000, episode_reward=1674.00 +/- 236.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=334000, episode_reward=1513.20 +/- 212.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=335000, episode_reward=1750.00 +/- 266.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=336000, episode_reward=1433.20 +/- 360.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=337000, episode_reward=1707.00 +/- 254.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=338000, episode_reward=1635.60 +/- 185.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=339000, episode_reward=1641.60 +/- 350.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=340000, episode_reward=1526.20 +/- 169.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=341000, episode_reward=1742.60 +/- 292.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=342000, episode_reward=1426.80 +/- 409.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=343000, episode_reward=1654.40 +/- 251.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=344000, episode_reward=1625.80 +/- 269.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=345000, episode_reward=1746.80 +/- 260.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=346000, episode_reward=1668.80 +/- 121.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=347000, episode_reward=1547.60 +/- 218.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=348000, episode_reward=1683.00 +/- 279.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=349000, episode_reward=1435.80 +/- 348.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=350000, episode_reward=1803.60 +/- 216.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=351000, episode_reward=1663.20 +/- 103.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=352000, episode_reward=1665.80 +/- 67.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=353000, episode_reward=1629.40 +/- 153.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=354000, episode_reward=1506.60 +/- 193.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=355000, episode_reward=1500.00 +/- 249.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=356000, episode_reward=1602.20 +/- 324.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=357000, episode_reward=1480.60 +/- 188.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=358000, episode_reward=1488.20 +/- 458.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=359000, episode_reward=1704.60 +/- 182.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=360000, episode_reward=1576.60 +/- 120.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=361000, episode_reward=1722.20 +/- 78.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=362000, episode_reward=1568.40 +/- 208.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=363000, episode_reward=1743.20 +/- 74.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=364000, episode_reward=1551.60 +/- 195.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=365000, episode_reward=1760.60 +/- 358.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=366000, episode_reward=1503.60 +/- 340.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=367000, episode_reward=1625.80 +/- 289.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=368000, episode_reward=1649.40 +/- 107.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=369000, episode_reward=1758.20 +/- 171.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=370000, episode_reward=1466.20 +/- 292.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=371000, episode_reward=1565.80 +/- 83.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=372000, episode_reward=1611.60 +/- 314.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=373000, episode_reward=1660.20 +/- 191.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=374000, episode_reward=1474.80 +/- 225.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=375000, episode_reward=1504.20 +/- 273.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=376000, episode_reward=1452.40 +/- 432.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=377000, episode_reward=1620.80 +/- 102.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=378000, episode_reward=1553.60 +/- 270.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=379000, episode_reward=1490.80 +/- 496.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=380000, episode_reward=1655.40 +/- 285.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=381000, episode_reward=1554.60 +/- 299.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=382000, episode_reward=1562.80 +/- 287.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=383000, episode_reward=1541.60 +/- 231.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=384000, episode_reward=1568.80 +/- 325.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=385000, episode_reward=1656.00 +/- 139.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=386000, episode_reward=1642.60 +/- 71.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=387000, episode_reward=1298.20 +/- 292.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=388000, episode_reward=1646.20 +/- 185.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=389000, episode_reward=1618.40 +/- 170.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=390000, episode_reward=1752.40 +/- 299.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=391000, episode_reward=1460.80 +/- 282.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=392000, episode_reward=1574.60 +/- 107.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=393000, episode_reward=1612.60 +/- 255.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=394000, episode_reward=1483.00 +/- 154.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=395000, episode_reward=1708.20 +/- 147.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=396000, episode_reward=1733.60 +/- 126.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=397000, episode_reward=1475.40 +/- 387.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=398000, episode_reward=1630.60 +/- 185.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=399000, episode_reward=1465.20 +/- 375.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=400000, episode_reward=1581.40 +/- 244.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=401000, episode_reward=1806.20 +/- 229.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=402000, episode_reward=1528.20 +/- 337.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=403000, episode_reward=1787.20 +/- 189.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=404000, episode_reward=1695.00 +/- 265.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=405000, episode_reward=1758.40 +/- 289.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=406000, episode_reward=1532.20 +/- 161.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=407000, episode_reward=1318.20 +/- 367.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=408000, episode_reward=1606.80 +/- 245.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=409000, episode_reward=1514.60 +/- 260.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=410000, episode_reward=1628.80 +/- 176.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=411000, episode_reward=1356.40 +/- 312.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=412000, episode_reward=1707.40 +/- 272.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=413000, episode_reward=1684.40 +/- 245.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=414000, episode_reward=1510.00 +/- 150.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=415000, episode_reward=1555.80 +/- 265.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=416000, episode_reward=1488.60 +/- 338.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=417000, episode_reward=1797.40 +/- 207.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=418000, episode_reward=1576.80 +/- 318.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=419000, episode_reward=1519.20 +/- 292.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=420000, episode_reward=1617.00 +/- 308.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=421000, episode_reward=1591.00 +/- 178.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=422000, episode_reward=1668.20 +/- 271.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=423000, episode_reward=1485.80 +/- 310.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=424000, episode_reward=1697.00 +/- 148.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=425000, episode_reward=1728.00 +/- 228.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=426000, episode_reward=1771.00 +/- 186.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=427000, episode_reward=1550.00 +/- 305.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=428000, episode_reward=1632.20 +/- 235.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=429000, episode_reward=1597.40 +/- 137.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=430000, episode_reward=1640.80 +/- 193.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=431000, episode_reward=1417.00 +/- 269.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=432000, episode_reward=1571.80 +/- 232.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=433000, episode_reward=1828.40 +/- 132.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=434000, episode_reward=1858.60 +/- 193.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=435000, episode_reward=1713.00 +/- 268.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=436000, episode_reward=1701.60 +/- 157.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=437000, episode_reward=1655.00 +/- 139.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=438000, episode_reward=1660.80 +/- 95.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=439000, episode_reward=1631.00 +/- 171.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=440000, episode_reward=1728.80 +/- 118.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=441000, episode_reward=1803.80 +/- 91.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=442000, episode_reward=1595.60 +/- 153.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=443000, episode_reward=1531.00 +/- 229.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=444000, episode_reward=1679.20 +/- 256.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=445000, episode_reward=1723.00 +/- 225.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=446000, episode_reward=1636.00 +/- 209.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=447000, episode_reward=1507.80 +/- 197.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=448000, episode_reward=1840.40 +/- 166.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=449000, episode_reward=1637.40 +/- 277.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=450000, episode_reward=1717.60 +/- 244.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=451000, episode_reward=1650.60 +/- 178.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=452000, episode_reward=1666.00 +/- 164.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=453000, episode_reward=1633.40 +/- 208.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=454000, episode_reward=1610.40 +/- 246.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=455000, episode_reward=1682.00 +/- 215.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=456000, episode_reward=1594.40 +/- 158.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=457000, episode_reward=1622.00 +/- 219.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=458000, episode_reward=1739.20 +/- 197.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=459000, episode_reward=1734.40 +/- 156.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=460000, episode_reward=1607.40 +/- 213.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=461000, episode_reward=1359.60 +/- 192.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=462000, episode_reward=1669.40 +/- 254.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=463000, episode_reward=1612.60 +/- 121.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=464000, episode_reward=1621.20 +/- 189.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=465000, episode_reward=1583.40 +/- 279.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=466000, episode_reward=1613.80 +/- 495.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=467000, episode_reward=1601.40 +/- 219.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=468000, episode_reward=1540.60 +/- 286.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=469000, episode_reward=1789.00 +/- 127.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=470000, episode_reward=1627.60 +/- 265.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=471000, episode_reward=1622.20 +/- 127.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=472000, episode_reward=1478.60 +/- 288.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=473000, episode_reward=1657.20 +/- 147.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=474000, episode_reward=1503.40 +/- 279.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=475000, episode_reward=1443.00 +/- 210.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=476000, episode_reward=1676.40 +/- 213.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=477000, episode_reward=1710.60 +/- 184.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=478000, episode_reward=1463.60 +/- 276.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=479000, episode_reward=1431.60 +/- 286.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=480000, episode_reward=1781.20 +/- 100.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=481000, episode_reward=1852.40 +/- 183.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=482000, episode_reward=1821.60 +/- 147.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=483000, episode_reward=1592.40 +/- 155.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=484000, episode_reward=1542.00 +/- 253.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=485000, episode_reward=1626.20 +/- 279.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=486000, episode_reward=1605.60 +/- 344.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=487000, episode_reward=1622.00 +/- 125.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=488000, episode_reward=1351.80 +/- 213.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=489000, episode_reward=1469.00 +/- 222.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=490000, episode_reward=1587.60 +/- 157.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=491000, episode_reward=1634.00 +/- 185.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=492000, episode_reward=1762.60 +/- 90.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=493000, episode_reward=1555.80 +/- 193.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=494000, episode_reward=1536.20 +/- 349.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=495000, episode_reward=1601.40 +/- 162.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=496000, episode_reward=1602.40 +/- 187.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=497000, episode_reward=1596.00 +/- 276.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=498000, episode_reward=1593.40 +/- 369.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=499000, episode_reward=1561.40 +/- 281.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=500000, episode_reward=1466.60 +/- 262.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=501000, episode_reward=1641.60 +/- 127.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Gamma = 0.8, Learning Rate = 0.003, Activation Function = Tanh, Neurons = 64, Process noise = 0.1, Measurement noise = 0.0\n",
      "Eval num_timesteps=1000, episode_reward=-7635.60 +/- 1879.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=-5192.20 +/- 1508.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3000, episode_reward=552.60 +/- 251.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4000, episode_reward=167.40 +/- 208.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=5000, episode_reward=100.40 +/- 280.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=6000, episode_reward=269.80 +/- 352.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=7000, episode_reward=406.20 +/- 349.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=8000, episode_reward=643.20 +/- 327.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=9000, episode_reward=426.80 +/- 293.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=10000, episode_reward=657.80 +/- 97.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=11000, episode_reward=836.00 +/- 166.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=12000, episode_reward=652.00 +/- 159.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=13000, episode_reward=432.20 +/- 236.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=14000, episode_reward=575.40 +/- 239.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=15000, episode_reward=556.40 +/- 255.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=16000, episode_reward=658.20 +/- 152.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=17000, episode_reward=665.80 +/- 281.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=18000, episode_reward=940.40 +/- 265.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=19000, episode_reward=949.00 +/- 234.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=20000, episode_reward=813.00 +/- 181.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=21000, episode_reward=865.20 +/- 143.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=22000, episode_reward=892.20 +/- 286.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=23000, episode_reward=864.20 +/- 224.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=24000, episode_reward=949.60 +/- 212.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=25000, episode_reward=1038.00 +/- 89.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=26000, episode_reward=1051.80 +/- 396.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=27000, episode_reward=1180.00 +/- 257.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=28000, episode_reward=1243.20 +/- 200.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=29000, episode_reward=1398.20 +/- 254.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=30000, episode_reward=1303.80 +/- 279.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=31000, episode_reward=1410.00 +/- 192.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=32000, episode_reward=298.40 +/- 2599.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=33000, episode_reward=1481.80 +/- 358.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=34000, episode_reward=1665.80 +/- 112.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=35000, episode_reward=1612.40 +/- 117.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=36000, episode_reward=1521.80 +/- 139.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=37000, episode_reward=1555.20 +/- 144.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=38000, episode_reward=1377.00 +/- 341.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=39000, episode_reward=1687.20 +/- 168.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=1525.60 +/- 184.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=41000, episode_reward=1439.40 +/- 371.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=42000, episode_reward=1257.40 +/- 191.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=43000, episode_reward=1366.60 +/- 331.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=44000, episode_reward=1463.60 +/- 233.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=45000, episode_reward=1595.20 +/- 249.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=46000, episode_reward=1621.40 +/- 194.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=47000, episode_reward=1399.60 +/- 161.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=48000, episode_reward=1529.20 +/- 260.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=49000, episode_reward=1506.60 +/- 323.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=50000, episode_reward=1376.40 +/- 219.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=51000, episode_reward=1524.80 +/- 250.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=52000, episode_reward=1560.60 +/- 278.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=53000, episode_reward=1336.40 +/- 176.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=54000, episode_reward=1444.00 +/- 314.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=55000, episode_reward=1234.60 +/- 320.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=56000, episode_reward=1583.20 +/- 260.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=57000, episode_reward=1565.60 +/- 70.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=58000, episode_reward=1528.80 +/- 236.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=59000, episode_reward=1637.80 +/- 135.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=60000, episode_reward=1477.40 +/- 226.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=61000, episode_reward=1532.00 +/- 263.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=62000, episode_reward=1569.20 +/- 156.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=63000, episode_reward=1557.00 +/- 266.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=64000, episode_reward=1503.00 +/- 266.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=65000, episode_reward=1474.00 +/- 287.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=66000, episode_reward=1589.80 +/- 123.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=67000, episode_reward=1446.80 +/- 385.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=68000, episode_reward=1406.40 +/- 425.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=69000, episode_reward=1640.80 +/- 162.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=70000, episode_reward=1380.80 +/- 280.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=71000, episode_reward=1744.80 +/- 130.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=72000, episode_reward=1570.00 +/- 220.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=73000, episode_reward=1603.00 +/- 251.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=74000, episode_reward=1424.60 +/- 195.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=75000, episode_reward=1641.60 +/- 196.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=76000, episode_reward=1625.80 +/- 107.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=77000, episode_reward=1529.80 +/- 233.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=78000, episode_reward=1638.40 +/- 128.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=79000, episode_reward=1506.80 +/- 182.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=80000, episode_reward=1645.20 +/- 302.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=81000, episode_reward=1398.40 +/- 338.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=82000, episode_reward=1306.80 +/- 213.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=83000, episode_reward=1552.80 +/- 311.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=84000, episode_reward=1657.20 +/- 281.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=85000, episode_reward=1614.60 +/- 204.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=86000, episode_reward=1212.60 +/- 254.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=87000, episode_reward=1557.00 +/- 153.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=88000, episode_reward=1554.80 +/- 257.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=89000, episode_reward=1367.80 +/- 327.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=90000, episode_reward=1379.20 +/- 291.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=91000, episode_reward=1533.00 +/- 319.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=92000, episode_reward=1446.80 +/- 159.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=93000, episode_reward=1537.40 +/- 175.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=94000, episode_reward=1547.20 +/- 105.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=95000, episode_reward=1532.20 +/- 146.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=96000, episode_reward=1574.00 +/- 199.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=97000, episode_reward=1546.60 +/- 224.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=98000, episode_reward=1449.80 +/- 345.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=99000, episode_reward=1539.80 +/- 235.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=100000, episode_reward=1575.20 +/- 282.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=101000, episode_reward=1378.80 +/- 433.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=102000, episode_reward=1781.20 +/- 247.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=103000, episode_reward=1723.40 +/- 97.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=104000, episode_reward=1639.80 +/- 156.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=105000, episode_reward=1429.80 +/- 184.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=106000, episode_reward=1461.40 +/- 256.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=107000, episode_reward=1347.60 +/- 319.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=108000, episode_reward=1551.00 +/- 417.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=109000, episode_reward=1637.40 +/- 353.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=110000, episode_reward=1490.40 +/- 224.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=111000, episode_reward=1590.00 +/- 209.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=112000, episode_reward=1261.80 +/- 296.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=113000, episode_reward=1347.00 +/- 247.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=114000, episode_reward=1480.00 +/- 222.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=115000, episode_reward=1513.80 +/- 171.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=116000, episode_reward=1479.40 +/- 170.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=117000, episode_reward=1506.00 +/- 130.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=118000, episode_reward=1491.00 +/- 180.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=119000, episode_reward=1499.80 +/- 151.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=120000, episode_reward=1413.00 +/- 188.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=121000, episode_reward=1188.80 +/- 335.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=122000, episode_reward=1336.20 +/- 390.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=123000, episode_reward=1749.60 +/- 149.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=124000, episode_reward=1716.80 +/- 167.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=125000, episode_reward=1687.00 +/- 269.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=126000, episode_reward=1455.60 +/- 246.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=127000, episode_reward=1691.80 +/- 113.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=128000, episode_reward=1539.20 +/- 132.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=129000, episode_reward=1551.40 +/- 201.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=130000, episode_reward=1607.20 +/- 176.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=131000, episode_reward=1590.40 +/- 223.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=132000, episode_reward=1642.80 +/- 375.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=133000, episode_reward=1589.60 +/- 263.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=134000, episode_reward=1417.80 +/- 258.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=135000, episode_reward=1457.80 +/- 222.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=136000, episode_reward=1267.00 +/- 429.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=137000, episode_reward=1497.60 +/- 141.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=138000, episode_reward=1761.00 +/- 53.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=139000, episode_reward=1577.40 +/- 270.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=140000, episode_reward=1611.00 +/- 258.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=141000, episode_reward=1426.00 +/- 277.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=142000, episode_reward=1680.60 +/- 159.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=143000, episode_reward=1517.60 +/- 177.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=144000, episode_reward=1517.00 +/- 137.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=145000, episode_reward=1521.40 +/- 148.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=146000, episode_reward=1344.80 +/- 401.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=147000, episode_reward=1589.20 +/- 89.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=148000, episode_reward=1441.20 +/- 167.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=149000, episode_reward=1578.20 +/- 196.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=150000, episode_reward=1718.20 +/- 146.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=151000, episode_reward=1441.00 +/- 213.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=152000, episode_reward=1506.00 +/- 266.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=153000, episode_reward=1582.80 +/- 253.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=154000, episode_reward=1483.80 +/- 244.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=155000, episode_reward=1384.40 +/- 189.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=156000, episode_reward=1395.20 +/- 238.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=157000, episode_reward=1393.40 +/- 186.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=158000, episode_reward=1514.40 +/- 270.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=159000, episode_reward=1478.40 +/- 128.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=160000, episode_reward=1640.20 +/- 252.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=161000, episode_reward=1534.80 +/- 167.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=162000, episode_reward=1389.00 +/- 362.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=163000, episode_reward=1382.80 +/- 356.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=164000, episode_reward=1483.40 +/- 277.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=165000, episode_reward=1533.80 +/- 291.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=166000, episode_reward=1437.60 +/- 394.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=167000, episode_reward=1504.40 +/- 252.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=168000, episode_reward=1701.20 +/- 161.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=169000, episode_reward=1576.20 +/- 156.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=170000, episode_reward=1466.80 +/- 330.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=171000, episode_reward=1565.80 +/- 239.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=172000, episode_reward=1528.20 +/- 356.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=173000, episode_reward=1578.60 +/- 168.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=174000, episode_reward=1656.20 +/- 203.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=175000, episode_reward=1551.80 +/- 155.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=176000, episode_reward=1475.40 +/- 235.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=177000, episode_reward=1439.00 +/- 183.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=178000, episode_reward=1610.20 +/- 160.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=179000, episode_reward=1752.40 +/- 187.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=180000, episode_reward=1513.00 +/- 129.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=181000, episode_reward=1582.00 +/- 274.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=182000, episode_reward=1515.00 +/- 165.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=183000, episode_reward=1554.40 +/- 348.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=184000, episode_reward=1695.00 +/- 266.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=185000, episode_reward=1645.20 +/- 89.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=186000, episode_reward=1579.60 +/- 259.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=187000, episode_reward=1512.20 +/- 134.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=188000, episode_reward=1635.00 +/- 229.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=189000, episode_reward=1529.40 +/- 198.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=190000, episode_reward=1510.60 +/- 233.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=191000, episode_reward=1608.80 +/- 141.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=192000, episode_reward=1665.60 +/- 236.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=193000, episode_reward=1497.60 +/- 233.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=194000, episode_reward=1736.80 +/- 239.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=195000, episode_reward=1686.40 +/- 138.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=196000, episode_reward=1519.20 +/- 76.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=197000, episode_reward=1474.20 +/- 209.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=198000, episode_reward=1598.80 +/- 165.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=199000, episode_reward=1653.20 +/- 90.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=200000, episode_reward=1383.80 +/- 210.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=201000, episode_reward=1497.80 +/- 277.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=202000, episode_reward=1715.80 +/- 122.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=203000, episode_reward=1286.80 +/- 211.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=204000, episode_reward=1634.40 +/- 168.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=205000, episode_reward=1511.00 +/- 178.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=206000, episode_reward=1465.80 +/- 213.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=207000, episode_reward=1348.20 +/- 232.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=208000, episode_reward=1399.00 +/- 139.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=209000, episode_reward=1450.60 +/- 193.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=210000, episode_reward=1437.40 +/- 278.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=211000, episode_reward=1448.80 +/- 358.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=212000, episode_reward=1341.20 +/- 240.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=213000, episode_reward=1670.40 +/- 265.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=214000, episode_reward=1448.20 +/- 350.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=215000, episode_reward=1576.80 +/- 344.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=216000, episode_reward=1611.00 +/- 225.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=217000, episode_reward=1521.40 +/- 210.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=218000, episode_reward=1426.00 +/- 206.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=219000, episode_reward=1473.20 +/- 223.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=220000, episode_reward=1336.00 +/- 333.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=221000, episode_reward=1555.80 +/- 229.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=222000, episode_reward=1426.60 +/- 205.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=223000, episode_reward=1668.60 +/- 231.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=224000, episode_reward=1358.00 +/- 296.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=225000, episode_reward=1665.40 +/- 120.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=226000, episode_reward=1362.60 +/- 288.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=227000, episode_reward=1636.40 +/- 114.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=228000, episode_reward=1525.20 +/- 165.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=229000, episode_reward=1372.00 +/- 267.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=230000, episode_reward=1289.00 +/- 154.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=231000, episode_reward=1518.40 +/- 353.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=232000, episode_reward=1447.20 +/- 189.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=233000, episode_reward=1223.60 +/- 328.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=234000, episode_reward=1434.40 +/- 242.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=235000, episode_reward=1703.20 +/- 168.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=236000, episode_reward=1508.40 +/- 268.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=237000, episode_reward=1490.20 +/- 247.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=238000, episode_reward=1492.00 +/- 91.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=239000, episode_reward=1352.80 +/- 151.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=240000, episode_reward=1517.40 +/- 163.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=241000, episode_reward=1283.40 +/- 210.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=242000, episode_reward=1381.00 +/- 295.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=243000, episode_reward=1517.00 +/- 254.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=244000, episode_reward=1280.60 +/- 234.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=245000, episode_reward=1308.80 +/- 282.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=246000, episode_reward=1540.00 +/- 297.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=247000, episode_reward=1508.40 +/- 295.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=248000, episode_reward=1468.60 +/- 181.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=249000, episode_reward=1429.40 +/- 555.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=250000, episode_reward=1614.80 +/- 255.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=251000, episode_reward=1271.80 +/- 151.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=252000, episode_reward=1432.20 +/- 210.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=253000, episode_reward=1510.20 +/- 296.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=254000, episode_reward=1476.80 +/- 361.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=255000, episode_reward=1445.40 +/- 235.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=256000, episode_reward=1354.80 +/- 247.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=257000, episode_reward=1844.80 +/- 197.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=258000, episode_reward=1535.20 +/- 395.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=259000, episode_reward=1593.20 +/- 142.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=260000, episode_reward=1456.40 +/- 151.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=261000, episode_reward=1708.60 +/- 341.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=262000, episode_reward=1336.00 +/- 336.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=263000, episode_reward=1371.20 +/- 95.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=264000, episode_reward=1431.40 +/- 182.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=265000, episode_reward=1531.60 +/- 300.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=266000, episode_reward=1633.00 +/- 91.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=267000, episode_reward=1549.80 +/- 194.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=268000, episode_reward=1522.00 +/- 230.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=269000, episode_reward=1593.60 +/- 228.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=270000, episode_reward=1578.00 +/- 240.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=271000, episode_reward=1555.00 +/- 290.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=272000, episode_reward=1551.80 +/- 267.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=273000, episode_reward=1465.60 +/- 112.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=274000, episode_reward=1417.00 +/- 376.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=275000, episode_reward=1626.40 +/- 100.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=276000, episode_reward=1509.60 +/- 173.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=277000, episode_reward=1364.80 +/- 296.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=278000, episode_reward=1395.20 +/- 273.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=279000, episode_reward=1581.40 +/- 196.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=280000, episode_reward=1420.60 +/- 316.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=281000, episode_reward=1486.40 +/- 232.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=282000, episode_reward=1185.20 +/- 389.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=283000, episode_reward=1435.00 +/- 68.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=284000, episode_reward=1554.00 +/- 282.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=285000, episode_reward=1582.40 +/- 260.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=286000, episode_reward=1401.60 +/- 217.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=287000, episode_reward=1685.60 +/- 257.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=288000, episode_reward=1601.00 +/- 149.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=289000, episode_reward=1383.40 +/- 365.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=290000, episode_reward=1588.60 +/- 343.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=291000, episode_reward=1338.20 +/- 143.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=292000, episode_reward=1438.20 +/- 299.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=293000, episode_reward=1313.60 +/- 248.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=294000, episode_reward=1656.00 +/- 175.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=295000, episode_reward=1444.00 +/- 265.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=296000, episode_reward=1440.20 +/- 279.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=297000, episode_reward=1671.60 +/- 149.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=298000, episode_reward=1621.00 +/- 217.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=299000, episode_reward=1483.20 +/- 313.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=300000, episode_reward=1580.60 +/- 308.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=301000, episode_reward=1402.60 +/- 156.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=302000, episode_reward=1631.40 +/- 92.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=303000, episode_reward=1457.80 +/- 80.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=304000, episode_reward=1527.40 +/- 365.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=305000, episode_reward=1697.40 +/- 275.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=306000, episode_reward=1533.80 +/- 280.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=307000, episode_reward=1478.00 +/- 300.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=308000, episode_reward=1471.20 +/- 171.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=309000, episode_reward=1247.80 +/- 298.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=310000, episode_reward=1511.60 +/- 180.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=311000, episode_reward=1301.80 +/- 153.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=312000, episode_reward=1589.40 +/- 235.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=313000, episode_reward=1694.40 +/- 177.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=314000, episode_reward=1472.20 +/- 273.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=315000, episode_reward=1333.00 +/- 369.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=316000, episode_reward=1544.80 +/- 207.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=317000, episode_reward=1578.20 +/- 222.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=318000, episode_reward=1645.80 +/- 107.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=319000, episode_reward=1597.00 +/- 129.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=320000, episode_reward=1661.80 +/- 484.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=321000, episode_reward=1498.00 +/- 375.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=322000, episode_reward=1609.40 +/- 211.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=323000, episode_reward=1550.40 +/- 190.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=324000, episode_reward=1281.40 +/- 254.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=325000, episode_reward=1459.40 +/- 239.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=326000, episode_reward=1747.20 +/- 184.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=327000, episode_reward=1525.40 +/- 209.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=328000, episode_reward=1653.00 +/- 192.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=329000, episode_reward=1552.60 +/- 181.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=330000, episode_reward=1584.80 +/- 153.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=331000, episode_reward=1529.80 +/- 104.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=332000, episode_reward=1658.00 +/- 131.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=333000, episode_reward=1721.40 +/- 97.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=334000, episode_reward=1555.20 +/- 220.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=335000, episode_reward=1715.60 +/- 180.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=336000, episode_reward=1634.20 +/- 230.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=337000, episode_reward=1561.60 +/- 229.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=338000, episode_reward=1648.00 +/- 99.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=339000, episode_reward=1691.80 +/- 184.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=340000, episode_reward=1704.20 +/- 150.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=341000, episode_reward=1592.20 +/- 70.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=342000, episode_reward=1494.00 +/- 245.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=343000, episode_reward=1667.20 +/- 167.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=344000, episode_reward=1676.00 +/- 176.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=345000, episode_reward=1551.60 +/- 197.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=346000, episode_reward=1598.40 +/- 278.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=347000, episode_reward=1642.80 +/- 236.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=348000, episode_reward=1730.60 +/- 270.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=349000, episode_reward=1436.00 +/- 165.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=350000, episode_reward=1522.00 +/- 227.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=351000, episode_reward=1610.20 +/- 103.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=352000, episode_reward=1608.00 +/- 89.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=353000, episode_reward=1408.40 +/- 367.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=354000, episode_reward=1574.20 +/- 177.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=355000, episode_reward=1595.00 +/- 83.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=356000, episode_reward=1519.40 +/- 161.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=357000, episode_reward=1462.40 +/- 330.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=358000, episode_reward=1808.40 +/- 66.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=359000, episode_reward=1409.80 +/- 170.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=360000, episode_reward=1460.60 +/- 286.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=361000, episode_reward=1386.60 +/- 200.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=362000, episode_reward=1515.40 +/- 179.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=363000, episode_reward=1476.00 +/- 259.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=364000, episode_reward=1530.60 +/- 218.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=365000, episode_reward=1579.20 +/- 179.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=366000, episode_reward=1623.60 +/- 230.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=367000, episode_reward=1593.80 +/- 227.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=368000, episode_reward=1775.00 +/- 223.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=369000, episode_reward=1297.20 +/- 183.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=370000, episode_reward=1328.80 +/- 109.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=371000, episode_reward=1521.40 +/- 78.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=372000, episode_reward=1709.60 +/- 180.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=373000, episode_reward=1615.60 +/- 341.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=374000, episode_reward=1658.40 +/- 156.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=375000, episode_reward=1374.20 +/- 241.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=376000, episode_reward=1655.00 +/- 148.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=377000, episode_reward=1630.00 +/- 211.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=378000, episode_reward=1526.80 +/- 105.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=379000, episode_reward=1440.40 +/- 180.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=380000, episode_reward=1605.20 +/- 153.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=381000, episode_reward=1652.40 +/- 119.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=382000, episode_reward=1389.60 +/- 303.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=383000, episode_reward=1660.20 +/- 139.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=384000, episode_reward=1693.00 +/- 126.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=385000, episode_reward=1493.00 +/- 279.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=386000, episode_reward=1631.80 +/- 232.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=387000, episode_reward=1735.60 +/- 155.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=388000, episode_reward=1537.40 +/- 216.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=389000, episode_reward=1647.40 +/- 307.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=390000, episode_reward=1676.40 +/- 269.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=391000, episode_reward=1400.80 +/- 196.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=392000, episode_reward=1673.20 +/- 96.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=393000, episode_reward=1630.20 +/- 116.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=394000, episode_reward=1643.00 +/- 214.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=395000, episode_reward=1415.00 +/- 445.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=396000, episode_reward=1306.20 +/- 114.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=397000, episode_reward=1630.40 +/- 177.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=398000, episode_reward=1608.20 +/- 274.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=399000, episode_reward=1478.80 +/- 359.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=400000, episode_reward=1650.00 +/- 171.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=401000, episode_reward=1505.20 +/- 297.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=402000, episode_reward=1718.40 +/- 188.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=403000, episode_reward=1718.40 +/- 364.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=404000, episode_reward=1511.20 +/- 244.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=405000, episode_reward=1491.80 +/- 377.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=406000, episode_reward=1480.80 +/- 214.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=407000, episode_reward=1545.20 +/- 222.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=408000, episode_reward=1684.40 +/- 163.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=409000, episode_reward=1773.20 +/- 215.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=410000, episode_reward=1495.00 +/- 126.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=411000, episode_reward=1429.00 +/- 181.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=412000, episode_reward=1613.00 +/- 200.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=413000, episode_reward=1594.40 +/- 230.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=414000, episode_reward=1717.00 +/- 249.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=415000, episode_reward=1561.40 +/- 99.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=416000, episode_reward=1431.00 +/- 247.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=417000, episode_reward=1560.80 +/- 68.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=418000, episode_reward=1441.20 +/- 184.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=419000, episode_reward=1710.20 +/- 125.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=420000, episode_reward=1455.20 +/- 228.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=421000, episode_reward=1631.60 +/- 240.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=422000, episode_reward=1700.40 +/- 228.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=423000, episode_reward=1619.80 +/- 227.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=424000, episode_reward=1596.00 +/- 268.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=425000, episode_reward=1634.00 +/- 120.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=426000, episode_reward=1582.40 +/- 189.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=427000, episode_reward=1566.20 +/- 166.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=428000, episode_reward=1574.60 +/- 148.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=429000, episode_reward=1244.80 +/- 318.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=430000, episode_reward=1697.00 +/- 300.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=431000, episode_reward=1566.40 +/- 232.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=432000, episode_reward=1320.60 +/- 264.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=433000, episode_reward=1582.60 +/- 263.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=434000, episode_reward=1561.40 +/- 165.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=435000, episode_reward=1520.00 +/- 256.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=436000, episode_reward=1749.40 +/- 109.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=437000, episode_reward=1538.60 +/- 270.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=438000, episode_reward=1505.00 +/- 98.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=439000, episode_reward=1472.40 +/- 72.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=440000, episode_reward=1631.60 +/- 178.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=441000, episode_reward=1308.80 +/- 436.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=442000, episode_reward=1490.20 +/- 275.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=443000, episode_reward=1550.00 +/- 178.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=444000, episode_reward=1614.60 +/- 220.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=445000, episode_reward=1662.00 +/- 101.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=446000, episode_reward=1513.00 +/- 178.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=447000, episode_reward=1629.00 +/- 157.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=448000, episode_reward=1655.20 +/- 343.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=449000, episode_reward=1675.00 +/- 186.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=450000, episode_reward=1530.40 +/- 323.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=451000, episode_reward=1516.00 +/- 211.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=452000, episode_reward=1679.20 +/- 136.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=453000, episode_reward=1536.20 +/- 324.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=454000, episode_reward=1663.60 +/- 230.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=455000, episode_reward=1645.00 +/- 351.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=456000, episode_reward=1666.00 +/- 106.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=457000, episode_reward=1595.00 +/- 266.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=458000, episode_reward=1712.00 +/- 328.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=459000, episode_reward=1785.40 +/- 103.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=460000, episode_reward=1707.60 +/- 197.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=461000, episode_reward=1527.40 +/- 399.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=462000, episode_reward=1653.80 +/- 178.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=463000, episode_reward=1536.40 +/- 335.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=464000, episode_reward=1892.20 +/- 177.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=465000, episode_reward=1670.00 +/- 150.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=466000, episode_reward=1640.20 +/- 66.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=467000, episode_reward=1546.80 +/- 416.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=468000, episode_reward=1649.00 +/- 100.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=469000, episode_reward=1735.80 +/- 253.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=470000, episode_reward=1679.00 +/- 99.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=471000, episode_reward=1551.20 +/- 327.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=472000, episode_reward=1677.80 +/- 264.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=473000, episode_reward=1529.40 +/- 179.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=474000, episode_reward=1703.20 +/- 198.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=475000, episode_reward=1544.60 +/- 154.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=476000, episode_reward=1585.80 +/- 117.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=477000, episode_reward=1673.00 +/- 152.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=478000, episode_reward=1729.60 +/- 134.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=479000, episode_reward=1607.60 +/- 214.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=480000, episode_reward=1641.60 +/- 115.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=481000, episode_reward=1760.40 +/- 249.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=482000, episode_reward=1588.20 +/- 266.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=483000, episode_reward=1684.40 +/- 229.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=484000, episode_reward=1691.80 +/- 165.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=485000, episode_reward=1419.20 +/- 255.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=486000, episode_reward=1589.80 +/- 358.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=487000, episode_reward=1566.20 +/- 288.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=488000, episode_reward=1629.20 +/- 196.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=489000, episode_reward=1636.00 +/- 338.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=490000, episode_reward=1691.40 +/- 145.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=491000, episode_reward=1575.80 +/- 304.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=492000, episode_reward=1371.60 +/- 165.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=493000, episode_reward=1638.40 +/- 346.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=494000, episode_reward=1765.00 +/- 139.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=495000, episode_reward=1636.40 +/- 215.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=496000, episode_reward=1648.60 +/- 127.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=497000, episode_reward=1685.80 +/- 91.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=498000, episode_reward=1600.80 +/- 202.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=499000, episode_reward=1691.00 +/- 255.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=500000, episode_reward=1743.00 +/- 194.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=501000, episode_reward=1636.20 +/- 108.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Gamma = 0.8, Learning Rate = 0.003, Activation Function = ReLU, Neurons = 8, Process noise = 0.1, Measurement noise = 0.0\n",
      "Eval num_timesteps=1000, episode_reward=-2391.60 +/- 64.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=-2437.40 +/- 43.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=3000, episode_reward=400.20 +/- 139.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4000, episode_reward=215.40 +/- 144.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=5000, episode_reward=527.40 +/- 160.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=6000, episode_reward=464.00 +/- 318.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=7000, episode_reward=346.80 +/- 135.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=8000, episode_reward=311.40 +/- 198.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=9000, episode_reward=344.00 +/- 228.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=10000, episode_reward=131.40 +/- 254.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=11000, episode_reward=312.80 +/- 207.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=12000, episode_reward=100.00 +/- 157.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=13000, episode_reward=395.00 +/- 122.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=14000, episode_reward=290.40 +/- 32.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=15000, episode_reward=311.40 +/- 203.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=16000, episode_reward=339.20 +/- 184.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=17000, episode_reward=349.80 +/- 195.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=18000, episode_reward=384.00 +/- 226.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=19000, episode_reward=726.00 +/- 274.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=20000, episode_reward=874.20 +/- 188.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=21000, episode_reward=863.40 +/- 335.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=22000, episode_reward=999.80 +/- 502.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=23000, episode_reward=1086.40 +/- 107.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=24000, episode_reward=886.80 +/- 207.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=25000, episode_reward=1260.60 +/- 412.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=26000, episode_reward=1193.80 +/- 456.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=27000, episode_reward=1399.40 +/- 175.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=28000, episode_reward=1617.80 +/- 268.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=29000, episode_reward=1526.60 +/- 99.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=30000, episode_reward=136.60 +/- 2410.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=31000, episode_reward=1500.40 +/- 166.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=32000, episode_reward=1524.40 +/- 267.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=33000, episode_reward=1299.60 +/- 290.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=34000, episode_reward=1124.80 +/- 327.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=35000, episode_reward=1196.40 +/- 421.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=36000, episode_reward=1295.00 +/- 109.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=37000, episode_reward=1370.40 +/- 191.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=38000, episode_reward=1410.40 +/- 188.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=39000, episode_reward=1530.80 +/- 132.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=40000, episode_reward=1749.60 +/- 168.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=41000, episode_reward=1402.40 +/- 178.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=42000, episode_reward=1435.20 +/- 171.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=43000, episode_reward=1402.60 +/- 148.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=44000, episode_reward=1527.40 +/- 167.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=45000, episode_reward=1434.20 +/- 117.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=46000, episode_reward=1384.60 +/- 359.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=47000, episode_reward=1595.60 +/- 315.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=48000, episode_reward=1306.20 +/- 109.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=49000, episode_reward=1467.80 +/- 290.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=50000, episode_reward=1602.60 +/- 388.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=51000, episode_reward=1361.60 +/- 363.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=52000, episode_reward=1314.20 +/- 214.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=53000, episode_reward=1539.40 +/- 197.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=54000, episode_reward=1421.40 +/- 460.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=55000, episode_reward=1565.20 +/- 136.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=56000, episode_reward=1357.40 +/- 304.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=57000, episode_reward=1454.20 +/- 373.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=58000, episode_reward=1570.80 +/- 253.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=59000, episode_reward=1352.60 +/- 402.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=60000, episode_reward=1481.00 +/- 274.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=61000, episode_reward=1518.80 +/- 198.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=62000, episode_reward=1627.80 +/- 243.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=63000, episode_reward=1575.40 +/- 283.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=64000, episode_reward=1319.00 +/- 370.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=65000, episode_reward=1597.20 +/- 160.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=66000, episode_reward=1558.00 +/- 231.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=67000, episode_reward=1359.20 +/- 216.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=68000, episode_reward=1608.20 +/- 119.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=69000, episode_reward=1591.80 +/- 139.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=70000, episode_reward=1443.60 +/- 369.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=71000, episode_reward=1434.80 +/- 278.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=72000, episode_reward=1605.60 +/- 187.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=73000, episode_reward=1515.40 +/- 272.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=74000, episode_reward=1415.40 +/- 204.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=75000, episode_reward=1464.60 +/- 340.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=76000, episode_reward=1571.80 +/- 435.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=77000, episode_reward=1457.20 +/- 220.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=78000, episode_reward=1665.80 +/- 234.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=79000, episode_reward=1503.00 +/- 258.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=80000, episode_reward=1568.60 +/- 180.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=81000, episode_reward=1488.60 +/- 187.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=82000, episode_reward=1491.60 +/- 191.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=83000, episode_reward=1611.60 +/- 131.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=84000, episode_reward=1479.20 +/- 187.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=85000, episode_reward=1771.00 +/- 183.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=86000, episode_reward=1604.20 +/- 156.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=87000, episode_reward=1486.20 +/- 192.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=88000, episode_reward=1484.80 +/- 289.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=89000, episode_reward=1340.20 +/- 402.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=90000, episode_reward=1777.20 +/- 248.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=91000, episode_reward=1567.60 +/- 152.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=92000, episode_reward=1591.00 +/- 105.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=93000, episode_reward=1522.60 +/- 260.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=94000, episode_reward=1433.40 +/- 204.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=95000, episode_reward=1699.60 +/- 187.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=96000, episode_reward=1491.60 +/- 255.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=97000, episode_reward=1434.60 +/- 260.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=98000, episode_reward=1456.80 +/- 281.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=99000, episode_reward=1749.80 +/- 128.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=100000, episode_reward=1582.40 +/- 248.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=101000, episode_reward=1643.40 +/- 389.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=102000, episode_reward=1484.60 +/- 267.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=103000, episode_reward=1341.20 +/- 257.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=104000, episode_reward=1394.40 +/- 196.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=105000, episode_reward=1509.60 +/- 184.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=106000, episode_reward=1523.00 +/- 216.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=107000, episode_reward=1424.40 +/- 230.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=108000, episode_reward=1614.60 +/- 106.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=109000, episode_reward=1415.40 +/- 192.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=110000, episode_reward=1344.40 +/- 259.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=111000, episode_reward=1280.60 +/- 382.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=112000, episode_reward=1601.00 +/- 181.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=113000, episode_reward=1630.00 +/- 98.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=114000, episode_reward=1661.80 +/- 136.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=115000, episode_reward=1461.60 +/- 569.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=116000, episode_reward=1573.00 +/- 157.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=117000, episode_reward=1563.60 +/- 200.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=118000, episode_reward=1562.80 +/- 162.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=119000, episode_reward=1586.60 +/- 190.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=120000, episode_reward=1577.40 +/- 230.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=121000, episode_reward=1340.60 +/- 217.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=122000, episode_reward=1465.60 +/- 262.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=123000, episode_reward=1389.40 +/- 165.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=124000, episode_reward=1583.40 +/- 228.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=125000, episode_reward=1638.00 +/- 84.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=126000, episode_reward=1490.00 +/- 242.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=127000, episode_reward=1366.00 +/- 302.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=128000, episode_reward=1504.80 +/- 131.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=129000, episode_reward=1296.40 +/- 173.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=130000, episode_reward=1348.40 +/- 281.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=131000, episode_reward=1579.80 +/- 210.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=132000, episode_reward=1547.20 +/- 228.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=133000, episode_reward=1618.80 +/- 184.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=134000, episode_reward=1565.20 +/- 134.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=135000, episode_reward=1551.00 +/- 85.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=136000, episode_reward=1622.60 +/- 161.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=137000, episode_reward=1686.60 +/- 259.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=138000, episode_reward=1359.40 +/- 121.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=139000, episode_reward=1602.40 +/- 452.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=140000, episode_reward=1551.00 +/- 82.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=141000, episode_reward=1539.40 +/- 232.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=142000, episode_reward=1534.80 +/- 217.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=143000, episode_reward=1671.40 +/- 76.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=144000, episode_reward=1416.40 +/- 256.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=145000, episode_reward=1513.40 +/- 262.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=146000, episode_reward=1574.80 +/- 322.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=147000, episode_reward=1548.00 +/- 210.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=148000, episode_reward=1600.00 +/- 172.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=149000, episode_reward=1688.20 +/- 86.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=150000, episode_reward=1640.00 +/- 298.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=151000, episode_reward=1410.40 +/- 351.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=152000, episode_reward=1655.40 +/- 304.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=153000, episode_reward=1369.40 +/- 178.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=154000, episode_reward=1575.60 +/- 299.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=155000, episode_reward=1421.00 +/- 249.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=156000, episode_reward=1535.20 +/- 186.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=157000, episode_reward=1639.00 +/- 342.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=158000, episode_reward=1405.80 +/- 211.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=159000, episode_reward=1681.40 +/- 187.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=160000, episode_reward=1650.40 +/- 227.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=161000, episode_reward=1672.40 +/- 291.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=162000, episode_reward=1415.40 +/- 214.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=163000, episode_reward=1685.60 +/- 379.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=164000, episode_reward=1533.40 +/- 371.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=165000, episode_reward=1496.20 +/- 316.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=166000, episode_reward=1606.60 +/- 200.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=167000, episode_reward=1373.40 +/- 286.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=168000, episode_reward=1833.00 +/- 208.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=169000, episode_reward=1536.20 +/- 305.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=170000, episode_reward=1663.20 +/- 115.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=171000, episode_reward=1711.40 +/- 212.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=172000, episode_reward=1728.60 +/- 221.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=173000, episode_reward=1502.00 +/- 278.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=174000, episode_reward=1512.80 +/- 269.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=175000, episode_reward=1655.20 +/- 225.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=176000, episode_reward=1684.20 +/- 138.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=177000, episode_reward=1596.20 +/- 171.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=178000, episode_reward=1717.00 +/- 119.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=179000, episode_reward=1684.60 +/- 204.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=180000, episode_reward=1385.20 +/- 312.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=181000, episode_reward=1682.20 +/- 135.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=182000, episode_reward=1629.80 +/- 138.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=183000, episode_reward=1603.40 +/- 229.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=184000, episode_reward=1462.40 +/- 220.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=185000, episode_reward=1824.60 +/- 284.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=186000, episode_reward=1538.20 +/- 174.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=187000, episode_reward=1719.00 +/- 212.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=188000, episode_reward=1425.00 +/- 305.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=189000, episode_reward=1732.60 +/- 250.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=190000, episode_reward=1492.80 +/- 262.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=191000, episode_reward=1570.80 +/- 286.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=192000, episode_reward=1493.80 +/- 293.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=193000, episode_reward=1697.60 +/- 147.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=194000, episode_reward=1670.60 +/- 276.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=195000, episode_reward=1705.40 +/- 108.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=196000, episode_reward=1501.00 +/- 155.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=197000, episode_reward=1664.20 +/- 185.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=198000, episode_reward=1440.00 +/- 294.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=199000, episode_reward=1642.20 +/- 240.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=200000, episode_reward=1745.20 +/- 83.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=201000, episode_reward=1483.20 +/- 163.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=202000, episode_reward=1535.60 +/- 369.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=203000, episode_reward=1441.20 +/- 175.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=204000, episode_reward=1489.60 +/- 113.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=205000, episode_reward=1465.00 +/- 206.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=206000, episode_reward=1356.40 +/- 426.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=207000, episode_reward=1657.80 +/- 243.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=208000, episode_reward=1640.40 +/- 142.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=209000, episode_reward=1412.40 +/- 394.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=210000, episode_reward=1825.60 +/- 228.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=211000, episode_reward=1634.20 +/- 113.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=212000, episode_reward=1571.00 +/- 194.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=213000, episode_reward=1640.40 +/- 128.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=214000, episode_reward=1703.00 +/- 157.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=215000, episode_reward=1648.60 +/- 254.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=216000, episode_reward=1576.00 +/- 152.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=217000, episode_reward=1444.20 +/- 179.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=218000, episode_reward=1361.60 +/- 278.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=219000, episode_reward=1551.40 +/- 168.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=220000, episode_reward=1655.40 +/- 108.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=221000, episode_reward=1565.60 +/- 117.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=222000, episode_reward=1741.20 +/- 159.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=223000, episode_reward=1303.60 +/- 140.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=224000, episode_reward=1770.60 +/- 220.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=225000, episode_reward=1409.40 +/- 268.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=226000, episode_reward=1399.60 +/- 235.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=227000, episode_reward=1511.60 +/- 231.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=228000, episode_reward=1583.80 +/- 151.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=229000, episode_reward=1619.40 +/- 237.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=230000, episode_reward=1629.60 +/- 258.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=231000, episode_reward=1387.60 +/- 196.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=232000, episode_reward=1648.60 +/- 393.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=233000, episode_reward=1317.00 +/- 154.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=234000, episode_reward=1647.40 +/- 285.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=235000, episode_reward=1231.20 +/- 330.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=236000, episode_reward=1550.20 +/- 126.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=237000, episode_reward=1385.20 +/- 205.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=238000, episode_reward=1590.20 +/- 219.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=239000, episode_reward=1451.00 +/- 344.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=240000, episode_reward=1648.40 +/- 233.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=241000, episode_reward=1815.00 +/- 161.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=242000, episode_reward=1263.80 +/- 277.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=243000, episode_reward=1586.80 +/- 105.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=244000, episode_reward=1568.00 +/- 225.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=245000, episode_reward=1363.20 +/- 220.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=246000, episode_reward=1591.20 +/- 330.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=247000, episode_reward=1581.00 +/- 176.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=248000, episode_reward=1317.60 +/- 178.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=249000, episode_reward=1542.20 +/- 67.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=250000, episode_reward=1550.60 +/- 139.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=251000, episode_reward=1450.00 +/- 230.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=252000, episode_reward=1639.00 +/- 113.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=253000, episode_reward=1634.40 +/- 244.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=254000, episode_reward=1703.60 +/- 112.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=255000, episode_reward=1597.20 +/- 278.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=256000, episode_reward=1616.60 +/- 155.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=257000, episode_reward=1516.40 +/- 249.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=258000, episode_reward=1467.60 +/- 345.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=259000, episode_reward=1324.40 +/- 248.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=260000, episode_reward=1590.20 +/- 268.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=261000, episode_reward=1531.20 +/- 286.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=262000, episode_reward=1629.40 +/- 225.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=263000, episode_reward=1849.60 +/- 262.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=264000, episode_reward=1613.60 +/- 242.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=265000, episode_reward=1872.40 +/- 123.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=266000, episode_reward=1560.80 +/- 203.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=267000, episode_reward=1737.20 +/- 323.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=268000, episode_reward=1704.20 +/- 247.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=269000, episode_reward=1607.00 +/- 283.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=270000, episode_reward=1520.20 +/- 301.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=271000, episode_reward=1653.80 +/- 154.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=272000, episode_reward=1473.00 +/- 137.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=273000, episode_reward=1534.60 +/- 211.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=274000, episode_reward=1183.20 +/- 464.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=275000, episode_reward=1417.40 +/- 144.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=276000, episode_reward=1656.00 +/- 210.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=277000, episode_reward=1435.00 +/- 157.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=278000, episode_reward=1556.80 +/- 169.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=279000, episode_reward=1684.40 +/- 184.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=280000, episode_reward=1644.20 +/- 314.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=281000, episode_reward=1557.20 +/- 191.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=282000, episode_reward=1465.00 +/- 226.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=283000, episode_reward=1678.80 +/- 319.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=284000, episode_reward=1555.60 +/- 204.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=285000, episode_reward=1599.40 +/- 192.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=286000, episode_reward=1503.40 +/- 215.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=287000, episode_reward=1647.40 +/- 301.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=288000, episode_reward=1588.80 +/- 229.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=289000, episode_reward=1534.20 +/- 73.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=290000, episode_reward=1541.20 +/- 160.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=291000, episode_reward=1569.80 +/- 195.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=292000, episode_reward=1723.20 +/- 168.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=293000, episode_reward=1617.20 +/- 188.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=294000, episode_reward=1668.40 +/- 311.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=295000, episode_reward=1521.00 +/- 77.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=296000, episode_reward=1731.60 +/- 159.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=297000, episode_reward=1629.00 +/- 220.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=298000, episode_reward=1629.60 +/- 267.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=299000, episode_reward=1462.60 +/- 282.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=300000, episode_reward=1606.80 +/- 316.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=301000, episode_reward=1462.40 +/- 231.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=302000, episode_reward=1542.80 +/- 106.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=303000, episode_reward=1773.60 +/- 160.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=304000, episode_reward=1360.40 +/- 215.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=305000, episode_reward=1429.80 +/- 358.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=306000, episode_reward=1495.00 +/- 97.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=307000, episode_reward=1657.00 +/- 113.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=308000, episode_reward=1719.00 +/- 183.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=309000, episode_reward=1528.80 +/- 229.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=310000, episode_reward=1788.60 +/- 164.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=311000, episode_reward=1692.20 +/- 372.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=312000, episode_reward=1579.80 +/- 231.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=313000, episode_reward=1560.80 +/- 495.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=314000, episode_reward=1590.20 +/- 318.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=315000, episode_reward=1707.40 +/- 68.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=316000, episode_reward=1604.00 +/- 283.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=317000, episode_reward=1578.40 +/- 114.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=318000, episode_reward=1729.20 +/- 293.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=319000, episode_reward=1739.80 +/- 124.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=320000, episode_reward=1617.40 +/- 364.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=321000, episode_reward=1665.40 +/- 223.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=322000, episode_reward=1587.20 +/- 259.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=323000, episode_reward=1655.40 +/- 117.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=324000, episode_reward=1692.80 +/- 174.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=325000, episode_reward=1705.20 +/- 227.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=326000, episode_reward=1710.40 +/- 182.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=327000, episode_reward=1725.60 +/- 174.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=328000, episode_reward=1617.20 +/- 149.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=329000, episode_reward=1481.80 +/- 185.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=330000, episode_reward=1573.60 +/- 214.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=331000, episode_reward=1556.20 +/- 266.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=332000, episode_reward=1577.00 +/- 223.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=333000, episode_reward=1735.20 +/- 37.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=334000, episode_reward=1452.60 +/- 149.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=335000, episode_reward=1668.20 +/- 215.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=336000, episode_reward=1723.80 +/- 196.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=337000, episode_reward=1540.40 +/- 104.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=338000, episode_reward=1447.00 +/- 166.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=339000, episode_reward=1606.80 +/- 84.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=340000, episode_reward=1432.40 +/- 129.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=341000, episode_reward=1581.60 +/- 201.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=342000, episode_reward=1574.60 +/- 178.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=343000, episode_reward=1662.00 +/- 306.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=344000, episode_reward=1675.80 +/- 379.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=345000, episode_reward=1602.60 +/- 286.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=346000, episode_reward=1591.20 +/- 109.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=347000, episode_reward=1590.60 +/- 115.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=348000, episode_reward=1665.60 +/- 135.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=349000, episode_reward=1508.00 +/- 380.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=350000, episode_reward=1760.00 +/- 135.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=351000, episode_reward=1696.00 +/- 254.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=352000, episode_reward=1573.80 +/- 67.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=353000, episode_reward=1539.20 +/- 287.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=354000, episode_reward=1656.60 +/- 141.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=355000, episode_reward=1430.00 +/- 292.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=356000, episode_reward=1848.80 +/- 247.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=357000, episode_reward=1594.00 +/- 258.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=358000, episode_reward=1613.80 +/- 311.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=359000, episode_reward=1695.00 +/- 89.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=360000, episode_reward=1536.60 +/- 481.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=361000, episode_reward=1601.20 +/- 288.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=362000, episode_reward=1824.80 +/- 204.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=363000, episode_reward=1499.40 +/- 137.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=364000, episode_reward=1571.20 +/- 318.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=365000, episode_reward=1551.20 +/- 327.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=366000, episode_reward=1797.60 +/- 81.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=367000, episode_reward=1392.20 +/- 137.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=368000, episode_reward=1785.20 +/- 80.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=369000, episode_reward=1729.80 +/- 225.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=370000, episode_reward=1449.80 +/- 195.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=371000, episode_reward=1679.20 +/- 333.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=372000, episode_reward=1689.60 +/- 95.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=373000, episode_reward=1665.60 +/- 209.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=374000, episode_reward=1775.00 +/- 104.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=375000, episode_reward=1793.00 +/- 169.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=376000, episode_reward=1600.60 +/- 155.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=377000, episode_reward=1619.20 +/- 107.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=378000, episode_reward=1501.00 +/- 190.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=379000, episode_reward=1462.00 +/- 358.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=380000, episode_reward=1634.20 +/- 348.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=381000, episode_reward=1555.40 +/- 322.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=382000, episode_reward=1621.00 +/- 229.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=383000, episode_reward=1638.80 +/- 278.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=384000, episode_reward=1486.60 +/- 186.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=385000, episode_reward=1596.60 +/- 155.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=386000, episode_reward=1576.80 +/- 281.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=387000, episode_reward=1414.40 +/- 122.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=388000, episode_reward=1628.40 +/- 257.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=389000, episode_reward=1442.00 +/- 219.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=390000, episode_reward=1464.20 +/- 214.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=391000, episode_reward=1579.20 +/- 204.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=392000, episode_reward=1501.60 +/- 290.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=393000, episode_reward=1554.20 +/- 267.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=394000, episode_reward=1657.20 +/- 279.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=395000, episode_reward=1676.80 +/- 223.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=396000, episode_reward=1639.40 +/- 238.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=397000, episode_reward=1577.00 +/- 473.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=398000, episode_reward=1649.80 +/- 67.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=399000, episode_reward=1716.00 +/- 189.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=400000, episode_reward=1699.00 +/- 162.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=401000, episode_reward=1608.40 +/- 162.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=402000, episode_reward=1613.40 +/- 302.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=403000, episode_reward=1779.00 +/- 183.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=404000, episode_reward=1473.40 +/- 118.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=405000, episode_reward=1612.60 +/- 206.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=406000, episode_reward=1566.40 +/- 235.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=407000, episode_reward=1592.40 +/- 124.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=408000, episode_reward=1494.60 +/- 100.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=409000, episode_reward=1672.00 +/- 133.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=410000, episode_reward=1719.40 +/- 212.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=411000, episode_reward=1795.80 +/- 80.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=412000, episode_reward=1700.40 +/- 189.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=413000, episode_reward=1612.60 +/- 202.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=414000, episode_reward=1556.60 +/- 150.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=415000, episode_reward=1731.60 +/- 73.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=416000, episode_reward=1724.60 +/- 115.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=417000, episode_reward=1594.40 +/- 175.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=418000, episode_reward=1540.60 +/- 344.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=419000, episode_reward=1628.20 +/- 395.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=420000, episode_reward=1571.00 +/- 360.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=421000, episode_reward=1399.20 +/- 378.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=422000, episode_reward=1537.40 +/- 308.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=423000, episode_reward=1674.80 +/- 299.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=424000, episode_reward=1660.80 +/- 237.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=425000, episode_reward=1543.80 +/- 184.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=426000, episode_reward=1589.00 +/- 224.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=427000, episode_reward=1476.60 +/- 267.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=428000, episode_reward=1770.80 +/- 233.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=429000, episode_reward=1633.00 +/- 207.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=430000, episode_reward=1379.80 +/- 305.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=431000, episode_reward=1454.20 +/- 153.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=432000, episode_reward=1417.00 +/- 182.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=433000, episode_reward=1685.20 +/- 236.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=434000, episode_reward=1543.00 +/- 115.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=435000, episode_reward=1536.60 +/- 123.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=436000, episode_reward=1570.80 +/- 360.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=437000, episode_reward=1723.60 +/- 113.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=438000, episode_reward=1661.40 +/- 153.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=439000, episode_reward=1680.40 +/- 232.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=440000, episode_reward=1688.80 +/- 171.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=441000, episode_reward=1633.80 +/- 98.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=442000, episode_reward=1587.80 +/- 413.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=443000, episode_reward=1438.60 +/- 360.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=444000, episode_reward=1766.00 +/- 259.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=445000, episode_reward=1389.40 +/- 284.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=446000, episode_reward=1446.40 +/- 226.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=447000, episode_reward=1721.40 +/- 219.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=448000, episode_reward=1423.60 +/- 258.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=449000, episode_reward=1679.40 +/- 292.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=450000, episode_reward=1537.40 +/- 304.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=451000, episode_reward=1510.20 +/- 297.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=452000, episode_reward=1476.00 +/- 312.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=453000, episode_reward=1809.20 +/- 144.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=454000, episode_reward=1681.40 +/- 248.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=455000, episode_reward=1534.00 +/- 483.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=456000, episode_reward=1632.80 +/- 106.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=457000, episode_reward=1522.40 +/- 220.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=458000, episode_reward=1492.80 +/- 326.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=459000, episode_reward=1644.00 +/- 276.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=460000, episode_reward=1680.20 +/- 306.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=461000, episode_reward=1715.00 +/- 255.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=462000, episode_reward=1533.80 +/- 109.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=463000, episode_reward=1721.80 +/- 256.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=464000, episode_reward=1668.20 +/- 270.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=465000, episode_reward=1490.20 +/- 338.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=466000, episode_reward=1558.00 +/- 267.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=467000, episode_reward=1632.80 +/- 125.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=468000, episode_reward=1489.00 +/- 331.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=469000, episode_reward=1613.20 +/- 190.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=470000, episode_reward=1562.00 +/- 179.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=471000, episode_reward=1600.80 +/- 143.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=472000, episode_reward=1521.00 +/- 263.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=473000, episode_reward=1561.60 +/- 297.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=474000, episode_reward=1673.20 +/- 192.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=475000, episode_reward=1560.20 +/- 430.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=476000, episode_reward=1652.80 +/- 325.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=477000, episode_reward=1789.00 +/- 212.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=478000, episode_reward=1780.20 +/- 210.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=479000, episode_reward=1648.80 +/- 226.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=480000, episode_reward=1593.20 +/- 230.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=481000, episode_reward=1518.40 +/- 110.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=482000, episode_reward=1633.00 +/- 258.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=483000, episode_reward=1411.60 +/- 211.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=484000, episode_reward=1522.40 +/- 318.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=485000, episode_reward=1664.00 +/- 121.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=486000, episode_reward=1677.20 +/- 238.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=487000, episode_reward=1531.60 +/- 225.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=488000, episode_reward=1790.00 +/- 303.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=489000, episode_reward=1582.80 +/- 275.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=490000, episode_reward=1632.20 +/- 272.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=491000, episode_reward=1490.60 +/- 171.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=492000, episode_reward=1552.20 +/- 205.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=493000, episode_reward=1724.00 +/- 88.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=494000, episode_reward=1692.00 +/- 150.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=495000, episode_reward=1590.20 +/- 121.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=496000, episode_reward=1759.40 +/- 203.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=497000, episode_reward=1728.40 +/- 166.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=498000, episode_reward=1697.60 +/- 143.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=499000, episode_reward=1746.60 +/- 102.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=500000, episode_reward=1642.60 +/- 181.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=501000, episode_reward=1679.00 +/- 177.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Gamma = 0.8, Learning Rate = 0.003, Activation Function = ReLU, Neurons = 32, Process noise = 0.1, Measurement noise = 0.0\n",
      "Eval num_timesteps=1000, episode_reward=-993.40 +/- 151.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=-1030.60 +/- 166.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=3000, episode_reward=-358.40 +/- 156.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4000, episode_reward=-472.20 +/- 92.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=5000, episode_reward=-78.60 +/- 120.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=6000, episode_reward=26.60 +/- 183.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=7000, episode_reward=99.80 +/- 140.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=8000, episode_reward=307.80 +/- 139.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=9000, episode_reward=255.60 +/- 74.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=10000, episode_reward=215.40 +/- 57.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=11000, episode_reward=147.60 +/- 144.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=12000, episode_reward=495.40 +/- 175.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=13000, episode_reward=363.00 +/- 294.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=14000, episode_reward=213.40 +/- 155.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=15000, episode_reward=330.60 +/- 218.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=16000, episode_reward=442.40 +/- 122.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=17000, episode_reward=888.00 +/- 259.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=18000, episode_reward=673.00 +/- 205.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=19000, episode_reward=1004.80 +/- 205.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=20000, episode_reward=1054.80 +/- 86.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=21000, episode_reward=1032.60 +/- 253.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=22000, episode_reward=764.00 +/- 251.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=23000, episode_reward=-2363.00 +/- 6760.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=24000, episode_reward=-720.00 +/- 3918.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=25000, episode_reward=1502.20 +/- 181.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=26000, episode_reward=1393.60 +/- 232.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=27000, episode_reward=-1214.00 +/- 5116.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=28000, episode_reward=1225.00 +/- 410.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=29000, episode_reward=1304.20 +/- 206.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=30000, episode_reward=1433.40 +/- 193.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=31000, episode_reward=1364.00 +/- 321.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=32000, episode_reward=1285.20 +/- 222.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=33000, episode_reward=1527.60 +/- 181.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=34000, episode_reward=1370.60 +/- 452.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=35000, episode_reward=1713.40 +/- 223.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=36000, episode_reward=1365.40 +/- 248.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=37000, episode_reward=1504.80 +/- 80.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=38000, episode_reward=1522.00 +/- 419.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=39000, episode_reward=1501.40 +/- 218.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=40000, episode_reward=1647.40 +/- 349.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=41000, episode_reward=1432.60 +/- 217.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=42000, episode_reward=1553.00 +/- 296.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=43000, episode_reward=1546.00 +/- 149.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=44000, episode_reward=1771.80 +/- 196.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=45000, episode_reward=1558.60 +/- 235.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=46000, episode_reward=1438.80 +/- 180.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=47000, episode_reward=1634.00 +/- 182.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=48000, episode_reward=1362.40 +/- 154.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=49000, episode_reward=1297.80 +/- 221.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=50000, episode_reward=1454.20 +/- 62.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=51000, episode_reward=1516.60 +/- 254.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=52000, episode_reward=1687.60 +/- 269.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=53000, episode_reward=1514.80 +/- 226.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=54000, episode_reward=1379.20 +/- 326.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=55000, episode_reward=1584.60 +/- 245.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=56000, episode_reward=1466.20 +/- 180.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=57000, episode_reward=1326.40 +/- 294.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=58000, episode_reward=1483.80 +/- 157.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=59000, episode_reward=1026.20 +/- 307.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=60000, episode_reward=1495.40 +/- 175.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=61000, episode_reward=1725.80 +/- 193.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=62000, episode_reward=1557.20 +/- 257.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=63000, episode_reward=1472.20 +/- 219.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=64000, episode_reward=1566.80 +/- 169.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=65000, episode_reward=1570.20 +/- 217.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=66000, episode_reward=1758.00 +/- 106.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=67000, episode_reward=1729.60 +/- 242.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=68000, episode_reward=1440.60 +/- 118.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=69000, episode_reward=1425.00 +/- 229.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=70000, episode_reward=1664.20 +/- 128.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=71000, episode_reward=1601.80 +/- 135.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=72000, episode_reward=1265.00 +/- 219.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=73000, episode_reward=1655.60 +/- 211.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=74000, episode_reward=1319.60 +/- 263.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=75000, episode_reward=1731.00 +/- 150.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=76000, episode_reward=1452.00 +/- 186.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=77000, episode_reward=1526.80 +/- 228.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=78000, episode_reward=1569.20 +/- 199.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=79000, episode_reward=1698.80 +/- 279.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=80000, episode_reward=1573.40 +/- 348.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=81000, episode_reward=1558.80 +/- 310.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=82000, episode_reward=1509.00 +/- 155.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=83000, episode_reward=1555.20 +/- 155.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=84000, episode_reward=1389.20 +/- 345.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=85000, episode_reward=1669.40 +/- 254.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=86000, episode_reward=1776.40 +/- 175.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=87000, episode_reward=1702.80 +/- 214.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=88000, episode_reward=1647.60 +/- 245.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=89000, episode_reward=1433.20 +/- 144.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=90000, episode_reward=1594.80 +/- 315.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=91000, episode_reward=1656.40 +/- 166.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=92000, episode_reward=1704.00 +/- 229.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=93000, episode_reward=1642.00 +/- 231.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=94000, episode_reward=1621.00 +/- 218.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=95000, episode_reward=1596.00 +/- 126.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=96000, episode_reward=1606.20 +/- 157.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=97000, episode_reward=1681.60 +/- 220.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=98000, episode_reward=1732.20 +/- 172.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=99000, episode_reward=1647.40 +/- 116.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=100000, episode_reward=1552.20 +/- 127.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=101000, episode_reward=1576.60 +/- 147.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=102000, episode_reward=1614.00 +/- 119.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=103000, episode_reward=1678.40 +/- 198.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=104000, episode_reward=1656.20 +/- 228.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=105000, episode_reward=1780.20 +/- 90.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=106000, episode_reward=1806.80 +/- 134.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=107000, episode_reward=1575.60 +/- 359.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=108000, episode_reward=1629.60 +/- 168.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=109000, episode_reward=1755.00 +/- 160.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=110000, episode_reward=1552.80 +/- 186.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=111000, episode_reward=1627.80 +/- 135.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=112000, episode_reward=1491.40 +/- 284.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=113000, episode_reward=1738.40 +/- 195.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=114000, episode_reward=1440.20 +/- 203.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=115000, episode_reward=1572.40 +/- 72.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=116000, episode_reward=1578.00 +/- 236.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=117000, episode_reward=1612.80 +/- 128.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=118000, episode_reward=1700.40 +/- 178.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=119000, episode_reward=1662.00 +/- 149.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=120000, episode_reward=1476.60 +/- 336.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=121000, episode_reward=1702.80 +/- 141.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=122000, episode_reward=1674.80 +/- 219.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=123000, episode_reward=1515.60 +/- 276.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=124000, episode_reward=1689.00 +/- 271.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=125000, episode_reward=1771.40 +/- 212.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=126000, episode_reward=1656.80 +/- 188.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=127000, episode_reward=1681.80 +/- 107.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=128000, episode_reward=1401.60 +/- 92.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=129000, episode_reward=1769.40 +/- 206.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=130000, episode_reward=1655.60 +/- 173.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=131000, episode_reward=1716.80 +/- 301.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=132000, episode_reward=1755.20 +/- 176.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=133000, episode_reward=1661.80 +/- 333.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=134000, episode_reward=1675.20 +/- 275.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=135000, episode_reward=1540.20 +/- 183.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=136000, episode_reward=1644.60 +/- 154.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=137000, episode_reward=1679.20 +/- 183.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=138000, episode_reward=1609.20 +/- 226.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=139000, episode_reward=1451.60 +/- 174.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=140000, episode_reward=1678.60 +/- 242.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=141000, episode_reward=1576.20 +/- 256.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=142000, episode_reward=1697.20 +/- 90.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=143000, episode_reward=1638.40 +/- 131.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=144000, episode_reward=1700.40 +/- 270.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=145000, episode_reward=1555.00 +/- 206.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=146000, episode_reward=1465.60 +/- 207.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=147000, episode_reward=1623.20 +/- 147.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=148000, episode_reward=1539.80 +/- 391.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=149000, episode_reward=1593.80 +/- 272.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=150000, episode_reward=1439.60 +/- 237.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=151000, episode_reward=1752.60 +/- 160.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=152000, episode_reward=1674.40 +/- 294.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=153000, episode_reward=1531.80 +/- 171.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=154000, episode_reward=1444.20 +/- 437.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=155000, episode_reward=1615.20 +/- 318.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=156000, episode_reward=1383.00 +/- 167.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=157000, episode_reward=1648.80 +/- 275.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=158000, episode_reward=1792.40 +/- 169.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=159000, episode_reward=1815.80 +/- 155.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=160000, episode_reward=1625.40 +/- 350.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=161000, episode_reward=1504.20 +/- 276.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=162000, episode_reward=1621.40 +/- 220.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=163000, episode_reward=1637.40 +/- 176.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=164000, episode_reward=1462.20 +/- 260.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=165000, episode_reward=1370.00 +/- 320.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=166000, episode_reward=1542.20 +/- 209.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=167000, episode_reward=1476.60 +/- 335.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=168000, episode_reward=1632.80 +/- 46.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=169000, episode_reward=1600.80 +/- 231.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=170000, episode_reward=1686.00 +/- 203.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=171000, episode_reward=1612.20 +/- 219.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=172000, episode_reward=1558.40 +/- 171.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=173000, episode_reward=1673.60 +/- 118.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=174000, episode_reward=1592.20 +/- 260.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=175000, episode_reward=1659.80 +/- 335.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=176000, episode_reward=1738.80 +/- 221.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=177000, episode_reward=1509.60 +/- 104.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=178000, episode_reward=1580.60 +/- 402.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=179000, episode_reward=1463.60 +/- 213.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=180000, episode_reward=1660.60 +/- 109.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=181000, episode_reward=1732.40 +/- 203.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=182000, episode_reward=1734.40 +/- 63.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=183000, episode_reward=1302.60 +/- 323.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=184000, episode_reward=1758.80 +/- 142.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=185000, episode_reward=1694.20 +/- 171.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=186000, episode_reward=1641.80 +/- 207.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=187000, episode_reward=1588.80 +/- 330.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=188000, episode_reward=1414.80 +/- 227.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=189000, episode_reward=1354.00 +/- 269.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=190000, episode_reward=1453.40 +/- 90.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=191000, episode_reward=1712.80 +/- 161.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=192000, episode_reward=1552.20 +/- 206.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=193000, episode_reward=1560.20 +/- 346.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=194000, episode_reward=1690.80 +/- 307.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=195000, episode_reward=1843.40 +/- 139.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=196000, episode_reward=1481.00 +/- 306.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=197000, episode_reward=1662.40 +/- 275.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=198000, episode_reward=1513.80 +/- 296.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=199000, episode_reward=1596.80 +/- 212.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=200000, episode_reward=1630.00 +/- 216.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=201000, episode_reward=1623.80 +/- 175.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=202000, episode_reward=1672.80 +/- 112.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=203000, episode_reward=1480.00 +/- 196.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=204000, episode_reward=1696.40 +/- 165.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=205000, episode_reward=1721.40 +/- 192.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=206000, episode_reward=1814.00 +/- 115.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=207000, episode_reward=1589.60 +/- 73.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=208000, episode_reward=1572.20 +/- 159.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=209000, episode_reward=1744.40 +/- 128.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=210000, episode_reward=1505.40 +/- 226.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=211000, episode_reward=1690.80 +/- 257.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=212000, episode_reward=1552.40 +/- 169.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=213000, episode_reward=1596.00 +/- 407.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=214000, episode_reward=1623.40 +/- 69.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=215000, episode_reward=1535.40 +/- 214.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=216000, episode_reward=1637.80 +/- 320.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=217000, episode_reward=1727.40 +/- 226.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=218000, episode_reward=1714.60 +/- 180.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=219000, episode_reward=1456.60 +/- 328.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=220000, episode_reward=1777.00 +/- 133.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=221000, episode_reward=1538.20 +/- 395.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=222000, episode_reward=1410.00 +/- 341.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=223000, episode_reward=1570.00 +/- 269.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=224000, episode_reward=1739.80 +/- 195.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=225000, episode_reward=1663.40 +/- 104.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=226000, episode_reward=1572.40 +/- 131.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=227000, episode_reward=1571.00 +/- 327.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=228000, episode_reward=1814.60 +/- 129.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=229000, episode_reward=1614.60 +/- 284.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=230000, episode_reward=1819.00 +/- 238.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=231000, episode_reward=1607.60 +/- 285.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=232000, episode_reward=1858.00 +/- 224.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=233000, episode_reward=1701.00 +/- 120.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=234000, episode_reward=1414.60 +/- 182.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=235000, episode_reward=1764.00 +/- 163.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=236000, episode_reward=1611.80 +/- 155.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=237000, episode_reward=1682.00 +/- 323.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=238000, episode_reward=1658.20 +/- 200.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=239000, episode_reward=1325.00 +/- 244.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=240000, episode_reward=1810.20 +/- 180.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=241000, episode_reward=1601.40 +/- 132.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=242000, episode_reward=1498.80 +/- 178.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=243000, episode_reward=1789.80 +/- 70.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=244000, episode_reward=1756.40 +/- 246.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=245000, episode_reward=1596.20 +/- 152.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=246000, episode_reward=1753.00 +/- 68.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=247000, episode_reward=1542.40 +/- 331.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=248000, episode_reward=1605.40 +/- 125.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=249000, episode_reward=1764.80 +/- 142.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=250000, episode_reward=1504.60 +/- 222.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=251000, episode_reward=1577.60 +/- 259.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=252000, episode_reward=1637.40 +/- 282.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=253000, episode_reward=1656.40 +/- 150.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=254000, episode_reward=1493.40 +/- 406.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=255000, episode_reward=1638.40 +/- 267.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=256000, episode_reward=1497.00 +/- 300.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=257000, episode_reward=1717.00 +/- 176.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=258000, episode_reward=1768.00 +/- 122.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=259000, episode_reward=1731.40 +/- 230.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=260000, episode_reward=1605.80 +/- 372.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=261000, episode_reward=1553.80 +/- 191.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=262000, episode_reward=1737.60 +/- 376.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=263000, episode_reward=1598.00 +/- 176.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=264000, episode_reward=1573.40 +/- 230.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=265000, episode_reward=1617.00 +/- 282.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=266000, episode_reward=1702.20 +/- 170.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=267000, episode_reward=1612.40 +/- 86.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=268000, episode_reward=1672.20 +/- 199.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=269000, episode_reward=1472.60 +/- 204.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=270000, episode_reward=1578.20 +/- 331.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=271000, episode_reward=1654.60 +/- 174.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=272000, episode_reward=1674.80 +/- 183.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=273000, episode_reward=1466.40 +/- 302.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=274000, episode_reward=1679.20 +/- 247.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=275000, episode_reward=1765.20 +/- 160.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=276000, episode_reward=1659.40 +/- 287.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=277000, episode_reward=1672.80 +/- 232.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=278000, episode_reward=1719.60 +/- 172.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=279000, episode_reward=1693.80 +/- 113.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=280000, episode_reward=1629.80 +/- 289.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=281000, episode_reward=1722.40 +/- 236.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=282000, episode_reward=1739.80 +/- 120.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=283000, episode_reward=1673.80 +/- 308.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=284000, episode_reward=1746.80 +/- 147.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=285000, episode_reward=1668.60 +/- 221.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=286000, episode_reward=1606.80 +/- 190.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=287000, episode_reward=1570.00 +/- 259.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=288000, episode_reward=1801.80 +/- 148.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=289000, episode_reward=1429.00 +/- 213.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=290000, episode_reward=1540.80 +/- 361.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=291000, episode_reward=1770.20 +/- 147.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=292000, episode_reward=1508.40 +/- 334.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=293000, episode_reward=1575.00 +/- 185.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=294000, episode_reward=1844.20 +/- 97.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=295000, episode_reward=1601.00 +/- 167.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=296000, episode_reward=1777.80 +/- 72.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=297000, episode_reward=1745.80 +/- 75.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=298000, episode_reward=1781.80 +/- 124.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=299000, episode_reward=1637.00 +/- 240.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=300000, episode_reward=1643.00 +/- 241.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=301000, episode_reward=1682.00 +/- 183.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=302000, episode_reward=1774.40 +/- 195.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=303000, episode_reward=1542.20 +/- 207.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=304000, episode_reward=1670.00 +/- 223.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=305000, episode_reward=1503.20 +/- 190.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=306000, episode_reward=1748.80 +/- 143.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=307000, episode_reward=1567.80 +/- 200.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=308000, episode_reward=1643.40 +/- 163.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=309000, episode_reward=1621.60 +/- 252.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=310000, episode_reward=1460.00 +/- 410.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=311000, episode_reward=1650.60 +/- 235.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=312000, episode_reward=1612.60 +/- 343.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=313000, episode_reward=1594.20 +/- 156.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=314000, episode_reward=1739.00 +/- 52.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=315000, episode_reward=1676.20 +/- 208.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=316000, episode_reward=1588.20 +/- 214.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=317000, episode_reward=1493.40 +/- 357.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=318000, episode_reward=1766.80 +/- 107.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=319000, episode_reward=1474.40 +/- 295.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=320000, episode_reward=1946.40 +/- 172.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=321000, episode_reward=1707.60 +/- 264.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=322000, episode_reward=1448.60 +/- 160.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=323000, episode_reward=1715.80 +/- 195.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=324000, episode_reward=1780.80 +/- 217.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=325000, episode_reward=1999.60 +/- 89.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=326000, episode_reward=1720.00 +/- 247.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=327000, episode_reward=1540.00 +/- 185.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=328000, episode_reward=1618.80 +/- 203.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=329000, episode_reward=1719.00 +/- 152.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=330000, episode_reward=1683.60 +/- 256.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=331000, episode_reward=1679.00 +/- 199.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=332000, episode_reward=1599.80 +/- 252.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=333000, episode_reward=1655.40 +/- 339.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=334000, episode_reward=1579.00 +/- 317.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=335000, episode_reward=1638.80 +/- 286.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=336000, episode_reward=1819.80 +/- 126.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=337000, episode_reward=1625.40 +/- 128.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=338000, episode_reward=1689.00 +/- 168.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=339000, episode_reward=1555.60 +/- 203.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=340000, episode_reward=1738.80 +/- 198.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=341000, episode_reward=1890.40 +/- 218.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=342000, episode_reward=1763.00 +/- 246.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=343000, episode_reward=1452.20 +/- 245.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=344000, episode_reward=1758.20 +/- 146.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=345000, episode_reward=1658.80 +/- 231.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=346000, episode_reward=1531.60 +/- 111.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=347000, episode_reward=1642.80 +/- 226.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=348000, episode_reward=1735.60 +/- 89.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=349000, episode_reward=1662.80 +/- 281.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=350000, episode_reward=1548.20 +/- 333.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=351000, episode_reward=1665.20 +/- 90.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=352000, episode_reward=1602.00 +/- 200.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=353000, episode_reward=1861.80 +/- 125.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=354000, episode_reward=1644.80 +/- 141.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=355000, episode_reward=1545.60 +/- 315.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=356000, episode_reward=1583.00 +/- 225.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=357000, episode_reward=1840.40 +/- 217.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=358000, episode_reward=1696.80 +/- 323.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=359000, episode_reward=1806.80 +/- 229.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=360000, episode_reward=1507.40 +/- 376.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=361000, episode_reward=1919.20 +/- 78.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=362000, episode_reward=1786.60 +/- 102.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=363000, episode_reward=1690.80 +/- 65.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=364000, episode_reward=1724.40 +/- 139.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=365000, episode_reward=1447.60 +/- 282.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=366000, episode_reward=1605.80 +/- 44.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=367000, episode_reward=1647.40 +/- 153.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=368000, episode_reward=1631.60 +/- 302.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=369000, episode_reward=1755.20 +/- 124.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=370000, episode_reward=1694.40 +/- 176.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=371000, episode_reward=1684.00 +/- 252.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=372000, episode_reward=1614.40 +/- 166.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=373000, episode_reward=1851.60 +/- 135.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=374000, episode_reward=1798.00 +/- 147.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=375000, episode_reward=1698.00 +/- 149.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=376000, episode_reward=1679.00 +/- 186.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=377000, episode_reward=1301.80 +/- 322.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=378000, episode_reward=1505.00 +/- 150.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=379000, episode_reward=1621.40 +/- 119.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=380000, episode_reward=1606.20 +/- 262.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=381000, episode_reward=1722.80 +/- 92.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=382000, episode_reward=1756.20 +/- 141.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=383000, episode_reward=1623.60 +/- 294.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=384000, episode_reward=1632.20 +/- 270.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=385000, episode_reward=1532.60 +/- 170.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=386000, episode_reward=1567.40 +/- 205.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=387000, episode_reward=1629.80 +/- 195.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=388000, episode_reward=1486.40 +/- 309.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=389000, episode_reward=1690.40 +/- 178.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=390000, episode_reward=1634.00 +/- 138.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=391000, episode_reward=1439.40 +/- 41.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=392000, episode_reward=1580.60 +/- 120.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=393000, episode_reward=1790.00 +/- 205.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=394000, episode_reward=1651.20 +/- 170.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=395000, episode_reward=1691.60 +/- 212.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=396000, episode_reward=1674.20 +/- 265.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=397000, episode_reward=1583.20 +/- 117.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=398000, episode_reward=1689.60 +/- 185.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=399000, episode_reward=1747.20 +/- 247.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=400000, episode_reward=1533.00 +/- 196.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=401000, episode_reward=1455.00 +/- 131.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=402000, episode_reward=1631.80 +/- 169.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=403000, episode_reward=1298.40 +/- 207.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=404000, episode_reward=1606.80 +/- 176.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=405000, episode_reward=1767.40 +/- 159.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=406000, episode_reward=1521.40 +/- 170.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=407000, episode_reward=1683.00 +/- 246.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=408000, episode_reward=1662.60 +/- 145.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=409000, episode_reward=1717.80 +/- 151.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=410000, episode_reward=1571.40 +/- 226.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=411000, episode_reward=1727.00 +/- 182.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=412000, episode_reward=1778.40 +/- 178.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=413000, episode_reward=1329.80 +/- 164.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=414000, episode_reward=1684.00 +/- 361.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=415000, episode_reward=1675.60 +/- 166.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=416000, episode_reward=1260.40 +/- 458.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=417000, episode_reward=1590.40 +/- 282.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=418000, episode_reward=1584.20 +/- 263.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=419000, episode_reward=1674.60 +/- 154.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=420000, episode_reward=1579.00 +/- 116.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=421000, episode_reward=1702.40 +/- 270.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=422000, episode_reward=1613.00 +/- 278.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=423000, episode_reward=1674.20 +/- 102.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=424000, episode_reward=1687.20 +/- 103.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=425000, episode_reward=1682.60 +/- 263.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=426000, episode_reward=1814.00 +/- 94.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=427000, episode_reward=1666.40 +/- 186.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=428000, episode_reward=1458.40 +/- 179.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=429000, episode_reward=1526.20 +/- 210.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=430000, episode_reward=1740.60 +/- 124.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=431000, episode_reward=1602.60 +/- 186.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=432000, episode_reward=1580.60 +/- 189.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=433000, episode_reward=1581.80 +/- 357.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=434000, episode_reward=1590.00 +/- 241.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=435000, episode_reward=1767.00 +/- 138.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=436000, episode_reward=1656.60 +/- 129.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=437000, episode_reward=1668.20 +/- 213.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=438000, episode_reward=1488.60 +/- 303.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=439000, episode_reward=1561.00 +/- 168.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=440000, episode_reward=1719.40 +/- 213.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=441000, episode_reward=1538.60 +/- 142.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=442000, episode_reward=1417.60 +/- 349.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=443000, episode_reward=1632.00 +/- 130.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=444000, episode_reward=1470.20 +/- 323.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=445000, episode_reward=1477.00 +/- 273.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=446000, episode_reward=1530.60 +/- 238.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=447000, episode_reward=1581.40 +/- 37.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=448000, episode_reward=1614.80 +/- 208.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=449000, episode_reward=1718.00 +/- 280.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=450000, episode_reward=1719.20 +/- 169.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=451000, episode_reward=1856.60 +/- 152.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=452000, episode_reward=1668.60 +/- 190.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=453000, episode_reward=1651.60 +/- 261.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=454000, episode_reward=1548.80 +/- 261.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=455000, episode_reward=1723.20 +/- 263.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=456000, episode_reward=1450.00 +/- 392.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=457000, episode_reward=1725.60 +/- 151.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=458000, episode_reward=1551.80 +/- 281.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=459000, episode_reward=1589.60 +/- 232.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=460000, episode_reward=1566.20 +/- 153.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=461000, episode_reward=1357.60 +/- 169.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=462000, episode_reward=1545.80 +/- 288.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=463000, episode_reward=1597.00 +/- 312.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=464000, episode_reward=1610.20 +/- 193.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=465000, episode_reward=1534.40 +/- 228.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=466000, episode_reward=1734.80 +/- 138.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=467000, episode_reward=1850.40 +/- 81.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=468000, episode_reward=1473.00 +/- 267.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=469000, episode_reward=1539.00 +/- 167.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=470000, episode_reward=1487.60 +/- 214.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=471000, episode_reward=1508.60 +/- 156.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=472000, episode_reward=1228.40 +/- 586.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=473000, episode_reward=1586.20 +/- 306.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=474000, episode_reward=1757.80 +/- 136.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=475000, episode_reward=1594.40 +/- 193.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=476000, episode_reward=1679.00 +/- 176.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=477000, episode_reward=1658.20 +/- 150.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=478000, episode_reward=1718.00 +/- 220.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=479000, episode_reward=1620.40 +/- 153.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=480000, episode_reward=1708.60 +/- 157.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=481000, episode_reward=1633.00 +/- 194.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=482000, episode_reward=1559.40 +/- 322.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=483000, episode_reward=1394.60 +/- 356.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=484000, episode_reward=1731.80 +/- 58.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=485000, episode_reward=1517.00 +/- 316.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=486000, episode_reward=1400.60 +/- 270.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=487000, episode_reward=1517.20 +/- 232.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=488000, episode_reward=1652.80 +/- 198.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=489000, episode_reward=1751.60 +/- 125.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=490000, episode_reward=1587.60 +/- 205.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=491000, episode_reward=1655.20 +/- 208.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=492000, episode_reward=1703.60 +/- 333.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=493000, episode_reward=1653.20 +/- 323.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=494000, episode_reward=1559.60 +/- 419.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=495000, episode_reward=1749.20 +/- 166.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=496000, episode_reward=1787.40 +/- 164.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=497000, episode_reward=1673.40 +/- 145.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=498000, episode_reward=1629.80 +/- 215.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=499000, episode_reward=1639.80 +/- 192.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=500000, episode_reward=1628.60 +/- 84.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=501000, episode_reward=1862.80 +/- 77.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Gamma = 0.8, Learning Rate = 0.003, Activation Function = ReLU, Neurons = 64, Process noise = 0.1, Measurement noise = 0.0\n",
      "Eval num_timesteps=1000, episode_reward=-2332.80 +/- 45.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=-2477.80 +/- 56.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=3000, episode_reward=-168.60 +/- 110.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4000, episode_reward=-194.80 +/- 173.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=5000, episode_reward=-709.60 +/- 80.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=6000, episode_reward=-593.20 +/- 117.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=7000, episode_reward=209.80 +/- 91.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=8000, episode_reward=324.20 +/- 122.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=9000, episode_reward=304.80 +/- 200.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=10000, episode_reward=436.80 +/- 122.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=11000, episode_reward=238.60 +/- 166.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=12000, episode_reward=361.00 +/- 96.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=13000, episode_reward=233.80 +/- 173.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=14000, episode_reward=278.40 +/- 207.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=15000, episode_reward=398.80 +/- 136.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=16000, episode_reward=377.00 +/- 132.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=17000, episode_reward=253.00 +/- 105.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=18000, episode_reward=227.80 +/- 114.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=19000, episode_reward=877.80 +/- 267.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=20000, episode_reward=630.60 +/- 139.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=21000, episode_reward=1097.20 +/- 249.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=22000, episode_reward=965.80 +/- 262.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=23000, episode_reward=1031.80 +/- 128.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=24000, episode_reward=1057.40 +/- 307.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=25000, episode_reward=679.20 +/- 255.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=26000, episode_reward=1135.60 +/- 136.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=27000, episode_reward=1108.00 +/- 308.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=28000, episode_reward=1177.80 +/- 185.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=29000, episode_reward=1382.40 +/- 72.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=30000, episode_reward=1256.00 +/- 123.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=31000, episode_reward=1450.40 +/- 194.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=32000, episode_reward=1175.40 +/- 286.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=33000, episode_reward=1336.60 +/- 274.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=34000, episode_reward=1138.20 +/- 373.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=35000, episode_reward=1652.60 +/- 119.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=36000, episode_reward=1487.20 +/- 131.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=37000, episode_reward=1490.40 +/- 382.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=38000, episode_reward=1462.20 +/- 240.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=39000, episode_reward=1338.40 +/- 233.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=40000, episode_reward=1574.00 +/- 253.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=41000, episode_reward=1596.00 +/- 200.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=42000, episode_reward=1581.20 +/- 230.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=43000, episode_reward=1395.60 +/- 222.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=44000, episode_reward=1363.80 +/- 224.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=45000, episode_reward=1346.00 +/- 187.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=46000, episode_reward=1635.80 +/- 158.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=47000, episode_reward=1399.40 +/- 335.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=48000, episode_reward=1610.20 +/- 283.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=49000, episode_reward=1409.40 +/- 204.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=50000, episode_reward=1587.60 +/- 209.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=51000, episode_reward=1659.60 +/- 138.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=52000, episode_reward=1361.00 +/- 288.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=53000, episode_reward=1802.00 +/- 171.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=54000, episode_reward=1510.00 +/- 169.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=55000, episode_reward=1626.40 +/- 280.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=56000, episode_reward=1746.20 +/- 306.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=57000, episode_reward=1533.20 +/- 268.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=58000, episode_reward=1508.40 +/- 315.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=59000, episode_reward=1506.20 +/- 268.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=60000, episode_reward=1356.20 +/- 153.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=61000, episode_reward=1286.00 +/- 318.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=62000, episode_reward=1782.40 +/- 141.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=63000, episode_reward=1492.00 +/- 167.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=64000, episode_reward=1584.40 +/- 259.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=65000, episode_reward=1623.00 +/- 315.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=66000, episode_reward=1498.80 +/- 175.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=67000, episode_reward=1482.40 +/- 188.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=68000, episode_reward=1215.60 +/- 186.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=69000, episode_reward=1492.60 +/- 211.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=70000, episode_reward=1795.80 +/- 114.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=71000, episode_reward=1699.80 +/- 215.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=72000, episode_reward=1550.80 +/- 299.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=73000, episode_reward=1479.20 +/- 141.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=74000, episode_reward=1654.40 +/- 259.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=75000, episode_reward=1448.20 +/- 168.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=76000, episode_reward=1290.60 +/- 243.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=77000, episode_reward=1406.40 +/- 520.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=78000, episode_reward=1616.40 +/- 148.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=79000, episode_reward=1532.00 +/- 260.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=80000, episode_reward=1553.40 +/- 315.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=81000, episode_reward=1343.20 +/- 255.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=82000, episode_reward=1591.60 +/- 189.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=83000, episode_reward=1651.80 +/- 82.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=84000, episode_reward=1445.20 +/- 250.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=85000, episode_reward=1621.20 +/- 215.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=86000, episode_reward=1534.00 +/- 327.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=87000, episode_reward=1477.20 +/- 314.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=88000, episode_reward=1169.60 +/- 364.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=89000, episode_reward=1643.20 +/- 220.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=90000, episode_reward=1562.20 +/- 261.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=91000, episode_reward=1568.00 +/- 93.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=92000, episode_reward=1546.80 +/- 239.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=93000, episode_reward=1539.00 +/- 197.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=94000, episode_reward=1656.20 +/- 135.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=95000, episode_reward=1647.60 +/- 181.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=96000, episode_reward=1441.00 +/- 240.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=97000, episode_reward=1576.20 +/- 159.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=98000, episode_reward=1367.20 +/- 290.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=99000, episode_reward=1391.40 +/- 253.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=100000, episode_reward=1553.00 +/- 258.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=101000, episode_reward=1303.20 +/- 140.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=102000, episode_reward=1532.20 +/- 361.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=103000, episode_reward=1615.20 +/- 194.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=104000, episode_reward=1563.20 +/- 144.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=105000, episode_reward=1500.40 +/- 100.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=106000, episode_reward=1518.20 +/- 338.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=107000, episode_reward=1681.60 +/- 256.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=108000, episode_reward=1422.00 +/- 170.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=109000, episode_reward=1407.40 +/- 217.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=110000, episode_reward=1428.40 +/- 220.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=111000, episode_reward=1593.00 +/- 40.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=112000, episode_reward=1557.80 +/- 147.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=113000, episode_reward=1426.40 +/- 273.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=114000, episode_reward=1478.20 +/- 131.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=115000, episode_reward=1438.00 +/- 468.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=116000, episode_reward=1409.40 +/- 189.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=117000, episode_reward=1194.00 +/- 248.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=118000, episode_reward=1648.20 +/- 88.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=119000, episode_reward=1695.00 +/- 224.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=120000, episode_reward=1390.20 +/- 290.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=121000, episode_reward=1744.60 +/- 219.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=122000, episode_reward=1547.40 +/- 263.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=123000, episode_reward=1543.20 +/- 239.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=124000, episode_reward=1481.40 +/- 190.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=125000, episode_reward=1827.60 +/- 185.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=126000, episode_reward=1507.80 +/- 246.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=127000, episode_reward=1615.60 +/- 158.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=128000, episode_reward=1524.40 +/- 251.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=129000, episode_reward=1639.60 +/- 136.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=130000, episode_reward=1514.40 +/- 209.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=131000, episode_reward=1476.40 +/- 280.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=132000, episode_reward=1348.40 +/- 290.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=133000, episode_reward=1603.20 +/- 257.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=134000, episode_reward=1450.60 +/- 227.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=135000, episode_reward=1696.00 +/- 231.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=136000, episode_reward=1487.20 +/- 216.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=137000, episode_reward=1695.00 +/- 218.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=138000, episode_reward=1761.80 +/- 96.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=139000, episode_reward=1469.60 +/- 232.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=140000, episode_reward=1710.60 +/- 149.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=141000, episode_reward=1682.60 +/- 122.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=142000, episode_reward=1753.40 +/- 160.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=143000, episode_reward=1458.80 +/- 236.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=144000, episode_reward=1557.80 +/- 188.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=145000, episode_reward=1750.20 +/- 208.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=146000, episode_reward=1736.20 +/- 92.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=147000, episode_reward=1473.00 +/- 281.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=148000, episode_reward=1709.40 +/- 132.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=149000, episode_reward=1573.40 +/- 123.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=150000, episode_reward=1456.20 +/- 230.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=151000, episode_reward=1585.20 +/- 98.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=152000, episode_reward=1529.20 +/- 295.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=153000, episode_reward=1539.80 +/- 268.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=154000, episode_reward=1634.20 +/- 267.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=155000, episode_reward=1657.40 +/- 255.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=156000, episode_reward=1490.20 +/- 185.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=157000, episode_reward=1637.60 +/- 274.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=158000, episode_reward=1712.40 +/- 194.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=159000, episode_reward=1402.20 +/- 155.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=160000, episode_reward=1527.40 +/- 495.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=161000, episode_reward=1664.40 +/- 118.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=162000, episode_reward=1779.80 +/- 188.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=163000, episode_reward=1639.20 +/- 147.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=164000, episode_reward=1725.60 +/- 133.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=165000, episode_reward=1473.00 +/- 179.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=166000, episode_reward=1861.20 +/- 142.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=167000, episode_reward=1751.40 +/- 221.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=168000, episode_reward=1766.80 +/- 138.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=169000, episode_reward=1575.00 +/- 255.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=170000, episode_reward=1738.60 +/- 103.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=171000, episode_reward=1716.60 +/- 219.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=172000, episode_reward=1392.40 +/- 223.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=173000, episode_reward=1351.20 +/- 224.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=174000, episode_reward=1630.00 +/- 291.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=175000, episode_reward=1813.80 +/- 340.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=176000, episode_reward=1614.20 +/- 302.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=177000, episode_reward=1747.20 +/- 98.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=178000, episode_reward=1552.80 +/- 168.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=179000, episode_reward=1518.40 +/- 220.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=180000, episode_reward=1519.40 +/- 320.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=181000, episode_reward=1543.60 +/- 238.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=182000, episode_reward=1762.60 +/- 230.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=183000, episode_reward=1698.20 +/- 315.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=184000, episode_reward=1473.40 +/- 298.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=185000, episode_reward=1533.00 +/- 118.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=186000, episode_reward=1568.20 +/- 145.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=187000, episode_reward=1657.80 +/- 225.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=188000, episode_reward=1658.20 +/- 253.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=189000, episode_reward=1592.40 +/- 157.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=190000, episode_reward=1460.20 +/- 191.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=191000, episode_reward=1779.00 +/- 153.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=192000, episode_reward=1565.00 +/- 193.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=193000, episode_reward=1521.40 +/- 240.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=194000, episode_reward=1267.00 +/- 327.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=195000, episode_reward=1400.40 +/- 208.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=196000, episode_reward=1505.40 +/- 117.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=197000, episode_reward=1788.40 +/- 27.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=198000, episode_reward=1467.60 +/- 353.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=199000, episode_reward=1525.20 +/- 370.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=200000, episode_reward=1503.20 +/- 180.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=201000, episode_reward=1602.00 +/- 112.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=202000, episode_reward=1444.80 +/- 353.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=203000, episode_reward=1418.60 +/- 495.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=204000, episode_reward=1617.80 +/- 224.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=205000, episode_reward=1525.20 +/- 303.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=206000, episode_reward=1596.80 +/- 139.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=207000, episode_reward=1526.60 +/- 227.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=208000, episode_reward=1517.40 +/- 175.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=209000, episode_reward=1790.60 +/- 173.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=210000, episode_reward=1666.40 +/- 274.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=211000, episode_reward=1810.60 +/- 187.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=212000, episode_reward=1504.20 +/- 158.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=213000, episode_reward=1401.20 +/- 239.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=214000, episode_reward=1581.60 +/- 260.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=215000, episode_reward=1424.20 +/- 265.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=216000, episode_reward=1509.00 +/- 171.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=217000, episode_reward=1544.40 +/- 301.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=218000, episode_reward=1567.40 +/- 279.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=219000, episode_reward=1646.00 +/- 132.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=220000, episode_reward=1577.00 +/- 148.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=221000, episode_reward=1429.20 +/- 232.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=222000, episode_reward=1516.40 +/- 257.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=223000, episode_reward=1626.80 +/- 336.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=224000, episode_reward=1474.80 +/- 199.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=225000, episode_reward=1612.00 +/- 153.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=226000, episode_reward=1471.40 +/- 262.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=227000, episode_reward=1512.60 +/- 326.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=228000, episode_reward=1427.20 +/- 233.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=229000, episode_reward=1474.80 +/- 267.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=230000, episode_reward=1629.00 +/- 210.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=231000, episode_reward=1575.00 +/- 174.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=232000, episode_reward=1450.40 +/- 225.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=233000, episode_reward=1598.20 +/- 150.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=234000, episode_reward=1602.80 +/- 112.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=235000, episode_reward=1659.80 +/- 148.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=236000, episode_reward=1422.20 +/- 314.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=237000, episode_reward=1738.20 +/- 67.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=238000, episode_reward=1491.60 +/- 189.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=239000, episode_reward=1669.80 +/- 148.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=240000, episode_reward=1459.20 +/- 356.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=241000, episode_reward=1658.20 +/- 263.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=242000, episode_reward=1813.80 +/- 168.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=243000, episode_reward=1564.80 +/- 203.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=244000, episode_reward=1691.20 +/- 119.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=245000, episode_reward=1636.80 +/- 92.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=246000, episode_reward=1437.40 +/- 338.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=247000, episode_reward=1652.60 +/- 207.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=248000, episode_reward=1491.20 +/- 241.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=249000, episode_reward=1542.40 +/- 109.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=250000, episode_reward=1655.80 +/- 107.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=251000, episode_reward=1776.00 +/- 160.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=252000, episode_reward=1544.20 +/- 245.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=253000, episode_reward=1617.40 +/- 309.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=254000, episode_reward=1727.60 +/- 211.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=255000, episode_reward=1559.00 +/- 164.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=256000, episode_reward=1640.00 +/- 288.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=257000, episode_reward=1563.00 +/- 313.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=258000, episode_reward=1452.80 +/- 270.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=259000, episode_reward=1560.00 +/- 290.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=260000, episode_reward=1419.60 +/- 246.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=261000, episode_reward=1650.60 +/- 228.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=262000, episode_reward=1562.00 +/- 134.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=263000, episode_reward=1360.20 +/- 246.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=264000, episode_reward=1590.20 +/- 140.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=265000, episode_reward=1716.80 +/- 146.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=266000, episode_reward=1604.20 +/- 323.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=267000, episode_reward=1620.00 +/- 219.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=268000, episode_reward=1644.20 +/- 182.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=269000, episode_reward=1638.60 +/- 197.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=270000, episode_reward=1602.60 +/- 293.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=271000, episode_reward=1459.20 +/- 170.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=272000, episode_reward=1690.20 +/- 250.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=273000, episode_reward=1350.60 +/- 238.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=274000, episode_reward=1593.40 +/- 275.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=275000, episode_reward=1425.60 +/- 298.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=276000, episode_reward=1681.80 +/- 173.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=277000, episode_reward=1595.20 +/- 204.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=278000, episode_reward=1763.60 +/- 51.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=279000, episode_reward=1613.20 +/- 323.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=280000, episode_reward=1666.80 +/- 208.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=281000, episode_reward=1412.00 +/- 301.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=282000, episode_reward=1713.20 +/- 281.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=283000, episode_reward=1576.40 +/- 244.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=284000, episode_reward=1727.20 +/- 200.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=285000, episode_reward=1679.60 +/- 171.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=286000, episode_reward=1683.40 +/- 243.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=287000, episode_reward=1615.80 +/- 211.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=288000, episode_reward=1641.40 +/- 190.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=289000, episode_reward=1606.20 +/- 127.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=290000, episode_reward=1595.80 +/- 191.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=291000, episode_reward=1481.40 +/- 257.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=292000, episode_reward=1665.40 +/- 367.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=293000, episode_reward=1530.60 +/- 307.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=294000, episode_reward=1566.80 +/- 99.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=295000, episode_reward=1597.80 +/- 211.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=296000, episode_reward=1560.40 +/- 183.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=297000, episode_reward=1445.00 +/- 127.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=298000, episode_reward=1552.40 +/- 523.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=299000, episode_reward=1738.00 +/- 189.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=300000, episode_reward=1449.60 +/- 199.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=301000, episode_reward=1660.20 +/- 262.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=302000, episode_reward=1586.60 +/- 347.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=303000, episode_reward=1529.40 +/- 217.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=304000, episode_reward=1551.00 +/- 233.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=305000, episode_reward=1650.00 +/- 156.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=306000, episode_reward=1468.80 +/- 133.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=307000, episode_reward=1675.00 +/- 118.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=308000, episode_reward=1638.60 +/- 394.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=309000, episode_reward=1369.80 +/- 548.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=310000, episode_reward=1491.40 +/- 326.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=311000, episode_reward=1686.00 +/- 71.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=312000, episode_reward=1536.60 +/- 118.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=313000, episode_reward=1590.00 +/- 150.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=314000, episode_reward=1695.20 +/- 107.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=315000, episode_reward=1672.40 +/- 157.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=316000, episode_reward=1743.20 +/- 122.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=317000, episode_reward=1680.40 +/- 194.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=318000, episode_reward=1790.40 +/- 233.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=319000, episode_reward=1472.20 +/- 201.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=320000, episode_reward=1568.60 +/- 269.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=321000, episode_reward=1687.40 +/- 252.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=322000, episode_reward=1395.80 +/- 229.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=323000, episode_reward=1558.80 +/- 150.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=324000, episode_reward=1751.00 +/- 177.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=325000, episode_reward=1523.40 +/- 148.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=326000, episode_reward=1528.20 +/- 240.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=327000, episode_reward=1693.20 +/- 240.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=328000, episode_reward=1469.20 +/- 204.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=329000, episode_reward=1451.20 +/- 113.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=330000, episode_reward=1572.00 +/- 442.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=331000, episode_reward=1629.00 +/- 220.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=332000, episode_reward=1407.20 +/- 253.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=333000, episode_reward=1545.80 +/- 148.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=334000, episode_reward=1299.60 +/- 224.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=335000, episode_reward=1636.00 +/- 181.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=336000, episode_reward=1661.80 +/- 240.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=337000, episode_reward=1795.80 +/- 245.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=338000, episode_reward=1528.60 +/- 174.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=339000, episode_reward=1621.00 +/- 238.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=340000, episode_reward=1620.20 +/- 156.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=341000, episode_reward=1671.00 +/- 233.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=342000, episode_reward=1533.60 +/- 149.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=343000, episode_reward=1573.80 +/- 126.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=344000, episode_reward=1675.40 +/- 177.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=345000, episode_reward=1388.80 +/- 316.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=346000, episode_reward=1468.60 +/- 155.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=347000, episode_reward=1322.40 +/- 175.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=348000, episode_reward=1632.40 +/- 139.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=349000, episode_reward=1679.60 +/- 169.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=350000, episode_reward=1664.60 +/- 317.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=351000, episode_reward=1712.20 +/- 179.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=352000, episode_reward=1689.20 +/- 241.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=353000, episode_reward=1432.60 +/- 63.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=354000, episode_reward=1698.60 +/- 132.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=355000, episode_reward=1436.20 +/- 217.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=356000, episode_reward=1623.40 +/- 314.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=357000, episode_reward=1592.60 +/- 183.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=358000, episode_reward=1588.80 +/- 176.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=359000, episode_reward=1681.00 +/- 215.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=360000, episode_reward=1783.80 +/- 334.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=361000, episode_reward=1627.00 +/- 189.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=362000, episode_reward=1787.80 +/- 227.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=363000, episode_reward=1375.60 +/- 396.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=364000, episode_reward=1665.60 +/- 164.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=365000, episode_reward=1464.60 +/- 581.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=366000, episode_reward=1286.20 +/- 202.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=367000, episode_reward=1570.00 +/- 312.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=368000, episode_reward=1630.00 +/- 273.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=369000, episode_reward=1499.20 +/- 314.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=370000, episode_reward=1603.60 +/- 228.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=371000, episode_reward=1697.80 +/- 181.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=372000, episode_reward=1731.20 +/- 46.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=373000, episode_reward=1561.20 +/- 234.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=374000, episode_reward=1235.20 +/- 340.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=375000, episode_reward=1331.80 +/- 179.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=376000, episode_reward=1713.80 +/- 102.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=377000, episode_reward=1686.80 +/- 243.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=378000, episode_reward=1793.60 +/- 155.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=379000, episode_reward=1749.60 +/- 232.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=380000, episode_reward=1512.00 +/- 265.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=381000, episode_reward=1453.80 +/- 152.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=382000, episode_reward=1376.60 +/- 345.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=383000, episode_reward=1673.60 +/- 141.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=384000, episode_reward=1570.40 +/- 356.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=385000, episode_reward=1486.80 +/- 548.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=386000, episode_reward=1442.40 +/- 176.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=387000, episode_reward=1634.60 +/- 189.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=388000, episode_reward=1563.20 +/- 214.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=389000, episode_reward=1693.60 +/- 239.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=390000, episode_reward=1528.80 +/- 329.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=391000, episode_reward=1716.40 +/- 221.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=392000, episode_reward=1560.00 +/- 381.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=393000, episode_reward=1489.20 +/- 320.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=394000, episode_reward=1401.20 +/- 300.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=395000, episode_reward=1662.60 +/- 134.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=396000, episode_reward=1525.60 +/- 208.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=397000, episode_reward=1506.60 +/- 289.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=398000, episode_reward=1645.40 +/- 334.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=399000, episode_reward=1537.00 +/- 261.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=400000, episode_reward=1657.40 +/- 209.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=401000, episode_reward=1584.20 +/- 207.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=402000, episode_reward=1545.80 +/- 266.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=403000, episode_reward=1710.00 +/- 209.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=404000, episode_reward=1633.80 +/- 212.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=405000, episode_reward=1395.80 +/- 383.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=406000, episode_reward=1657.80 +/- 214.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=407000, episode_reward=1695.60 +/- 286.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=408000, episode_reward=1558.00 +/- 233.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=409000, episode_reward=1699.80 +/- 277.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=410000, episode_reward=1473.80 +/- 314.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=411000, episode_reward=1768.00 +/- 92.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=412000, episode_reward=1732.00 +/- 237.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=413000, episode_reward=1656.00 +/- 279.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=414000, episode_reward=1433.00 +/- 125.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=415000, episode_reward=1682.80 +/- 338.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=416000, episode_reward=1652.20 +/- 224.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=417000, episode_reward=1457.80 +/- 319.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=418000, episode_reward=1602.20 +/- 61.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=419000, episode_reward=1567.80 +/- 227.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=420000, episode_reward=1638.60 +/- 104.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=421000, episode_reward=1730.20 +/- 324.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=422000, episode_reward=1794.80 +/- 72.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=423000, episode_reward=1555.00 +/- 236.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=424000, episode_reward=1565.40 +/- 255.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=425000, episode_reward=1704.20 +/- 130.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=426000, episode_reward=1645.40 +/- 174.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=427000, episode_reward=1683.00 +/- 255.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=428000, episode_reward=1422.00 +/- 198.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=429000, episode_reward=1448.60 +/- 168.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=430000, episode_reward=1765.80 +/- 84.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=431000, episode_reward=1627.60 +/- 226.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=432000, episode_reward=1836.20 +/- 210.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=433000, episode_reward=1492.40 +/- 406.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=434000, episode_reward=1326.80 +/- 408.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=435000, episode_reward=1589.00 +/- 140.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=436000, episode_reward=1559.20 +/- 272.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=437000, episode_reward=1639.60 +/- 224.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=438000, episode_reward=1654.40 +/- 163.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=439000, episode_reward=1692.60 +/- 265.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=440000, episode_reward=1439.40 +/- 382.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=441000, episode_reward=1690.00 +/- 255.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=442000, episode_reward=1459.60 +/- 177.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=443000, episode_reward=1553.20 +/- 226.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=444000, episode_reward=1595.80 +/- 246.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=445000, episode_reward=1582.20 +/- 316.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=446000, episode_reward=1629.60 +/- 200.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=447000, episode_reward=1672.00 +/- 218.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=448000, episode_reward=1427.20 +/- 237.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=449000, episode_reward=1806.00 +/- 126.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=450000, episode_reward=1910.00 +/- 186.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=451000, episode_reward=1757.80 +/- 191.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=452000, episode_reward=1587.80 +/- 163.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=453000, episode_reward=1702.20 +/- 130.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=454000, episode_reward=1552.20 +/- 316.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=455000, episode_reward=1803.00 +/- 202.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=456000, episode_reward=1722.40 +/- 176.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=457000, episode_reward=1868.60 +/- 121.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=458000, episode_reward=1666.40 +/- 187.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=459000, episode_reward=1475.80 +/- 250.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=460000, episode_reward=1507.40 +/- 323.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=461000, episode_reward=1620.80 +/- 120.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=462000, episode_reward=1699.20 +/- 296.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=463000, episode_reward=1485.80 +/- 229.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=464000, episode_reward=1729.00 +/- 217.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=465000, episode_reward=1534.80 +/- 418.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=466000, episode_reward=1398.40 +/- 347.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=467000, episode_reward=1525.20 +/- 121.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=468000, episode_reward=1396.80 +/- 361.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=469000, episode_reward=1522.00 +/- 252.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=470000, episode_reward=1441.00 +/- 230.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=471000, episode_reward=1567.20 +/- 180.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=472000, episode_reward=1588.00 +/- 279.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=473000, episode_reward=1757.20 +/- 252.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=474000, episode_reward=1549.40 +/- 308.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=475000, episode_reward=1539.40 +/- 159.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=476000, episode_reward=1534.60 +/- 207.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=477000, episode_reward=1248.80 +/- 270.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=478000, episode_reward=1627.00 +/- 312.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=479000, episode_reward=1730.60 +/- 267.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=480000, episode_reward=1318.20 +/- 406.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=481000, episode_reward=1602.80 +/- 180.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=482000, episode_reward=1457.40 +/- 164.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=483000, episode_reward=1498.60 +/- 212.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=484000, episode_reward=1716.20 +/- 127.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=485000, episode_reward=1612.80 +/- 334.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=486000, episode_reward=1638.00 +/- 368.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=487000, episode_reward=1607.20 +/- 127.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=488000, episode_reward=1685.80 +/- 150.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=489000, episode_reward=1472.00 +/- 306.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=490000, episode_reward=1604.60 +/- 338.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=491000, episode_reward=1656.00 +/- 254.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=492000, episode_reward=1669.80 +/- 189.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=493000, episode_reward=1870.80 +/- 128.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=494000, episode_reward=1621.00 +/- 93.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=495000, episode_reward=1740.40 +/- 182.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=496000, episode_reward=1515.00 +/- 308.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=497000, episode_reward=1780.40 +/- 244.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=498000, episode_reward=1559.60 +/- 327.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=499000, episode_reward=1681.60 +/- 234.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=500000, episode_reward=1841.80 +/- 140.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=501000, episode_reward=1496.20 +/- 197.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Gamma = 0.8, Learning Rate = 0.0003, Activation Function = Tanh, Neurons = 8, Process noise = 0.1, Measurement noise = 0.0\n",
      "Eval num_timesteps=1000, episode_reward=34.60 +/- 46.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=-74.40 +/- 190.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=3000, episode_reward=-3163.00 +/- 3708.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=4000, episode_reward=-2605.80 +/- 2414.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=5000, episode_reward=-5129.20 +/- 2176.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=6000, episode_reward=-5887.60 +/- 1366.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=7000, episode_reward=-149.80 +/- 151.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=8000, episode_reward=209.60 +/- 105.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=9000, episode_reward=297.20 +/- 123.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=10000, episode_reward=195.80 +/- 167.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=11000, episode_reward=385.80 +/- 77.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=12000, episode_reward=323.80 +/- 222.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=13000, episode_reward=378.20 +/- 97.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=14000, episode_reward=329.80 +/- 161.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=15000, episode_reward=226.60 +/- 118.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=16000, episode_reward=155.00 +/- 272.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=17000, episode_reward=267.20 +/- 134.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=18000, episode_reward=182.60 +/- 160.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=19000, episode_reward=506.80 +/- 187.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=20000, episode_reward=378.80 +/- 156.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=21000, episode_reward=307.80 +/- 156.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=22000, episode_reward=276.60 +/- 109.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=23000, episode_reward=119.40 +/- 203.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=24000, episode_reward=333.20 +/- 239.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=25000, episode_reward=247.80 +/- 146.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=26000, episode_reward=157.20 +/- 156.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=27000, episode_reward=82.80 +/- 351.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=28000, episode_reward=363.40 +/- 126.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=29000, episode_reward=383.00 +/- 243.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=30000, episode_reward=-187.60 +/- 253.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=31000, episode_reward=2.40 +/- 301.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=32000, episode_reward=-220.80 +/- 369.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=33000, episode_reward=-171.00 +/- 350.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=34000, episode_reward=-89.40 +/- 224.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=35000, episode_reward=-168.40 +/- 366.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=36000, episode_reward=130.80 +/- 361.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=37000, episode_reward=75.20 +/- 224.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=38000, episode_reward=182.60 +/- 215.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=39000, episode_reward=126.60 +/- 187.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=40000, episode_reward=127.00 +/- 160.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=41000, episode_reward=178.00 +/- 336.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=42000, episode_reward=88.20 +/- 219.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=43000, episode_reward=283.60 +/- 378.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=44000, episode_reward=230.00 +/- 171.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=45000, episode_reward=298.00 +/- 130.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=46000, episode_reward=183.20 +/- 120.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=47000, episode_reward=375.00 +/- 193.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=48000, episode_reward=318.40 +/- 106.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=49000, episode_reward=201.40 +/- 191.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=50000, episode_reward=137.80 +/- 143.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=51000, episode_reward=323.80 +/- 88.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=52000, episode_reward=292.80 +/- 110.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=53000, episode_reward=239.60 +/- 264.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=54000, episode_reward=249.00 +/- 167.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=55000, episode_reward=361.20 +/- 159.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=56000, episode_reward=312.40 +/- 220.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=57000, episode_reward=309.40 +/- 237.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=58000, episode_reward=434.60 +/- 129.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=59000, episode_reward=379.20 +/- 190.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=60000, episode_reward=548.00 +/- 359.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=61000, episode_reward=653.60 +/- 253.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=62000, episode_reward=1109.00 +/- 119.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=63000, episode_reward=567.80 +/- 182.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=64000, episode_reward=814.20 +/- 245.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=65000, episode_reward=957.80 +/- 173.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=66000, episode_reward=1082.80 +/- 113.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=67000, episode_reward=699.00 +/- 248.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=68000, episode_reward=876.20 +/- 289.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=69000, episode_reward=808.80 +/- 171.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=70000, episode_reward=771.60 +/- 209.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=71000, episode_reward=898.20 +/- 449.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=72000, episode_reward=863.00 +/- 452.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=73000, episode_reward=850.20 +/- 225.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=74000, episode_reward=1213.60 +/- 176.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=75000, episode_reward=912.60 +/- 197.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=76000, episode_reward=1046.00 +/- 179.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=77000, episode_reward=571.20 +/- 606.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=78000, episode_reward=76.40 +/- 1378.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=79000, episode_reward=593.80 +/- 580.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=80000, episode_reward=913.60 +/- 272.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=81000, episode_reward=679.80 +/- 115.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=82000, episode_reward=803.80 +/- 412.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=83000, episode_reward=1144.20 +/- 282.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=84000, episode_reward=-881.40 +/- 3286.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=85000, episode_reward=974.60 +/- 172.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=86000, episode_reward=857.80 +/- 263.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=87000, episode_reward=842.60 +/- 249.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=88000, episode_reward=919.80 +/- 410.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=89000, episode_reward=849.20 +/- 305.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=90000, episode_reward=813.20 +/- 357.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=91000, episode_reward=1057.80 +/- 95.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=92000, episode_reward=1025.80 +/- 324.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=93000, episode_reward=961.60 +/- 354.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=94000, episode_reward=1063.40 +/- 106.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=95000, episode_reward=951.40 +/- 274.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=96000, episode_reward=1040.60 +/- 116.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=97000, episode_reward=870.40 +/- 173.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=98000, episode_reward=897.20 +/- 231.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=99000, episode_reward=883.60 +/- 144.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=100000, episode_reward=1080.80 +/- 325.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=101000, episode_reward=1125.00 +/- 319.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=102000, episode_reward=1193.00 +/- 281.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=103000, episode_reward=689.80 +/- 344.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=104000, episode_reward=1075.00 +/- 303.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=105000, episode_reward=1051.00 +/- 252.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=106000, episode_reward=997.80 +/- 193.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=107000, episode_reward=1159.80 +/- 147.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=108000, episode_reward=864.80 +/- 190.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=109000, episode_reward=986.00 +/- 182.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=110000, episode_reward=911.00 +/- 222.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=111000, episode_reward=830.60 +/- 213.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=112000, episode_reward=862.60 +/- 182.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=113000, episode_reward=463.40 +/- 487.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=114000, episode_reward=923.20 +/- 339.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=115000, episode_reward=798.80 +/- 358.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=116000, episode_reward=1037.60 +/- 202.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=117000, episode_reward=719.20 +/- 433.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=118000, episode_reward=675.00 +/- 377.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=119000, episode_reward=957.20 +/- 224.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=120000, episode_reward=960.60 +/- 304.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=121000, episode_reward=874.60 +/- 249.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=122000, episode_reward=1224.20 +/- 329.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=123000, episode_reward=837.40 +/- 141.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=124000, episode_reward=1083.40 +/- 192.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=125000, episode_reward=1266.80 +/- 105.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=126000, episode_reward=1306.20 +/- 156.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=127000, episode_reward=1327.40 +/- 315.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=128000, episode_reward=1129.00 +/- 115.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=129000, episode_reward=1139.40 +/- 197.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=130000, episode_reward=1205.20 +/- 233.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=131000, episode_reward=982.00 +/- 223.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=132000, episode_reward=1386.60 +/- 321.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=133000, episode_reward=1104.20 +/- 273.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=134000, episode_reward=1357.60 +/- 194.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=135000, episode_reward=1052.00 +/- 239.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=136000, episode_reward=1089.80 +/- 215.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=137000, episode_reward=1193.60 +/- 233.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=138000, episode_reward=1314.00 +/- 211.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=139000, episode_reward=1324.80 +/- 156.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=140000, episode_reward=1187.20 +/- 196.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=141000, episode_reward=1425.60 +/- 196.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=142000, episode_reward=1371.20 +/- 269.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=143000, episode_reward=1276.00 +/- 195.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=144000, episode_reward=1451.60 +/- 202.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=145000, episode_reward=976.20 +/- 233.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=146000, episode_reward=1215.80 +/- 226.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=147000, episode_reward=1278.40 +/- 186.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=148000, episode_reward=1229.60 +/- 102.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=149000, episode_reward=1326.20 +/- 194.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=150000, episode_reward=1301.20 +/- 310.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=151000, episode_reward=1274.00 +/- 305.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=152000, episode_reward=-599.60 +/- 3210.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=153000, episode_reward=1215.00 +/- 175.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=154000, episode_reward=1082.40 +/- 363.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=155000, episode_reward=1354.00 +/- 193.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=156000, episode_reward=1162.80 +/- 308.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=157000, episode_reward=1429.60 +/- 251.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=158000, episode_reward=1196.40 +/- 235.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=159000, episode_reward=1345.40 +/- 283.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=160000, episode_reward=1174.40 +/- 179.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=161000, episode_reward=1452.60 +/- 295.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=162000, episode_reward=1259.00 +/- 175.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=163000, episode_reward=1145.40 +/- 266.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=164000, episode_reward=1401.40 +/- 189.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=165000, episode_reward=1336.80 +/- 92.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=166000, episode_reward=1248.60 +/- 275.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=167000, episode_reward=1402.40 +/- 153.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=168000, episode_reward=1147.60 +/- 193.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=169000, episode_reward=1246.60 +/- 426.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=170000, episode_reward=1421.80 +/- 197.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=171000, episode_reward=1490.00 +/- 230.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=172000, episode_reward=1493.80 +/- 197.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=173000, episode_reward=1277.60 +/- 326.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=174000, episode_reward=1250.80 +/- 120.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=175000, episode_reward=1016.60 +/- 435.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=176000, episode_reward=1325.40 +/- 209.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=177000, episode_reward=1106.80 +/- 569.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=178000, episode_reward=1560.00 +/- 87.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=179000, episode_reward=1374.80 +/- 183.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=180000, episode_reward=1253.40 +/- 310.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=181000, episode_reward=1500.20 +/- 192.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=182000, episode_reward=1345.20 +/- 282.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=183000, episode_reward=1303.80 +/- 180.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=184000, episode_reward=1282.80 +/- 121.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=185000, episode_reward=1432.40 +/- 410.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=186000, episode_reward=1424.60 +/- 313.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=187000, episode_reward=1065.40 +/- 725.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=188000, episode_reward=1191.80 +/- 474.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=189000, episode_reward=1525.60 +/- 187.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=190000, episode_reward=954.40 +/- 192.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=191000, episode_reward=1081.20 +/- 403.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=192000, episode_reward=1078.00 +/- 255.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=193000, episode_reward=1404.60 +/- 242.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=194000, episode_reward=1066.60 +/- 335.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=195000, episode_reward=1165.60 +/- 149.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=196000, episode_reward=1497.60 +/- 257.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=197000, episode_reward=1611.20 +/- 157.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=198000, episode_reward=1330.20 +/- 437.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=199000, episode_reward=1344.60 +/- 183.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=200000, episode_reward=1381.00 +/- 177.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=201000, episode_reward=1326.60 +/- 185.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=202000, episode_reward=1383.40 +/- 259.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=203000, episode_reward=1345.40 +/- 206.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=204000, episode_reward=1298.40 +/- 195.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=205000, episode_reward=1286.00 +/- 157.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=206000, episode_reward=1169.00 +/- 427.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=207000, episode_reward=1410.60 +/- 235.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=208000, episode_reward=1461.60 +/- 105.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=209000, episode_reward=1431.40 +/- 283.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=210000, episode_reward=1396.40 +/- 241.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=211000, episode_reward=1386.80 +/- 175.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=212000, episode_reward=1203.80 +/- 377.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=213000, episode_reward=1310.00 +/- 335.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=214000, episode_reward=1272.00 +/- 342.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=215000, episode_reward=1265.80 +/- 384.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=216000, episode_reward=1207.40 +/- 240.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=217000, episode_reward=1540.20 +/- 215.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=218000, episode_reward=1407.00 +/- 235.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=219000, episode_reward=1283.60 +/- 355.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=220000, episode_reward=1428.60 +/- 204.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=221000, episode_reward=1526.60 +/- 239.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=222000, episode_reward=1325.20 +/- 149.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=223000, episode_reward=1162.20 +/- 316.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=224000, episode_reward=1363.00 +/- 165.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=225000, episode_reward=1580.60 +/- 144.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=226000, episode_reward=1333.80 +/- 307.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=227000, episode_reward=1385.20 +/- 266.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=228000, episode_reward=1414.20 +/- 168.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=229000, episode_reward=1246.40 +/- 225.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=230000, episode_reward=1411.60 +/- 336.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=231000, episode_reward=1186.00 +/- 298.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=232000, episode_reward=1467.60 +/- 142.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=233000, episode_reward=1406.40 +/- 107.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=234000, episode_reward=1165.20 +/- 194.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=235000, episode_reward=1386.60 +/- 211.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=236000, episode_reward=1372.80 +/- 134.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=237000, episode_reward=1461.00 +/- 174.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=238000, episode_reward=1350.00 +/- 234.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=239000, episode_reward=1187.60 +/- 225.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=240000, episode_reward=1340.40 +/- 166.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=241000, episode_reward=1474.20 +/- 183.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=242000, episode_reward=1167.20 +/- 231.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=243000, episode_reward=1334.80 +/- 453.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=244000, episode_reward=1434.40 +/- 151.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=245000, episode_reward=1466.00 +/- 153.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=246000, episode_reward=1561.00 +/- 271.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=247000, episode_reward=1562.20 +/- 95.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=248000, episode_reward=1385.40 +/- 152.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=249000, episode_reward=1594.00 +/- 159.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=250000, episode_reward=1010.80 +/- 180.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=251000, episode_reward=1480.20 +/- 145.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=252000, episode_reward=1422.00 +/- 128.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=253000, episode_reward=1308.80 +/- 208.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=254000, episode_reward=1426.00 +/- 244.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=255000, episode_reward=1407.80 +/- 325.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=256000, episode_reward=1558.80 +/- 132.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=257000, episode_reward=1347.80 +/- 182.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=258000, episode_reward=1310.20 +/- 247.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=259000, episode_reward=1116.80 +/- 289.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=260000, episode_reward=1191.80 +/- 218.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=261000, episode_reward=1425.60 +/- 168.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=262000, episode_reward=1121.20 +/- 261.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=263000, episode_reward=1111.00 +/- 255.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=264000, episode_reward=1245.00 +/- 258.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=265000, episode_reward=1489.80 +/- 179.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=266000, episode_reward=1310.20 +/- 260.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=267000, episode_reward=1362.60 +/- 219.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=268000, episode_reward=1508.20 +/- 395.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=269000, episode_reward=1454.20 +/- 306.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=270000, episode_reward=1475.40 +/- 248.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=271000, episode_reward=1309.00 +/- 223.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=272000, episode_reward=1673.60 +/- 138.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=273000, episode_reward=1425.00 +/- 244.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=274000, episode_reward=1375.60 +/- 118.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=275000, episode_reward=1383.60 +/- 226.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=276000, episode_reward=1402.00 +/- 240.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=277000, episode_reward=1514.00 +/- 257.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=278000, episode_reward=1469.80 +/- 133.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=279000, episode_reward=1245.80 +/- 107.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=280000, episode_reward=1674.40 +/- 186.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=281000, episode_reward=1365.40 +/- 156.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=282000, episode_reward=1403.60 +/- 191.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=283000, episode_reward=1357.80 +/- 135.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=284000, episode_reward=1455.60 +/- 181.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=285000, episode_reward=1476.80 +/- 205.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=286000, episode_reward=1408.20 +/- 174.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=287000, episode_reward=1486.60 +/- 160.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=288000, episode_reward=1355.40 +/- 151.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=289000, episode_reward=1202.40 +/- 213.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=290000, episode_reward=1355.80 +/- 196.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=291000, episode_reward=1465.40 +/- 244.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=292000, episode_reward=1470.00 +/- 150.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=293000, episode_reward=1367.40 +/- 446.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=294000, episode_reward=1343.20 +/- 247.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=295000, episode_reward=1486.80 +/- 110.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=296000, episode_reward=1420.60 +/- 182.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=297000, episode_reward=1497.80 +/- 279.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=298000, episode_reward=1433.20 +/- 191.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=299000, episode_reward=1438.00 +/- 152.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=300000, episode_reward=1473.60 +/- 270.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=301000, episode_reward=1331.80 +/- 264.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=302000, episode_reward=1427.20 +/- 199.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=303000, episode_reward=1280.20 +/- 233.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=304000, episode_reward=1445.80 +/- 323.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=305000, episode_reward=1551.60 +/- 220.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=306000, episode_reward=1252.00 +/- 158.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=307000, episode_reward=1154.00 +/- 249.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=308000, episode_reward=1386.20 +/- 267.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=309000, episode_reward=1221.40 +/- 117.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=310000, episode_reward=1332.00 +/- 63.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=311000, episode_reward=1277.60 +/- 266.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=312000, episode_reward=1395.00 +/- 226.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=313000, episode_reward=1367.40 +/- 146.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=314000, episode_reward=1430.80 +/- 197.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=315000, episode_reward=1331.60 +/- 360.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=316000, episode_reward=1332.00 +/- 147.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=317000, episode_reward=1379.00 +/- 187.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=318000, episode_reward=1478.60 +/- 344.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=319000, episode_reward=1189.00 +/- 401.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=320000, episode_reward=1282.80 +/- 168.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=321000, episode_reward=1537.40 +/- 233.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=322000, episode_reward=1166.60 +/- 193.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=323000, episode_reward=1361.20 +/- 260.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=324000, episode_reward=1330.00 +/- 247.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=325000, episode_reward=1271.80 +/- 160.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=326000, episode_reward=1358.80 +/- 316.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=327000, episode_reward=1426.60 +/- 138.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=328000, episode_reward=1550.40 +/- 123.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=329000, episode_reward=1214.20 +/- 204.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=330000, episode_reward=1293.00 +/- 247.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=331000, episode_reward=1194.00 +/- 210.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=332000, episode_reward=1284.40 +/- 245.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=333000, episode_reward=1394.60 +/- 297.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=334000, episode_reward=1457.20 +/- 132.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=335000, episode_reward=1492.20 +/- 195.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=336000, episode_reward=1469.40 +/- 125.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=337000, episode_reward=1580.40 +/- 146.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=338000, episode_reward=1429.80 +/- 104.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=339000, episode_reward=1437.40 +/- 347.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=340000, episode_reward=1516.20 +/- 269.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=341000, episode_reward=1511.60 +/- 91.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=342000, episode_reward=1440.00 +/- 207.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=343000, episode_reward=1557.60 +/- 136.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=344000, episode_reward=1428.40 +/- 141.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=345000, episode_reward=1260.60 +/- 304.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=346000, episode_reward=1435.60 +/- 114.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=347000, episode_reward=1150.40 +/- 193.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=348000, episode_reward=1544.60 +/- 191.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=349000, episode_reward=1325.60 +/- 162.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=350000, episode_reward=1388.20 +/- 215.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=351000, episode_reward=1469.60 +/- 184.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=352000, episode_reward=1400.80 +/- 191.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=353000, episode_reward=1367.00 +/- 139.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=354000, episode_reward=1458.60 +/- 286.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=355000, episode_reward=1290.00 +/- 154.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=356000, episode_reward=1418.00 +/- 301.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=357000, episode_reward=1331.80 +/- 253.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=358000, episode_reward=1341.80 +/- 213.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=359000, episode_reward=1297.40 +/- 252.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=360000, episode_reward=1368.00 +/- 393.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=361000, episode_reward=1450.00 +/- 143.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=362000, episode_reward=1464.20 +/- 164.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=363000, episode_reward=1198.20 +/- 212.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=364000, episode_reward=1390.00 +/- 314.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=365000, episode_reward=1308.80 +/- 173.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=366000, episode_reward=1474.00 +/- 133.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=367000, episode_reward=1319.60 +/- 379.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=368000, episode_reward=1447.20 +/- 223.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=369000, episode_reward=1337.60 +/- 398.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=370000, episode_reward=1355.00 +/- 85.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=371000, episode_reward=1394.20 +/- 67.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=372000, episode_reward=1309.40 +/- 109.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=373000, episode_reward=1347.80 +/- 233.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=374000, episode_reward=1420.60 +/- 185.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=375000, episode_reward=1113.40 +/- 215.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=376000, episode_reward=1464.80 +/- 164.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=377000, episode_reward=1232.80 +/- 367.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=378000, episode_reward=1481.20 +/- 207.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=379000, episode_reward=1442.80 +/- 135.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=380000, episode_reward=1118.80 +/- 157.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=381000, episode_reward=1281.80 +/- 426.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=382000, episode_reward=1274.00 +/- 277.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=383000, episode_reward=1308.40 +/- 120.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=384000, episode_reward=1537.60 +/- 133.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=385000, episode_reward=1381.80 +/- 95.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=386000, episode_reward=1300.20 +/- 145.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=387000, episode_reward=1366.00 +/- 221.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=388000, episode_reward=1370.80 +/- 192.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=389000, episode_reward=1123.80 +/- 153.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=390000, episode_reward=1102.40 +/- 200.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=391000, episode_reward=1108.80 +/- 185.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=392000, episode_reward=1640.60 +/- 158.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=393000, episode_reward=1232.00 +/- 184.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=394000, episode_reward=1135.80 +/- 122.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=395000, episode_reward=1384.20 +/- 217.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=396000, episode_reward=1362.40 +/- 274.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=397000, episode_reward=1183.60 +/- 229.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=398000, episode_reward=1131.40 +/- 421.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=399000, episode_reward=1342.60 +/- 252.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=400000, episode_reward=1252.60 +/- 231.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=401000, episode_reward=1547.80 +/- 253.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=402000, episode_reward=1229.40 +/- 190.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=403000, episode_reward=1249.40 +/- 440.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=404000, episode_reward=1213.80 +/- 250.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=405000, episode_reward=1381.60 +/- 211.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=406000, episode_reward=1483.00 +/- 282.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=407000, episode_reward=1416.60 +/- 282.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=408000, episode_reward=1417.60 +/- 331.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=409000, episode_reward=1376.00 +/- 241.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=410000, episode_reward=1468.80 +/- 253.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=411000, episode_reward=1312.60 +/- 188.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=412000, episode_reward=1419.00 +/- 211.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=413000, episode_reward=1435.20 +/- 276.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=414000, episode_reward=1478.60 +/- 201.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=415000, episode_reward=1431.00 +/- 160.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=416000, episode_reward=1417.80 +/- 140.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=417000, episode_reward=1415.80 +/- 283.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=418000, episode_reward=1222.40 +/- 141.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=419000, episode_reward=1426.80 +/- 203.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=420000, episode_reward=1305.40 +/- 232.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=421000, episode_reward=1322.60 +/- 247.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=422000, episode_reward=1383.60 +/- 257.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=423000, episode_reward=1343.00 +/- 146.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=424000, episode_reward=1469.20 +/- 264.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=425000, episode_reward=1245.00 +/- 180.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=426000, episode_reward=1240.20 +/- 167.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=427000, episode_reward=1324.60 +/- 250.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=428000, episode_reward=1302.20 +/- 181.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=429000, episode_reward=1458.40 +/- 164.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=430000, episode_reward=1310.60 +/- 291.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=431000, episode_reward=1271.40 +/- 354.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=432000, episode_reward=1293.40 +/- 180.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=433000, episode_reward=1359.20 +/- 253.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=434000, episode_reward=1202.60 +/- 225.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=435000, episode_reward=1272.60 +/- 231.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=436000, episode_reward=1395.00 +/- 258.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=437000, episode_reward=1453.20 +/- 159.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=438000, episode_reward=1418.40 +/- 251.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=439000, episode_reward=1225.60 +/- 263.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=440000, episode_reward=1254.00 +/- 246.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=441000, episode_reward=1423.80 +/- 160.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=442000, episode_reward=1213.40 +/- 272.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=443000, episode_reward=1507.20 +/- 103.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=444000, episode_reward=1464.80 +/- 155.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=445000, episode_reward=1452.60 +/- 279.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=446000, episode_reward=1347.60 +/- 348.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=447000, episode_reward=1416.60 +/- 170.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=448000, episode_reward=1468.60 +/- 161.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=449000, episode_reward=1499.20 +/- 210.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=450000, episode_reward=1432.40 +/- 92.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=451000, episode_reward=1464.40 +/- 239.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=452000, episode_reward=1490.80 +/- 111.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=453000, episode_reward=1265.80 +/- 274.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=454000, episode_reward=1179.60 +/- 336.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=455000, episode_reward=1299.00 +/- 241.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=456000, episode_reward=1351.00 +/- 262.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=457000, episode_reward=1233.00 +/- 348.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=458000, episode_reward=1551.40 +/- 170.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=459000, episode_reward=1179.00 +/- 205.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=460000, episode_reward=1519.00 +/- 167.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=461000, episode_reward=1354.60 +/- 242.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=462000, episode_reward=1376.60 +/- 301.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=463000, episode_reward=1344.40 +/- 313.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=464000, episode_reward=1124.60 +/- 239.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=465000, episode_reward=1468.00 +/- 139.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=466000, episode_reward=1457.00 +/- 173.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=467000, episode_reward=1230.60 +/- 201.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=468000, episode_reward=1332.20 +/- 172.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=469000, episode_reward=1289.40 +/- 271.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=470000, episode_reward=1247.20 +/- 386.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=471000, episode_reward=1462.60 +/- 131.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=472000, episode_reward=1475.40 +/- 214.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=473000, episode_reward=1313.20 +/- 257.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=474000, episode_reward=1324.20 +/- 306.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=475000, episode_reward=1394.20 +/- 138.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=476000, episode_reward=1325.80 +/- 173.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=477000, episode_reward=1564.80 +/- 183.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=478000, episode_reward=1484.20 +/- 154.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=479000, episode_reward=1487.60 +/- 168.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=480000, episode_reward=1207.80 +/- 290.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=481000, episode_reward=1208.40 +/- 264.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=482000, episode_reward=1280.80 +/- 385.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=483000, episode_reward=1466.20 +/- 87.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=484000, episode_reward=1527.80 +/- 154.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=485000, episode_reward=1341.00 +/- 288.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=486000, episode_reward=1481.80 +/- 145.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=487000, episode_reward=1324.60 +/- 152.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=488000, episode_reward=1408.20 +/- 211.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=489000, episode_reward=1486.40 +/- 183.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=490000, episode_reward=1301.80 +/- 134.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=491000, episode_reward=1406.60 +/- 135.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=492000, episode_reward=1349.20 +/- 149.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=493000, episode_reward=1344.60 +/- 146.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=494000, episode_reward=1224.00 +/- 177.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=495000, episode_reward=1273.40 +/- 329.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=496000, episode_reward=1106.00 +/- 141.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=497000, episode_reward=1279.20 +/- 233.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=498000, episode_reward=1372.80 +/- 185.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=499000, episode_reward=1264.80 +/- 231.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=500000, episode_reward=1301.00 +/- 243.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=501000, episode_reward=1329.60 +/- 219.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Gamma = 0.8, Learning Rate = 0.0003, Activation Function = Tanh, Neurons = 32, Process noise = 0.1, Measurement noise = 0.0\n",
      "Eval num_timesteps=1000, episode_reward=-512.40 +/- 209.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=-404.20 +/- 159.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3000, episode_reward=415.80 +/- 98.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4000, episode_reward=380.40 +/- 114.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=5000, episode_reward=402.80 +/- 282.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=6000, episode_reward=207.40 +/- 275.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=7000, episode_reward=459.00 +/- 154.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=8000, episode_reward=244.40 +/- 65.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=9000, episode_reward=364.00 +/- 77.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=10000, episode_reward=248.60 +/- 229.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=11000, episode_reward=370.00 +/- 202.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=12000, episode_reward=382.80 +/- 116.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=13000, episode_reward=459.40 +/- 105.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=14000, episode_reward=341.00 +/- 138.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=15000, episode_reward=180.60 +/- 189.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=16000, episode_reward=175.20 +/- 187.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=17000, episode_reward=187.20 +/- 180.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=18000, episode_reward=284.60 +/- 163.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=19000, episode_reward=342.80 +/- 43.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=20000, episode_reward=120.40 +/- 198.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=21000, episode_reward=215.40 +/- 212.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=22000, episode_reward=294.20 +/- 58.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=23000, episode_reward=292.20 +/- 191.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=24000, episode_reward=511.40 +/- 246.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=25000, episode_reward=484.80 +/- 175.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=26000, episode_reward=324.80 +/- 320.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=27000, episode_reward=578.00 +/- 242.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=28000, episode_reward=592.40 +/- 255.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=29000, episode_reward=838.00 +/- 429.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=30000, episode_reward=788.40 +/- 207.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=31000, episode_reward=919.80 +/- 236.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=32000, episode_reward=1078.40 +/- 143.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=33000, episode_reward=903.20 +/- 172.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=34000, episode_reward=1250.80 +/- 319.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=35000, episode_reward=1030.20 +/- 156.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=36000, episode_reward=1168.00 +/- 239.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=37000, episode_reward=1120.40 +/- 91.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=38000, episode_reward=1159.00 +/- 164.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=39000, episode_reward=-1554.60 +/- 5651.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=40000, episode_reward=-2332.00 +/- 6648.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=41000, episode_reward=1292.60 +/- 345.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=42000, episode_reward=1268.80 +/- 197.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=43000, episode_reward=550.00 +/- 1002.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=44000, episode_reward=1263.20 +/- 201.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=45000, episode_reward=1220.40 +/- 267.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=46000, episode_reward=1223.20 +/- 335.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=47000, episode_reward=985.40 +/- 352.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=48000, episode_reward=-2285.40 +/- 6856.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=49000, episode_reward=1183.80 +/- 266.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=50000, episode_reward=-331.60 +/- 3310.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=51000, episode_reward=1031.60 +/- 195.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=52000, episode_reward=1095.80 +/- 71.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=53000, episode_reward=1116.00 +/- 278.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=54000, episode_reward=1124.80 +/- 121.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=55000, episode_reward=966.80 +/- 340.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=56000, episode_reward=1012.80 +/- 90.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=57000, episode_reward=1123.60 +/- 249.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=58000, episode_reward=1054.20 +/- 157.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=59000, episode_reward=1116.40 +/- 243.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=60000, episode_reward=1132.80 +/- 231.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=61000, episode_reward=-2010.00 +/- 6544.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=62000, episode_reward=1137.40 +/- 228.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=63000, episode_reward=1315.40 +/- 124.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=64000, episode_reward=1114.60 +/- 121.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=65000, episode_reward=1019.60 +/- 317.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=66000, episode_reward=-1852.60 +/- 6341.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=67000, episode_reward=-557.60 +/- 3778.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=68000, episode_reward=-6909.60 +/- 6969.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=69000, episode_reward=1076.40 +/- 126.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=70000, episode_reward=-2368.80 +/- 7415.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=71000, episode_reward=1349.00 +/- 203.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=72000, episode_reward=1100.80 +/- 217.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=73000, episode_reward=-107.40 +/- 2459.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=74000, episode_reward=-360.60 +/- 3549.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=75000, episode_reward=1081.60 +/- 362.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=76000, episode_reward=1379.00 +/- 152.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=77000, episode_reward=1089.80 +/- 135.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=78000, episode_reward=1247.40 +/- 238.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=79000, episode_reward=1392.60 +/- 169.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=80000, episode_reward=1262.60 +/- 468.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=81000, episode_reward=1355.80 +/- 330.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=82000, episode_reward=1337.40 +/- 209.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=83000, episode_reward=1290.60 +/- 198.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=84000, episode_reward=-1924.60 +/- 7166.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=85000, episode_reward=1527.60 +/- 175.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=86000, episode_reward=1170.40 +/- 129.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=87000, episode_reward=-830.00 +/- 4616.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=88000, episode_reward=1614.20 +/- 188.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=89000, episode_reward=1531.40 +/- 71.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=90000, episode_reward=1369.60 +/- 267.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=91000, episode_reward=1241.60 +/- 428.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=92000, episode_reward=1634.80 +/- 171.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=93000, episode_reward=1393.00 +/- 244.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=94000, episode_reward=1414.60 +/- 168.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=95000, episode_reward=1633.60 +/- 199.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=96000, episode_reward=1455.00 +/- 233.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=97000, episode_reward=1299.60 +/- 327.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=98000, episode_reward=1256.00 +/- 309.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=99000, episode_reward=1422.20 +/- 146.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=100000, episode_reward=1509.80 +/- 305.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=101000, episode_reward=1556.60 +/- 259.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=102000, episode_reward=1486.60 +/- 252.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=103000, episode_reward=1724.60 +/- 172.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=104000, episode_reward=1625.80 +/- 167.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=105000, episode_reward=1617.60 +/- 106.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=106000, episode_reward=1573.80 +/- 185.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=107000, episode_reward=1602.80 +/- 357.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=108000, episode_reward=1432.80 +/- 237.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=109000, episode_reward=1464.40 +/- 249.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=110000, episode_reward=1704.80 +/- 288.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=111000, episode_reward=1673.60 +/- 303.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=112000, episode_reward=1186.20 +/- 261.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=113000, episode_reward=1521.60 +/- 188.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=114000, episode_reward=1268.80 +/- 496.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=115000, episode_reward=1720.80 +/- 128.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=116000, episode_reward=1381.80 +/- 406.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=117000, episode_reward=1663.80 +/- 178.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=118000, episode_reward=1460.60 +/- 267.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=119000, episode_reward=1516.40 +/- 501.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=120000, episode_reward=1818.00 +/- 171.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=121000, episode_reward=1458.20 +/- 208.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=122000, episode_reward=1685.20 +/- 244.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=123000, episode_reward=1666.00 +/- 164.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=124000, episode_reward=1491.40 +/- 183.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=125000, episode_reward=1655.00 +/- 198.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=126000, episode_reward=1598.80 +/- 181.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=127000, episode_reward=1616.40 +/- 188.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=128000, episode_reward=1544.40 +/- 252.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=129000, episode_reward=1732.60 +/- 219.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=130000, episode_reward=1360.80 +/- 127.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=131000, episode_reward=1362.20 +/- 381.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=132000, episode_reward=1326.20 +/- 339.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=133000, episode_reward=1350.20 +/- 272.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=134000, episode_reward=1591.80 +/- 335.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=135000, episode_reward=1343.40 +/- 402.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=136000, episode_reward=1393.20 +/- 198.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=137000, episode_reward=1379.20 +/- 329.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=138000, episode_reward=1591.60 +/- 217.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=139000, episode_reward=1369.80 +/- 226.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=140000, episode_reward=1654.60 +/- 83.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=141000, episode_reward=1565.60 +/- 307.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=142000, episode_reward=1694.00 +/- 232.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=143000, episode_reward=1563.40 +/- 111.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=144000, episode_reward=1450.00 +/- 398.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=145000, episode_reward=1549.00 +/- 188.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=146000, episode_reward=1569.60 +/- 218.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=147000, episode_reward=1610.20 +/- 108.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=148000, episode_reward=1632.40 +/- 404.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=149000, episode_reward=1548.60 +/- 102.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=150000, episode_reward=1647.60 +/- 173.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=151000, episode_reward=1705.80 +/- 127.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=152000, episode_reward=1363.20 +/- 256.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=153000, episode_reward=1391.40 +/- 172.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=154000, episode_reward=1302.60 +/- 291.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=155000, episode_reward=1502.80 +/- 106.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=156000, episode_reward=1660.00 +/- 175.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=157000, episode_reward=1471.20 +/- 130.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=158000, episode_reward=1400.00 +/- 125.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=159000, episode_reward=1550.80 +/- 324.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=160000, episode_reward=1840.80 +/- 108.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=161000, episode_reward=1695.00 +/- 157.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=162000, episode_reward=1508.40 +/- 208.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=163000, episode_reward=1584.60 +/- 149.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=164000, episode_reward=1606.80 +/- 180.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=165000, episode_reward=1596.80 +/- 150.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=166000, episode_reward=1502.20 +/- 349.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=167000, episode_reward=1707.60 +/- 112.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=168000, episode_reward=1503.80 +/- 255.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=169000, episode_reward=1637.40 +/- 176.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=170000, episode_reward=1635.20 +/- 164.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=171000, episode_reward=1565.20 +/- 162.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=172000, episode_reward=1469.80 +/- 251.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=173000, episode_reward=1633.80 +/- 149.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=174000, episode_reward=1678.80 +/- 168.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=175000, episode_reward=1510.80 +/- 340.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=176000, episode_reward=1434.60 +/- 219.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=177000, episode_reward=1751.20 +/- 135.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=178000, episode_reward=1790.80 +/- 223.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=179000, episode_reward=1567.00 +/- 195.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=180000, episode_reward=1472.20 +/- 294.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=181000, episode_reward=1321.20 +/- 191.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=182000, episode_reward=1489.80 +/- 284.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=183000, episode_reward=1609.00 +/- 362.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=184000, episode_reward=1535.00 +/- 99.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=185000, episode_reward=1504.00 +/- 450.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=186000, episode_reward=1546.60 +/- 358.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=187000, episode_reward=1527.40 +/- 180.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=188000, episode_reward=1547.20 +/- 64.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=189000, episode_reward=1650.80 +/- 211.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=190000, episode_reward=1710.20 +/- 180.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=191000, episode_reward=1733.80 +/- 124.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=192000, episode_reward=1535.60 +/- 215.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=193000, episode_reward=1397.40 +/- 235.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=194000, episode_reward=1537.00 +/- 203.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=195000, episode_reward=1599.20 +/- 212.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=196000, episode_reward=1642.40 +/- 176.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=197000, episode_reward=1611.40 +/- 207.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=198000, episode_reward=1589.40 +/- 181.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=199000, episode_reward=1772.20 +/- 133.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=200000, episode_reward=1509.40 +/- 379.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=201000, episode_reward=1761.00 +/- 178.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=202000, episode_reward=1548.60 +/- 284.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=203000, episode_reward=1549.40 +/- 172.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=204000, episode_reward=1479.00 +/- 261.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=205000, episode_reward=1655.20 +/- 193.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=206000, episode_reward=1514.20 +/- 301.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=207000, episode_reward=1527.40 +/- 334.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=208000, episode_reward=1763.20 +/- 188.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=209000, episode_reward=1732.00 +/- 260.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=210000, episode_reward=1667.00 +/- 104.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=211000, episode_reward=1673.00 +/- 142.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=212000, episode_reward=1431.40 +/- 101.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=213000, episode_reward=1449.80 +/- 347.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=214000, episode_reward=1342.60 +/- 214.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=215000, episode_reward=1492.00 +/- 339.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=216000, episode_reward=1751.00 +/- 52.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=217000, episode_reward=1530.20 +/- 409.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=218000, episode_reward=1768.40 +/- 156.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=219000, episode_reward=1580.60 +/- 254.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=220000, episode_reward=1737.20 +/- 138.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=221000, episode_reward=1466.60 +/- 349.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=222000, episode_reward=1381.60 +/- 157.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=223000, episode_reward=1657.40 +/- 222.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=224000, episode_reward=1544.80 +/- 232.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=225000, episode_reward=1569.00 +/- 303.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=226000, episode_reward=1708.20 +/- 161.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=227000, episode_reward=1534.40 +/- 287.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=228000, episode_reward=1809.40 +/- 152.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=229000, episode_reward=1508.80 +/- 336.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=230000, episode_reward=1620.80 +/- 131.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=231000, episode_reward=1450.80 +/- 493.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=232000, episode_reward=1426.00 +/- 206.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=233000, episode_reward=1667.80 +/- 331.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=234000, episode_reward=1519.20 +/- 129.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=235000, episode_reward=1555.60 +/- 217.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=236000, episode_reward=1734.60 +/- 308.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=237000, episode_reward=1487.60 +/- 258.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=238000, episode_reward=1730.40 +/- 159.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=239000, episode_reward=1639.40 +/- 194.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=240000, episode_reward=1754.20 +/- 116.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=241000, episode_reward=1385.60 +/- 374.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=242000, episode_reward=1555.40 +/- 207.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=243000, episode_reward=1434.40 +/- 277.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=244000, episode_reward=1567.00 +/- 195.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=245000, episode_reward=1527.60 +/- 127.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=246000, episode_reward=1693.00 +/- 364.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=247000, episode_reward=1507.40 +/- 102.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=248000, episode_reward=1486.60 +/- 173.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=249000, episode_reward=1548.40 +/- 155.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=250000, episode_reward=1528.20 +/- 331.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=251000, episode_reward=1610.00 +/- 194.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=252000, episode_reward=1569.60 +/- 85.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=253000, episode_reward=1611.00 +/- 296.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=254000, episode_reward=1585.80 +/- 149.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=255000, episode_reward=1548.80 +/- 178.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=256000, episode_reward=1544.00 +/- 187.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=257000, episode_reward=1624.20 +/- 189.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=258000, episode_reward=1636.40 +/- 264.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=259000, episode_reward=1417.20 +/- 513.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=260000, episode_reward=1778.40 +/- 433.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=261000, episode_reward=1586.80 +/- 269.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=262000, episode_reward=1665.80 +/- 181.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=263000, episode_reward=1688.60 +/- 163.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=264000, episode_reward=1451.80 +/- 396.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=265000, episode_reward=1447.60 +/- 272.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=266000, episode_reward=1522.60 +/- 209.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=267000, episode_reward=1550.40 +/- 162.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=268000, episode_reward=1596.00 +/- 269.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=269000, episode_reward=1293.20 +/- 233.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=270000, episode_reward=1424.80 +/- 267.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=271000, episode_reward=1690.80 +/- 262.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=272000, episode_reward=1542.20 +/- 225.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=273000, episode_reward=1596.40 +/- 199.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=274000, episode_reward=1676.40 +/- 313.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=275000, episode_reward=1719.60 +/- 176.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=276000, episode_reward=1792.00 +/- 173.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=277000, episode_reward=1799.40 +/- 303.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=278000, episode_reward=1553.40 +/- 250.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=279000, episode_reward=1283.80 +/- 388.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=280000, episode_reward=1669.20 +/- 223.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=281000, episode_reward=1633.40 +/- 186.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=282000, episode_reward=1591.00 +/- 237.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=283000, episode_reward=1620.40 +/- 292.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=284000, episode_reward=1415.80 +/- 263.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=285000, episode_reward=1596.80 +/- 228.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=286000, episode_reward=1788.20 +/- 70.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=287000, episode_reward=1638.60 +/- 140.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=288000, episode_reward=1423.40 +/- 338.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=289000, episode_reward=1653.20 +/- 247.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=290000, episode_reward=1443.40 +/- 301.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=291000, episode_reward=1516.60 +/- 358.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=292000, episode_reward=1564.80 +/- 247.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=293000, episode_reward=1321.40 +/- 363.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=294000, episode_reward=1637.00 +/- 290.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=295000, episode_reward=1549.40 +/- 234.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=296000, episode_reward=1364.60 +/- 157.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=297000, episode_reward=1520.00 +/- 204.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=298000, episode_reward=1585.20 +/- 173.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=299000, episode_reward=1413.20 +/- 356.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=300000, episode_reward=1459.00 +/- 242.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=301000, episode_reward=1635.40 +/- 197.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=302000, episode_reward=1601.00 +/- 216.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=303000, episode_reward=1567.00 +/- 166.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=304000, episode_reward=1754.00 +/- 175.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=305000, episode_reward=1788.80 +/- 100.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=306000, episode_reward=1640.20 +/- 150.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=307000, episode_reward=1417.80 +/- 138.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=308000, episode_reward=1436.60 +/- 180.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=309000, episode_reward=1672.80 +/- 84.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=310000, episode_reward=1487.20 +/- 277.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=311000, episode_reward=1544.40 +/- 252.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=312000, episode_reward=1802.40 +/- 217.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=313000, episode_reward=1314.80 +/- 357.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=314000, episode_reward=1553.60 +/- 151.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=315000, episode_reward=1520.40 +/- 258.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=316000, episode_reward=1629.60 +/- 187.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=317000, episode_reward=1635.40 +/- 138.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=318000, episode_reward=1545.60 +/- 291.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=319000, episode_reward=1460.00 +/- 128.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=320000, episode_reward=1541.60 +/- 80.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=321000, episode_reward=1757.80 +/- 240.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=322000, episode_reward=1304.20 +/- 401.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=323000, episode_reward=1348.40 +/- 347.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=324000, episode_reward=1604.00 +/- 375.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=325000, episode_reward=1604.40 +/- 159.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=326000, episode_reward=1688.20 +/- 108.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=327000, episode_reward=1731.20 +/- 171.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=328000, episode_reward=1696.80 +/- 119.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=329000, episode_reward=1531.00 +/- 221.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=330000, episode_reward=1519.60 +/- 272.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=331000, episode_reward=1593.40 +/- 341.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=332000, episode_reward=1324.40 +/- 300.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=333000, episode_reward=1636.00 +/- 177.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=334000, episode_reward=1450.40 +/- 304.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=335000, episode_reward=1696.60 +/- 289.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=336000, episode_reward=1511.60 +/- 309.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=337000, episode_reward=1586.40 +/- 315.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=338000, episode_reward=1823.00 +/- 171.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=339000, episode_reward=1562.60 +/- 279.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=340000, episode_reward=1561.80 +/- 290.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=341000, episode_reward=1666.80 +/- 117.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=342000, episode_reward=1597.00 +/- 370.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=343000, episode_reward=1459.60 +/- 474.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=344000, episode_reward=1632.80 +/- 270.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=345000, episode_reward=1727.20 +/- 136.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=346000, episode_reward=1660.60 +/- 254.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=347000, episode_reward=1656.00 +/- 147.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=348000, episode_reward=1628.60 +/- 279.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=349000, episode_reward=1536.00 +/- 365.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=350000, episode_reward=1675.80 +/- 70.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=351000, episode_reward=1547.60 +/- 453.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=352000, episode_reward=1663.40 +/- 258.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=353000, episode_reward=1779.60 +/- 119.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=354000, episode_reward=1552.20 +/- 372.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=355000, episode_reward=1548.40 +/- 259.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=356000, episode_reward=1673.20 +/- 293.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=357000, episode_reward=1701.20 +/- 250.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=358000, episode_reward=1787.80 +/- 237.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=359000, episode_reward=1780.40 +/- 287.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=360000, episode_reward=1614.80 +/- 298.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=361000, episode_reward=1603.00 +/- 271.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=362000, episode_reward=1560.60 +/- 313.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=363000, episode_reward=1597.40 +/- 221.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=364000, episode_reward=1576.40 +/- 330.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=365000, episode_reward=1455.40 +/- 140.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=366000, episode_reward=1573.80 +/- 128.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=367000, episode_reward=1636.60 +/- 142.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=368000, episode_reward=1659.60 +/- 183.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=369000, episode_reward=1668.80 +/- 147.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=370000, episode_reward=1731.20 +/- 188.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=371000, episode_reward=1528.20 +/- 236.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=372000, episode_reward=1614.00 +/- 190.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=373000, episode_reward=1523.60 +/- 322.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=374000, episode_reward=1590.20 +/- 295.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=375000, episode_reward=1490.20 +/- 227.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=376000, episode_reward=1566.60 +/- 316.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=377000, episode_reward=1600.20 +/- 317.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=378000, episode_reward=1782.80 +/- 154.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=379000, episode_reward=1839.20 +/- 148.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=380000, episode_reward=1634.40 +/- 262.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=381000, episode_reward=1865.40 +/- 207.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=382000, episode_reward=1703.80 +/- 138.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=383000, episode_reward=1504.40 +/- 358.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=384000, episode_reward=1581.20 +/- 95.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=385000, episode_reward=1692.80 +/- 382.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=386000, episode_reward=1589.80 +/- 148.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=387000, episode_reward=1501.00 +/- 260.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=388000, episode_reward=1637.00 +/- 241.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=389000, episode_reward=1578.80 +/- 162.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=390000, episode_reward=1617.40 +/- 321.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=391000, episode_reward=1506.60 +/- 102.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=392000, episode_reward=1516.40 +/- 192.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=393000, episode_reward=1501.20 +/- 344.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=394000, episode_reward=1688.40 +/- 282.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=395000, episode_reward=1562.60 +/- 290.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=396000, episode_reward=1677.80 +/- 380.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=397000, episode_reward=1754.40 +/- 244.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=398000, episode_reward=1778.40 +/- 128.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=399000, episode_reward=1581.60 +/- 139.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=400000, episode_reward=1604.00 +/- 372.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=401000, episode_reward=1571.20 +/- 331.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=402000, episode_reward=1558.00 +/- 278.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=403000, episode_reward=1525.40 +/- 238.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=404000, episode_reward=1620.40 +/- 139.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=405000, episode_reward=1587.40 +/- 260.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=406000, episode_reward=1587.40 +/- 440.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=407000, episode_reward=1682.40 +/- 188.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=408000, episode_reward=1466.40 +/- 207.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=409000, episode_reward=1757.80 +/- 159.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=410000, episode_reward=1724.60 +/- 47.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=411000, episode_reward=1532.00 +/- 267.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=412000, episode_reward=1788.00 +/- 181.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=413000, episode_reward=1843.20 +/- 72.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=414000, episode_reward=1477.20 +/- 245.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=415000, episode_reward=1622.00 +/- 265.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=416000, episode_reward=1595.60 +/- 243.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=417000, episode_reward=1562.80 +/- 300.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=418000, episode_reward=1584.00 +/- 303.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=419000, episode_reward=1531.20 +/- 319.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=420000, episode_reward=1517.00 +/- 190.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=421000, episode_reward=1686.80 +/- 213.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=422000, episode_reward=1601.00 +/- 270.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=423000, episode_reward=1651.40 +/- 152.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=424000, episode_reward=1586.20 +/- 102.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=425000, episode_reward=1610.20 +/- 227.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=426000, episode_reward=1617.80 +/- 371.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=427000, episode_reward=1774.80 +/- 179.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=428000, episode_reward=1526.40 +/- 207.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=429000, episode_reward=1818.20 +/- 282.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=430000, episode_reward=1675.80 +/- 320.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=431000, episode_reward=1643.60 +/- 185.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=432000, episode_reward=1568.20 +/- 343.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=433000, episode_reward=1822.40 +/- 324.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=434000, episode_reward=1464.60 +/- 241.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=435000, episode_reward=1478.80 +/- 195.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=436000, episode_reward=1636.20 +/- 139.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=437000, episode_reward=1719.00 +/- 53.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=438000, episode_reward=1590.20 +/- 90.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=439000, episode_reward=1676.00 +/- 332.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=440000, episode_reward=1808.00 +/- 133.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=441000, episode_reward=1606.20 +/- 301.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=442000, episode_reward=1611.40 +/- 315.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=443000, episode_reward=1791.00 +/- 208.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=444000, episode_reward=1548.80 +/- 287.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=445000, episode_reward=1615.80 +/- 133.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=446000, episode_reward=1746.80 +/- 164.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=447000, episode_reward=1529.20 +/- 269.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=448000, episode_reward=1565.00 +/- 336.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=449000, episode_reward=1487.60 +/- 307.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=450000, episode_reward=1587.80 +/- 381.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=451000, episode_reward=1765.40 +/- 239.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=452000, episode_reward=1755.00 +/- 251.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=453000, episode_reward=1786.40 +/- 204.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=454000, episode_reward=1850.00 +/- 93.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=455000, episode_reward=1777.40 +/- 199.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=456000, episode_reward=1490.60 +/- 175.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=457000, episode_reward=1689.60 +/- 183.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=458000, episode_reward=1719.20 +/- 222.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=459000, episode_reward=1535.80 +/- 220.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=460000, episode_reward=1561.40 +/- 172.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=461000, episode_reward=1581.20 +/- 161.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=462000, episode_reward=1501.20 +/- 154.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=463000, episode_reward=1687.40 +/- 310.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=464000, episode_reward=1785.20 +/- 237.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=465000, episode_reward=1558.60 +/- 161.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=466000, episode_reward=1513.60 +/- 209.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=467000, episode_reward=1689.80 +/- 121.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=468000, episode_reward=1632.60 +/- 184.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=469000, episode_reward=1833.60 +/- 87.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=470000, episode_reward=1655.60 +/- 210.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=471000, episode_reward=1407.20 +/- 119.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=472000, episode_reward=1469.80 +/- 117.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=473000, episode_reward=1766.00 +/- 110.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=474000, episode_reward=1754.00 +/- 188.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=475000, episode_reward=1663.40 +/- 183.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=476000, episode_reward=1703.20 +/- 268.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=477000, episode_reward=1649.20 +/- 442.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=478000, episode_reward=1694.80 +/- 174.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=479000, episode_reward=1814.00 +/- 226.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=480000, episode_reward=1653.40 +/- 202.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=481000, episode_reward=1718.20 +/- 202.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=482000, episode_reward=1576.40 +/- 117.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=483000, episode_reward=1787.80 +/- 103.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=484000, episode_reward=1792.20 +/- 202.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=485000, episode_reward=1666.80 +/- 161.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=486000, episode_reward=1584.80 +/- 205.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=487000, episode_reward=1530.80 +/- 289.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=488000, episode_reward=1711.20 +/- 155.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=489000, episode_reward=1758.60 +/- 70.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=490000, episode_reward=1595.40 +/- 221.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=491000, episode_reward=1470.80 +/- 157.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=492000, episode_reward=1476.00 +/- 180.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=493000, episode_reward=1544.60 +/- 176.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=494000, episode_reward=1532.00 +/- 109.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=495000, episode_reward=1534.20 +/- 333.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=496000, episode_reward=1717.20 +/- 292.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=497000, episode_reward=1730.60 +/- 255.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=498000, episode_reward=1352.00 +/- 114.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=499000, episode_reward=1530.80 +/- 164.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=500000, episode_reward=1658.40 +/- 332.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=501000, episode_reward=1536.80 +/- 259.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Gamma = 0.8, Learning Rate = 0.0003, Activation Function = Tanh, Neurons = 64, Process noise = 0.1, Measurement noise = 0.0\n",
      "Eval num_timesteps=1000, episode_reward=-18960.00 +/- 128.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=-19012.00 +/- 72.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=3000, episode_reward=-525.40 +/- 303.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4000, episode_reward=-493.40 +/- 277.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=5000, episode_reward=579.60 +/- 108.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=6000, episode_reward=425.00 +/- 308.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=7000, episode_reward=390.00 +/- 95.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=8000, episode_reward=291.60 +/- 239.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=9000, episode_reward=330.80 +/- 325.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=10000, episode_reward=389.40 +/- 136.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=11000, episode_reward=381.40 +/- 227.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=12000, episode_reward=258.60 +/- 178.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=13000, episode_reward=329.00 +/- 148.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=14000, episode_reward=496.00 +/- 105.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=15000, episode_reward=376.00 +/- 312.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=16000, episode_reward=503.80 +/- 145.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=17000, episode_reward=328.80 +/- 169.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=18000, episode_reward=171.60 +/- 99.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=19000, episode_reward=484.40 +/- 209.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=20000, episode_reward=379.20 +/- 79.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=21000, episode_reward=261.40 +/- 252.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=22000, episode_reward=253.00 +/- 108.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=23000, episode_reward=384.60 +/- 262.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=24000, episode_reward=401.60 +/- 221.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=25000, episode_reward=286.20 +/- 124.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=26000, episode_reward=151.60 +/- 199.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=27000, episode_reward=216.80 +/- 98.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=28000, episode_reward=178.60 +/- 140.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=29000, episode_reward=489.00 +/- 316.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=30000, episode_reward=421.20 +/- 383.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=31000, episode_reward=538.80 +/- 434.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=32000, episode_reward=697.60 +/- 186.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=33000, episode_reward=617.60 +/- 246.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=34000, episode_reward=674.80 +/- 149.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=35000, episode_reward=905.80 +/- 189.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=36000, episode_reward=871.40 +/- 362.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=37000, episode_reward=975.40 +/- 479.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=38000, episode_reward=977.00 +/- 371.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=39000, episode_reward=939.80 +/- 244.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=40000, episode_reward=816.60 +/- 376.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=41000, episode_reward=636.40 +/- 296.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=42000, episode_reward=972.40 +/- 323.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=43000, episode_reward=738.00 +/- 137.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=44000, episode_reward=1200.60 +/- 209.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=45000, episode_reward=1015.00 +/- 259.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=46000, episode_reward=1064.00 +/- 444.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=47000, episode_reward=1126.40 +/- 183.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=48000, episode_reward=1384.80 +/- 214.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=49000, episode_reward=1195.40 +/- 238.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=50000, episode_reward=-390.40 +/- 3402.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=51000, episode_reward=140.60 +/- 2260.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=52000, episode_reward=1232.40 +/- 270.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=53000, episode_reward=1252.40 +/- 282.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=54000, episode_reward=1068.40 +/- 448.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=55000, episode_reward=957.40 +/- 207.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=56000, episode_reward=-57.40 +/- 2522.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=57000, episode_reward=1293.20 +/- 226.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=58000, episode_reward=-997.00 +/- 4226.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=59000, episode_reward=994.80 +/- 179.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=60000, episode_reward=432.20 +/- 687.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=61000, episode_reward=989.80 +/- 109.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=62000, episode_reward=1101.40 +/- 277.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=63000, episode_reward=832.20 +/- 315.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=64000, episode_reward=1045.80 +/- 335.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=65000, episode_reward=24.40 +/- 2238.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=66000, episode_reward=1440.80 +/- 246.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=67000, episode_reward=1226.80 +/- 401.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=68000, episode_reward=586.00 +/- 1063.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=69000, episode_reward=1247.20 +/- 174.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=70000, episode_reward=1119.80 +/- 458.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=71000, episode_reward=1416.40 +/- 323.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=72000, episode_reward=1205.20 +/- 208.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=73000, episode_reward=1416.40 +/- 174.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=74000, episode_reward=1399.40 +/- 246.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=75000, episode_reward=1559.00 +/- 251.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=76000, episode_reward=1276.00 +/- 270.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=77000, episode_reward=1375.60 +/- 266.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=78000, episode_reward=1406.40 +/- 324.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=79000, episode_reward=1232.60 +/- 298.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=80000, episode_reward=659.40 +/- 1104.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=81000, episode_reward=1590.80 +/- 316.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=82000, episode_reward=1341.40 +/- 355.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=83000, episode_reward=-926.00 +/- 4938.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=84000, episode_reward=1580.00 +/- 270.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=85000, episode_reward=-2186.20 +/- 7312.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=86000, episode_reward=-1497.80 +/- 6393.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=87000, episode_reward=1606.40 +/- 234.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=88000, episode_reward=1579.60 +/- 190.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=89000, episode_reward=1412.80 +/- 232.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=90000, episode_reward=1374.20 +/- 214.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=91000, episode_reward=1528.00 +/- 414.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=92000, episode_reward=1413.00 +/- 259.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=93000, episode_reward=1433.60 +/- 201.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=94000, episode_reward=1718.00 +/- 122.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=95000, episode_reward=1352.00 +/- 368.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=96000, episode_reward=1554.00 +/- 232.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=97000, episode_reward=1353.60 +/- 289.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=98000, episode_reward=1393.00 +/- 229.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=99000, episode_reward=1494.40 +/- 480.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=100000, episode_reward=1407.00 +/- 423.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=101000, episode_reward=1743.20 +/- 274.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=102000, episode_reward=1528.00 +/- 154.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=103000, episode_reward=1578.40 +/- 293.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=104000, episode_reward=1374.00 +/- 163.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=105000, episode_reward=1457.80 +/- 155.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=106000, episode_reward=1566.80 +/- 150.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=107000, episode_reward=1359.60 +/- 228.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=108000, episode_reward=1446.60 +/- 402.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=109000, episode_reward=1521.80 +/- 284.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=110000, episode_reward=1438.00 +/- 196.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=111000, episode_reward=1375.20 +/- 300.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=112000, episode_reward=1164.60 +/- 295.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=113000, episode_reward=1544.80 +/- 366.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=114000, episode_reward=1474.40 +/- 280.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=115000, episode_reward=1375.80 +/- 275.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=116000, episode_reward=1411.40 +/- 162.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=117000, episode_reward=1507.40 +/- 204.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=118000, episode_reward=1622.60 +/- 146.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=119000, episode_reward=1502.40 +/- 233.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=120000, episode_reward=1534.00 +/- 245.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=121000, episode_reward=1701.20 +/- 223.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=122000, episode_reward=1673.40 +/- 227.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=123000, episode_reward=1558.20 +/- 258.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=124000, episode_reward=1684.60 +/- 222.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=125000, episode_reward=1696.00 +/- 276.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=126000, episode_reward=1589.20 +/- 148.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=127000, episode_reward=1552.80 +/- 176.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=128000, episode_reward=1691.20 +/- 182.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=129000, episode_reward=1662.80 +/- 200.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=130000, episode_reward=1517.00 +/- 287.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=131000, episode_reward=1409.20 +/- 142.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=132000, episode_reward=1640.20 +/- 330.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=133000, episode_reward=1564.20 +/- 201.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=134000, episode_reward=1566.80 +/- 105.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=135000, episode_reward=1552.40 +/- 212.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=136000, episode_reward=1521.80 +/- 72.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=137000, episode_reward=1460.60 +/- 178.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=138000, episode_reward=1375.20 +/- 141.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=139000, episode_reward=1541.00 +/- 132.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=140000, episode_reward=1366.40 +/- 224.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=141000, episode_reward=1344.60 +/- 129.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=142000, episode_reward=1485.20 +/- 200.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=143000, episode_reward=1453.60 +/- 79.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=144000, episode_reward=1516.80 +/- 316.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=145000, episode_reward=1549.80 +/- 349.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=146000, episode_reward=1392.60 +/- 225.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=147000, episode_reward=1161.00 +/- 390.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=148000, episode_reward=1414.60 +/- 308.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=149000, episode_reward=1524.80 +/- 102.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=150000, episode_reward=1450.40 +/- 150.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=151000, episode_reward=1288.80 +/- 266.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=152000, episode_reward=1522.20 +/- 205.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=153000, episode_reward=1327.00 +/- 313.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=154000, episode_reward=1465.00 +/- 286.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=155000, episode_reward=1602.80 +/- 158.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=156000, episode_reward=1547.80 +/- 213.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=157000, episode_reward=1511.60 +/- 225.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=158000, episode_reward=1458.00 +/- 342.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=159000, episode_reward=1682.40 +/- 266.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=160000, episode_reward=1443.40 +/- 189.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=161000, episode_reward=1596.40 +/- 235.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=162000, episode_reward=1494.60 +/- 329.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=163000, episode_reward=1624.20 +/- 122.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=164000, episode_reward=1617.80 +/- 228.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=165000, episode_reward=1611.40 +/- 144.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=166000, episode_reward=1550.80 +/- 236.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=167000, episode_reward=1525.20 +/- 284.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=168000, episode_reward=1640.00 +/- 192.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=169000, episode_reward=1727.80 +/- 139.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=170000, episode_reward=1520.00 +/- 235.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=171000, episode_reward=1336.60 +/- 358.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=172000, episode_reward=1585.00 +/- 211.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=173000, episode_reward=1621.80 +/- 404.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=174000, episode_reward=1569.40 +/- 209.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=175000, episode_reward=1618.40 +/- 146.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=176000, episode_reward=1808.20 +/- 179.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=177000, episode_reward=1542.40 +/- 102.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=178000, episode_reward=1664.80 +/- 200.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=179000, episode_reward=1411.00 +/- 270.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=180000, episode_reward=1387.20 +/- 266.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=181000, episode_reward=1507.00 +/- 211.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=182000, episode_reward=1581.40 +/- 173.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=183000, episode_reward=1593.40 +/- 98.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=184000, episode_reward=1567.60 +/- 275.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=185000, episode_reward=1599.60 +/- 173.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=186000, episode_reward=1345.40 +/- 74.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=187000, episode_reward=1550.60 +/- 123.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=188000, episode_reward=1635.00 +/- 162.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=189000, episode_reward=1646.80 +/- 250.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=190000, episode_reward=1369.00 +/- 184.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=191000, episode_reward=1353.60 +/- 180.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=192000, episode_reward=1662.40 +/- 137.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=193000, episode_reward=1647.80 +/- 188.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=194000, episode_reward=1603.60 +/- 120.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=195000, episode_reward=1475.40 +/- 354.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=196000, episode_reward=1474.40 +/- 334.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=197000, episode_reward=1496.60 +/- 243.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=198000, episode_reward=1339.80 +/- 284.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=199000, episode_reward=1446.00 +/- 215.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=200000, episode_reward=1420.60 +/- 353.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=201000, episode_reward=1493.00 +/- 215.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=202000, episode_reward=1715.60 +/- 237.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=203000, episode_reward=1475.60 +/- 318.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=204000, episode_reward=1557.00 +/- 320.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=205000, episode_reward=1214.40 +/- 229.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=206000, episode_reward=1672.00 +/- 147.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=207000, episode_reward=1297.40 +/- 491.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=208000, episode_reward=1712.20 +/- 321.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=209000, episode_reward=1570.40 +/- 90.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=210000, episode_reward=1388.60 +/- 135.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=211000, episode_reward=1428.40 +/- 235.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=212000, episode_reward=1651.00 +/- 124.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=213000, episode_reward=1517.60 +/- 95.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=214000, episode_reward=1612.40 +/- 147.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=215000, episode_reward=1490.80 +/- 174.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=216000, episode_reward=1548.80 +/- 209.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=217000, episode_reward=1442.00 +/- 114.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=218000, episode_reward=1639.00 +/- 123.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=219000, episode_reward=1445.80 +/- 206.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=220000, episode_reward=1385.40 +/- 272.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=221000, episode_reward=1609.80 +/- 232.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=222000, episode_reward=1624.40 +/- 111.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=223000, episode_reward=1647.60 +/- 210.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=224000, episode_reward=1549.40 +/- 249.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=225000, episode_reward=1505.20 +/- 290.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=226000, episode_reward=1670.00 +/- 197.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=227000, episode_reward=1680.40 +/- 219.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=228000, episode_reward=1501.80 +/- 215.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=229000, episode_reward=1589.40 +/- 151.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=230000, episode_reward=1558.00 +/- 186.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=231000, episode_reward=1706.00 +/- 315.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=232000, episode_reward=1661.20 +/- 175.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=233000, episode_reward=1569.20 +/- 300.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=234000, episode_reward=1808.60 +/- 128.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=235000, episode_reward=1577.80 +/- 177.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=236000, episode_reward=1654.00 +/- 212.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=237000, episode_reward=1516.40 +/- 190.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=238000, episode_reward=1622.80 +/- 281.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=239000, episode_reward=1499.60 +/- 162.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=240000, episode_reward=1558.80 +/- 73.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=241000, episode_reward=1453.60 +/- 86.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=242000, episode_reward=1366.00 +/- 462.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=243000, episode_reward=1372.40 +/- 347.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=244000, episode_reward=1687.20 +/- 190.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=245000, episode_reward=1540.60 +/- 245.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=246000, episode_reward=1308.60 +/- 245.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=247000, episode_reward=1424.40 +/- 206.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=248000, episode_reward=1620.60 +/- 149.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=249000, episode_reward=1503.20 +/- 278.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=250000, episode_reward=1462.00 +/- 403.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=251000, episode_reward=1478.20 +/- 219.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=252000, episode_reward=1456.40 +/- 296.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=253000, episode_reward=1651.20 +/- 288.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=254000, episode_reward=1673.80 +/- 283.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=255000, episode_reward=1629.60 +/- 129.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=256000, episode_reward=1714.00 +/- 142.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=257000, episode_reward=1611.80 +/- 274.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=258000, episode_reward=1553.80 +/- 67.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=259000, episode_reward=1534.20 +/- 212.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=260000, episode_reward=1618.00 +/- 150.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=261000, episode_reward=1469.60 +/- 273.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=262000, episode_reward=1530.20 +/- 224.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=263000, episode_reward=1851.00 +/- 115.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=264000, episode_reward=1702.80 +/- 231.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=265000, episode_reward=1687.40 +/- 62.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=266000, episode_reward=1652.80 +/- 151.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=267000, episode_reward=1550.20 +/- 226.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=268000, episode_reward=1499.40 +/- 327.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=269000, episode_reward=1429.20 +/- 402.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=270000, episode_reward=1469.40 +/- 149.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=271000, episode_reward=1372.80 +/- 267.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=272000, episode_reward=1584.00 +/- 166.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=273000, episode_reward=1596.60 +/- 229.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=274000, episode_reward=1494.40 +/- 334.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=275000, episode_reward=1605.60 +/- 229.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=276000, episode_reward=1752.60 +/- 173.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=277000, episode_reward=1656.60 +/- 215.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=278000, episode_reward=1689.20 +/- 68.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=279000, episode_reward=1395.40 +/- 253.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=280000, episode_reward=1501.60 +/- 299.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=281000, episode_reward=1666.00 +/- 195.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=282000, episode_reward=1815.40 +/- 109.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=283000, episode_reward=1645.00 +/- 274.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=284000, episode_reward=1506.00 +/- 284.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=285000, episode_reward=1558.00 +/- 180.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=286000, episode_reward=1647.40 +/- 183.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=287000, episode_reward=1536.80 +/- 331.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=288000, episode_reward=1561.60 +/- 112.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=289000, episode_reward=1470.60 +/- 83.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=290000, episode_reward=1453.20 +/- 268.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=291000, episode_reward=1544.40 +/- 293.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=292000, episode_reward=1541.00 +/- 541.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=293000, episode_reward=1471.00 +/- 217.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=294000, episode_reward=1475.20 +/- 178.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=295000, episode_reward=1522.00 +/- 252.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=296000, episode_reward=1648.80 +/- 216.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=297000, episode_reward=1670.20 +/- 211.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=298000, episode_reward=1338.40 +/- 180.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=299000, episode_reward=1541.60 +/- 262.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=300000, episode_reward=1829.20 +/- 126.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=301000, episode_reward=1579.40 +/- 234.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=302000, episode_reward=1603.60 +/- 228.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=303000, episode_reward=1456.60 +/- 260.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=304000, episode_reward=1453.80 +/- 372.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=305000, episode_reward=1412.40 +/- 454.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=306000, episode_reward=1409.40 +/- 432.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=307000, episode_reward=1731.80 +/- 101.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=308000, episode_reward=1664.80 +/- 150.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=309000, episode_reward=1467.80 +/- 309.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=310000, episode_reward=1447.80 +/- 326.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=311000, episode_reward=1411.80 +/- 256.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=312000, episode_reward=1692.40 +/- 256.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=313000, episode_reward=1440.20 +/- 76.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=314000, episode_reward=1550.80 +/- 255.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=315000, episode_reward=1625.00 +/- 232.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=316000, episode_reward=1425.60 +/- 328.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=317000, episode_reward=1695.40 +/- 245.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=318000, episode_reward=1623.80 +/- 187.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=319000, episode_reward=1452.60 +/- 384.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=320000, episode_reward=1616.20 +/- 351.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=321000, episode_reward=1595.20 +/- 139.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=322000, episode_reward=1667.80 +/- 176.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=323000, episode_reward=1571.60 +/- 164.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=324000, episode_reward=1477.20 +/- 248.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=325000, episode_reward=1505.00 +/- 155.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=326000, episode_reward=1560.60 +/- 205.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=327000, episode_reward=1466.60 +/- 512.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=328000, episode_reward=1633.60 +/- 169.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=329000, episode_reward=1580.40 +/- 184.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=330000, episode_reward=1585.40 +/- 146.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=331000, episode_reward=1397.20 +/- 351.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=332000, episode_reward=1449.60 +/- 383.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=333000, episode_reward=1448.20 +/- 190.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=334000, episode_reward=1615.80 +/- 336.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=335000, episode_reward=1642.00 +/- 158.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=336000, episode_reward=1568.40 +/- 71.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=337000, episode_reward=1570.20 +/- 189.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=338000, episode_reward=1652.40 +/- 281.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=339000, episode_reward=1665.60 +/- 246.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=340000, episode_reward=1497.80 +/- 274.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=341000, episode_reward=1661.20 +/- 206.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=342000, episode_reward=1682.60 +/- 156.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=343000, episode_reward=1498.80 +/- 296.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=344000, episode_reward=1520.40 +/- 187.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=345000, episode_reward=1533.00 +/- 214.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=346000, episode_reward=1655.00 +/- 357.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=347000, episode_reward=1494.80 +/- 142.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=348000, episode_reward=1460.60 +/- 402.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=349000, episode_reward=1420.60 +/- 281.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=350000, episode_reward=1526.60 +/- 235.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=351000, episode_reward=1597.20 +/- 281.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=352000, episode_reward=1388.40 +/- 52.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=353000, episode_reward=1655.80 +/- 223.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=354000, episode_reward=1511.80 +/- 176.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=355000, episode_reward=1564.20 +/- 176.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=356000, episode_reward=1727.20 +/- 167.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=357000, episode_reward=1656.80 +/- 237.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=358000, episode_reward=1496.40 +/- 229.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=359000, episode_reward=1500.80 +/- 294.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=360000, episode_reward=1696.60 +/- 250.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=361000, episode_reward=1584.00 +/- 242.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=362000, episode_reward=1611.40 +/- 133.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=363000, episode_reward=1582.40 +/- 240.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=364000, episode_reward=1478.40 +/- 276.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=365000, episode_reward=1572.20 +/- 148.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=366000, episode_reward=1523.60 +/- 216.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=367000, episode_reward=1549.40 +/- 294.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=368000, episode_reward=1513.40 +/- 267.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=369000, episode_reward=1576.00 +/- 195.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=370000, episode_reward=1321.20 +/- 224.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=371000, episode_reward=1560.60 +/- 270.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=372000, episode_reward=1616.40 +/- 163.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=373000, episode_reward=1553.00 +/- 225.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=374000, episode_reward=1452.40 +/- 172.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=375000, episode_reward=1461.80 +/- 254.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=376000, episode_reward=1615.00 +/- 147.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=377000, episode_reward=1616.20 +/- 129.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=378000, episode_reward=1686.00 +/- 317.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=379000, episode_reward=1854.00 +/- 162.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=380000, episode_reward=1548.20 +/- 267.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=381000, episode_reward=1431.60 +/- 281.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=382000, episode_reward=1554.00 +/- 226.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=383000, episode_reward=1590.20 +/- 151.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=384000, episode_reward=1521.00 +/- 201.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=385000, episode_reward=1343.60 +/- 563.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=386000, episode_reward=1748.40 +/- 270.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=387000, episode_reward=1508.60 +/- 284.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=388000, episode_reward=1670.40 +/- 176.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=389000, episode_reward=1652.80 +/- 353.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=390000, episode_reward=1338.80 +/- 307.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=391000, episode_reward=1798.20 +/- 139.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=392000, episode_reward=1443.00 +/- 117.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=393000, episode_reward=1696.40 +/- 173.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=394000, episode_reward=1630.20 +/- 310.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=395000, episode_reward=1626.60 +/- 129.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=396000, episode_reward=1689.00 +/- 401.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=397000, episode_reward=1682.80 +/- 215.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=398000, episode_reward=1377.40 +/- 258.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=399000, episode_reward=1489.60 +/- 264.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=400000, episode_reward=1681.20 +/- 121.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=401000, episode_reward=1764.00 +/- 258.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=402000, episode_reward=1545.00 +/- 271.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=403000, episode_reward=1484.00 +/- 326.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=404000, episode_reward=1812.00 +/- 111.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=405000, episode_reward=1600.20 +/- 105.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=406000, episode_reward=1713.40 +/- 231.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=407000, episode_reward=1757.40 +/- 66.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=408000, episode_reward=1518.00 +/- 251.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=409000, episode_reward=1479.40 +/- 406.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=410000, episode_reward=1665.20 +/- 236.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=411000, episode_reward=1758.80 +/- 187.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=412000, episode_reward=1517.80 +/- 284.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=413000, episode_reward=1615.40 +/- 129.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=414000, episode_reward=1742.60 +/- 221.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=415000, episode_reward=1360.20 +/- 233.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=416000, episode_reward=1643.00 +/- 210.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=417000, episode_reward=1699.00 +/- 325.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=418000, episode_reward=1806.40 +/- 93.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=419000, episode_reward=1808.40 +/- 280.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=420000, episode_reward=1386.60 +/- 265.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=421000, episode_reward=1673.40 +/- 336.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=422000, episode_reward=1510.60 +/- 132.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=423000, episode_reward=1530.20 +/- 182.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=424000, episode_reward=1613.60 +/- 196.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=425000, episode_reward=1717.40 +/- 192.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=426000, episode_reward=1632.00 +/- 234.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=427000, episode_reward=1659.40 +/- 231.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=428000, episode_reward=1580.80 +/- 221.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=429000, episode_reward=1697.60 +/- 138.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=430000, episode_reward=1780.60 +/- 235.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=431000, episode_reward=1607.60 +/- 248.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=432000, episode_reward=1607.60 +/- 175.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=433000, episode_reward=1568.20 +/- 238.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=434000, episode_reward=1625.60 +/- 212.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=435000, episode_reward=1704.00 +/- 251.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=436000, episode_reward=1485.20 +/- 204.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=437000, episode_reward=1640.60 +/- 198.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=438000, episode_reward=1647.60 +/- 190.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=439000, episode_reward=1568.40 +/- 253.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=440000, episode_reward=1493.20 +/- 303.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=441000, episode_reward=1770.00 +/- 116.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=442000, episode_reward=1576.60 +/- 227.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=443000, episode_reward=1726.80 +/- 295.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=444000, episode_reward=1460.40 +/- 231.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=445000, episode_reward=1462.40 +/- 368.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=446000, episode_reward=1778.40 +/- 132.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=447000, episode_reward=1616.80 +/- 242.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=448000, episode_reward=1535.60 +/- 186.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=449000, episode_reward=1496.60 +/- 238.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=450000, episode_reward=1747.80 +/- 158.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=451000, episode_reward=1622.20 +/- 306.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=452000, episode_reward=1516.80 +/- 181.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=453000, episode_reward=1442.00 +/- 464.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=454000, episode_reward=1661.00 +/- 86.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=455000, episode_reward=1677.60 +/- 124.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=456000, episode_reward=1746.60 +/- 201.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=457000, episode_reward=1754.60 +/- 138.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=458000, episode_reward=1395.60 +/- 115.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=459000, episode_reward=1717.00 +/- 103.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=460000, episode_reward=1430.20 +/- 86.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=461000, episode_reward=1421.40 +/- 253.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=462000, episode_reward=1528.00 +/- 142.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=463000, episode_reward=1548.80 +/- 289.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=464000, episode_reward=1802.80 +/- 174.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=465000, episode_reward=1597.20 +/- 129.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=466000, episode_reward=1733.80 +/- 203.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=467000, episode_reward=1522.80 +/- 241.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=468000, episode_reward=1703.80 +/- 250.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=469000, episode_reward=1694.00 +/- 197.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=470000, episode_reward=1597.60 +/- 127.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=471000, episode_reward=1490.20 +/- 428.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=472000, episode_reward=1560.00 +/- 232.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=473000, episode_reward=1669.00 +/- 147.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=474000, episode_reward=1739.60 +/- 179.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=475000, episode_reward=1715.20 +/- 291.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=476000, episode_reward=1855.60 +/- 149.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=477000, episode_reward=1573.60 +/- 338.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=478000, episode_reward=1680.00 +/- 132.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=479000, episode_reward=1841.40 +/- 146.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=480000, episode_reward=1619.20 +/- 193.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=481000, episode_reward=1572.80 +/- 134.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=482000, episode_reward=1646.40 +/- 290.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=483000, episode_reward=1656.00 +/- 170.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=484000, episode_reward=1515.80 +/- 244.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=485000, episode_reward=1587.20 +/- 289.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=486000, episode_reward=1684.60 +/- 269.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=487000, episode_reward=1474.40 +/- 301.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=488000, episode_reward=1471.40 +/- 227.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=489000, episode_reward=1551.00 +/- 246.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=490000, episode_reward=1736.20 +/- 238.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=491000, episode_reward=1595.00 +/- 173.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=492000, episode_reward=1611.00 +/- 319.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=493000, episode_reward=1457.00 +/- 189.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=494000, episode_reward=1605.00 +/- 180.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=495000, episode_reward=1577.80 +/- 234.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=496000, episode_reward=1575.20 +/- 192.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=497000, episode_reward=1539.60 +/- 279.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=498000, episode_reward=1476.20 +/- 163.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=499000, episode_reward=1646.40 +/- 291.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=500000, episode_reward=1602.40 +/- 153.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=501000, episode_reward=1410.00 +/- 289.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Gamma = 0.8, Learning Rate = 0.0003, Activation Function = ReLU, Neurons = 8, Process noise = 0.1, Measurement noise = 0.0\n",
      "Eval num_timesteps=1000, episode_reward=-7467.00 +/- 73.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=-7508.00 +/- 79.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=3000, episode_reward=165.20 +/- 249.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4000, episode_reward=218.80 +/- 140.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=5000, episode_reward=-14.20 +/- 304.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=6000, episode_reward=-285.00 +/- 188.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=7000, episode_reward=-514.00 +/- 256.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=8000, episode_reward=-223.80 +/- 273.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=9000, episode_reward=94.00 +/- 158.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=10000, episode_reward=412.40 +/- 125.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=11000, episode_reward=61.40 +/- 174.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=12000, episode_reward=271.00 +/- 147.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=13000, episode_reward=440.60 +/- 155.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=14000, episode_reward=539.00 +/- 81.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=15000, episode_reward=327.60 +/- 164.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=16000, episode_reward=400.80 +/- 187.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=17000, episode_reward=493.00 +/- 97.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=18000, episode_reward=287.00 +/- 205.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=19000, episode_reward=430.80 +/- 89.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=20000, episode_reward=425.20 +/- 277.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=21000, episode_reward=353.20 +/- 274.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=22000, episode_reward=534.20 +/- 223.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=23000, episode_reward=203.00 +/- 92.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=24000, episode_reward=250.60 +/- 227.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=25000, episode_reward=385.20 +/- 103.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=26000, episode_reward=153.60 +/- 193.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=27000, episode_reward=199.40 +/- 96.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=28000, episode_reward=307.40 +/- 211.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=29000, episode_reward=176.20 +/- 321.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=30000, episode_reward=319.80 +/- 246.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=31000, episode_reward=199.40 +/- 145.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=32000, episode_reward=268.40 +/- 131.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=33000, episode_reward=176.40 +/- 126.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=34000, episode_reward=403.80 +/- 168.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=35000, episode_reward=155.20 +/- 132.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=36000, episode_reward=319.80 +/- 136.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=37000, episode_reward=333.40 +/- 217.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=38000, episode_reward=213.60 +/- 99.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=39000, episode_reward=464.00 +/- 155.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=40000, episode_reward=279.40 +/- 135.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=41000, episode_reward=431.20 +/- 114.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=42000, episode_reward=325.40 +/- 100.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=43000, episode_reward=139.80 +/- 185.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=44000, episode_reward=222.20 +/- 134.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=45000, episode_reward=335.20 +/- 105.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=46000, episode_reward=167.80 +/- 175.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=47000, episode_reward=343.00 +/- 100.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=48000, episode_reward=130.00 +/- 237.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=49000, episode_reward=362.00 +/- 139.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=50000, episode_reward=247.40 +/- 159.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=51000, episode_reward=265.00 +/- 158.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=52000, episode_reward=299.20 +/- 289.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=53000, episode_reward=172.40 +/- 269.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=54000, episode_reward=398.40 +/- 89.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=55000, episode_reward=150.20 +/- 98.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=56000, episode_reward=320.00 +/- 238.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=57000, episode_reward=228.20 +/- 181.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=58000, episode_reward=328.60 +/- 210.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=59000, episode_reward=249.40 +/- 97.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=60000, episode_reward=377.60 +/- 255.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=61000, episode_reward=301.20 +/- 190.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=62000, episode_reward=299.00 +/- 135.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=63000, episode_reward=434.20 +/- 116.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=64000, episode_reward=431.40 +/- 209.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=65000, episode_reward=193.00 +/- 133.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=66000, episode_reward=431.60 +/- 281.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=67000, episode_reward=179.80 +/- 117.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=68000, episode_reward=278.20 +/- 241.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=69000, episode_reward=455.40 +/- 145.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=70000, episode_reward=364.80 +/- 137.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=71000, episode_reward=434.00 +/- 126.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=72000, episode_reward=411.00 +/- 197.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=73000, episode_reward=330.00 +/- 115.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=74000, episode_reward=75.80 +/- 104.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=75000, episode_reward=204.40 +/- 131.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=76000, episode_reward=333.60 +/- 200.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=77000, episode_reward=168.40 +/- 249.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=78000, episode_reward=256.40 +/- 64.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=79000, episode_reward=306.20 +/- 399.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=80000, episode_reward=391.20 +/- 124.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=81000, episode_reward=358.20 +/- 76.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=82000, episode_reward=467.40 +/- 211.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=83000, episode_reward=208.80 +/- 206.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=84000, episode_reward=285.80 +/- 161.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=85000, episode_reward=345.40 +/- 182.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=86000, episode_reward=356.00 +/- 214.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=87000, episode_reward=290.20 +/- 193.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=88000, episode_reward=231.00 +/- 256.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=89000, episode_reward=323.00 +/- 219.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=90000, episode_reward=442.40 +/- 135.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=91000, episode_reward=281.20 +/- 254.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=92000, episode_reward=282.60 +/- 141.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=93000, episode_reward=370.40 +/- 122.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=94000, episode_reward=254.00 +/- 241.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=95000, episode_reward=235.00 +/- 148.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=96000, episode_reward=395.00 +/- 97.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=97000, episode_reward=314.00 +/- 240.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=98000, episode_reward=400.00 +/- 295.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=99000, episode_reward=427.20 +/- 154.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=100000, episode_reward=425.40 +/- 140.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=101000, episode_reward=332.00 +/- 106.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=102000, episode_reward=306.40 +/- 151.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=103000, episode_reward=247.40 +/- 307.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=104000, episode_reward=301.80 +/- 197.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=105000, episode_reward=309.00 +/- 162.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=106000, episode_reward=388.20 +/- 94.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=107000, episode_reward=247.80 +/- 290.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=108000, episode_reward=350.20 +/- 203.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=109000, episode_reward=435.80 +/- 154.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=110000, episode_reward=174.80 +/- 89.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=111000, episode_reward=550.40 +/- 274.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=112000, episode_reward=446.00 +/- 204.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=113000, episode_reward=884.80 +/- 156.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=114000, episode_reward=685.80 +/- 324.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=115000, episode_reward=803.60 +/- 294.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=116000, episode_reward=695.00 +/- 195.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=117000, episode_reward=854.40 +/- 171.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=118000, episode_reward=1018.00 +/- 351.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=119000, episode_reward=839.40 +/- 291.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=120000, episode_reward=904.80 +/- 268.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=121000, episode_reward=1033.00 +/- 242.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=122000, episode_reward=849.20 +/- 388.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=123000, episode_reward=793.60 +/- 462.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=124000, episode_reward=970.60 +/- 256.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=125000, episode_reward=737.40 +/- 193.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=126000, episode_reward=940.00 +/- 371.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=127000, episode_reward=979.20 +/- 253.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=128000, episode_reward=1117.80 +/- 269.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=129000, episode_reward=937.60 +/- 256.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=130000, episode_reward=906.40 +/- 355.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=131000, episode_reward=1062.20 +/- 268.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=132000, episode_reward=1098.40 +/- 348.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=133000, episode_reward=100.00 +/- 2249.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=134000, episode_reward=1318.40 +/- 241.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=135000, episode_reward=968.80 +/- 332.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=136000, episode_reward=855.80 +/- 141.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=137000, episode_reward=216.40 +/- 1594.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=138000, episode_reward=1172.20 +/- 160.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=139000, episode_reward=811.20 +/- 330.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=140000, episode_reward=861.00 +/- 541.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=141000, episode_reward=-1242.00 +/- 4339.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=142000, episode_reward=1002.60 +/- 71.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=143000, episode_reward=144.40 +/- 1979.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=144000, episode_reward=835.00 +/- 358.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=145000, episode_reward=908.60 +/- 255.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=146000, episode_reward=843.60 +/- 299.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=147000, episode_reward=1021.60 +/- 150.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=148000, episode_reward=1043.40 +/- 299.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=149000, episode_reward=1171.40 +/- 223.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=150000, episode_reward=916.00 +/- 204.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=151000, episode_reward=941.60 +/- 208.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=152000, episode_reward=996.20 +/- 311.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=153000, episode_reward=1110.80 +/- 145.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=154000, episode_reward=857.40 +/- 141.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=155000, episode_reward=933.00 +/- 135.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=156000, episode_reward=845.60 +/- 118.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=157000, episode_reward=-2076.80 +/- 6270.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=158000, episode_reward=988.40 +/- 289.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=159000, episode_reward=989.60 +/- 193.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=160000, episode_reward=1044.80 +/- 306.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=161000, episode_reward=1001.00 +/- 250.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=162000, episode_reward=1110.80 +/- 185.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=163000, episode_reward=1079.40 +/- 205.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=164000, episode_reward=852.00 +/- 213.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=165000, episode_reward=-925.80 +/- 3836.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=166000, episode_reward=764.60 +/- 249.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=167000, episode_reward=1159.40 +/- 123.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=168000, episode_reward=764.20 +/- 373.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=169000, episode_reward=690.20 +/- 418.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=170000, episode_reward=491.80 +/- 895.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=171000, episode_reward=1009.80 +/- 189.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=172000, episode_reward=934.80 +/- 320.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=173000, episode_reward=999.40 +/- 332.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=174000, episode_reward=-2245.00 +/- 6632.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=175000, episode_reward=1040.00 +/- 162.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=176000, episode_reward=901.00 +/- 338.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=177000, episode_reward=960.20 +/- 265.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=178000, episode_reward=804.60 +/- 204.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=179000, episode_reward=-2059.40 +/- 5785.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=180000, episode_reward=900.40 +/- 358.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=181000, episode_reward=951.00 +/- 370.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=182000, episode_reward=866.00 +/- 152.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=183000, episode_reward=1076.20 +/- 197.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=184000, episode_reward=1314.00 +/- 316.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=185000, episode_reward=945.80 +/- 265.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=186000, episode_reward=927.40 +/- 193.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=187000, episode_reward=973.00 +/- 375.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=188000, episode_reward=1121.80 +/- 154.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=189000, episode_reward=1196.40 +/- 122.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=190000, episode_reward=916.60 +/- 239.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=191000, episode_reward=942.40 +/- 314.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=192000, episode_reward=1229.80 +/- 400.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=193000, episode_reward=1059.20 +/- 366.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=194000, episode_reward=1104.20 +/- 362.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=195000, episode_reward=1218.80 +/- 437.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=196000, episode_reward=1193.20 +/- 291.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=197000, episode_reward=1144.40 +/- 315.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=198000, episode_reward=1149.40 +/- 167.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=199000, episode_reward=1468.40 +/- 212.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=200000, episode_reward=1138.20 +/- 206.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=201000, episode_reward=1319.80 +/- 446.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=202000, episode_reward=1448.60 +/- 128.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=203000, episode_reward=1528.60 +/- 233.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=204000, episode_reward=1467.80 +/- 246.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=205000, episode_reward=1069.80 +/- 349.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=206000, episode_reward=1352.20 +/- 114.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=207000, episode_reward=1259.60 +/- 214.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=208000, episode_reward=-1268.20 +/- 4304.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=209000, episode_reward=1213.00 +/- 423.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=210000, episode_reward=729.60 +/- 143.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=211000, episode_reward=997.40 +/- 315.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=212000, episode_reward=1270.80 +/- 77.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=213000, episode_reward=1148.20 +/- 263.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=214000, episode_reward=1184.00 +/- 290.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=215000, episode_reward=1334.40 +/- 280.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=216000, episode_reward=1320.60 +/- 186.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=217000, episode_reward=1548.60 +/- 112.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=218000, episode_reward=1489.40 +/- 275.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=219000, episode_reward=1486.00 +/- 278.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=220000, episode_reward=1570.60 +/- 303.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=221000, episode_reward=1198.00 +/- 99.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=222000, episode_reward=1239.60 +/- 258.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=223000, episode_reward=1453.60 +/- 423.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=224000, episode_reward=1311.60 +/- 255.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=225000, episode_reward=1410.40 +/- 366.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=226000, episode_reward=1536.40 +/- 260.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=227000, episode_reward=1147.40 +/- 316.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=228000, episode_reward=1238.20 +/- 112.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=229000, episode_reward=1221.00 +/- 695.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=230000, episode_reward=1245.40 +/- 278.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=231000, episode_reward=1413.60 +/- 115.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=232000, episode_reward=1055.80 +/- 126.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=233000, episode_reward=1598.60 +/- 253.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=234000, episode_reward=1478.80 +/- 298.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=235000, episode_reward=1502.20 +/- 197.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=236000, episode_reward=1260.20 +/- 392.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=237000, episode_reward=1391.40 +/- 267.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=238000, episode_reward=1221.40 +/- 275.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=239000, episode_reward=1591.40 +/- 100.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=240000, episode_reward=1508.60 +/- 250.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=241000, episode_reward=1496.80 +/- 298.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=242000, episode_reward=1275.40 +/- 250.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=243000, episode_reward=1369.60 +/- 299.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=244000, episode_reward=1212.00 +/- 259.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=245000, episode_reward=1406.00 +/- 216.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=246000, episode_reward=1160.40 +/- 192.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=247000, episode_reward=1539.60 +/- 260.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=248000, episode_reward=1279.20 +/- 88.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=249000, episode_reward=1435.00 +/- 240.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=250000, episode_reward=1302.00 +/- 200.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=251000, episode_reward=1406.40 +/- 259.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=252000, episode_reward=1251.60 +/- 501.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=253000, episode_reward=1490.00 +/- 345.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=254000, episode_reward=1237.40 +/- 248.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=255000, episode_reward=1470.40 +/- 233.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=256000, episode_reward=1653.60 +/- 183.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=257000, episode_reward=1472.60 +/- 231.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=258000, episode_reward=1507.00 +/- 218.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=259000, episode_reward=1467.20 +/- 450.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=260000, episode_reward=1499.20 +/- 252.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=261000, episode_reward=1396.00 +/- 293.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=262000, episode_reward=1472.40 +/- 221.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=263000, episode_reward=1605.80 +/- 273.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=264000, episode_reward=1421.40 +/- 203.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=265000, episode_reward=1589.20 +/- 237.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=266000, episode_reward=1510.40 +/- 461.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=267000, episode_reward=1520.00 +/- 288.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=268000, episode_reward=1559.00 +/- 167.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=269000, episode_reward=1752.20 +/- 130.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=270000, episode_reward=1578.80 +/- 193.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=271000, episode_reward=1288.00 +/- 274.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=272000, episode_reward=1156.60 +/- 96.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=273000, episode_reward=1400.80 +/- 410.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=274000, episode_reward=1521.40 +/- 292.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=275000, episode_reward=1428.00 +/- 353.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=276000, episode_reward=1666.40 +/- 106.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=277000, episode_reward=1732.40 +/- 164.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=278000, episode_reward=1707.20 +/- 208.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=279000, episode_reward=1689.20 +/- 156.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=280000, episode_reward=1477.00 +/- 235.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=281000, episode_reward=1849.20 +/- 163.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=282000, episode_reward=1528.40 +/- 378.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=283000, episode_reward=386.60 +/- 1924.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=284000, episode_reward=1371.60 +/- 340.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=285000, episode_reward=1671.60 +/- 182.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=286000, episode_reward=1652.00 +/- 330.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=287000, episode_reward=1858.60 +/- 292.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=288000, episode_reward=1474.40 +/- 397.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=289000, episode_reward=1643.60 +/- 167.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=290000, episode_reward=1635.40 +/- 146.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=291000, episode_reward=1455.80 +/- 257.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=292000, episode_reward=1535.40 +/- 361.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=293000, episode_reward=1206.40 +/- 255.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=294000, episode_reward=1445.20 +/- 169.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=295000, episode_reward=1554.80 +/- 247.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=296000, episode_reward=1462.20 +/- 261.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=297000, episode_reward=1619.20 +/- 169.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=298000, episode_reward=1694.80 +/- 129.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=299000, episode_reward=1454.00 +/- 187.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=300000, episode_reward=1613.20 +/- 226.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=301000, episode_reward=1496.20 +/- 118.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=302000, episode_reward=1386.80 +/- 338.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=303000, episode_reward=1452.20 +/- 188.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=304000, episode_reward=1634.80 +/- 253.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=305000, episode_reward=1403.40 +/- 184.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=306000, episode_reward=1527.40 +/- 150.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=307000, episode_reward=1616.00 +/- 95.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=308000, episode_reward=1537.20 +/- 181.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=309000, episode_reward=1708.60 +/- 158.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=310000, episode_reward=1542.40 +/- 176.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=311000, episode_reward=1566.80 +/- 244.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=312000, episode_reward=1479.00 +/- 218.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=313000, episode_reward=1517.40 +/- 228.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=314000, episode_reward=1708.80 +/- 316.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=315000, episode_reward=1558.20 +/- 167.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=316000, episode_reward=1619.40 +/- 195.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=317000, episode_reward=1519.80 +/- 205.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=318000, episode_reward=1715.00 +/- 158.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=319000, episode_reward=1511.00 +/- 389.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=320000, episode_reward=1401.00 +/- 173.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=321000, episode_reward=1401.80 +/- 306.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=322000, episode_reward=1467.20 +/- 213.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=323000, episode_reward=1607.60 +/- 81.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=324000, episode_reward=1877.20 +/- 129.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=325000, episode_reward=1460.60 +/- 504.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=326000, episode_reward=1539.20 +/- 361.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=327000, episode_reward=1711.20 +/- 238.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=328000, episode_reward=1319.00 +/- 289.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=329000, episode_reward=1465.80 +/- 209.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=330000, episode_reward=1592.20 +/- 165.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=331000, episode_reward=1485.40 +/- 423.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=332000, episode_reward=1620.20 +/- 267.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=333000, episode_reward=1352.80 +/- 130.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=334000, episode_reward=1442.20 +/- 265.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=335000, episode_reward=1431.60 +/- 208.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=336000, episode_reward=1581.40 +/- 276.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=337000, episode_reward=1525.20 +/- 170.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=338000, episode_reward=1438.00 +/- 197.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=339000, episode_reward=1424.40 +/- 216.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=340000, episode_reward=1307.00 +/- 120.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=341000, episode_reward=1553.20 +/- 233.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=342000, episode_reward=1709.00 +/- 316.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=343000, episode_reward=1763.00 +/- 276.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=344000, episode_reward=1592.20 +/- 242.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=345000, episode_reward=1413.40 +/- 163.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=346000, episode_reward=1476.80 +/- 201.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=347000, episode_reward=1705.40 +/- 215.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=348000, episode_reward=1741.40 +/- 216.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=349000, episode_reward=1664.60 +/- 362.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=350000, episode_reward=1561.00 +/- 223.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=351000, episode_reward=1532.80 +/- 141.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=352000, episode_reward=1478.20 +/- 157.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=353000, episode_reward=1435.40 +/- 44.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=354000, episode_reward=1325.00 +/- 205.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=355000, episode_reward=1574.40 +/- 151.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=356000, episode_reward=1524.40 +/- 155.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=357000, episode_reward=1654.20 +/- 109.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=358000, episode_reward=1614.20 +/- 166.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=359000, episode_reward=1573.40 +/- 146.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=360000, episode_reward=1497.60 +/- 204.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=361000, episode_reward=1645.80 +/- 247.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=362000, episode_reward=1521.80 +/- 163.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=363000, episode_reward=1338.40 +/- 186.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=364000, episode_reward=1399.80 +/- 244.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=365000, episode_reward=1510.60 +/- 361.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=366000, episode_reward=1605.00 +/- 151.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=367000, episode_reward=1529.00 +/- 354.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=368000, episode_reward=1589.80 +/- 193.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=369000, episode_reward=1752.80 +/- 213.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=370000, episode_reward=1429.60 +/- 307.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=371000, episode_reward=1748.60 +/- 210.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=372000, episode_reward=1502.00 +/- 344.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=373000, episode_reward=1557.60 +/- 211.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=374000, episode_reward=1625.40 +/- 223.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=375000, episode_reward=1485.00 +/- 315.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=376000, episode_reward=1586.60 +/- 232.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=377000, episode_reward=1530.80 +/- 207.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=378000, episode_reward=1357.80 +/- 151.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=379000, episode_reward=1481.00 +/- 296.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=380000, episode_reward=1720.60 +/- 114.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=381000, episode_reward=1849.60 +/- 106.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=382000, episode_reward=1478.80 +/- 291.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=383000, episode_reward=1482.40 +/- 197.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=384000, episode_reward=1549.20 +/- 157.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=385000, episode_reward=1534.40 +/- 249.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=386000, episode_reward=1334.40 +/- 182.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=387000, episode_reward=1289.00 +/- 371.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=388000, episode_reward=1496.00 +/- 147.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=389000, episode_reward=1400.40 +/- 442.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=390000, episode_reward=1522.80 +/- 136.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=391000, episode_reward=1407.40 +/- 347.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=392000, episode_reward=1315.80 +/- 243.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=393000, episode_reward=1575.20 +/- 286.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=394000, episode_reward=1638.20 +/- 132.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=395000, episode_reward=1681.80 +/- 255.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=396000, episode_reward=1482.40 +/- 185.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=397000, episode_reward=1334.40 +/- 119.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=398000, episode_reward=1705.40 +/- 186.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=399000, episode_reward=1639.20 +/- 218.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=400000, episode_reward=1463.80 +/- 163.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=401000, episode_reward=1512.80 +/- 197.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=402000, episode_reward=1527.40 +/- 310.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=403000, episode_reward=1552.80 +/- 204.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=404000, episode_reward=1679.00 +/- 216.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=405000, episode_reward=1626.60 +/- 295.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=406000, episode_reward=1558.20 +/- 108.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=407000, episode_reward=1541.00 +/- 339.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=408000, episode_reward=1484.40 +/- 259.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=409000, episode_reward=1509.40 +/- 261.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=410000, episode_reward=1631.60 +/- 258.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=411000, episode_reward=1564.80 +/- 100.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=412000, episode_reward=1446.60 +/- 166.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=413000, episode_reward=1637.20 +/- 225.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=414000, episode_reward=1464.20 +/- 372.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=415000, episode_reward=1535.60 +/- 160.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=416000, episode_reward=1485.20 +/- 245.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=417000, episode_reward=1595.40 +/- 312.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=418000, episode_reward=1584.00 +/- 218.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=419000, episode_reward=1618.00 +/- 198.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=420000, episode_reward=1346.40 +/- 370.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=421000, episode_reward=1268.40 +/- 444.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=422000, episode_reward=1569.60 +/- 307.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=423000, episode_reward=1718.20 +/- 135.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=424000, episode_reward=1686.00 +/- 88.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=425000, episode_reward=1510.00 +/- 277.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=426000, episode_reward=1453.60 +/- 211.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=427000, episode_reward=1542.20 +/- 364.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=428000, episode_reward=1744.80 +/- 239.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=429000, episode_reward=1490.40 +/- 235.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=430000, episode_reward=1652.20 +/- 225.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=431000, episode_reward=1642.20 +/- 220.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=432000, episode_reward=1582.20 +/- 183.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=433000, episode_reward=1596.80 +/- 166.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=434000, episode_reward=1706.00 +/- 151.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=435000, episode_reward=1695.20 +/- 159.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=436000, episode_reward=1479.40 +/- 118.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=437000, episode_reward=1590.00 +/- 192.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=438000, episode_reward=1515.00 +/- 306.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=439000, episode_reward=1467.20 +/- 189.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=440000, episode_reward=1542.40 +/- 160.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=441000, episode_reward=1587.20 +/- 169.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=442000, episode_reward=1234.60 +/- 299.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=443000, episode_reward=1516.00 +/- 209.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=444000, episode_reward=1788.80 +/- 197.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=445000, episode_reward=1661.00 +/- 176.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=446000, episode_reward=1468.20 +/- 298.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=447000, episode_reward=1445.60 +/- 142.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=448000, episode_reward=1608.60 +/- 171.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=449000, episode_reward=1522.40 +/- 378.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=450000, episode_reward=1403.40 +/- 286.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=451000, episode_reward=1536.40 +/- 376.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=452000, episode_reward=1316.80 +/- 264.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=453000, episode_reward=1479.20 +/- 507.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=454000, episode_reward=1488.80 +/- 302.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=455000, episode_reward=1460.40 +/- 201.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=456000, episode_reward=1247.40 +/- 341.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=457000, episode_reward=1536.40 +/- 308.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=458000, episode_reward=1633.00 +/- 191.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=459000, episode_reward=1618.20 +/- 178.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=460000, episode_reward=1150.00 +/- 571.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=461000, episode_reward=1657.60 +/- 302.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=462000, episode_reward=1383.20 +/- 235.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=463000, episode_reward=1525.00 +/- 114.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=464000, episode_reward=1375.00 +/- 325.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=465000, episode_reward=1498.60 +/- 278.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=466000, episode_reward=1502.80 +/- 176.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=467000, episode_reward=1709.20 +/- 125.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=468000, episode_reward=1661.20 +/- 190.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=469000, episode_reward=1585.40 +/- 212.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=470000, episode_reward=1610.20 +/- 175.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=471000, episode_reward=1570.80 +/- 247.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=472000, episode_reward=1502.60 +/- 350.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=473000, episode_reward=1401.80 +/- 176.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=474000, episode_reward=1521.80 +/- 344.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=475000, episode_reward=1670.20 +/- 51.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=476000, episode_reward=1374.20 +/- 274.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=477000, episode_reward=1334.40 +/- 76.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=478000, episode_reward=1450.60 +/- 271.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=479000, episode_reward=1586.00 +/- 146.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=480000, episode_reward=1491.60 +/- 260.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=481000, episode_reward=1507.60 +/- 209.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=482000, episode_reward=1274.80 +/- 192.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=483000, episode_reward=1352.00 +/- 155.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=484000, episode_reward=1650.80 +/- 196.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=485000, episode_reward=1468.20 +/- 147.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=486000, episode_reward=1665.40 +/- 113.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=487000, episode_reward=1520.00 +/- 130.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=488000, episode_reward=1664.20 +/- 114.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=489000, episode_reward=1393.40 +/- 66.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=490000, episode_reward=1862.40 +/- 176.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=491000, episode_reward=1612.80 +/- 404.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=492000, episode_reward=1297.80 +/- 152.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=493000, episode_reward=1316.60 +/- 176.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=494000, episode_reward=1224.20 +/- 294.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=495000, episode_reward=1397.80 +/- 222.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=496000, episode_reward=1533.40 +/- 414.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=497000, episode_reward=1506.40 +/- 229.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=498000, episode_reward=1617.80 +/- 163.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=499000, episode_reward=1504.20 +/- 279.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=500000, episode_reward=1667.40 +/- 174.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=501000, episode_reward=1547.20 +/- 170.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Gamma = 0.8, Learning Rate = 0.0003, Activation Function = ReLU, Neurons = 32, Process noise = 0.1, Measurement noise = 0.0\n",
      "Eval num_timesteps=1000, episode_reward=-1851.60 +/- 701.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=-1655.20 +/- 977.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3000, episode_reward=-4442.80 +/- 1970.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=4000, episode_reward=-4114.60 +/- 2168.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=5000, episode_reward=-5205.00 +/- 1272.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=6000, episode_reward=-5129.00 +/- 1676.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=7000, episode_reward=-5366.60 +/- 1701.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=8000, episode_reward=-5785.40 +/- 1782.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=9000, episode_reward=146.40 +/- 93.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=10000, episode_reward=60.80 +/- 234.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=11000, episode_reward=225.20 +/- 206.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=12000, episode_reward=257.80 +/- 91.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=13000, episode_reward=393.00 +/- 228.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=14000, episode_reward=309.60 +/- 202.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=15000, episode_reward=368.80 +/- 220.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=16000, episode_reward=331.40 +/- 335.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=17000, episode_reward=209.20 +/- 43.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=18000, episode_reward=186.80 +/- 121.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=19000, episode_reward=374.80 +/- 252.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=20000, episode_reward=374.40 +/- 118.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=21000, episode_reward=143.20 +/- 301.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=22000, episode_reward=245.20 +/- 134.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=23000, episode_reward=317.40 +/- 197.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=24000, episode_reward=253.40 +/- 167.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=25000, episode_reward=197.40 +/- 87.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=26000, episode_reward=130.20 +/- 151.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=27000, episode_reward=286.00 +/- 187.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=28000, episode_reward=203.00 +/- 283.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=29000, episode_reward=306.20 +/- 204.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=30000, episode_reward=127.80 +/- 175.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=31000, episode_reward=335.20 +/- 86.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=32000, episode_reward=248.00 +/- 131.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=33000, episode_reward=444.40 +/- 255.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=34000, episode_reward=401.80 +/- 83.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=35000, episode_reward=500.40 +/- 172.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=36000, episode_reward=195.40 +/- 182.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=37000, episode_reward=403.60 +/- 219.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=38000, episode_reward=352.20 +/- 154.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=39000, episode_reward=299.60 +/- 144.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=40000, episode_reward=404.00 +/- 141.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=41000, episode_reward=288.00 +/- 243.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=42000, episode_reward=299.00 +/- 153.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=43000, episode_reward=301.60 +/- 77.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=44000, episode_reward=258.60 +/- 111.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=45000, episode_reward=313.20 +/- 153.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=46000, episode_reward=436.00 +/- 153.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=47000, episode_reward=231.00 +/- 174.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=48000, episode_reward=387.80 +/- 110.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=49000, episode_reward=418.00 +/- 183.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=50000, episode_reward=443.40 +/- 121.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=51000, episode_reward=374.20 +/- 175.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=52000, episode_reward=236.00 +/- 309.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=53000, episode_reward=180.80 +/- 288.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=54000, episode_reward=467.20 +/- 176.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=55000, episode_reward=126.80 +/- 167.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=56000, episode_reward=548.80 +/- 263.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=57000, episode_reward=413.20 +/- 160.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=58000, episode_reward=650.40 +/- 146.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=59000, episode_reward=358.80 +/- 155.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=60000, episode_reward=666.00 +/- 177.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=61000, episode_reward=753.80 +/- 279.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=62000, episode_reward=699.20 +/- 328.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=63000, episode_reward=524.20 +/- 304.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=64000, episode_reward=909.20 +/- 187.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=65000, episode_reward=831.00 +/- 285.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=66000, episode_reward=894.40 +/- 233.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=67000, episode_reward=876.80 +/- 360.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=68000, episode_reward=782.80 +/- 448.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=69000, episode_reward=-2133.00 +/- 6294.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=70000, episode_reward=882.20 +/- 336.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=71000, episode_reward=1128.60 +/- 249.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=72000, episode_reward=1320.20 +/- 127.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=73000, episode_reward=-1189.80 +/- 3724.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=74000, episode_reward=-1186.60 +/- 4575.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=75000, episode_reward=-1213.20 +/- 4263.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=76000, episode_reward=1298.60 +/- 238.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=77000, episode_reward=1261.60 +/- 243.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=78000, episode_reward=1292.60 +/- 235.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=79000, episode_reward=-853.20 +/- 4018.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=80000, episode_reward=1172.40 +/- 213.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=81000, episode_reward=1247.40 +/- 124.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=82000, episode_reward=1010.60 +/- 114.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=83000, episode_reward=-1214.80 +/- 4404.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=84000, episode_reward=902.20 +/- 455.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=85000, episode_reward=799.20 +/- 339.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=86000, episode_reward=1084.60 +/- 204.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=87000, episode_reward=1175.80 +/- 268.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=88000, episode_reward=1172.40 +/- 200.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=89000, episode_reward=924.80 +/- 201.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=90000, episode_reward=1222.00 +/- 217.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=91000, episode_reward=1237.80 +/- 254.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=92000, episode_reward=1159.00 +/- 183.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=93000, episode_reward=1290.20 +/- 262.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=94000, episode_reward=1092.00 +/- 373.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=95000, episode_reward=965.00 +/- 204.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=96000, episode_reward=1163.00 +/- 187.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=97000, episode_reward=1281.00 +/- 265.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=98000, episode_reward=-4.80 +/- 2360.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=99000, episode_reward=941.40 +/- 410.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=100000, episode_reward=1382.40 +/- 229.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=101000, episode_reward=1009.20 +/- 238.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=102000, episode_reward=-649.60 +/- 4008.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=103000, episode_reward=1155.40 +/- 216.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=104000, episode_reward=-4260.40 +/- 6652.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=105000, episode_reward=1203.20 +/- 175.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=106000, episode_reward=972.80 +/- 288.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=107000, episode_reward=-492.20 +/- 2955.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=108000, episode_reward=1392.40 +/- 184.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=109000, episode_reward=977.80 +/- 188.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=110000, episode_reward=1173.20 +/- 282.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=111000, episode_reward=-349.60 +/- 2416.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=112000, episode_reward=397.40 +/- 1245.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=113000, episode_reward=1219.60 +/- 264.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=114000, episode_reward=1147.40 +/- 342.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=115000, episode_reward=1447.40 +/- 72.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=116000, episode_reward=908.00 +/- 233.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=117000, episode_reward=1265.60 +/- 346.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=118000, episode_reward=-644.40 +/- 3063.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=119000, episode_reward=1075.00 +/- 387.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=120000, episode_reward=1139.20 +/- 372.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=121000, episode_reward=1302.20 +/- 411.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=122000, episode_reward=1101.80 +/- 177.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=123000, episode_reward=1267.40 +/- 383.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=124000, episode_reward=1265.20 +/- 183.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=125000, episode_reward=1166.40 +/- 208.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=126000, episode_reward=1066.80 +/- 406.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=127000, episode_reward=1185.60 +/- 378.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=128000, episode_reward=1428.40 +/- 215.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=129000, episode_reward=1284.00 +/- 151.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=130000, episode_reward=1537.60 +/- 309.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=131000, episode_reward=1522.00 +/- 366.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=132000, episode_reward=1480.80 +/- 331.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=133000, episode_reward=1156.20 +/- 230.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=134000, episode_reward=1520.60 +/- 272.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=135000, episode_reward=1179.20 +/- 342.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=136000, episode_reward=1484.40 +/- 384.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=137000, episode_reward=1487.60 +/- 292.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=138000, episode_reward=1179.20 +/- 378.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=139000, episode_reward=-2314.80 +/- 7248.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=140000, episode_reward=1419.00 +/- 258.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=141000, episode_reward=1398.40 +/- 189.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=142000, episode_reward=1227.60 +/- 99.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=143000, episode_reward=1365.80 +/- 212.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=144000, episode_reward=1314.20 +/- 257.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=145000, episode_reward=1371.20 +/- 139.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=146000, episode_reward=-1019.00 +/- 4856.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=147000, episode_reward=1550.80 +/- 305.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=148000, episode_reward=1578.20 +/- 138.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=149000, episode_reward=1365.60 +/- 355.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=150000, episode_reward=1450.00 +/- 268.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=151000, episode_reward=1306.20 +/- 200.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=152000, episode_reward=1418.60 +/- 98.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=153000, episode_reward=1489.00 +/- 289.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=154000, episode_reward=1624.20 +/- 135.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=155000, episode_reward=1663.40 +/- 248.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=156000, episode_reward=1396.20 +/- 136.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=157000, episode_reward=1530.20 +/- 309.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=158000, episode_reward=1391.80 +/- 234.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=159000, episode_reward=1349.60 +/- 324.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=160000, episode_reward=1431.60 +/- 156.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=161000, episode_reward=1205.40 +/- 332.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=162000, episode_reward=1292.80 +/- 357.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=163000, episode_reward=1597.80 +/- 243.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=164000, episode_reward=1570.60 +/- 255.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=165000, episode_reward=1425.20 +/- 245.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=166000, episode_reward=1486.00 +/- 138.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=167000, episode_reward=1557.20 +/- 256.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=168000, episode_reward=1533.20 +/- 292.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=169000, episode_reward=1531.80 +/- 225.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=170000, episode_reward=-276.00 +/- 3860.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=171000, episode_reward=1389.40 +/- 246.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=172000, episode_reward=1560.60 +/- 249.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=173000, episode_reward=1619.00 +/- 180.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=174000, episode_reward=1404.00 +/- 321.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=175000, episode_reward=1463.20 +/- 191.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=176000, episode_reward=1582.20 +/- 168.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=177000, episode_reward=1627.00 +/- 259.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=178000, episode_reward=1501.00 +/- 230.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=179000, episode_reward=1556.00 +/- 183.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=180000, episode_reward=1252.00 +/- 423.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=181000, episode_reward=1487.80 +/- 272.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=182000, episode_reward=1392.80 +/- 168.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=183000, episode_reward=1691.40 +/- 240.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=184000, episode_reward=1468.40 +/- 334.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=185000, episode_reward=1498.40 +/- 167.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=186000, episode_reward=1408.00 +/- 345.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=187000, episode_reward=1510.40 +/- 220.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=188000, episode_reward=1644.20 +/- 428.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=189000, episode_reward=1479.20 +/- 205.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=190000, episode_reward=1507.60 +/- 84.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=191000, episode_reward=1629.20 +/- 226.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=192000, episode_reward=1532.60 +/- 384.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=193000, episode_reward=1479.60 +/- 228.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=194000, episode_reward=1578.40 +/- 283.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=195000, episode_reward=1679.80 +/- 223.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=196000, episode_reward=1702.80 +/- 155.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=197000, episode_reward=1675.40 +/- 289.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=198000, episode_reward=1517.80 +/- 257.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=199000, episode_reward=1488.00 +/- 262.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=200000, episode_reward=1540.20 +/- 160.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=201000, episode_reward=1519.60 +/- 292.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=202000, episode_reward=1519.80 +/- 165.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=203000, episode_reward=1656.60 +/- 209.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=204000, episode_reward=1725.40 +/- 116.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=205000, episode_reward=1632.80 +/- 214.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=206000, episode_reward=1610.00 +/- 293.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=207000, episode_reward=1757.20 +/- 100.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=208000, episode_reward=1645.80 +/- 156.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=209000, episode_reward=1509.60 +/- 192.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=210000, episode_reward=1757.40 +/- 143.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=211000, episode_reward=1590.80 +/- 69.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=212000, episode_reward=1716.20 +/- 282.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=213000, episode_reward=1560.20 +/- 126.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=214000, episode_reward=1276.80 +/- 509.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=215000, episode_reward=1433.00 +/- 113.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=216000, episode_reward=1655.00 +/- 169.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=217000, episode_reward=1518.00 +/- 387.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=218000, episode_reward=1544.80 +/- 132.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=219000, episode_reward=1603.00 +/- 276.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=220000, episode_reward=1845.20 +/- 194.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=221000, episode_reward=1562.80 +/- 278.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=222000, episode_reward=1702.40 +/- 272.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=223000, episode_reward=1595.00 +/- 172.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=224000, episode_reward=1586.80 +/- 219.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=225000, episode_reward=1461.40 +/- 243.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=226000, episode_reward=1561.20 +/- 66.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=227000, episode_reward=1490.00 +/- 344.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=228000, episode_reward=1658.40 +/- 215.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=229000, episode_reward=1509.60 +/- 174.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=230000, episode_reward=1470.20 +/- 329.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=231000, episode_reward=1603.20 +/- 245.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=232000, episode_reward=1771.60 +/- 115.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=233000, episode_reward=1635.20 +/- 150.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=234000, episode_reward=1598.40 +/- 311.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=235000, episode_reward=1449.60 +/- 383.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=236000, episode_reward=1583.40 +/- 310.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=237000, episode_reward=1524.20 +/- 262.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=238000, episode_reward=1672.60 +/- 129.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=239000, episode_reward=1883.00 +/- 97.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=240000, episode_reward=1432.20 +/- 329.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=241000, episode_reward=1587.00 +/- 282.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=242000, episode_reward=1601.20 +/- 205.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=243000, episode_reward=1712.00 +/- 258.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=244000, episode_reward=1671.20 +/- 148.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=245000, episode_reward=1444.00 +/- 146.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=246000, episode_reward=1477.40 +/- 167.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=247000, episode_reward=1540.60 +/- 318.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=248000, episode_reward=1644.20 +/- 143.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=249000, episode_reward=1813.00 +/- 159.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=250000, episode_reward=1789.60 +/- 115.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=251000, episode_reward=1566.20 +/- 210.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=252000, episode_reward=1449.40 +/- 251.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=253000, episode_reward=1481.80 +/- 364.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=254000, episode_reward=1480.40 +/- 289.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=255000, episode_reward=1566.60 +/- 195.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=256000, episode_reward=1409.40 +/- 217.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=257000, episode_reward=1566.80 +/- 265.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=258000, episode_reward=1511.00 +/- 269.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=259000, episode_reward=1566.40 +/- 76.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=260000, episode_reward=1429.00 +/- 229.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=261000, episode_reward=1548.60 +/- 160.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=262000, episode_reward=1570.20 +/- 327.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=263000, episode_reward=1615.60 +/- 324.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=264000, episode_reward=1580.80 +/- 185.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=265000, episode_reward=1495.00 +/- 178.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=266000, episode_reward=1675.60 +/- 191.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=267000, episode_reward=1266.60 +/- 365.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=268000, episode_reward=1488.80 +/- 239.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=269000, episode_reward=1376.80 +/- 166.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=270000, episode_reward=1506.00 +/- 321.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=271000, episode_reward=1495.40 +/- 235.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=272000, episode_reward=1603.20 +/- 169.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=273000, episode_reward=1324.20 +/- 327.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=274000, episode_reward=1522.00 +/- 187.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=275000, episode_reward=1562.60 +/- 232.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=276000, episode_reward=1471.20 +/- 318.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=277000, episode_reward=1560.60 +/- 143.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=278000, episode_reward=1242.20 +/- 271.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=279000, episode_reward=1656.80 +/- 113.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=280000, episode_reward=1760.60 +/- 243.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=281000, episode_reward=1759.40 +/- 168.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=282000, episode_reward=1379.00 +/- 194.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=283000, episode_reward=1477.80 +/- 189.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=284000, episode_reward=1593.00 +/- 135.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=285000, episode_reward=1554.40 +/- 107.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=286000, episode_reward=1611.80 +/- 172.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=287000, episode_reward=1486.60 +/- 245.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=288000, episode_reward=1526.20 +/- 161.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=289000, episode_reward=1567.40 +/- 299.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=290000, episode_reward=1744.60 +/- 288.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=291000, episode_reward=1548.80 +/- 270.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=292000, episode_reward=1561.60 +/- 226.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=293000, episode_reward=1701.80 +/- 216.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=294000, episode_reward=1530.40 +/- 275.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=295000, episode_reward=1739.60 +/- 119.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=296000, episode_reward=1669.60 +/- 108.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=297000, episode_reward=1611.60 +/- 162.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=298000, episode_reward=1670.00 +/- 175.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=299000, episode_reward=1657.80 +/- 142.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=300000, episode_reward=1338.00 +/- 197.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=301000, episode_reward=1475.00 +/- 242.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=302000, episode_reward=1488.40 +/- 277.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=303000, episode_reward=1631.00 +/- 233.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=304000, episode_reward=1812.60 +/- 245.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=305000, episode_reward=1458.80 +/- 273.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=306000, episode_reward=1742.00 +/- 97.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=307000, episode_reward=1673.80 +/- 213.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=308000, episode_reward=1563.00 +/- 295.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=309000, episode_reward=1554.20 +/- 132.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=310000, episode_reward=1658.80 +/- 361.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=311000, episode_reward=1657.00 +/- 200.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=312000, episode_reward=1472.60 +/- 209.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=313000, episode_reward=1623.80 +/- 269.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=314000, episode_reward=1743.60 +/- 166.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=315000, episode_reward=1621.60 +/- 152.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=316000, episode_reward=1652.80 +/- 85.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=317000, episode_reward=1442.00 +/- 221.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=318000, episode_reward=1572.60 +/- 209.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=319000, episode_reward=1703.40 +/- 232.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=320000, episode_reward=1716.20 +/- 185.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=321000, episode_reward=1597.00 +/- 271.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=322000, episode_reward=1537.40 +/- 193.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=323000, episode_reward=1201.20 +/- 362.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=324000, episode_reward=1638.80 +/- 164.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=325000, episode_reward=1662.60 +/- 420.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=326000, episode_reward=1518.40 +/- 189.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=327000, episode_reward=1623.00 +/- 207.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=328000, episode_reward=1490.60 +/- 197.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=329000, episode_reward=1711.80 +/- 204.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=330000, episode_reward=1700.80 +/- 161.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=331000, episode_reward=1544.60 +/- 65.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=332000, episode_reward=1699.20 +/- 377.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=333000, episode_reward=1583.00 +/- 229.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=334000, episode_reward=1721.40 +/- 273.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=335000, episode_reward=1592.60 +/- 227.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=336000, episode_reward=1405.60 +/- 296.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=337000, episode_reward=1617.80 +/- 291.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=338000, episode_reward=1685.20 +/- 122.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=339000, episode_reward=1612.60 +/- 188.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=340000, episode_reward=1767.80 +/- 203.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=341000, episode_reward=1569.00 +/- 163.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=342000, episode_reward=1645.40 +/- 187.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=343000, episode_reward=1559.20 +/- 178.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=344000, episode_reward=1556.00 +/- 167.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=345000, episode_reward=1703.40 +/- 201.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=346000, episode_reward=1712.00 +/- 400.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=347000, episode_reward=1683.00 +/- 149.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=348000, episode_reward=1631.00 +/- 205.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=349000, episode_reward=1475.00 +/- 216.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=350000, episode_reward=1697.60 +/- 296.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=351000, episode_reward=1767.60 +/- 185.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=352000, episode_reward=1334.60 +/- 258.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=353000, episode_reward=1566.40 +/- 169.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=354000, episode_reward=1490.80 +/- 157.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=355000, episode_reward=1295.80 +/- 127.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=356000, episode_reward=1670.80 +/- 154.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=357000, episode_reward=1635.40 +/- 156.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=358000, episode_reward=1516.80 +/- 257.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=359000, episode_reward=1324.80 +/- 147.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=360000, episode_reward=1580.20 +/- 276.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=361000, episode_reward=1470.40 +/- 259.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=362000, episode_reward=1603.40 +/- 66.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=363000, episode_reward=1737.60 +/- 370.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=364000, episode_reward=1518.00 +/- 133.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=365000, episode_reward=1635.60 +/- 118.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=366000, episode_reward=1393.60 +/- 143.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=367000, episode_reward=1487.20 +/- 266.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=368000, episode_reward=1467.40 +/- 178.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=369000, episode_reward=1503.80 +/- 401.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=370000, episode_reward=1746.80 +/- 323.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=371000, episode_reward=1613.80 +/- 332.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=372000, episode_reward=1470.80 +/- 329.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=373000, episode_reward=1714.60 +/- 123.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=374000, episode_reward=1642.00 +/- 147.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=375000, episode_reward=1377.20 +/- 170.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=376000, episode_reward=1541.00 +/- 128.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=377000, episode_reward=1631.20 +/- 345.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=378000, episode_reward=1571.40 +/- 274.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=379000, episode_reward=1622.60 +/- 127.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=380000, episode_reward=1670.00 +/- 305.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=381000, episode_reward=1337.00 +/- 284.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=382000, episode_reward=1431.40 +/- 196.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=383000, episode_reward=1458.00 +/- 182.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=384000, episode_reward=1657.40 +/- 353.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=385000, episode_reward=1532.60 +/- 286.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=386000, episode_reward=1470.60 +/- 266.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=387000, episode_reward=1520.80 +/- 239.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=388000, episode_reward=1507.80 +/- 304.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=389000, episode_reward=1733.00 +/- 221.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=390000, episode_reward=1478.60 +/- 296.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=391000, episode_reward=1614.80 +/- 219.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=392000, episode_reward=1566.00 +/- 244.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=393000, episode_reward=1692.20 +/- 133.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=394000, episode_reward=1579.80 +/- 91.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=395000, episode_reward=1690.80 +/- 201.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=396000, episode_reward=1689.20 +/- 195.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=397000, episode_reward=1574.60 +/- 291.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=398000, episode_reward=1843.80 +/- 120.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=399000, episode_reward=1544.80 +/- 338.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=400000, episode_reward=1476.60 +/- 347.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=401000, episode_reward=1400.40 +/- 363.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=402000, episode_reward=1612.20 +/- 264.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=403000, episode_reward=1743.80 +/- 146.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=404000, episode_reward=1431.00 +/- 217.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=405000, episode_reward=1721.00 +/- 432.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=406000, episode_reward=1384.00 +/- 102.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=407000, episode_reward=1633.00 +/- 151.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=408000, episode_reward=1620.00 +/- 228.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=409000, episode_reward=1515.60 +/- 149.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=410000, episode_reward=1695.20 +/- 154.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=411000, episode_reward=1482.00 +/- 242.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=412000, episode_reward=1458.80 +/- 147.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=413000, episode_reward=1677.60 +/- 226.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=414000, episode_reward=1732.20 +/- 392.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=415000, episode_reward=1303.20 +/- 324.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=416000, episode_reward=1585.40 +/- 318.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=417000, episode_reward=1808.40 +/- 217.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=418000, episode_reward=1422.20 +/- 242.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=419000, episode_reward=1736.00 +/- 112.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=420000, episode_reward=1561.80 +/- 117.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=421000, episode_reward=1393.60 +/- 152.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=422000, episode_reward=1512.80 +/- 356.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=423000, episode_reward=1617.80 +/- 206.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=424000, episode_reward=1562.20 +/- 198.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=425000, episode_reward=1407.00 +/- 368.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=426000, episode_reward=1399.20 +/- 282.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=427000, episode_reward=1656.80 +/- 257.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=428000, episode_reward=1757.20 +/- 151.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=429000, episode_reward=1225.20 +/- 289.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=430000, episode_reward=1600.80 +/- 214.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=431000, episode_reward=1772.80 +/- 181.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=432000, episode_reward=1603.40 +/- 234.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=433000, episode_reward=1621.60 +/- 197.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=434000, episode_reward=1600.40 +/- 203.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=435000, episode_reward=1534.20 +/- 180.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=436000, episode_reward=1455.40 +/- 324.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=437000, episode_reward=1545.80 +/- 191.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=438000, episode_reward=1634.80 +/- 389.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=439000, episode_reward=1674.80 +/- 146.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=440000, episode_reward=1470.40 +/- 343.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=441000, episode_reward=1606.80 +/- 126.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=442000, episode_reward=1642.80 +/- 240.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=443000, episode_reward=1574.60 +/- 140.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=444000, episode_reward=1769.00 +/- 153.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=445000, episode_reward=1652.60 +/- 203.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=446000, episode_reward=1350.80 +/- 103.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=447000, episode_reward=1610.20 +/- 161.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=448000, episode_reward=1687.60 +/- 220.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=449000, episode_reward=1557.40 +/- 229.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=450000, episode_reward=1659.20 +/- 135.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=451000, episode_reward=1457.80 +/- 175.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=452000, episode_reward=1656.80 +/- 200.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=453000, episode_reward=1596.00 +/- 402.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=454000, episode_reward=1416.80 +/- 356.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=455000, episode_reward=1368.80 +/- 152.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=456000, episode_reward=1494.20 +/- 373.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=457000, episode_reward=1528.80 +/- 299.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=458000, episode_reward=1598.20 +/- 314.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=459000, episode_reward=1594.00 +/- 141.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=460000, episode_reward=1697.40 +/- 179.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=461000, episode_reward=1509.60 +/- 268.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=462000, episode_reward=1587.60 +/- 153.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=463000, episode_reward=1420.80 +/- 214.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=464000, episode_reward=1440.00 +/- 134.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=465000, episode_reward=1755.20 +/- 181.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=466000, episode_reward=1605.60 +/- 150.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=467000, episode_reward=1440.80 +/- 71.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=468000, episode_reward=1662.20 +/- 278.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=469000, episode_reward=1266.20 +/- 287.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=470000, episode_reward=1588.40 +/- 201.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=471000, episode_reward=1517.80 +/- 214.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=472000, episode_reward=1658.80 +/- 200.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=473000, episode_reward=1473.40 +/- 280.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=474000, episode_reward=1497.00 +/- 345.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=475000, episode_reward=1484.80 +/- 359.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=476000, episode_reward=1698.00 +/- 162.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=477000, episode_reward=1605.40 +/- 199.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=478000, episode_reward=1685.60 +/- 92.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=479000, episode_reward=1491.60 +/- 209.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=480000, episode_reward=1488.40 +/- 302.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=481000, episode_reward=1544.80 +/- 271.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=482000, episode_reward=1584.80 +/- 223.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=483000, episode_reward=1576.60 +/- 254.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=484000, episode_reward=1413.60 +/- 210.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=485000, episode_reward=1559.60 +/- 159.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=486000, episode_reward=1457.60 +/- 58.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=487000, episode_reward=1751.80 +/- 202.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=488000, episode_reward=1703.80 +/- 277.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=489000, episode_reward=1376.00 +/- 204.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=490000, episode_reward=1599.20 +/- 288.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=491000, episode_reward=1613.40 +/- 240.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=492000, episode_reward=1772.00 +/- 162.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=493000, episode_reward=1503.80 +/- 349.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=494000, episode_reward=1421.80 +/- 349.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=495000, episode_reward=1791.60 +/- 198.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=496000, episode_reward=1621.20 +/- 189.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=497000, episode_reward=1561.40 +/- 161.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=498000, episode_reward=1708.40 +/- 190.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=499000, episode_reward=1806.20 +/- 242.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=500000, episode_reward=1638.20 +/- 254.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=501000, episode_reward=1759.60 +/- 177.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Gamma = 0.8, Learning Rate = 0.0003, Activation Function = ReLU, Neurons = 64, Process noise = 0.1, Measurement noise = 0.0\n",
      "Eval num_timesteps=1000, episode_reward=-7462.00 +/- 70.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=-7508.00 +/- 50.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=3000, episode_reward=-514.40 +/- 468.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4000, episode_reward=-630.00 +/- 147.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=5000, episode_reward=-55.40 +/- 129.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=6000, episode_reward=-189.40 +/- 72.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=7000, episode_reward=77.60 +/- 226.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=8000, episode_reward=62.20 +/- 337.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=9000, episode_reward=258.80 +/- 326.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=10000, episode_reward=164.00 +/- 258.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=11000, episode_reward=362.00 +/- 169.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=12000, episode_reward=447.80 +/- 212.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=13000, episode_reward=288.40 +/- 81.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=14000, episode_reward=387.80 +/- 87.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=15000, episode_reward=230.20 +/- 325.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=16000, episode_reward=104.20 +/- 150.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=17000, episode_reward=462.20 +/- 159.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=18000, episode_reward=561.40 +/- 142.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=19000, episode_reward=147.20 +/- 97.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=20000, episode_reward=331.00 +/- 252.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=21000, episode_reward=387.40 +/- 253.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=22000, episode_reward=289.00 +/- 240.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=23000, episode_reward=324.60 +/- 178.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=24000, episode_reward=351.60 +/- 205.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=25000, episode_reward=146.20 +/- 256.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=26000, episode_reward=125.60 +/- 163.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=27000, episode_reward=180.60 +/- 166.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=28000, episode_reward=229.00 +/- 220.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=29000, episode_reward=286.80 +/- 273.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=30000, episode_reward=222.80 +/- 154.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=31000, episode_reward=248.20 +/- 178.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=32000, episode_reward=230.00 +/- 262.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=33000, episode_reward=297.20 +/- 191.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=34000, episode_reward=440.40 +/- 161.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=35000, episode_reward=130.40 +/- 276.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=36000, episode_reward=301.00 +/- 101.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=37000, episode_reward=446.20 +/- 109.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=38000, episode_reward=435.80 +/- 188.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=39000, episode_reward=245.20 +/- 126.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=40000, episode_reward=353.40 +/- 283.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=41000, episode_reward=276.60 +/- 356.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=42000, episode_reward=277.00 +/- 240.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=43000, episode_reward=474.40 +/- 158.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=44000, episode_reward=176.20 +/- 424.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=45000, episode_reward=341.40 +/- 192.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=46000, episode_reward=404.80 +/- 165.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=47000, episode_reward=274.60 +/- 266.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=48000, episode_reward=240.00 +/- 226.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=49000, episode_reward=353.20 +/- 212.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=50000, episode_reward=652.20 +/- 228.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=51000, episode_reward=448.80 +/- 201.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=52000, episode_reward=292.80 +/- 316.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=53000, episode_reward=462.20 +/- 157.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=54000, episode_reward=971.40 +/- 306.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=55000, episode_reward=803.00 +/- 180.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=56000, episode_reward=960.80 +/- 371.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=57000, episode_reward=991.60 +/- 322.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=58000, episode_reward=782.40 +/- 237.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=59000, episode_reward=826.60 +/- 312.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=60000, episode_reward=1116.00 +/- 157.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=61000, episode_reward=870.60 +/- 209.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=62000, episode_reward=1088.60 +/- 178.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=63000, episode_reward=-979.40 +/- 3629.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=64000, episode_reward=-2474.20 +/- 6558.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=65000, episode_reward=1242.40 +/- 423.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=66000, episode_reward=1199.80 +/- 194.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=67000, episode_reward=-1755.80 +/- 5993.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=68000, episode_reward=-1337.80 +/- 5466.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=69000, episode_reward=934.40 +/- 380.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=70000, episode_reward=1216.00 +/- 235.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=71000, episode_reward=1097.80 +/- 260.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=72000, episode_reward=764.20 +/- 191.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=73000, episode_reward=1075.80 +/- 377.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=74000, episode_reward=1173.60 +/- 182.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=75000, episode_reward=1144.00 +/- 239.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=76000, episode_reward=1054.20 +/- 83.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=77000, episode_reward=986.00 +/- 393.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=78000, episode_reward=1323.40 +/- 236.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=79000, episode_reward=975.40 +/- 190.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=80000, episode_reward=1022.20 +/- 281.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=81000, episode_reward=1039.80 +/- 148.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=82000, episode_reward=862.00 +/- 314.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=83000, episode_reward=906.20 +/- 194.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=84000, episode_reward=1114.80 +/- 199.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=85000, episode_reward=1156.60 +/- 240.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=86000, episode_reward=1148.40 +/- 253.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=87000, episode_reward=-169.20 +/- 3067.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=88000, episode_reward=1033.80 +/- 331.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=89000, episode_reward=1215.40 +/- 221.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=90000, episode_reward=-2649.60 +/- 6232.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=91000, episode_reward=1212.60 +/- 221.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=92000, episode_reward=1270.60 +/- 238.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=93000, episode_reward=-806.80 +/- 4050.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=94000, episode_reward=895.00 +/- 225.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=95000, episode_reward=1562.80 +/- 165.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=96000, episode_reward=1316.20 +/- 276.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=97000, episode_reward=1330.60 +/- 279.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=98000, episode_reward=1260.60 +/- 182.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=99000, episode_reward=1117.00 +/- 163.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=100000, episode_reward=-2139.00 +/- 6669.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=101000, episode_reward=963.20 +/- 811.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=102000, episode_reward=1030.20 +/- 202.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=103000, episode_reward=1386.40 +/- 144.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=104000, episode_reward=1185.40 +/- 240.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=105000, episode_reward=-1243.40 +/- 5331.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=106000, episode_reward=1297.60 +/- 214.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=107000, episode_reward=1265.20 +/- 143.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=108000, episode_reward=1537.60 +/- 373.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=109000, episode_reward=1051.80 +/- 195.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=110000, episode_reward=1145.20 +/- 199.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=111000, episode_reward=1198.60 +/- 454.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=112000, episode_reward=1173.80 +/- 263.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=113000, episode_reward=1169.00 +/- 380.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=114000, episode_reward=1369.40 +/- 186.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=115000, episode_reward=-2039.20 +/- 6473.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=116000, episode_reward=1191.20 +/- 209.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=117000, episode_reward=824.80 +/- 810.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=118000, episode_reward=1381.60 +/- 305.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=119000, episode_reward=1200.20 +/- 355.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=120000, episode_reward=1062.00 +/- 230.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=121000, episode_reward=1314.00 +/- 448.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=122000, episode_reward=1242.40 +/- 337.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=123000, episode_reward=1102.80 +/- 259.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=124000, episode_reward=1361.60 +/- 249.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=125000, episode_reward=1497.00 +/- 219.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=126000, episode_reward=1288.40 +/- 293.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=127000, episode_reward=1363.80 +/- 171.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=128000, episode_reward=1300.80 +/- 233.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=129000, episode_reward=1353.20 +/- 430.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=130000, episode_reward=1545.80 +/- 177.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=131000, episode_reward=1142.80 +/- 788.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=132000, episode_reward=1304.20 +/- 258.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=133000, episode_reward=1581.40 +/- 358.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=134000, episode_reward=1634.60 +/- 208.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=135000, episode_reward=1177.20 +/- 294.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=136000, episode_reward=381.80 +/- 1949.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=137000, episode_reward=1349.60 +/- 81.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=138000, episode_reward=1358.80 +/- 232.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=139000, episode_reward=1536.20 +/- 340.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=140000, episode_reward=1618.60 +/- 89.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=141000, episode_reward=1367.60 +/- 145.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=142000, episode_reward=1366.60 +/- 101.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=143000, episode_reward=1554.60 +/- 276.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=144000, episode_reward=1455.00 +/- 248.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=145000, episode_reward=1561.80 +/- 277.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=146000, episode_reward=1693.80 +/- 186.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=147000, episode_reward=1343.80 +/- 349.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=148000, episode_reward=1609.80 +/- 107.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=149000, episode_reward=1172.40 +/- 372.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=150000, episode_reward=1538.60 +/- 233.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=151000, episode_reward=1569.40 +/- 315.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=152000, episode_reward=1257.80 +/- 176.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=153000, episode_reward=1527.00 +/- 149.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=154000, episode_reward=1341.60 +/- 373.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=155000, episode_reward=1552.00 +/- 211.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=156000, episode_reward=1413.40 +/- 309.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=157000, episode_reward=1364.60 +/- 266.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=158000, episode_reward=1626.40 +/- 166.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=159000, episode_reward=-448.60 +/- 3624.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=160000, episode_reward=1263.40 +/- 577.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=161000, episode_reward=1362.80 +/- 391.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=162000, episode_reward=1350.20 +/- 464.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=163000, episode_reward=1483.60 +/- 234.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=164000, episode_reward=1690.40 +/- 108.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=165000, episode_reward=1538.00 +/- 221.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=166000, episode_reward=1409.40 +/- 414.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=167000, episode_reward=1463.20 +/- 209.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=168000, episode_reward=1596.40 +/- 73.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=169000, episode_reward=1386.40 +/- 102.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=170000, episode_reward=1380.20 +/- 241.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=171000, episode_reward=1246.00 +/- 299.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=172000, episode_reward=1443.80 +/- 236.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=173000, episode_reward=1518.60 +/- 186.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=174000, episode_reward=1634.80 +/- 147.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=175000, episode_reward=1469.40 +/- 165.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=176000, episode_reward=1405.80 +/- 244.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=177000, episode_reward=1478.20 +/- 249.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=178000, episode_reward=1537.40 +/- 317.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=179000, episode_reward=1506.20 +/- 191.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=180000, episode_reward=1733.80 +/- 235.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=181000, episode_reward=1267.40 +/- 193.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=182000, episode_reward=1506.40 +/- 234.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=183000, episode_reward=1394.80 +/- 235.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=184000, episode_reward=1368.60 +/- 255.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=185000, episode_reward=1659.80 +/- 185.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=186000, episode_reward=1524.40 +/- 132.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=187000, episode_reward=1555.60 +/- 213.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=188000, episode_reward=1513.60 +/- 147.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=189000, episode_reward=1111.20 +/- 284.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=190000, episode_reward=1441.20 +/- 123.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=191000, episode_reward=1647.00 +/- 254.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=192000, episode_reward=1508.60 +/- 356.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=193000, episode_reward=1647.20 +/- 202.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=194000, episode_reward=1645.80 +/- 104.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=195000, episode_reward=1653.40 +/- 82.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=196000, episode_reward=1729.20 +/- 157.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=197000, episode_reward=1305.80 +/- 191.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=198000, episode_reward=1691.40 +/- 212.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=199000, episode_reward=1523.20 +/- 269.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=200000, episode_reward=1515.40 +/- 241.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=201000, episode_reward=1604.00 +/- 243.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=202000, episode_reward=1270.80 +/- 239.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=203000, episode_reward=1396.20 +/- 184.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=204000, episode_reward=1427.40 +/- 154.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=205000, episode_reward=1527.80 +/- 336.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=206000, episode_reward=1537.60 +/- 422.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=207000, episode_reward=1522.40 +/- 147.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=208000, episode_reward=1464.00 +/- 252.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=209000, episode_reward=1278.60 +/- 275.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=210000, episode_reward=1391.60 +/- 234.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=211000, episode_reward=1749.40 +/- 126.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=212000, episode_reward=1436.80 +/- 321.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=213000, episode_reward=1270.40 +/- 300.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=214000, episode_reward=1321.20 +/- 246.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=215000, episode_reward=1739.80 +/- 102.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=216000, episode_reward=1560.40 +/- 210.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=217000, episode_reward=1627.60 +/- 185.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=218000, episode_reward=1423.00 +/- 344.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=219000, episode_reward=1513.40 +/- 301.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=220000, episode_reward=1700.80 +/- 181.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=221000, episode_reward=1296.40 +/- 156.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=222000, episode_reward=1613.60 +/- 153.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=223000, episode_reward=1415.80 +/- 347.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=224000, episode_reward=1652.40 +/- 237.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=225000, episode_reward=1670.40 +/- 213.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=226000, episode_reward=1656.80 +/- 115.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=227000, episode_reward=1715.00 +/- 173.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=228000, episode_reward=1379.80 +/- 185.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=229000, episode_reward=1495.20 +/- 345.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=230000, episode_reward=1305.60 +/- 182.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=231000, episode_reward=1568.60 +/- 202.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=232000, episode_reward=1498.20 +/- 217.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=233000, episode_reward=1618.00 +/- 273.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=234000, episode_reward=1728.40 +/- 316.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=235000, episode_reward=1509.00 +/- 184.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=236000, episode_reward=1596.00 +/- 91.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=237000, episode_reward=1516.80 +/- 142.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=238000, episode_reward=1690.00 +/- 176.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=239000, episode_reward=1456.60 +/- 196.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=240000, episode_reward=1565.00 +/- 189.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=241000, episode_reward=1494.40 +/- 269.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=242000, episode_reward=1467.00 +/- 257.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=243000, episode_reward=1545.20 +/- 325.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=244000, episode_reward=1502.60 +/- 248.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=245000, episode_reward=1657.20 +/- 196.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=246000, episode_reward=1763.40 +/- 216.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=247000, episode_reward=1259.00 +/- 385.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=248000, episode_reward=1660.40 +/- 345.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=249000, episode_reward=1643.80 +/- 144.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=250000, episode_reward=1445.20 +/- 313.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=251000, episode_reward=1471.00 +/- 233.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=252000, episode_reward=1712.00 +/- 115.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=253000, episode_reward=1532.20 +/- 256.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=254000, episode_reward=1414.20 +/- 367.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=255000, episode_reward=1640.40 +/- 42.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=256000, episode_reward=1395.00 +/- 179.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=257000, episode_reward=1534.00 +/- 224.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=258000, episode_reward=1422.40 +/- 285.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=259000, episode_reward=1537.20 +/- 202.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=260000, episode_reward=1549.00 +/- 168.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=261000, episode_reward=1460.80 +/- 303.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=262000, episode_reward=1452.40 +/- 123.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=263000, episode_reward=1465.40 +/- 410.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=264000, episode_reward=1241.40 +/- 170.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=265000, episode_reward=1337.60 +/- 329.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=266000, episode_reward=1481.00 +/- 201.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=267000, episode_reward=1442.60 +/- 307.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=268000, episode_reward=1583.60 +/- 134.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=269000, episode_reward=1629.60 +/- 307.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=270000, episode_reward=1686.40 +/- 196.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=271000, episode_reward=1663.60 +/- 102.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=272000, episode_reward=1636.60 +/- 190.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=273000, episode_reward=1533.80 +/- 424.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=274000, episode_reward=1563.60 +/- 241.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=275000, episode_reward=1412.20 +/- 225.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=276000, episode_reward=1334.40 +/- 151.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=277000, episode_reward=1522.40 +/- 109.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=278000, episode_reward=1309.20 +/- 373.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=279000, episode_reward=1646.80 +/- 186.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=280000, episode_reward=1570.40 +/- 263.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=281000, episode_reward=1490.40 +/- 270.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=282000, episode_reward=1534.00 +/- 219.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=283000, episode_reward=1636.00 +/- 185.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=284000, episode_reward=1445.60 +/- 305.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=285000, episode_reward=1313.60 +/- 362.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=286000, episode_reward=1525.00 +/- 266.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=287000, episode_reward=1533.40 +/- 135.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=288000, episode_reward=1593.80 +/- 211.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=289000, episode_reward=1459.80 +/- 240.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=290000, episode_reward=1492.00 +/- 234.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=291000, episode_reward=1543.80 +/- 234.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=292000, episode_reward=1624.00 +/- 95.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=293000, episode_reward=1554.00 +/- 234.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=294000, episode_reward=1569.20 +/- 327.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=295000, episode_reward=1476.20 +/- 155.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=296000, episode_reward=1485.40 +/- 266.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=297000, episode_reward=1596.20 +/- 117.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=298000, episode_reward=1596.60 +/- 164.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=299000, episode_reward=1461.00 +/- 170.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=300000, episode_reward=1643.00 +/- 169.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=301000, episode_reward=1650.00 +/- 320.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=302000, episode_reward=1584.60 +/- 211.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=303000, episode_reward=1449.20 +/- 155.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=304000, episode_reward=1591.20 +/- 214.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=305000, episode_reward=1591.80 +/- 259.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=306000, episode_reward=1599.40 +/- 147.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=307000, episode_reward=1627.20 +/- 358.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=308000, episode_reward=1533.40 +/- 147.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=309000, episode_reward=1370.60 +/- 215.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=310000, episode_reward=1440.20 +/- 347.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=311000, episode_reward=1526.60 +/- 273.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=312000, episode_reward=1528.80 +/- 224.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=313000, episode_reward=1597.80 +/- 326.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=314000, episode_reward=1559.40 +/- 238.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=315000, episode_reward=1470.60 +/- 282.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=316000, episode_reward=1578.80 +/- 363.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=317000, episode_reward=1340.80 +/- 322.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=318000, episode_reward=1328.00 +/- 301.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=319000, episode_reward=1556.20 +/- 166.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=320000, episode_reward=1661.40 +/- 251.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=321000, episode_reward=1777.60 +/- 212.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=322000, episode_reward=1709.00 +/- 177.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=323000, episode_reward=1449.40 +/- 188.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=324000, episode_reward=1647.60 +/- 252.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=325000, episode_reward=1667.00 +/- 240.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=326000, episode_reward=1413.00 +/- 218.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=327000, episode_reward=1404.00 +/- 104.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=328000, episode_reward=1528.20 +/- 220.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=329000, episode_reward=1608.00 +/- 262.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=330000, episode_reward=1623.60 +/- 299.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=331000, episode_reward=1670.20 +/- 243.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=332000, episode_reward=1633.00 +/- 152.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=333000, episode_reward=1459.80 +/- 144.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=334000, episode_reward=1620.20 +/- 293.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=335000, episode_reward=1474.40 +/- 390.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=336000, episode_reward=1506.80 +/- 203.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=337000, episode_reward=1500.20 +/- 241.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=338000, episode_reward=1558.80 +/- 229.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=339000, episode_reward=1580.40 +/- 169.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=340000, episode_reward=1441.80 +/- 280.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=341000, episode_reward=1617.00 +/- 262.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=342000, episode_reward=1606.60 +/- 135.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=343000, episode_reward=1513.00 +/- 328.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=344000, episode_reward=1533.20 +/- 245.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=345000, episode_reward=1548.60 +/- 184.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=346000, episode_reward=1709.40 +/- 226.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=347000, episode_reward=1629.80 +/- 298.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=348000, episode_reward=1531.80 +/- 395.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=349000, episode_reward=1367.60 +/- 310.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=350000, episode_reward=1838.60 +/- 121.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=351000, episode_reward=1424.20 +/- 154.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=352000, episode_reward=1520.20 +/- 257.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=353000, episode_reward=1494.40 +/- 238.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=354000, episode_reward=1592.20 +/- 233.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=355000, episode_reward=1438.80 +/- 328.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=356000, episode_reward=1283.40 +/- 231.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=357000, episode_reward=1506.20 +/- 241.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=358000, episode_reward=1420.60 +/- 219.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=359000, episode_reward=1509.00 +/- 164.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=360000, episode_reward=1783.40 +/- 262.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=361000, episode_reward=1573.40 +/- 107.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=362000, episode_reward=1536.20 +/- 196.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=363000, episode_reward=1577.80 +/- 272.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=364000, episode_reward=1641.40 +/- 266.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=365000, episode_reward=1529.60 +/- 113.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=366000, episode_reward=1789.20 +/- 225.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=367000, episode_reward=1535.80 +/- 375.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=368000, episode_reward=1547.40 +/- 288.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=369000, episode_reward=1401.60 +/- 141.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=370000, episode_reward=1542.60 +/- 274.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=371000, episode_reward=1535.40 +/- 312.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=372000, episode_reward=1554.40 +/- 172.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=373000, episode_reward=1552.80 +/- 243.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=374000, episode_reward=1366.00 +/- 416.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=375000, episode_reward=1509.80 +/- 352.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=376000, episode_reward=1295.20 +/- 403.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=377000, episode_reward=1637.60 +/- 179.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=378000, episode_reward=1472.00 +/- 184.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=379000, episode_reward=1723.80 +/- 159.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=380000, episode_reward=1667.80 +/- 217.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=381000, episode_reward=1618.80 +/- 234.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=382000, episode_reward=1704.60 +/- 113.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=383000, episode_reward=1630.40 +/- 178.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=384000, episode_reward=1539.40 +/- 201.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=385000, episode_reward=1617.80 +/- 196.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=386000, episode_reward=1765.00 +/- 29.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=387000, episode_reward=1500.20 +/- 355.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=388000, episode_reward=1405.20 +/- 185.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=389000, episode_reward=1697.40 +/- 226.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=390000, episode_reward=1742.60 +/- 140.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=391000, episode_reward=1537.20 +/- 251.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=392000, episode_reward=1650.20 +/- 238.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=393000, episode_reward=1514.80 +/- 271.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=394000, episode_reward=1599.60 +/- 133.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=395000, episode_reward=1640.80 +/- 191.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=396000, episode_reward=1489.80 +/- 250.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=397000, episode_reward=1563.60 +/- 152.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=398000, episode_reward=1665.60 +/- 128.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=399000, episode_reward=1628.40 +/- 182.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=400000, episode_reward=1469.20 +/- 213.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=401000, episode_reward=1546.80 +/- 256.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=402000, episode_reward=1757.20 +/- 168.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=403000, episode_reward=1554.80 +/- 290.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=404000, episode_reward=1629.20 +/- 157.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=405000, episode_reward=1726.80 +/- 57.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=406000, episode_reward=1581.60 +/- 157.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=407000, episode_reward=1571.40 +/- 240.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=408000, episode_reward=1570.40 +/- 160.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=409000, episode_reward=1520.80 +/- 159.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=410000, episode_reward=1684.80 +/- 115.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=411000, episode_reward=1801.60 +/- 106.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=412000, episode_reward=1555.00 +/- 286.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=413000, episode_reward=1528.60 +/- 245.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=414000, episode_reward=1382.00 +/- 109.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=415000, episode_reward=1582.80 +/- 60.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=416000, episode_reward=1814.40 +/- 146.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=417000, episode_reward=1672.60 +/- 150.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=418000, episode_reward=1551.00 +/- 137.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=419000, episode_reward=1617.60 +/- 200.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=420000, episode_reward=1492.60 +/- 219.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=421000, episode_reward=1680.20 +/- 278.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=422000, episode_reward=1710.80 +/- 152.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=423000, episode_reward=1690.20 +/- 160.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=424000, episode_reward=1484.40 +/- 337.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=425000, episode_reward=1592.60 +/- 295.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=426000, episode_reward=1532.80 +/- 363.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=427000, episode_reward=1664.20 +/- 311.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=428000, episode_reward=1582.40 +/- 247.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=429000, episode_reward=1455.00 +/- 223.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=430000, episode_reward=1508.40 +/- 293.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=431000, episode_reward=1538.40 +/- 139.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=432000, episode_reward=1394.20 +/- 228.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=433000, episode_reward=1466.00 +/- 235.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=434000, episode_reward=1538.00 +/- 199.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=435000, episode_reward=1768.80 +/- 219.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=436000, episode_reward=1819.00 +/- 155.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=437000, episode_reward=1563.60 +/- 210.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=438000, episode_reward=1493.40 +/- 218.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=439000, episode_reward=1648.20 +/- 103.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=440000, episode_reward=1563.20 +/- 265.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=441000, episode_reward=1520.40 +/- 409.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=442000, episode_reward=1778.60 +/- 136.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=443000, episode_reward=1627.60 +/- 227.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=444000, episode_reward=1747.40 +/- 201.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=445000, episode_reward=1684.40 +/- 327.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=446000, episode_reward=1505.00 +/- 183.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=447000, episode_reward=1799.20 +/- 172.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=448000, episode_reward=1610.60 +/- 258.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=449000, episode_reward=1600.00 +/- 191.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=450000, episode_reward=1649.60 +/- 115.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=451000, episode_reward=1551.00 +/- 207.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=452000, episode_reward=1807.00 +/- 82.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=453000, episode_reward=1350.80 +/- 221.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=454000, episode_reward=1475.80 +/- 285.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=455000, episode_reward=1572.80 +/- 243.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=456000, episode_reward=1634.40 +/- 159.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=457000, episode_reward=1676.00 +/- 188.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=458000, episode_reward=1551.80 +/- 285.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=459000, episode_reward=1661.40 +/- 152.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=460000, episode_reward=1681.40 +/- 199.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=461000, episode_reward=1450.00 +/- 262.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=462000, episode_reward=1609.60 +/- 259.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=463000, episode_reward=1506.00 +/- 339.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=464000, episode_reward=1539.80 +/- 221.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=465000, episode_reward=1624.00 +/- 253.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=466000, episode_reward=1558.80 +/- 102.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=467000, episode_reward=1391.20 +/- 256.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=468000, episode_reward=1485.60 +/- 224.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=469000, episode_reward=1516.20 +/- 261.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=470000, episode_reward=1619.60 +/- 153.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=471000, episode_reward=1637.80 +/- 196.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=472000, episode_reward=1451.20 +/- 144.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=473000, episode_reward=1401.60 +/- 271.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=474000, episode_reward=1629.40 +/- 206.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=475000, episode_reward=1634.80 +/- 143.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=476000, episode_reward=1586.40 +/- 133.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=477000, episode_reward=1534.00 +/- 201.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=478000, episode_reward=1526.40 +/- 95.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=479000, episode_reward=1597.80 +/- 253.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=480000, episode_reward=1506.00 +/- 169.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=481000, episode_reward=1610.60 +/- 151.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=482000, episode_reward=1601.20 +/- 338.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=483000, episode_reward=1713.20 +/- 157.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=484000, episode_reward=1479.60 +/- 183.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=485000, episode_reward=1694.40 +/- 237.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=486000, episode_reward=1660.40 +/- 167.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=487000, episode_reward=1553.80 +/- 353.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=488000, episode_reward=1567.40 +/- 116.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=489000, episode_reward=1542.80 +/- 222.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=490000, episode_reward=1510.00 +/- 189.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=491000, episode_reward=1466.80 +/- 67.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=492000, episode_reward=1580.80 +/- 152.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=493000, episode_reward=1451.40 +/- 301.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=494000, episode_reward=1549.80 +/- 104.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=495000, episode_reward=1443.00 +/- 214.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=496000, episode_reward=1693.80 +/- 306.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=497000, episode_reward=1518.00 +/- 252.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=498000, episode_reward=1718.40 +/- 134.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=499000, episode_reward=1446.00 +/- 269.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=500000, episode_reward=1653.80 +/- 172.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=501000, episode_reward=1432.80 +/- 147.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Gamma = 0.8, Learning Rate = 3e-05, Activation Function = Tanh, Neurons = 8, Process noise = 0.1, Measurement noise = 0.0\n",
      "Eval num_timesteps=1000, episode_reward=-3464.00 +/- 70.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=-3520.00 +/- 53.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=3000, episode_reward=-71.00 +/- 359.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4000, episode_reward=52.20 +/- 185.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=5000, episode_reward=-1526.00 +/- 557.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=6000, episode_reward=-1203.80 +/- 205.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=7000, episode_reward=83.40 +/- 205.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=8000, episode_reward=-24.00 +/- 164.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=9000, episode_reward=61.00 +/- 235.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=10000, episode_reward=-18.00 +/- 116.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=11000, episode_reward=106.60 +/- 239.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=12000, episode_reward=-62.80 +/- 210.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=13000, episode_reward=-18.60 +/- 70.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=14000, episode_reward=77.60 +/- 154.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=15000, episode_reward=-129.80 +/- 176.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=16000, episode_reward=-198.40 +/- 217.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=17000, episode_reward=71.20 +/- 143.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=18000, episode_reward=3.60 +/- 203.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=19000, episode_reward=74.00 +/- 166.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=20000, episode_reward=245.80 +/- 116.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=21000, episode_reward=-67.00 +/- 206.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=22000, episode_reward=-5.00 +/- 357.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=23000, episode_reward=-209.20 +/- 136.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=24000, episode_reward=-167.40 +/- 154.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=25000, episode_reward=-313.40 +/- 231.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=26000, episode_reward=-13.00 +/- 135.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=27000, episode_reward=-248.20 +/- 266.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=28000, episode_reward=-523.60 +/- 378.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=29000, episode_reward=-486.00 +/- 113.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=30000, episode_reward=-609.60 +/- 347.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=31000, episode_reward=-453.40 +/- 250.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=32000, episode_reward=-344.00 +/- 330.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=33000, episode_reward=-356.60 +/- 283.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=34000, episode_reward=-529.40 +/- 188.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=35000, episode_reward=-404.80 +/- 336.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=36000, episode_reward=-605.20 +/- 500.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=37000, episode_reward=-741.60 +/- 264.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=38000, episode_reward=-917.20 +/- 156.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=39000, episode_reward=-515.80 +/- 181.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=40000, episode_reward=-428.00 +/- 172.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=41000, episode_reward=-695.40 +/- 72.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=42000, episode_reward=-516.80 +/- 517.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=43000, episode_reward=-423.20 +/- 171.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=44000, episode_reward=-395.60 +/- 122.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=45000, episode_reward=-463.80 +/- 376.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=46000, episode_reward=-199.80 +/- 183.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=47000, episode_reward=-155.80 +/- 325.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=48000, episode_reward=-108.40 +/- 183.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=49000, episode_reward=-445.60 +/- 339.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=50000, episode_reward=-541.80 +/- 415.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=51000, episode_reward=-335.60 +/- 239.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=52000, episode_reward=-139.00 +/- 137.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=53000, episode_reward=-191.80 +/- 191.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=54000, episode_reward=-173.00 +/- 303.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=55000, episode_reward=-503.00 +/- 256.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=56000, episode_reward=-460.80 +/- 143.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=57000, episode_reward=-170.40 +/- 343.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=58000, episode_reward=-320.40 +/- 253.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=59000, episode_reward=-379.20 +/- 306.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=60000, episode_reward=-398.60 +/- 153.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=61000, episode_reward=-207.40 +/- 235.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=62000, episode_reward=-226.40 +/- 129.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=63000, episode_reward=-149.00 +/- 217.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=64000, episode_reward=-126.80 +/- 172.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=65000, episode_reward=-314.20 +/- 131.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=66000, episode_reward=-261.00 +/- 168.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=67000, episode_reward=-309.60 +/- 318.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=68000, episode_reward=-423.00 +/- 256.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=69000, episode_reward=-171.20 +/- 282.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=70000, episode_reward=12.20 +/- 231.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=71000, episode_reward=-309.40 +/- 176.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=72000, episode_reward=-246.60 +/- 202.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=73000, episode_reward=-211.60 +/- 255.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=74000, episode_reward=-241.20 +/- 178.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=75000, episode_reward=-224.80 +/- 300.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=76000, episode_reward=-306.40 +/- 170.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=77000, episode_reward=-251.20 +/- 344.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=78000, episode_reward=-555.80 +/- 160.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=79000, episode_reward=-294.00 +/- 157.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=80000, episode_reward=-271.40 +/- 156.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=81000, episode_reward=-152.00 +/- 211.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=82000, episode_reward=-414.60 +/- 276.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=83000, episode_reward=-378.80 +/- 91.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=84000, episode_reward=-362.40 +/- 199.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=85000, episode_reward=-569.40 +/- 243.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=86000, episode_reward=-265.00 +/- 283.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=87000, episode_reward=-477.00 +/- 246.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=88000, episode_reward=5.00 +/- 224.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=89000, episode_reward=-94.40 +/- 336.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=90000, episode_reward=-159.40 +/- 134.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=91000, episode_reward=-238.00 +/- 356.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=92000, episode_reward=-324.00 +/- 124.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=93000, episode_reward=101.00 +/- 308.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=94000, episode_reward=11.40 +/- 255.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=95000, episode_reward=-263.40 +/- 257.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=96000, episode_reward=-187.60 +/- 202.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=97000, episode_reward=77.80 +/- 203.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=98000, episode_reward=-121.60 +/- 257.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=99000, episode_reward=-171.00 +/- 252.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=100000, episode_reward=-33.60 +/- 157.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=101000, episode_reward=-98.00 +/- 184.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=102000, episode_reward=-67.40 +/- 267.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=103000, episode_reward=51.60 +/- 216.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=104000, episode_reward=41.20 +/- 236.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=105000, episode_reward=-57.40 +/- 208.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=106000, episode_reward=112.40 +/- 259.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=107000, episode_reward=-49.80 +/- 227.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=108000, episode_reward=90.60 +/- 287.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=109000, episode_reward=-102.40 +/- 147.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=110000, episode_reward=-12.80 +/- 259.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=111000, episode_reward=-2.20 +/- 189.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=112000, episode_reward=75.20 +/- 163.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=113000, episode_reward=68.60 +/- 113.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=114000, episode_reward=157.00 +/- 104.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=115000, episode_reward=148.60 +/- 133.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=116000, episode_reward=100.60 +/- 225.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=117000, episode_reward=-99.80 +/- 105.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=118000, episode_reward=194.40 +/- 252.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=119000, episode_reward=96.60 +/- 291.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=120000, episode_reward=-4.60 +/- 172.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=121000, episode_reward=-25.40 +/- 292.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=122000, episode_reward=-84.00 +/- 271.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=123000, episode_reward=-0.40 +/- 181.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=124000, episode_reward=164.60 +/- 354.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=125000, episode_reward=86.80 +/- 157.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=126000, episode_reward=245.60 +/- 200.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=127000, episode_reward=30.00 +/- 60.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=128000, episode_reward=11.20 +/- 229.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=129000, episode_reward=83.40 +/- 149.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=130000, episode_reward=121.40 +/- 80.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=131000, episode_reward=110.40 +/- 217.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=132000, episode_reward=-9.80 +/- 273.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=133000, episode_reward=52.40 +/- 163.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=134000, episode_reward=74.40 +/- 118.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=135000, episode_reward=231.40 +/- 315.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=136000, episode_reward=105.60 +/- 150.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=137000, episode_reward=-120.60 +/- 267.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=138000, episode_reward=21.60 +/- 184.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=139000, episode_reward=120.60 +/- 160.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=140000, episode_reward=134.40 +/- 250.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=141000, episode_reward=8.40 +/- 155.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=142000, episode_reward=48.20 +/- 235.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=143000, episode_reward=81.40 +/- 250.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=144000, episode_reward=215.40 +/- 213.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=145000, episode_reward=229.20 +/- 201.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=146000, episode_reward=175.20 +/- 150.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=147000, episode_reward=182.60 +/- 115.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=148000, episode_reward=243.40 +/- 205.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=149000, episode_reward=250.40 +/- 270.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=150000, episode_reward=49.20 +/- 303.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=151000, episode_reward=140.80 +/- 312.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=152000, episode_reward=212.80 +/- 148.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=153000, episode_reward=202.40 +/- 174.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=154000, episode_reward=229.80 +/- 231.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=155000, episode_reward=176.60 +/- 138.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=156000, episode_reward=328.60 +/- 168.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=157000, episode_reward=273.20 +/- 237.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=158000, episode_reward=184.60 +/- 251.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=159000, episode_reward=232.60 +/- 238.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=160000, episode_reward=239.40 +/- 98.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=161000, episode_reward=99.60 +/- 222.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=162000, episode_reward=382.40 +/- 228.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=163000, episode_reward=254.80 +/- 135.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=164000, episode_reward=140.60 +/- 178.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=165000, episode_reward=482.20 +/- 156.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=166000, episode_reward=142.20 +/- 222.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=167000, episode_reward=124.20 +/- 205.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=168000, episode_reward=262.40 +/- 216.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=169000, episode_reward=150.00 +/- 194.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=170000, episode_reward=302.60 +/- 179.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=171000, episode_reward=147.60 +/- 273.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=172000, episode_reward=115.80 +/- 280.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=173000, episode_reward=293.60 +/- 130.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=174000, episode_reward=178.60 +/- 105.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=175000, episode_reward=275.00 +/- 163.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=176000, episode_reward=295.20 +/- 130.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=177000, episode_reward=269.60 +/- 80.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=178000, episode_reward=202.20 +/- 238.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=179000, episode_reward=222.20 +/- 126.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=180000, episode_reward=133.00 +/- 166.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=181000, episode_reward=368.00 +/- 98.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=182000, episode_reward=62.20 +/- 146.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=183000, episode_reward=193.20 +/- 146.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=184000, episode_reward=162.60 +/- 164.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=185000, episode_reward=161.40 +/- 184.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=186000, episode_reward=183.00 +/- 192.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=187000, episode_reward=291.60 +/- 115.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=188000, episode_reward=260.40 +/- 88.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=189000, episode_reward=197.60 +/- 270.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=190000, episode_reward=386.60 +/- 150.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=191000, episode_reward=340.00 +/- 148.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=192000, episode_reward=230.20 +/- 221.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=193000, episode_reward=94.80 +/- 263.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=194000, episode_reward=348.60 +/- 144.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=195000, episode_reward=253.00 +/- 243.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=196000, episode_reward=298.60 +/- 195.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=197000, episode_reward=122.20 +/- 290.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=198000, episode_reward=153.80 +/- 241.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=199000, episode_reward=381.80 +/- 240.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=200000, episode_reward=305.00 +/- 124.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=201000, episode_reward=164.00 +/- 95.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=202000, episode_reward=177.40 +/- 108.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=203000, episode_reward=199.40 +/- 263.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=204000, episode_reward=285.00 +/- 202.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=205000, episode_reward=327.20 +/- 129.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=206000, episode_reward=201.40 +/- 228.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=207000, episode_reward=293.20 +/- 116.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=208000, episode_reward=216.00 +/- 133.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=209000, episode_reward=226.60 +/- 90.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=210000, episode_reward=344.00 +/- 175.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=211000, episode_reward=278.00 +/- 121.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=212000, episode_reward=47.60 +/- 138.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=213000, episode_reward=317.80 +/- 216.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=214000, episode_reward=223.40 +/- 203.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=215000, episode_reward=202.20 +/- 138.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=216000, episode_reward=271.80 +/- 230.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=217000, episode_reward=152.20 +/- 194.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=218000, episode_reward=178.20 +/- 217.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=219000, episode_reward=222.60 +/- 173.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=220000, episode_reward=216.00 +/- 137.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=221000, episode_reward=62.00 +/- 212.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=222000, episode_reward=359.40 +/- 180.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=223000, episode_reward=163.60 +/- 148.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=224000, episode_reward=92.80 +/- 234.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=225000, episode_reward=202.80 +/- 109.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=226000, episode_reward=163.00 +/- 233.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=227000, episode_reward=274.00 +/- 218.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=228000, episode_reward=143.40 +/- 161.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=229000, episode_reward=222.00 +/- 135.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=230000, episode_reward=301.80 +/- 254.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=231000, episode_reward=169.20 +/- 209.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=232000, episode_reward=156.40 +/- 192.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=233000, episode_reward=100.20 +/- 203.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=234000, episode_reward=311.80 +/- 259.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=235000, episode_reward=260.40 +/- 95.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=236000, episode_reward=439.00 +/- 146.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=237000, episode_reward=165.20 +/- 129.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=238000, episode_reward=163.20 +/- 380.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=239000, episode_reward=204.00 +/- 257.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=240000, episode_reward=233.40 +/- 138.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=241000, episode_reward=267.80 +/- 182.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=242000, episode_reward=297.00 +/- 177.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=243000, episode_reward=233.80 +/- 140.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=244000, episode_reward=310.40 +/- 150.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=245000, episode_reward=299.40 +/- 184.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=246000, episode_reward=166.60 +/- 248.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=247000, episode_reward=164.40 +/- 156.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=248000, episode_reward=246.00 +/- 165.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=249000, episode_reward=266.20 +/- 101.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=250000, episode_reward=294.20 +/- 215.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=251000, episode_reward=274.80 +/- 151.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=252000, episode_reward=151.80 +/- 167.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=253000, episode_reward=225.40 +/- 73.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=254000, episode_reward=367.80 +/- 218.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=255000, episode_reward=38.80 +/- 195.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=256000, episode_reward=229.80 +/- 191.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=257000, episode_reward=67.00 +/- 181.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=258000, episode_reward=227.60 +/- 164.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=259000, episode_reward=76.20 +/- 238.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=260000, episode_reward=340.20 +/- 84.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=261000, episode_reward=199.20 +/- 249.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=262000, episode_reward=178.80 +/- 139.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=263000, episode_reward=171.80 +/- 126.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=264000, episode_reward=187.40 +/- 178.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=265000, episode_reward=343.20 +/- 151.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=266000, episode_reward=296.40 +/- 60.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=267000, episode_reward=168.60 +/- 239.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=268000, episode_reward=306.00 +/- 116.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=269000, episode_reward=272.00 +/- 141.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=270000, episode_reward=269.60 +/- 237.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=271000, episode_reward=164.20 +/- 206.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=272000, episode_reward=118.00 +/- 131.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=273000, episode_reward=283.60 +/- 165.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=274000, episode_reward=237.00 +/- 158.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=275000, episode_reward=313.80 +/- 191.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=276000, episode_reward=160.40 +/- 94.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=277000, episode_reward=10.20 +/- 213.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=278000, episode_reward=173.00 +/- 243.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=279000, episode_reward=301.80 +/- 133.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=280000, episode_reward=297.80 +/- 184.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=281000, episode_reward=251.80 +/- 137.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=282000, episode_reward=260.00 +/- 156.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=283000, episode_reward=295.40 +/- 254.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=284000, episode_reward=334.80 +/- 176.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=285000, episode_reward=170.00 +/- 234.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=286000, episode_reward=278.60 +/- 134.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=287000, episode_reward=215.80 +/- 238.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=288000, episode_reward=134.20 +/- 196.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=289000, episode_reward=327.00 +/- 181.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=290000, episode_reward=143.60 +/- 81.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=291000, episode_reward=499.00 +/- 201.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=292000, episode_reward=416.40 +/- 119.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=293000, episode_reward=181.40 +/- 306.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=294000, episode_reward=330.00 +/- 80.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=295000, episode_reward=313.40 +/- 115.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=296000, episode_reward=231.00 +/- 211.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=297000, episode_reward=161.40 +/- 87.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=298000, episode_reward=355.40 +/- 105.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=299000, episode_reward=135.20 +/- 262.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=300000, episode_reward=253.20 +/- 187.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=301000, episode_reward=123.80 +/- 78.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=302000, episode_reward=247.80 +/- 65.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=303000, episode_reward=225.00 +/- 163.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=304000, episode_reward=209.00 +/- 90.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=305000, episode_reward=129.00 +/- 192.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=306000, episode_reward=328.40 +/- 110.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=307000, episode_reward=235.00 +/- 127.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=308000, episode_reward=193.40 +/- 149.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=309000, episode_reward=177.80 +/- 121.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=310000, episode_reward=182.60 +/- 185.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=311000, episode_reward=344.40 +/- 289.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=312000, episode_reward=394.00 +/- 199.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=313000, episode_reward=226.00 +/- 168.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=314000, episode_reward=177.20 +/- 220.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=315000, episode_reward=305.80 +/- 109.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=316000, episode_reward=110.20 +/- 238.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=317000, episode_reward=227.00 +/- 220.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=318000, episode_reward=328.20 +/- 87.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=319000, episode_reward=320.20 +/- 144.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=320000, episode_reward=281.60 +/- 296.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=321000, episode_reward=380.60 +/- 225.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=322000, episode_reward=399.20 +/- 194.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=323000, episode_reward=304.00 +/- 80.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=324000, episode_reward=317.20 +/- 258.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=325000, episode_reward=290.80 +/- 218.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=326000, episode_reward=290.40 +/- 215.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=327000, episode_reward=207.80 +/- 136.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=328000, episode_reward=418.20 +/- 61.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=329000, episode_reward=360.60 +/- 68.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=330000, episode_reward=390.20 +/- 247.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=331000, episode_reward=143.00 +/- 172.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=332000, episode_reward=481.40 +/- 96.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=333000, episode_reward=340.40 +/- 137.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=334000, episode_reward=232.60 +/- 97.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=335000, episode_reward=299.40 +/- 474.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=336000, episode_reward=288.20 +/- 103.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=337000, episode_reward=364.80 +/- 269.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=338000, episode_reward=210.60 +/- 224.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=339000, episode_reward=246.80 +/- 109.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=340000, episode_reward=266.60 +/- 190.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=341000, episode_reward=385.40 +/- 164.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=342000, episode_reward=319.40 +/- 119.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=343000, episode_reward=480.20 +/- 159.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=344000, episode_reward=407.20 +/- 159.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=345000, episode_reward=377.20 +/- 263.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=346000, episode_reward=275.40 +/- 243.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=347000, episode_reward=281.80 +/- 61.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=348000, episode_reward=207.80 +/- 84.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=349000, episode_reward=499.00 +/- 260.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=350000, episode_reward=336.00 +/- 131.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=351000, episode_reward=350.40 +/- 212.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=352000, episode_reward=356.60 +/- 156.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=353000, episode_reward=358.80 +/- 172.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=354000, episode_reward=233.80 +/- 312.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=355000, episode_reward=312.80 +/- 208.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=356000, episode_reward=198.80 +/- 46.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=357000, episode_reward=424.20 +/- 196.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=358000, episode_reward=576.60 +/- 248.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=359000, episode_reward=382.60 +/- 298.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=360000, episode_reward=392.80 +/- 181.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=361000, episode_reward=196.20 +/- 169.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=362000, episode_reward=493.60 +/- 302.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=363000, episode_reward=499.40 +/- 245.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=364000, episode_reward=402.20 +/- 213.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=365000, episode_reward=557.80 +/- 298.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=366000, episode_reward=530.80 +/- 249.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=367000, episode_reward=539.60 +/- 252.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=368000, episode_reward=465.80 +/- 297.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=369000, episode_reward=416.40 +/- 153.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=370000, episode_reward=569.00 +/- 169.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=371000, episode_reward=358.80 +/- 39.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=372000, episode_reward=475.20 +/- 171.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=373000, episode_reward=402.80 +/- 269.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=374000, episode_reward=368.20 +/- 287.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=375000, episode_reward=483.20 +/- 232.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=376000, episode_reward=287.20 +/- 279.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=377000, episode_reward=499.40 +/- 95.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=378000, episode_reward=350.00 +/- 200.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=379000, episode_reward=501.40 +/- 184.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=380000, episode_reward=371.20 +/- 72.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=381000, episode_reward=429.80 +/- 221.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=382000, episode_reward=480.00 +/- 135.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=383000, episode_reward=569.00 +/- 180.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=384000, episode_reward=360.40 +/- 215.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=385000, episode_reward=371.40 +/- 148.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=386000, episode_reward=521.40 +/- 319.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=387000, episode_reward=364.60 +/- 211.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=388000, episode_reward=382.60 +/- 123.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=389000, episode_reward=310.00 +/- 250.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=390000, episode_reward=414.60 +/- 135.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=391000, episode_reward=427.80 +/- 118.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=392000, episode_reward=501.80 +/- 253.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=393000, episode_reward=515.00 +/- 246.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=394000, episode_reward=273.60 +/- 177.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=395000, episode_reward=470.00 +/- 125.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=396000, episode_reward=627.40 +/- 185.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=397000, episode_reward=410.60 +/- 260.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=398000, episode_reward=383.20 +/- 320.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=399000, episode_reward=347.20 +/- 219.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=400000, episode_reward=252.00 +/- 161.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=401000, episode_reward=643.20 +/- 206.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=402000, episode_reward=556.00 +/- 141.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=403000, episode_reward=513.60 +/- 231.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=404000, episode_reward=465.00 +/- 50.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=405000, episode_reward=579.40 +/- 238.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=406000, episode_reward=375.40 +/- 197.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=407000, episode_reward=398.80 +/- 257.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=408000, episode_reward=481.60 +/- 349.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=409000, episode_reward=508.40 +/- 320.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=410000, episode_reward=439.80 +/- 266.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=411000, episode_reward=724.00 +/- 81.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=412000, episode_reward=551.20 +/- 281.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=413000, episode_reward=498.80 +/- 346.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=414000, episode_reward=456.00 +/- 253.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=415000, episode_reward=384.80 +/- 301.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=416000, episode_reward=422.40 +/- 155.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=417000, episode_reward=422.60 +/- 247.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=418000, episode_reward=715.00 +/- 186.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=419000, episode_reward=639.20 +/- 114.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=420000, episode_reward=481.80 +/- 199.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=421000, episode_reward=722.80 +/- 189.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=422000, episode_reward=261.40 +/- 173.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=423000, episode_reward=376.00 +/- 373.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=424000, episode_reward=688.60 +/- 284.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=425000, episode_reward=393.80 +/- 129.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=426000, episode_reward=642.00 +/- 204.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=427000, episode_reward=406.40 +/- 320.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=428000, episode_reward=806.20 +/- 142.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=429000, episode_reward=575.60 +/- 181.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=430000, episode_reward=435.40 +/- 241.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=431000, episode_reward=443.60 +/- 159.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=432000, episode_reward=787.80 +/- 111.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=433000, episode_reward=516.60 +/- 453.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=434000, episode_reward=535.00 +/- 269.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=435000, episode_reward=786.00 +/- 205.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=436000, episode_reward=550.60 +/- 257.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=437000, episode_reward=575.20 +/- 210.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=438000, episode_reward=766.80 +/- 321.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=439000, episode_reward=679.80 +/- 202.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=440000, episode_reward=549.80 +/- 307.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=441000, episode_reward=794.60 +/- 183.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=442000, episode_reward=664.60 +/- 273.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=443000, episode_reward=758.20 +/- 243.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=444000, episode_reward=592.80 +/- 112.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=445000, episode_reward=492.80 +/- 189.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=446000, episode_reward=605.00 +/- 245.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=447000, episode_reward=682.00 +/- 245.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=448000, episode_reward=648.60 +/- 302.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=449000, episode_reward=788.40 +/- 175.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=450000, episode_reward=576.40 +/- 270.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=451000, episode_reward=835.00 +/- 178.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=452000, episode_reward=727.80 +/- 248.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=453000, episode_reward=798.00 +/- 165.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=454000, episode_reward=589.60 +/- 122.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=455000, episode_reward=756.00 +/- 265.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=456000, episode_reward=689.40 +/- 100.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=457000, episode_reward=676.20 +/- 238.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=458000, episode_reward=470.60 +/- 112.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=459000, episode_reward=544.60 +/- 415.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=460000, episode_reward=403.80 +/- 350.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=461000, episode_reward=638.40 +/- 275.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=462000, episode_reward=432.60 +/- 193.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=463000, episode_reward=471.60 +/- 348.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=464000, episode_reward=694.60 +/- 183.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=465000, episode_reward=508.40 +/- 273.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=466000, episode_reward=519.20 +/- 183.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=467000, episode_reward=510.20 +/- 364.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=468000, episode_reward=522.80 +/- 305.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=469000, episode_reward=393.80 +/- 136.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=470000, episode_reward=452.40 +/- 283.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=471000, episode_reward=506.60 +/- 89.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=472000, episode_reward=653.20 +/- 235.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=473000, episode_reward=510.40 +/- 231.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=474000, episode_reward=367.40 +/- 264.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=475000, episode_reward=642.60 +/- 102.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=476000, episode_reward=652.60 +/- 123.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=477000, episode_reward=574.80 +/- 83.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=478000, episode_reward=438.80 +/- 249.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=479000, episode_reward=415.80 +/- 354.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=480000, episode_reward=858.40 +/- 300.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=481000, episode_reward=877.20 +/- 352.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=482000, episode_reward=956.40 +/- 73.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=483000, episode_reward=622.60 +/- 152.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=484000, episode_reward=591.00 +/- 334.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=485000, episode_reward=444.20 +/- 187.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=486000, episode_reward=488.40 +/- 274.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=487000, episode_reward=552.60 +/- 338.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=488000, episode_reward=643.20 +/- 331.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=489000, episode_reward=675.00 +/- 300.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=490000, episode_reward=587.00 +/- 253.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=491000, episode_reward=648.20 +/- 341.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=492000, episode_reward=614.00 +/- 412.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=493000, episode_reward=591.80 +/- 241.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=494000, episode_reward=709.20 +/- 259.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=495000, episode_reward=682.00 +/- 223.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=496000, episode_reward=718.80 +/- 305.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=497000, episode_reward=774.80 +/- 210.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=498000, episode_reward=746.20 +/- 300.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=499000, episode_reward=426.40 +/- 454.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=500000, episode_reward=781.00 +/- 104.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=501000, episode_reward=626.80 +/- 143.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Gamma = 0.8, Learning Rate = 3e-05, Activation Function = Tanh, Neurons = 32, Process noise = 0.1, Measurement noise = 0.0\n",
      "Eval num_timesteps=1000, episode_reward=-1475.00 +/- 86.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=-1530.00 +/- 93.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=3000, episode_reward=178.80 +/- 205.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4000, episode_reward=216.20 +/- 144.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=5000, episode_reward=-6441.40 +/- 1798.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=6000, episode_reward=-4569.00 +/- 1155.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=7000, episode_reward=67.20 +/- 198.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=8000, episode_reward=169.20 +/- 186.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=9000, episode_reward=592.80 +/- 242.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=10000, episode_reward=335.20 +/- 142.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=11000, episode_reward=216.40 +/- 341.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=12000, episode_reward=344.00 +/- 188.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=13000, episode_reward=233.00 +/- 209.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=14000, episode_reward=442.00 +/- 155.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=15000, episode_reward=223.40 +/- 155.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=16000, episode_reward=27.60 +/- 168.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=17000, episode_reward=-44.60 +/- 184.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=18000, episode_reward=-154.20 +/- 271.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=19000, episode_reward=-60.20 +/- 284.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=20000, episode_reward=-497.60 +/- 307.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=21000, episode_reward=202.00 +/- 244.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=22000, episode_reward=288.80 +/- 210.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=23000, episode_reward=-15.60 +/- 151.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=24000, episode_reward=34.60 +/- 199.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=25000, episode_reward=73.80 +/- 526.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=26000, episode_reward=-192.40 +/- 491.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=27000, episode_reward=-0.60 +/- 192.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=28000, episode_reward=-76.00 +/- 467.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=29000, episode_reward=-142.60 +/- 502.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=30000, episode_reward=-293.40 +/- 638.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=31000, episode_reward=-129.40 +/- 372.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=32000, episode_reward=-238.20 +/- 593.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=33000, episode_reward=-312.20 +/- 490.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=34000, episode_reward=-27.40 +/- 442.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=35000, episode_reward=-14.40 +/- 406.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=36000, episode_reward=-285.40 +/- 501.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=37000, episode_reward=111.00 +/- 379.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=38000, episode_reward=-223.80 +/- 265.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=39000, episode_reward=0.40 +/- 362.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=40000, episode_reward=-211.40 +/- 716.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=41000, episode_reward=-892.00 +/- 579.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=42000, episode_reward=-338.00 +/- 643.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=43000, episode_reward=-0.20 +/- 292.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=44000, episode_reward=-435.20 +/- 258.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=45000, episode_reward=-50.80 +/- 336.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=46000, episode_reward=-602.00 +/- 787.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=47000, episode_reward=-279.40 +/- 431.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=48000, episode_reward=-505.40 +/- 1096.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=49000, episode_reward=-450.00 +/- 530.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=50000, episode_reward=-240.40 +/- 280.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=51000, episode_reward=47.40 +/- 614.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=52000, episode_reward=-292.80 +/- 450.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=53000, episode_reward=0.00 +/- 387.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=54000, episode_reward=-650.20 +/- 1207.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=55000, episode_reward=-546.40 +/- 433.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=56000, episode_reward=28.20 +/- 154.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=57000, episode_reward=-135.00 +/- 492.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=58000, episode_reward=-187.40 +/- 693.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=59000, episode_reward=10.40 +/- 410.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=60000, episode_reward=-204.60 +/- 510.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=61000, episode_reward=23.40 +/- 623.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=62000, episode_reward=34.00 +/- 436.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=63000, episode_reward=-216.00 +/- 504.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=64000, episode_reward=-142.80 +/- 433.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=65000, episode_reward=-607.80 +/- 543.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=66000, episode_reward=-778.00 +/- 717.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=67000, episode_reward=-288.80 +/- 734.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=68000, episode_reward=157.20 +/- 331.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=69000, episode_reward=138.40 +/- 359.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=70000, episode_reward=-1042.00 +/- 1268.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=71000, episode_reward=-37.20 +/- 475.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=72000, episode_reward=-162.80 +/- 281.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=73000, episode_reward=29.60 +/- 336.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=74000, episode_reward=-374.80 +/- 407.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=75000, episode_reward=-411.80 +/- 600.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=76000, episode_reward=-153.60 +/- 307.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=77000, episode_reward=-91.20 +/- 650.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=78000, episode_reward=49.20 +/- 234.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=79000, episode_reward=-118.80 +/- 608.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=80000, episode_reward=-160.00 +/- 601.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=81000, episode_reward=118.60 +/- 222.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=82000, episode_reward=-298.00 +/- 545.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=83000, episode_reward=-240.40 +/- 426.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=84000, episode_reward=-659.60 +/- 599.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=85000, episode_reward=109.80 +/- 134.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=86000, episode_reward=199.60 +/- 170.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=87000, episode_reward=-19.00 +/- 304.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=88000, episode_reward=-18.20 +/- 158.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=89000, episode_reward=-151.40 +/- 687.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=90000, episode_reward=-4.40 +/- 227.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=91000, episode_reward=-277.80 +/- 475.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=92000, episode_reward=-347.60 +/- 638.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=93000, episode_reward=-3.80 +/- 397.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=94000, episode_reward=137.40 +/- 206.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=95000, episode_reward=-57.40 +/- 286.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=96000, episode_reward=-145.00 +/- 523.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=97000, episode_reward=127.20 +/- 301.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=98000, episode_reward=-176.60 +/- 725.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=99000, episode_reward=-29.40 +/- 379.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=100000, episode_reward=-78.20 +/- 276.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=101000, episode_reward=-58.80 +/- 320.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=102000, episode_reward=115.20 +/- 280.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=103000, episode_reward=-88.80 +/- 221.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=104000, episode_reward=-42.60 +/- 249.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=105000, episode_reward=-218.00 +/- 451.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=106000, episode_reward=-50.60 +/- 371.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=107000, episode_reward=-86.40 +/- 286.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=108000, episode_reward=-180.00 +/- 750.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=109000, episode_reward=-26.20 +/- 319.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=110000, episode_reward=0.40 +/- 505.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=111000, episode_reward=-50.00 +/- 577.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=112000, episode_reward=-31.60 +/- 142.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=113000, episode_reward=-448.40 +/- 945.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=114000, episode_reward=117.40 +/- 324.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=115000, episode_reward=-370.20 +/- 791.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=116000, episode_reward=226.20 +/- 207.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=117000, episode_reward=55.40 +/- 217.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=118000, episode_reward=-149.20 +/- 266.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=119000, episode_reward=-337.60 +/- 1027.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=120000, episode_reward=287.80 +/- 242.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=121000, episode_reward=20.60 +/- 273.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=122000, episode_reward=-55.80 +/- 422.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=123000, episode_reward=3.80 +/- 65.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=124000, episode_reward=143.00 +/- 362.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=125000, episode_reward=179.60 +/- 225.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=126000, episode_reward=31.20 +/- 255.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=127000, episode_reward=-43.40 +/- 347.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=128000, episode_reward=3.60 +/- 283.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=129000, episode_reward=-49.80 +/- 377.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=130000, episode_reward=237.60 +/- 42.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=131000, episode_reward=195.40 +/- 150.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=132000, episode_reward=55.40 +/- 303.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=133000, episode_reward=193.40 +/- 181.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=134000, episode_reward=269.60 +/- 213.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=135000, episode_reward=228.40 +/- 235.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=136000, episode_reward=223.80 +/- 222.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=137000, episode_reward=141.80 +/- 136.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=138000, episode_reward=90.00 +/- 371.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=139000, episode_reward=194.00 +/- 255.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=140000, episode_reward=105.80 +/- 252.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=141000, episode_reward=232.60 +/- 116.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=142000, episode_reward=333.60 +/- 198.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=143000, episode_reward=116.60 +/- 197.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=144000, episode_reward=307.40 +/- 57.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=145000, episode_reward=418.60 +/- 145.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=146000, episode_reward=61.00 +/- 282.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=147000, episode_reward=166.20 +/- 245.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=148000, episode_reward=330.60 +/- 238.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=149000, episode_reward=374.20 +/- 66.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=150000, episode_reward=277.40 +/- 279.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=151000, episode_reward=298.60 +/- 195.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=152000, episode_reward=312.60 +/- 194.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=153000, episode_reward=277.80 +/- 239.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=154000, episode_reward=268.40 +/- 162.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=155000, episode_reward=115.00 +/- 132.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=156000, episode_reward=124.20 +/- 192.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=157000, episode_reward=390.60 +/- 68.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=158000, episode_reward=237.00 +/- 103.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=159000, episode_reward=375.40 +/- 207.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=160000, episode_reward=258.00 +/- 53.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=161000, episode_reward=218.40 +/- 82.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=162000, episode_reward=484.60 +/- 108.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=163000, episode_reward=346.40 +/- 181.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=164000, episode_reward=414.20 +/- 214.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=165000, episode_reward=372.20 +/- 190.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=166000, episode_reward=225.20 +/- 144.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=167000, episode_reward=356.00 +/- 128.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=168000, episode_reward=435.20 +/- 202.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=169000, episode_reward=187.80 +/- 192.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=170000, episode_reward=147.60 +/- 88.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=171000, episode_reward=251.60 +/- 157.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=172000, episode_reward=325.60 +/- 203.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=173000, episode_reward=446.00 +/- 159.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=174000, episode_reward=377.20 +/- 192.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=175000, episode_reward=211.20 +/- 230.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=176000, episode_reward=159.20 +/- 235.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=177000, episode_reward=386.20 +/- 178.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=178000, episode_reward=344.00 +/- 240.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=179000, episode_reward=217.00 +/- 205.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=180000, episode_reward=289.00 +/- 193.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=181000, episode_reward=268.60 +/- 150.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=182000, episode_reward=309.40 +/- 58.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=183000, episode_reward=171.20 +/- 104.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=184000, episode_reward=256.40 +/- 184.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=185000, episode_reward=301.20 +/- 165.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=186000, episode_reward=209.80 +/- 173.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=187000, episode_reward=468.00 +/- 139.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=188000, episode_reward=484.00 +/- 193.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=189000, episode_reward=295.00 +/- 152.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=190000, episode_reward=394.40 +/- 152.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=191000, episode_reward=263.00 +/- 146.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=192000, episode_reward=176.20 +/- 95.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=193000, episode_reward=220.20 +/- 156.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=194000, episode_reward=281.80 +/- 177.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=195000, episode_reward=263.00 +/- 147.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=196000, episode_reward=197.20 +/- 141.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=197000, episode_reward=217.60 +/- 156.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=198000, episode_reward=217.40 +/- 138.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=199000, episode_reward=327.00 +/- 72.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=200000, episode_reward=486.60 +/- 51.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=201000, episode_reward=291.20 +/- 145.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=202000, episode_reward=270.40 +/- 54.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=203000, episode_reward=418.20 +/- 198.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=204000, episode_reward=281.40 +/- 169.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=205000, episode_reward=297.00 +/- 262.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=206000, episode_reward=248.00 +/- 177.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=207000, episode_reward=283.00 +/- 167.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=208000, episode_reward=311.20 +/- 91.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=209000, episode_reward=269.60 +/- 71.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=210000, episode_reward=291.00 +/- 167.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=211000, episode_reward=313.80 +/- 148.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=212000, episode_reward=366.00 +/- 85.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=213000, episode_reward=347.20 +/- 210.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=214000, episode_reward=282.80 +/- 169.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=215000, episode_reward=288.60 +/- 203.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=216000, episode_reward=336.00 +/- 199.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=217000, episode_reward=299.40 +/- 76.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=218000, episode_reward=403.20 +/- 213.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=219000, episode_reward=301.00 +/- 74.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=220000, episode_reward=325.60 +/- 191.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=221000, episode_reward=176.60 +/- 108.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=222000, episode_reward=209.80 +/- 96.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=223000, episode_reward=337.20 +/- 113.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=224000, episode_reward=325.20 +/- 140.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=225000, episode_reward=309.80 +/- 61.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=226000, episode_reward=304.60 +/- 135.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=227000, episode_reward=314.00 +/- 230.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=228000, episode_reward=475.40 +/- 42.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=229000, episode_reward=429.40 +/- 147.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=230000, episode_reward=341.00 +/- 208.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=231000, episode_reward=263.00 +/- 132.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=232000, episode_reward=399.80 +/- 181.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=233000, episode_reward=264.80 +/- 258.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=234000, episode_reward=456.60 +/- 45.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=235000, episode_reward=278.60 +/- 123.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=236000, episode_reward=317.00 +/- 153.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=237000, episode_reward=239.60 +/- 136.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=238000, episode_reward=302.40 +/- 157.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=239000, episode_reward=314.00 +/- 118.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=240000, episode_reward=323.80 +/- 201.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=241000, episode_reward=274.80 +/- 132.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=242000, episode_reward=412.80 +/- 207.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=243000, episode_reward=218.60 +/- 105.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=244000, episode_reward=277.00 +/- 246.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=245000, episode_reward=398.00 +/- 162.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=246000, episode_reward=308.60 +/- 111.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=247000, episode_reward=263.20 +/- 195.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=248000, episode_reward=336.20 +/- 262.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=249000, episode_reward=256.20 +/- 201.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=250000, episode_reward=339.60 +/- 96.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=251000, episode_reward=332.20 +/- 202.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=252000, episode_reward=399.40 +/- 101.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=253000, episode_reward=299.60 +/- 190.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=254000, episode_reward=409.00 +/- 132.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=255000, episode_reward=304.00 +/- 168.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=256000, episode_reward=330.40 +/- 202.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=257000, episode_reward=384.80 +/- 31.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=258000, episode_reward=212.00 +/- 135.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=259000, episode_reward=206.20 +/- 200.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=260000, episode_reward=357.80 +/- 138.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=261000, episode_reward=175.80 +/- 253.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=262000, episode_reward=277.40 +/- 280.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=263000, episode_reward=301.40 +/- 130.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=264000, episode_reward=242.00 +/- 146.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=265000, episode_reward=210.40 +/- 264.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=266000, episode_reward=260.20 +/- 152.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=267000, episode_reward=403.20 +/- 209.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=268000, episode_reward=302.60 +/- 255.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=269000, episode_reward=402.00 +/- 247.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=270000, episode_reward=328.60 +/- 201.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=271000, episode_reward=358.80 +/- 121.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=272000, episode_reward=300.00 +/- 128.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=273000, episode_reward=176.00 +/- 269.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=274000, episode_reward=176.20 +/- 199.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=275000, episode_reward=253.40 +/- 225.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=276000, episode_reward=386.60 +/- 279.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=277000, episode_reward=297.20 +/- 46.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=278000, episode_reward=181.40 +/- 255.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=279000, episode_reward=276.80 +/- 181.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=280000, episode_reward=455.60 +/- 203.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=281000, episode_reward=222.00 +/- 145.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=282000, episode_reward=333.40 +/- 157.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=283000, episode_reward=405.80 +/- 182.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=284000, episode_reward=410.20 +/- 127.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=285000, episode_reward=334.00 +/- 188.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=286000, episode_reward=439.60 +/- 155.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=287000, episode_reward=440.20 +/- 121.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=288000, episode_reward=271.60 +/- 218.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=289000, episode_reward=301.80 +/- 187.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=290000, episode_reward=353.60 +/- 228.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=291000, episode_reward=343.40 +/- 64.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=292000, episode_reward=280.00 +/- 135.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=293000, episode_reward=477.00 +/- 125.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=294000, episode_reward=414.40 +/- 111.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=295000, episode_reward=344.40 +/- 77.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=296000, episode_reward=421.20 +/- 204.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=297000, episode_reward=186.80 +/- 103.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=298000, episode_reward=389.00 +/- 78.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=299000, episode_reward=352.80 +/- 168.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=300000, episode_reward=347.80 +/- 159.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=301000, episode_reward=235.60 +/- 158.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=302000, episode_reward=417.20 +/- 211.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=303000, episode_reward=410.60 +/- 179.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=304000, episode_reward=316.60 +/- 154.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=305000, episode_reward=258.00 +/- 105.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=306000, episode_reward=308.00 +/- 144.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=307000, episode_reward=300.60 +/- 85.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=308000, episode_reward=323.40 +/- 213.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=309000, episode_reward=171.40 +/- 244.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=310000, episode_reward=344.60 +/- 223.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=311000, episode_reward=259.80 +/- 209.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=312000, episode_reward=422.40 +/- 172.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=313000, episode_reward=365.20 +/- 202.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=314000, episode_reward=177.40 +/- 162.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=315000, episode_reward=315.40 +/- 170.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=316000, episode_reward=385.20 +/- 176.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=317000, episode_reward=276.40 +/- 117.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=318000, episode_reward=308.20 +/- 102.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=319000, episode_reward=294.00 +/- 203.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=320000, episode_reward=380.80 +/- 209.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=321000, episode_reward=281.60 +/- 115.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=322000, episode_reward=147.80 +/- 264.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=323000, episode_reward=429.80 +/- 149.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=324000, episode_reward=242.20 +/- 195.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=325000, episode_reward=364.00 +/- 140.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=326000, episode_reward=259.40 +/- 246.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=327000, episode_reward=97.20 +/- 231.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=328000, episode_reward=175.20 +/- 88.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=329000, episode_reward=415.00 +/- 157.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=330000, episode_reward=133.60 +/- 67.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=331000, episode_reward=293.40 +/- 249.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=332000, episode_reward=114.40 +/- 188.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=333000, episode_reward=267.60 +/- 141.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=334000, episode_reward=461.20 +/- 193.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=335000, episode_reward=212.60 +/- 135.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=336000, episode_reward=191.00 +/- 291.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=337000, episode_reward=416.00 +/- 179.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=338000, episode_reward=204.00 +/- 96.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=339000, episode_reward=316.60 +/- 280.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=340000, episode_reward=299.80 +/- 157.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=341000, episode_reward=313.80 +/- 215.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=342000, episode_reward=444.60 +/- 142.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=343000, episode_reward=226.80 +/- 148.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=344000, episode_reward=263.00 +/- 233.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=345000, episode_reward=323.40 +/- 257.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=346000, episode_reward=338.20 +/- 211.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=347000, episode_reward=273.60 +/- 181.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=348000, episode_reward=344.80 +/- 164.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=349000, episode_reward=414.00 +/- 105.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=350000, episode_reward=210.40 +/- 185.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=351000, episode_reward=296.80 +/- 185.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=352000, episode_reward=277.80 +/- 186.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=353000, episode_reward=166.80 +/- 136.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=354000, episode_reward=440.60 +/- 68.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=355000, episode_reward=382.20 +/- 170.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=356000, episode_reward=429.00 +/- 148.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=357000, episode_reward=321.80 +/- 75.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=358000, episode_reward=337.00 +/- 152.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=359000, episode_reward=294.00 +/- 183.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=360000, episode_reward=250.20 +/- 147.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=361000, episode_reward=413.20 +/- 192.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=362000, episode_reward=245.40 +/- 200.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=363000, episode_reward=296.20 +/- 119.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=364000, episode_reward=205.40 +/- 155.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=365000, episode_reward=331.80 +/- 107.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=366000, episode_reward=358.80 +/- 173.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=367000, episode_reward=241.20 +/- 188.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=368000, episode_reward=365.40 +/- 162.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=369000, episode_reward=263.80 +/- 218.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=370000, episode_reward=339.80 +/- 98.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=371000, episode_reward=356.20 +/- 184.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=372000, episode_reward=316.20 +/- 290.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=373000, episode_reward=403.00 +/- 138.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=374000, episode_reward=291.60 +/- 159.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=375000, episode_reward=392.40 +/- 64.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=376000, episode_reward=293.80 +/- 142.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=377000, episode_reward=206.80 +/- 158.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=378000, episode_reward=434.00 +/- 125.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=379000, episode_reward=362.80 +/- 231.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=380000, episode_reward=323.80 +/- 254.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=381000, episode_reward=361.80 +/- 127.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=382000, episode_reward=284.60 +/- 162.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=383000, episode_reward=480.00 +/- 95.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=384000, episode_reward=303.20 +/- 58.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=385000, episode_reward=403.40 +/- 135.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=386000, episode_reward=411.20 +/- 172.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=387000, episode_reward=253.00 +/- 195.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=388000, episode_reward=407.20 +/- 211.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=389000, episode_reward=359.60 +/- 106.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=390000, episode_reward=396.80 +/- 133.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=391000, episode_reward=334.60 +/- 139.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=392000, episode_reward=266.60 +/- 124.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=393000, episode_reward=349.20 +/- 136.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=394000, episode_reward=308.40 +/- 98.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=395000, episode_reward=297.00 +/- 95.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=396000, episode_reward=292.40 +/- 298.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=397000, episode_reward=249.20 +/- 218.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=398000, episode_reward=318.40 +/- 151.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=399000, episode_reward=198.20 +/- 118.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=400000, episode_reward=409.80 +/- 184.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=401000, episode_reward=303.80 +/- 252.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=402000, episode_reward=386.40 +/- 174.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=403000, episode_reward=267.20 +/- 84.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=404000, episode_reward=339.40 +/- 158.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=405000, episode_reward=304.40 +/- 182.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=406000, episode_reward=312.80 +/- 180.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=407000, episode_reward=225.80 +/- 164.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=408000, episode_reward=340.00 +/- 103.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=409000, episode_reward=300.60 +/- 133.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=410000, episode_reward=344.00 +/- 178.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=411000, episode_reward=261.80 +/- 170.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=412000, episode_reward=344.80 +/- 179.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=413000, episode_reward=285.20 +/- 202.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=414000, episode_reward=430.60 +/- 152.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=415000, episode_reward=367.60 +/- 134.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=416000, episode_reward=290.20 +/- 262.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=417000, episode_reward=340.80 +/- 233.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=418000, episode_reward=269.60 +/- 199.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=419000, episode_reward=340.20 +/- 91.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=420000, episode_reward=304.00 +/- 151.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=421000, episode_reward=438.80 +/- 152.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=422000, episode_reward=293.00 +/- 129.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=423000, episode_reward=149.00 +/- 257.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=424000, episode_reward=250.40 +/- 235.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=425000, episode_reward=204.80 +/- 246.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=426000, episode_reward=198.00 +/- 179.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=427000, episode_reward=221.00 +/- 203.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=428000, episode_reward=96.20 +/- 108.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=429000, episode_reward=276.60 +/- 150.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=430000, episode_reward=195.60 +/- 201.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=431000, episode_reward=23.40 +/- 123.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=432000, episode_reward=268.00 +/- 255.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=433000, episode_reward=361.20 +/- 159.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=434000, episode_reward=233.80 +/- 204.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=435000, episode_reward=427.00 +/- 193.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=436000, episode_reward=334.20 +/- 153.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=437000, episode_reward=348.40 +/- 158.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=438000, episode_reward=286.40 +/- 207.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=439000, episode_reward=170.80 +/- 285.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=440000, episode_reward=263.40 +/- 277.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=441000, episode_reward=423.80 +/- 131.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=442000, episode_reward=432.00 +/- 182.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=443000, episode_reward=383.40 +/- 148.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=444000, episode_reward=53.80 +/- 170.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=445000, episode_reward=264.20 +/- 126.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=446000, episode_reward=255.80 +/- 132.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=447000, episode_reward=174.40 +/- 139.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=448000, episode_reward=282.80 +/- 192.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=449000, episode_reward=384.80 +/- 238.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=450000, episode_reward=271.40 +/- 98.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=451000, episode_reward=11.80 +/- 151.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=452000, episode_reward=264.20 +/- 174.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=453000, episode_reward=374.40 +/- 90.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=454000, episode_reward=303.00 +/- 127.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=455000, episode_reward=190.00 +/- 156.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=456000, episode_reward=466.80 +/- 102.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=457000, episode_reward=201.00 +/- 233.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=458000, episode_reward=281.40 +/- 172.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=459000, episode_reward=340.40 +/- 166.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=460000, episode_reward=376.00 +/- 243.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=461000, episode_reward=333.00 +/- 275.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=462000, episode_reward=330.80 +/- 138.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=463000, episode_reward=260.80 +/- 191.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=464000, episode_reward=193.40 +/- 145.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=465000, episode_reward=227.20 +/- 143.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=466000, episode_reward=282.60 +/- 115.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=467000, episode_reward=222.20 +/- 77.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=468000, episode_reward=328.00 +/- 107.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=469000, episode_reward=124.60 +/- 188.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=470000, episode_reward=266.60 +/- 243.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=471000, episode_reward=270.00 +/- 202.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=472000, episode_reward=399.40 +/- 125.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=473000, episode_reward=288.80 +/- 193.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=474000, episode_reward=220.00 +/- 193.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=475000, episode_reward=447.00 +/- 149.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=476000, episode_reward=261.20 +/- 165.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=477000, episode_reward=248.20 +/- 130.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=478000, episode_reward=264.40 +/- 191.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=479000, episode_reward=384.60 +/- 246.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=480000, episode_reward=315.60 +/- 77.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=481000, episode_reward=327.80 +/- 100.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=482000, episode_reward=419.40 +/- 93.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=483000, episode_reward=192.40 +/- 139.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=484000, episode_reward=256.20 +/- 202.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=485000, episode_reward=408.40 +/- 84.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=486000, episode_reward=248.60 +/- 163.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=487000, episode_reward=226.80 +/- 80.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=488000, episode_reward=204.40 +/- 145.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=489000, episode_reward=140.00 +/- 84.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=490000, episode_reward=237.00 +/- 182.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=491000, episode_reward=267.60 +/- 314.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=492000, episode_reward=206.00 +/- 123.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=493000, episode_reward=285.60 +/- 91.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=494000, episode_reward=359.80 +/- 84.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=495000, episode_reward=205.40 +/- 233.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=496000, episode_reward=243.20 +/- 87.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=497000, episode_reward=342.20 +/- 247.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=498000, episode_reward=204.00 +/- 176.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=499000, episode_reward=434.20 +/- 141.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=500000, episode_reward=336.60 +/- 304.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=501000, episode_reward=304.60 +/- 144.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Gamma = 0.8, Learning Rate = 3e-05, Activation Function = Tanh, Neurons = 64, Process noise = 0.1, Measurement noise = 0.0\n",
      "Eval num_timesteps=1000, episode_reward=-5730.00 +/- 693.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=-5898.40 +/- 594.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=3000, episode_reward=58.40 +/- 191.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4000, episode_reward=232.20 +/- 292.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=5000, episode_reward=233.20 +/- 191.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=6000, episode_reward=160.20 +/- 162.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=7000, episode_reward=308.80 +/- 75.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=8000, episode_reward=325.20 +/- 82.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=9000, episode_reward=435.80 +/- 129.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=10000, episode_reward=330.40 +/- 161.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=11000, episode_reward=340.40 +/- 124.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=12000, episode_reward=334.40 +/- 203.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=13000, episode_reward=244.80 +/- 210.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=14000, episode_reward=304.80 +/- 96.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=15000, episode_reward=354.20 +/- 215.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=16000, episode_reward=339.40 +/- 341.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=17000, episode_reward=451.40 +/- 121.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=18000, episode_reward=426.00 +/- 249.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=19000, episode_reward=35.00 +/- 169.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=20000, episode_reward=314.00 +/- 180.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=21000, episode_reward=119.00 +/- 193.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=22000, episode_reward=13.80 +/- 135.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=23000, episode_reward=282.80 +/- 147.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=24000, episode_reward=55.20 +/- 207.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=25000, episode_reward=271.00 +/- 241.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=26000, episode_reward=362.80 +/- 218.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=27000, episode_reward=316.20 +/- 330.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=28000, episode_reward=297.20 +/- 316.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=29000, episode_reward=361.00 +/- 319.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=30000, episode_reward=241.20 +/- 138.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=31000, episode_reward=303.80 +/- 84.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=32000, episode_reward=316.60 +/- 250.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=33000, episode_reward=361.40 +/- 195.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=34000, episode_reward=233.00 +/- 124.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=35000, episode_reward=390.20 +/- 148.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=36000, episode_reward=368.20 +/- 185.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=37000, episode_reward=372.40 +/- 167.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=38000, episode_reward=314.60 +/- 243.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=39000, episode_reward=302.20 +/- 54.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=40000, episode_reward=429.60 +/- 275.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=41000, episode_reward=356.40 +/- 183.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=42000, episode_reward=260.20 +/- 333.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=43000, episode_reward=201.60 +/- 94.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=44000, episode_reward=343.00 +/- 260.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=45000, episode_reward=180.00 +/- 141.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=46000, episode_reward=493.80 +/- 154.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=47000, episode_reward=429.00 +/- 269.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=48000, episode_reward=469.80 +/- 117.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=49000, episode_reward=119.60 +/- 85.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=50000, episode_reward=391.60 +/- 154.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=51000, episode_reward=251.60 +/- 150.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=52000, episode_reward=451.00 +/- 138.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=53000, episode_reward=306.80 +/- 153.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=54000, episode_reward=307.80 +/- 172.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=55000, episode_reward=284.00 +/- 152.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=56000, episode_reward=272.60 +/- 111.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=57000, episode_reward=204.20 +/- 67.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=58000, episode_reward=315.80 +/- 116.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=59000, episode_reward=435.60 +/- 163.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=60000, episode_reward=293.20 +/- 77.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=61000, episode_reward=311.20 +/- 254.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=62000, episode_reward=140.40 +/- 340.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=63000, episode_reward=411.40 +/- 188.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=64000, episode_reward=366.60 +/- 205.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=65000, episode_reward=410.80 +/- 311.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=66000, episode_reward=327.00 +/- 72.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=67000, episode_reward=309.60 +/- 61.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=68000, episode_reward=523.80 +/- 170.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=69000, episode_reward=279.00 +/- 175.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=70000, episode_reward=305.40 +/- 187.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=71000, episode_reward=359.60 +/- 143.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=72000, episode_reward=358.40 +/- 218.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=73000, episode_reward=161.00 +/- 289.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=74000, episode_reward=264.80 +/- 195.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=75000, episode_reward=274.20 +/- 117.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=76000, episode_reward=311.80 +/- 149.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=77000, episode_reward=341.00 +/- 100.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=78000, episode_reward=396.40 +/- 205.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=79000, episode_reward=284.00 +/- 170.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=80000, episode_reward=324.40 +/- 379.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=81000, episode_reward=311.20 +/- 214.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=82000, episode_reward=137.00 +/- 151.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=83000, episode_reward=441.80 +/- 120.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=84000, episode_reward=423.80 +/- 86.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=85000, episode_reward=412.80 +/- 197.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=86000, episode_reward=418.20 +/- 134.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=87000, episode_reward=247.40 +/- 171.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=88000, episode_reward=390.80 +/- 198.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=89000, episode_reward=306.40 +/- 185.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=90000, episode_reward=422.00 +/- 270.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=91000, episode_reward=217.80 +/- 114.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=92000, episode_reward=313.60 +/- 144.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=93000, episode_reward=497.20 +/- 128.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=94000, episode_reward=284.20 +/- 166.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=95000, episode_reward=280.00 +/- 88.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=96000, episode_reward=474.80 +/- 195.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=97000, episode_reward=325.40 +/- 128.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=98000, episode_reward=356.40 +/- 123.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=99000, episode_reward=253.00 +/- 235.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=100000, episode_reward=448.80 +/- 109.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=101000, episode_reward=220.20 +/- 107.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=102000, episode_reward=353.20 +/- 162.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=103000, episode_reward=508.00 +/- 207.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=104000, episode_reward=427.20 +/- 206.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=105000, episode_reward=249.00 +/- 174.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=106000, episode_reward=320.00 +/- 218.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=107000, episode_reward=386.40 +/- 85.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=108000, episode_reward=543.20 +/- 110.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=109000, episode_reward=426.80 +/- 222.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=110000, episode_reward=338.00 +/- 151.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=111000, episode_reward=194.40 +/- 194.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=112000, episode_reward=355.60 +/- 219.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=113000, episode_reward=389.80 +/- 398.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=114000, episode_reward=216.60 +/- 61.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=115000, episode_reward=342.00 +/- 75.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=116000, episode_reward=350.40 +/- 156.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=117000, episode_reward=335.00 +/- 180.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=118000, episode_reward=213.20 +/- 221.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=119000, episode_reward=332.60 +/- 124.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=120000, episode_reward=182.80 +/- 83.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=121000, episode_reward=364.80 +/- 148.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=122000, episode_reward=388.80 +/- 107.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=123000, episode_reward=332.00 +/- 172.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=124000, episode_reward=464.20 +/- 126.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=125000, episode_reward=435.60 +/- 222.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=126000, episode_reward=435.40 +/- 62.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=127000, episode_reward=334.80 +/- 150.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=128000, episode_reward=310.00 +/- 134.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=129000, episode_reward=155.20 +/- 231.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=130000, episode_reward=203.80 +/- 180.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=131000, episode_reward=332.60 +/- 182.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=132000, episode_reward=256.40 +/- 174.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=133000, episode_reward=288.80 +/- 239.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=134000, episode_reward=242.20 +/- 234.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=135000, episode_reward=276.20 +/- 182.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=136000, episode_reward=482.60 +/- 181.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=137000, episode_reward=272.40 +/- 140.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=138000, episode_reward=377.20 +/- 306.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=139000, episode_reward=317.00 +/- 208.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=140000, episode_reward=364.00 +/- 129.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=141000, episode_reward=313.20 +/- 137.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=142000, episode_reward=382.20 +/- 122.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=143000, episode_reward=325.20 +/- 174.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=144000, episode_reward=447.40 +/- 213.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=145000, episode_reward=349.80 +/- 91.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=146000, episode_reward=377.60 +/- 228.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=147000, episode_reward=450.20 +/- 247.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=148000, episode_reward=333.40 +/- 119.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=149000, episode_reward=248.00 +/- 148.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=150000, episode_reward=298.40 +/- 138.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=151000, episode_reward=384.40 +/- 264.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=152000, episode_reward=385.80 +/- 87.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=153000, episode_reward=469.20 +/- 152.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=154000, episode_reward=368.80 +/- 128.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=155000, episode_reward=587.80 +/- 222.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=156000, episode_reward=349.60 +/- 236.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=157000, episode_reward=260.20 +/- 183.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=158000, episode_reward=422.20 +/- 223.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=159000, episode_reward=280.40 +/- 161.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=160000, episode_reward=313.00 +/- 156.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=161000, episode_reward=382.80 +/- 198.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=162000, episode_reward=466.60 +/- 218.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=163000, episode_reward=413.40 +/- 219.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=164000, episode_reward=402.00 +/- 149.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=165000, episode_reward=392.60 +/- 209.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=166000, episode_reward=418.80 +/- 162.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=167000, episode_reward=286.00 +/- 186.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=168000, episode_reward=187.60 +/- 140.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=169000, episode_reward=313.20 +/- 175.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=170000, episode_reward=460.60 +/- 74.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=171000, episode_reward=312.80 +/- 169.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=172000, episode_reward=382.20 +/- 269.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=173000, episode_reward=360.80 +/- 225.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=174000, episode_reward=156.00 +/- 228.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=175000, episode_reward=193.00 +/- 197.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=176000, episode_reward=477.00 +/- 74.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=177000, episode_reward=314.20 +/- 255.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=178000, episode_reward=354.80 +/- 99.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=179000, episode_reward=345.80 +/- 245.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=180000, episode_reward=492.80 +/- 139.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=181000, episode_reward=274.40 +/- 363.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=182000, episode_reward=515.20 +/- 170.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=183000, episode_reward=311.60 +/- 163.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=184000, episode_reward=362.60 +/- 128.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=185000, episode_reward=318.20 +/- 125.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=186000, episode_reward=284.80 +/- 234.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=187000, episode_reward=450.80 +/- 139.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=188000, episode_reward=377.40 +/- 185.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=189000, episode_reward=371.60 +/- 277.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=190000, episode_reward=360.60 +/- 198.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=191000, episode_reward=445.40 +/- 125.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=192000, episode_reward=370.00 +/- 109.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=193000, episode_reward=346.40 +/- 212.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=194000, episode_reward=218.40 +/- 240.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=195000, episode_reward=301.20 +/- 164.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=196000, episode_reward=310.60 +/- 209.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=197000, episode_reward=172.80 +/- 196.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=198000, episode_reward=199.20 +/- 96.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=199000, episode_reward=388.60 +/- 187.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=200000, episode_reward=333.00 +/- 187.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=201000, episode_reward=349.00 +/- 213.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=202000, episode_reward=225.80 +/- 174.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=203000, episode_reward=268.40 +/- 139.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=204000, episode_reward=253.20 +/- 183.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=205000, episode_reward=427.40 +/- 231.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=206000, episode_reward=504.40 +/- 166.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=207000, episode_reward=253.00 +/- 185.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=208000, episode_reward=300.60 +/- 81.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=209000, episode_reward=344.20 +/- 136.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=210000, episode_reward=346.40 +/- 225.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=211000, episode_reward=273.20 +/- 146.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=212000, episode_reward=433.20 +/- 206.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=213000, episode_reward=127.40 +/- 327.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=214000, episode_reward=376.40 +/- 208.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=215000, episode_reward=403.80 +/- 177.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=216000, episode_reward=463.60 +/- 218.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=217000, episode_reward=295.80 +/- 149.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=218000, episode_reward=382.80 +/- 125.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=219000, episode_reward=276.20 +/- 125.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=220000, episode_reward=354.20 +/- 227.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=221000, episode_reward=188.20 +/- 94.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=222000, episode_reward=375.80 +/- 176.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=223000, episode_reward=375.40 +/- 93.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=224000, episode_reward=334.00 +/- 195.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=225000, episode_reward=463.60 +/- 105.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=226000, episode_reward=276.20 +/- 184.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=227000, episode_reward=395.80 +/- 156.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=228000, episode_reward=202.20 +/- 164.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=229000, episode_reward=255.80 +/- 160.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=230000, episode_reward=259.60 +/- 119.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=231000, episode_reward=585.80 +/- 142.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=232000, episode_reward=432.60 +/- 249.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=233000, episode_reward=86.20 +/- 296.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=234000, episode_reward=434.00 +/- 111.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=235000, episode_reward=563.40 +/- 148.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=236000, episode_reward=215.20 +/- 204.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=237000, episode_reward=363.20 +/- 151.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=238000, episode_reward=292.40 +/- 240.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=239000, episode_reward=339.20 +/- 124.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=240000, episode_reward=388.80 +/- 154.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=241000, episode_reward=414.40 +/- 120.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=242000, episode_reward=391.00 +/- 179.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=243000, episode_reward=286.60 +/- 279.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=244000, episode_reward=243.80 +/- 138.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=245000, episode_reward=234.00 +/- 148.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=246000, episode_reward=199.20 +/- 141.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=247000, episode_reward=321.80 +/- 222.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=248000, episode_reward=386.80 +/- 128.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=249000, episode_reward=303.20 +/- 143.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=250000, episode_reward=388.00 +/- 204.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=251000, episode_reward=321.80 +/- 295.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=252000, episode_reward=400.00 +/- 160.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=253000, episode_reward=480.40 +/- 140.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=254000, episode_reward=456.00 +/- 249.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=255000, episode_reward=364.20 +/- 129.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=256000, episode_reward=254.60 +/- 236.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=257000, episode_reward=442.60 +/- 285.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=258000, episode_reward=339.00 +/- 149.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=259000, episode_reward=421.20 +/- 210.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=260000, episode_reward=255.40 +/- 262.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=261000, episode_reward=450.20 +/- 232.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=262000, episode_reward=330.20 +/- 173.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=263000, episode_reward=498.40 +/- 217.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=264000, episode_reward=398.00 +/- 184.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=265000, episode_reward=138.80 +/- 263.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=266000, episode_reward=391.00 +/- 354.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=267000, episode_reward=254.20 +/- 251.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=268000, episode_reward=384.60 +/- 189.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=269000, episode_reward=332.40 +/- 133.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=270000, episode_reward=264.80 +/- 129.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=271000, episode_reward=317.80 +/- 142.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=272000, episode_reward=519.60 +/- 99.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=273000, episode_reward=345.20 +/- 117.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=274000, episode_reward=383.00 +/- 118.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=275000, episode_reward=455.80 +/- 224.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=276000, episode_reward=370.40 +/- 112.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=277000, episode_reward=462.00 +/- 30.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=278000, episode_reward=113.40 +/- 147.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=279000, episode_reward=336.80 +/- 119.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=280000, episode_reward=332.80 +/- 152.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=281000, episode_reward=368.80 +/- 118.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=282000, episode_reward=299.60 +/- 97.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=283000, episode_reward=207.00 +/- 239.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=284000, episode_reward=329.00 +/- 137.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=285000, episode_reward=330.00 +/- 167.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=286000, episode_reward=346.80 +/- 141.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=287000, episode_reward=295.80 +/- 276.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=288000, episode_reward=370.40 +/- 222.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=289000, episode_reward=337.80 +/- 117.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=290000, episode_reward=422.80 +/- 128.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=291000, episode_reward=302.00 +/- 154.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=292000, episode_reward=341.00 +/- 218.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=293000, episode_reward=516.60 +/- 252.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=294000, episode_reward=326.20 +/- 306.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=295000, episode_reward=375.80 +/- 63.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=296000, episode_reward=435.40 +/- 240.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=297000, episode_reward=331.60 +/- 180.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=298000, episode_reward=333.20 +/- 221.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=299000, episode_reward=351.80 +/- 142.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=300000, episode_reward=277.20 +/- 129.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=301000, episode_reward=291.00 +/- 151.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=302000, episode_reward=338.80 +/- 231.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=303000, episode_reward=264.20 +/- 144.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=304000, episode_reward=346.20 +/- 204.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=305000, episode_reward=326.80 +/- 213.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=306000, episode_reward=325.40 +/- 122.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=307000, episode_reward=478.20 +/- 138.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=308000, episode_reward=356.40 +/- 225.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=309000, episode_reward=312.60 +/- 207.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=310000, episode_reward=338.60 +/- 265.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=311000, episode_reward=213.20 +/- 155.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=312000, episode_reward=358.00 +/- 184.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=313000, episode_reward=306.00 +/- 178.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=314000, episode_reward=427.60 +/- 238.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=315000, episode_reward=118.80 +/- 231.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=316000, episode_reward=360.40 +/- 203.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=317000, episode_reward=302.00 +/- 176.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=318000, episode_reward=352.40 +/- 294.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=319000, episode_reward=302.80 +/- 117.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=320000, episode_reward=314.00 +/- 211.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=321000, episode_reward=285.80 +/- 171.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=322000, episode_reward=439.80 +/- 189.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=323000, episode_reward=402.40 +/- 148.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=324000, episode_reward=410.80 +/- 202.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=325000, episode_reward=383.20 +/- 187.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=326000, episode_reward=335.60 +/- 92.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=327000, episode_reward=483.80 +/- 217.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=328000, episode_reward=382.80 +/- 69.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=329000, episode_reward=177.20 +/- 170.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=330000, episode_reward=515.60 +/- 116.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=331000, episode_reward=219.20 +/- 317.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=332000, episode_reward=395.20 +/- 123.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=333000, episode_reward=453.80 +/- 215.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=334000, episode_reward=327.40 +/- 118.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=335000, episode_reward=380.40 +/- 73.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=336000, episode_reward=282.60 +/- 250.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=337000, episode_reward=414.60 +/- 257.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=338000, episode_reward=390.80 +/- 200.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=339000, episode_reward=337.00 +/- 72.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=340000, episode_reward=264.00 +/- 190.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=341000, episode_reward=390.40 +/- 154.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=342000, episode_reward=372.00 +/- 201.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=343000, episode_reward=352.40 +/- 244.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=344000, episode_reward=290.60 +/- 137.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=345000, episode_reward=270.20 +/- 164.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=346000, episode_reward=165.20 +/- 144.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=347000, episode_reward=427.20 +/- 224.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=348000, episode_reward=402.40 +/- 231.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=349000, episode_reward=291.40 +/- 244.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=350000, episode_reward=398.80 +/- 139.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=351000, episode_reward=432.80 +/- 152.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=352000, episode_reward=479.60 +/- 152.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=353000, episode_reward=434.20 +/- 152.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=354000, episode_reward=300.40 +/- 78.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=355000, episode_reward=392.00 +/- 157.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=356000, episode_reward=293.40 +/- 139.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=357000, episode_reward=256.20 +/- 235.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=358000, episode_reward=372.40 +/- 331.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=359000, episode_reward=334.40 +/- 337.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=360000, episode_reward=440.80 +/- 200.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=361000, episode_reward=331.40 +/- 245.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=362000, episode_reward=326.00 +/- 119.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=363000, episode_reward=304.20 +/- 175.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=364000, episode_reward=218.40 +/- 202.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=365000, episode_reward=475.60 +/- 269.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=366000, episode_reward=451.20 +/- 167.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=367000, episode_reward=293.20 +/- 208.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=368000, episode_reward=376.60 +/- 254.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=369000, episode_reward=311.60 +/- 340.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=370000, episode_reward=437.00 +/- 126.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=371000, episode_reward=327.60 +/- 120.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=372000, episode_reward=503.40 +/- 247.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=373000, episode_reward=386.60 +/- 223.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=374000, episode_reward=272.80 +/- 164.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=375000, episode_reward=349.60 +/- 244.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=376000, episode_reward=345.40 +/- 331.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=377000, episode_reward=257.60 +/- 82.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=378000, episode_reward=160.20 +/- 193.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=379000, episode_reward=294.00 +/- 306.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=380000, episode_reward=248.00 +/- 83.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=381000, episode_reward=157.60 +/- 197.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=382000, episode_reward=421.00 +/- 254.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=383000, episode_reward=358.00 +/- 194.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=384000, episode_reward=211.80 +/- 165.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=385000, episode_reward=439.60 +/- 124.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=386000, episode_reward=420.80 +/- 204.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=387000, episode_reward=218.60 +/- 186.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=388000, episode_reward=262.80 +/- 171.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=389000, episode_reward=367.40 +/- 150.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=390000, episode_reward=251.00 +/- 199.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=391000, episode_reward=235.00 +/- 113.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=392000, episode_reward=327.00 +/- 78.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=393000, episode_reward=378.00 +/- 228.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=394000, episode_reward=314.40 +/- 173.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=395000, episode_reward=408.00 +/- 354.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=396000, episode_reward=288.60 +/- 121.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=397000, episode_reward=337.60 +/- 198.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=398000, episode_reward=201.60 +/- 212.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=399000, episode_reward=383.80 +/- 190.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=400000, episode_reward=171.20 +/- 134.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=401000, episode_reward=300.40 +/- 139.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=402000, episode_reward=259.40 +/- 141.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=403000, episode_reward=417.60 +/- 195.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=404000, episode_reward=519.60 +/- 263.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=405000, episode_reward=123.20 +/- 208.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=406000, episode_reward=248.60 +/- 205.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=407000, episode_reward=401.80 +/- 157.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=408000, episode_reward=157.60 +/- 129.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=409000, episode_reward=241.00 +/- 216.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=410000, episode_reward=259.40 +/- 127.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=411000, episode_reward=488.00 +/- 99.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=412000, episode_reward=445.80 +/- 185.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=413000, episode_reward=242.80 +/- 127.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=414000, episode_reward=160.80 +/- 153.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=415000, episode_reward=409.80 +/- 189.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=416000, episode_reward=412.20 +/- 250.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=417000, episode_reward=277.80 +/- 275.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=418000, episode_reward=329.00 +/- 164.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=419000, episode_reward=222.60 +/- 212.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=420000, episode_reward=428.80 +/- 221.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=421000, episode_reward=382.80 +/- 142.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=422000, episode_reward=351.60 +/- 183.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=423000, episode_reward=441.80 +/- 150.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=424000, episode_reward=327.20 +/- 89.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=425000, episode_reward=229.60 +/- 226.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=426000, episode_reward=431.60 +/- 186.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=427000, episode_reward=287.60 +/- 164.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=428000, episode_reward=460.80 +/- 292.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=429000, episode_reward=248.00 +/- 257.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=430000, episode_reward=417.20 +/- 159.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=431000, episode_reward=369.20 +/- 203.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=432000, episode_reward=299.20 +/- 148.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=433000, episode_reward=529.40 +/- 308.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=434000, episode_reward=356.40 +/- 109.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=435000, episode_reward=269.20 +/- 97.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=436000, episode_reward=604.80 +/- 195.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=437000, episode_reward=523.00 +/- 198.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=438000, episode_reward=675.80 +/- 203.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=439000, episode_reward=797.80 +/- 221.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=440000, episode_reward=983.80 +/- 266.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=441000, episode_reward=798.60 +/- 300.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=442000, episode_reward=719.20 +/- 310.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=443000, episode_reward=1051.60 +/- 110.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=444000, episode_reward=843.40 +/- 143.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=445000, episode_reward=1274.60 +/- 298.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=446000, episode_reward=998.00 +/- 391.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=447000, episode_reward=785.80 +/- 245.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=448000, episode_reward=757.00 +/- 313.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=449000, episode_reward=943.40 +/- 86.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=450000, episode_reward=973.20 +/- 275.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=451000, episode_reward=740.20 +/- 725.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=452000, episode_reward=972.80 +/- 237.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=453000, episode_reward=1198.60 +/- 323.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=454000, episode_reward=972.20 +/- 327.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=455000, episode_reward=1092.00 +/- 233.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=456000, episode_reward=1175.20 +/- 217.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=457000, episode_reward=1270.00 +/- 178.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=458000, episode_reward=-2514.60 +/- 7126.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=459000, episode_reward=-2394.20 +/- 6696.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=460000, episode_reward=548.20 +/- 1120.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=461000, episode_reward=1057.60 +/- 270.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=462000, episode_reward=-2103.80 +/- 6533.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=463000, episode_reward=922.00 +/- 320.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=464000, episode_reward=960.80 +/- 267.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=465000, episode_reward=-298.00 +/- 3069.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=466000, episode_reward=-2244.60 +/- 6699.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=467000, episode_reward=1107.80 +/- 353.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=468000, episode_reward=1213.80 +/- 125.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=469000, episode_reward=552.40 +/- 896.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=470000, episode_reward=-2832.40 +/- 6411.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=471000, episode_reward=-670.00 +/- 3292.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=472000, episode_reward=-5247.60 +/- 6057.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=473000, episode_reward=1116.20 +/- 136.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=474000, episode_reward=-1702.60 +/- 5595.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=475000, episode_reward=1155.80 +/- 245.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=476000, episode_reward=997.00 +/- 375.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=477000, episode_reward=1036.20 +/- 267.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=478000, episode_reward=129.60 +/- 1437.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=479000, episode_reward=1151.00 +/- 236.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=480000, episode_reward=1181.60 +/- 332.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=481000, episode_reward=1154.20 +/- 174.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=482000, episode_reward=-3203.20 +/- 6239.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=483000, episode_reward=-1819.20 +/- 5967.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=484000, episode_reward=-219.00 +/- 2898.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=485000, episode_reward=25.20 +/- 2460.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=486000, episode_reward=99.00 +/- 1923.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=487000, episode_reward=-879.80 +/- 3174.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=488000, episode_reward=-1552.80 +/- 5014.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=489000, episode_reward=1294.40 +/- 188.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=490000, episode_reward=-597.60 +/- 3546.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=491000, episode_reward=-1443.80 +/- 5331.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=492000, episode_reward=-2463.80 +/- 6786.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=493000, episode_reward=1252.60 +/- 263.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=494000, episode_reward=-176.40 +/- 2378.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=495000, episode_reward=1085.00 +/- 91.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=496000, episode_reward=675.40 +/- 1075.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=497000, episode_reward=1266.00 +/- 105.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=498000, episode_reward=1004.00 +/- 257.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=499000, episode_reward=-4454.80 +/- 6981.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=500000, episode_reward=847.80 +/- 690.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=501000, episode_reward=1122.40 +/- 135.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Gamma = 0.8, Learning Rate = 3e-05, Activation Function = ReLU, Neurons = 8, Process noise = 0.1, Measurement noise = 0.0\n",
      "Eval num_timesteps=1000, episode_reward=-1217.40 +/- 91.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=-1254.80 +/- 153.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=3000, episode_reward=-433.40 +/- 201.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4000, episode_reward=-421.80 +/- 197.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=5000, episode_reward=-2926.20 +/- 609.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=6000, episode_reward=-3418.00 +/- 796.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=7000, episode_reward=-367.60 +/- 244.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=8000, episode_reward=-266.40 +/- 371.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=9000, episode_reward=-222.40 +/- 418.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=10000, episode_reward=-4.60 +/- 235.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=11000, episode_reward=-6497.00 +/- 1686.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=12000, episode_reward=-5179.60 +/- 712.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=13000, episode_reward=-404.20 +/- 513.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=14000, episode_reward=-53.60 +/- 496.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=15000, episode_reward=-414.60 +/- 581.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=16000, episode_reward=-705.60 +/- 303.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=17000, episode_reward=-8292.00 +/- 781.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=18000, episode_reward=-6467.00 +/- 2593.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=19000, episode_reward=-7284.80 +/- 661.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=20000, episode_reward=-6451.00 +/- 1767.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=21000, episode_reward=-72.20 +/- 346.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=22000, episode_reward=222.80 +/- 280.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=23000, episode_reward=-5677.80 +/- 1503.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=24000, episode_reward=-4792.80 +/- 1342.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=25000, episode_reward=-6524.20 +/- 2748.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=26000, episode_reward=-7020.60 +/- 2143.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=27000, episode_reward=-5919.80 +/- 1448.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=28000, episode_reward=-5312.60 +/- 1843.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=29000, episode_reward=167.80 +/- 174.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=30000, episode_reward=209.80 +/- 172.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=31000, episode_reward=249.00 +/- 183.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=32000, episode_reward=184.00 +/- 240.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=33000, episode_reward=470.60 +/- 253.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=34000, episode_reward=246.00 +/- 284.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=35000, episode_reward=182.60 +/- 200.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=36000, episode_reward=241.60 +/- 154.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=37000, episode_reward=374.40 +/- 123.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=38000, episode_reward=359.20 +/- 254.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=39000, episode_reward=435.40 +/- 138.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=40000, episode_reward=287.40 +/- 355.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=41000, episode_reward=345.80 +/- 144.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=42000, episode_reward=315.40 +/- 162.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=43000, episode_reward=303.20 +/- 307.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=44000, episode_reward=309.40 +/- 197.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=45000, episode_reward=438.00 +/- 174.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=46000, episode_reward=312.80 +/- 204.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=47000, episode_reward=304.60 +/- 161.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=48000, episode_reward=436.20 +/- 138.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=49000, episode_reward=242.60 +/- 375.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=50000, episode_reward=273.20 +/- 143.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=51000, episode_reward=330.20 +/- 97.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=52000, episode_reward=265.40 +/- 220.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=53000, episode_reward=212.00 +/- 371.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=54000, episode_reward=397.80 +/- 195.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=55000, episode_reward=207.80 +/- 126.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=56000, episode_reward=337.20 +/- 251.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=57000, episode_reward=400.80 +/- 235.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=58000, episode_reward=329.00 +/- 202.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=59000, episode_reward=347.40 +/- 182.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=60000, episode_reward=180.00 +/- 144.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=61000, episode_reward=236.40 +/- 207.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=62000, episode_reward=500.80 +/- 72.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=63000, episode_reward=405.40 +/- 152.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=64000, episode_reward=396.80 +/- 68.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=65000, episode_reward=490.20 +/- 305.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=66000, episode_reward=327.60 +/- 223.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=67000, episode_reward=320.80 +/- 112.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=68000, episode_reward=364.00 +/- 102.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=69000, episode_reward=245.20 +/- 172.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=70000, episode_reward=407.20 +/- 199.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=71000, episode_reward=488.80 +/- 274.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=72000, episode_reward=333.60 +/- 147.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=73000, episode_reward=199.60 +/- 285.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=74000, episode_reward=-22.40 +/- 152.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=75000, episode_reward=85.40 +/- 138.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=76000, episode_reward=81.40 +/- 113.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=77000, episode_reward=107.20 +/- 66.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=78000, episode_reward=68.20 +/- 149.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=79000, episode_reward=67.60 +/- 111.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=80000, episode_reward=58.40 +/- 155.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=81000, episode_reward=54.60 +/- 138.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=82000, episode_reward=112.80 +/- 140.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=83000, episode_reward=206.00 +/- 117.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=84000, episode_reward=47.80 +/- 222.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=85000, episode_reward=5.60 +/- 117.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=86000, episode_reward=144.60 +/- 94.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=87000, episode_reward=175.60 +/- 62.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=88000, episode_reward=-5.00 +/- 211.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=89000, episode_reward=53.00 +/- 197.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=90000, episode_reward=-11.20 +/- 154.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=91000, episode_reward=93.00 +/- 125.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=92000, episode_reward=22.20 +/- 145.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=93000, episode_reward=56.00 +/- 189.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=94000, episode_reward=25.20 +/- 143.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=95000, episode_reward=17.40 +/- 180.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=96000, episode_reward=131.40 +/- 121.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=97000, episode_reward=39.80 +/- 142.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=98000, episode_reward=78.80 +/- 120.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=99000, episode_reward=122.40 +/- 53.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=100000, episode_reward=-100.20 +/- 125.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=101000, episode_reward=89.40 +/- 93.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=102000, episode_reward=197.40 +/- 189.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=103000, episode_reward=114.00 +/- 114.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=104000, episode_reward=107.20 +/- 179.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=105000, episode_reward=-106.00 +/- 171.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=106000, episode_reward=171.20 +/- 41.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=107000, episode_reward=119.20 +/- 128.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=108000, episode_reward=49.60 +/- 227.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=109000, episode_reward=58.80 +/- 122.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=110000, episode_reward=-67.40 +/- 195.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=111000, episode_reward=-34.00 +/- 173.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=112000, episode_reward=39.80 +/- 132.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=113000, episode_reward=148.40 +/- 89.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=114000, episode_reward=-18.60 +/- 148.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=115000, episode_reward=131.40 +/- 140.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=116000, episode_reward=-51.40 +/- 158.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=117000, episode_reward=-36.40 +/- 138.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=118000, episode_reward=32.60 +/- 171.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=119000, episode_reward=26.40 +/- 135.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=120000, episode_reward=120.40 +/- 116.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=121000, episode_reward=133.20 +/- 132.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=122000, episode_reward=6.60 +/- 156.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=123000, episode_reward=5.40 +/- 154.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=124000, episode_reward=115.60 +/- 114.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=125000, episode_reward=77.80 +/- 147.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=126000, episode_reward=124.60 +/- 80.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=127000, episode_reward=97.00 +/- 174.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=128000, episode_reward=17.40 +/- 129.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=129000, episode_reward=151.00 +/- 85.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=130000, episode_reward=27.60 +/- 147.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=131000, episode_reward=39.20 +/- 188.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=132000, episode_reward=-70.60 +/- 174.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=133000, episode_reward=177.40 +/- 131.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=134000, episode_reward=76.40 +/- 134.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=135000, episode_reward=131.80 +/- 103.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=136000, episode_reward=37.80 +/- 136.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=137000, episode_reward=-83.40 +/- 127.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=138000, episode_reward=199.60 +/- 252.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=139000, episode_reward=173.20 +/- 184.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=140000, episode_reward=42.40 +/- 161.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=141000, episode_reward=-51.60 +/- 137.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=142000, episode_reward=97.60 +/- 58.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=143000, episode_reward=11.00 +/- 137.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=144000, episode_reward=375.60 +/- 168.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=145000, episode_reward=286.60 +/- 62.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=146000, episode_reward=127.80 +/- 110.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=147000, episode_reward=32.60 +/- 145.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=148000, episode_reward=450.00 +/- 172.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=149000, episode_reward=203.20 +/- 147.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=150000, episode_reward=428.00 +/- 212.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=151000, episode_reward=260.40 +/- 153.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=152000, episode_reward=261.00 +/- 94.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=153000, episode_reward=303.20 +/- 231.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=154000, episode_reward=295.40 +/- 194.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=155000, episode_reward=362.40 +/- 163.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=156000, episode_reward=495.60 +/- 153.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=157000, episode_reward=332.60 +/- 142.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=158000, episode_reward=364.00 +/- 237.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=159000, episode_reward=159.80 +/- 256.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=160000, episode_reward=413.80 +/- 305.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=161000, episode_reward=371.60 +/- 238.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=162000, episode_reward=251.80 +/- 154.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=163000, episode_reward=194.40 +/- 89.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=164000, episode_reward=309.00 +/- 244.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=165000, episode_reward=197.80 +/- 142.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=166000, episode_reward=251.60 +/- 306.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=167000, episode_reward=515.00 +/- 225.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=168000, episode_reward=438.60 +/- 36.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=169000, episode_reward=554.80 +/- 169.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=170000, episode_reward=276.80 +/- 140.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=171000, episode_reward=299.80 +/- 215.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=172000, episode_reward=256.60 +/- 210.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=173000, episode_reward=315.00 +/- 282.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=174000, episode_reward=296.80 +/- 84.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=175000, episode_reward=294.00 +/- 160.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=176000, episode_reward=628.60 +/- 36.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=177000, episode_reward=372.80 +/- 127.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=178000, episode_reward=374.40 +/- 148.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=179000, episode_reward=219.00 +/- 172.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=180000, episode_reward=387.00 +/- 53.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=181000, episode_reward=442.40 +/- 181.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=182000, episode_reward=243.60 +/- 157.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=183000, episode_reward=229.20 +/- 116.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=184000, episode_reward=301.40 +/- 163.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=185000, episode_reward=352.60 +/- 139.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=186000, episode_reward=452.40 +/- 124.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=187000, episode_reward=409.00 +/- 165.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=188000, episode_reward=236.60 +/- 151.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=189000, episode_reward=261.40 +/- 180.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=190000, episode_reward=160.20 +/- 99.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=191000, episode_reward=339.60 +/- 85.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=192000, episode_reward=138.80 +/- 39.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=193000, episode_reward=336.20 +/- 210.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=194000, episode_reward=384.60 +/- 259.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=195000, episode_reward=323.60 +/- 138.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=196000, episode_reward=495.20 +/- 147.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=197000, episode_reward=245.60 +/- 180.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=198000, episode_reward=407.60 +/- 341.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=199000, episode_reward=274.60 +/- 71.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=200000, episode_reward=356.40 +/- 132.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=201000, episode_reward=285.60 +/- 193.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=202000, episode_reward=357.20 +/- 300.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=203000, episode_reward=407.40 +/- 186.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=204000, episode_reward=430.80 +/- 202.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=205000, episode_reward=325.80 +/- 159.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=206000, episode_reward=250.60 +/- 178.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=207000, episode_reward=402.20 +/- 148.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=208000, episode_reward=460.40 +/- 70.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=209000, episode_reward=211.40 +/- 219.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=210000, episode_reward=318.60 +/- 216.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=211000, episode_reward=288.40 +/- 168.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=212000, episode_reward=425.20 +/- 87.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=213000, episode_reward=267.60 +/- 186.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=214000, episode_reward=203.00 +/- 192.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=215000, episode_reward=311.00 +/- 108.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=216000, episode_reward=430.40 +/- 279.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=217000, episode_reward=402.00 +/- 168.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=218000, episode_reward=316.40 +/- 165.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=219000, episode_reward=312.80 +/- 149.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=220000, episode_reward=337.00 +/- 164.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=221000, episode_reward=267.20 +/- 78.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=222000, episode_reward=360.40 +/- 179.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=223000, episode_reward=129.20 +/- 188.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=224000, episode_reward=200.80 +/- 136.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=225000, episode_reward=377.20 +/- 129.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=226000, episode_reward=333.00 +/- 160.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=227000, episode_reward=175.80 +/- 268.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=228000, episode_reward=278.00 +/- 109.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=229000, episode_reward=351.20 +/- 154.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=230000, episode_reward=397.60 +/- 116.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=231000, episode_reward=353.20 +/- 291.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=232000, episode_reward=315.00 +/- 360.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=233000, episode_reward=410.20 +/- 246.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=234000, episode_reward=599.20 +/- 94.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=235000, episode_reward=362.60 +/- 205.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=236000, episode_reward=417.20 +/- 43.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=237000, episode_reward=301.40 +/- 254.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=238000, episode_reward=282.00 +/- 196.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=239000, episode_reward=519.00 +/- 173.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=240000, episode_reward=448.60 +/- 103.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=241000, episode_reward=514.20 +/- 55.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=242000, episode_reward=200.60 +/- 150.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=243000, episode_reward=387.20 +/- 230.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=244000, episode_reward=73.80 +/- 168.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=245000, episode_reward=375.40 +/- 131.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=246000, episode_reward=223.80 +/- 191.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=247000, episode_reward=391.00 +/- 209.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=248000, episode_reward=506.60 +/- 133.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=249000, episode_reward=402.00 +/- 140.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=250000, episode_reward=368.80 +/- 173.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=251000, episode_reward=290.60 +/- 217.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=252000, episode_reward=248.20 +/- 95.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=253000, episode_reward=389.40 +/- 179.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=254000, episode_reward=380.60 +/- 116.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=255000, episode_reward=212.00 +/- 196.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=256000, episode_reward=201.40 +/- 99.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=257000, episode_reward=333.40 +/- 114.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=258000, episode_reward=316.80 +/- 125.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=259000, episode_reward=409.80 +/- 131.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=260000, episode_reward=412.40 +/- 209.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=261000, episode_reward=363.20 +/- 245.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=262000, episode_reward=467.40 +/- 187.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=263000, episode_reward=299.20 +/- 150.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=264000, episode_reward=349.80 +/- 234.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=265000, episode_reward=268.80 +/- 90.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=266000, episode_reward=281.00 +/- 166.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=267000, episode_reward=371.80 +/- 98.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=268000, episode_reward=400.80 +/- 104.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=269000, episode_reward=452.60 +/- 161.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=270000, episode_reward=404.80 +/- 126.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=271000, episode_reward=341.80 +/- 52.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=272000, episode_reward=143.80 +/- 139.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=273000, episode_reward=181.00 +/- 102.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=274000, episode_reward=423.00 +/- 109.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=275000, episode_reward=427.60 +/- 227.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=276000, episode_reward=409.20 +/- 98.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=277000, episode_reward=182.60 +/- 203.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=278000, episode_reward=291.00 +/- 155.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=279000, episode_reward=417.80 +/- 91.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=280000, episode_reward=395.40 +/- 174.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=281000, episode_reward=202.00 +/- 217.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=282000, episode_reward=410.20 +/- 109.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=283000, episode_reward=238.80 +/- 129.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=284000, episode_reward=427.60 +/- 158.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=285000, episode_reward=488.20 +/- 123.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=286000, episode_reward=425.40 +/- 201.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=287000, episode_reward=341.60 +/- 146.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=288000, episode_reward=329.60 +/- 216.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=289000, episode_reward=266.80 +/- 216.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=290000, episode_reward=342.20 +/- 180.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=291000, episode_reward=412.20 +/- 178.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=292000, episode_reward=363.80 +/- 212.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=293000, episode_reward=270.00 +/- 183.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=294000, episode_reward=205.60 +/- 102.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=295000, episode_reward=447.60 +/- 133.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=296000, episode_reward=373.00 +/- 112.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=297000, episode_reward=274.00 +/- 174.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=298000, episode_reward=329.20 +/- 76.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=299000, episode_reward=302.20 +/- 140.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=300000, episode_reward=300.00 +/- 243.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=301000, episode_reward=246.20 +/- 167.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=302000, episode_reward=355.60 +/- 241.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=303000, episode_reward=324.40 +/- 325.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=304000, episode_reward=409.00 +/- 172.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=305000, episode_reward=370.80 +/- 41.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=306000, episode_reward=362.60 +/- 93.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=307000, episode_reward=276.00 +/- 275.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=308000, episode_reward=484.80 +/- 141.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=309000, episode_reward=472.00 +/- 285.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=310000, episode_reward=291.40 +/- 251.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=311000, episode_reward=164.20 +/- 190.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=312000, episode_reward=395.20 +/- 192.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=313000, episode_reward=295.20 +/- 228.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=314000, episode_reward=364.00 +/- 161.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=315000, episode_reward=256.80 +/- 106.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=316000, episode_reward=452.20 +/- 118.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=317000, episode_reward=468.60 +/- 85.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=318000, episode_reward=483.40 +/- 170.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=319000, episode_reward=386.20 +/- 174.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=320000, episode_reward=409.60 +/- 95.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=321000, episode_reward=447.40 +/- 115.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=322000, episode_reward=460.60 +/- 131.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=323000, episode_reward=310.00 +/- 160.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=324000, episode_reward=250.80 +/- 262.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=325000, episode_reward=393.20 +/- 102.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=326000, episode_reward=302.60 +/- 184.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=327000, episode_reward=276.40 +/- 221.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=328000, episode_reward=365.40 +/- 195.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=329000, episode_reward=346.20 +/- 214.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=330000, episode_reward=260.20 +/- 72.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=331000, episode_reward=346.80 +/- 311.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=332000, episode_reward=477.20 +/- 142.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=333000, episode_reward=319.40 +/- 308.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=334000, episode_reward=153.80 +/- 161.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=335000, episode_reward=233.40 +/- 159.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=336000, episode_reward=375.60 +/- 169.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=337000, episode_reward=316.80 +/- 145.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=338000, episode_reward=492.20 +/- 135.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=339000, episode_reward=401.20 +/- 101.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=340000, episode_reward=296.60 +/- 140.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=341000, episode_reward=355.80 +/- 218.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=342000, episode_reward=168.20 +/- 167.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=343000, episode_reward=326.40 +/- 117.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=344000, episode_reward=316.60 +/- 166.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=345000, episode_reward=274.60 +/- 208.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=346000, episode_reward=315.20 +/- 161.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=347000, episode_reward=180.80 +/- 259.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=348000, episode_reward=417.40 +/- 136.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=349000, episode_reward=405.60 +/- 124.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=350000, episode_reward=275.80 +/- 176.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=351000, episode_reward=289.40 +/- 240.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=352000, episode_reward=139.40 +/- 162.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=353000, episode_reward=226.20 +/- 128.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=354000, episode_reward=284.00 +/- 158.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=355000, episode_reward=195.60 +/- 181.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=356000, episode_reward=327.20 +/- 152.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=357000, episode_reward=293.00 +/- 173.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=358000, episode_reward=333.00 +/- 52.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=359000, episode_reward=411.20 +/- 196.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=360000, episode_reward=297.20 +/- 161.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=361000, episode_reward=373.00 +/- 129.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=362000, episode_reward=253.80 +/- 122.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=363000, episode_reward=296.40 +/- 233.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=364000, episode_reward=258.40 +/- 328.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=365000, episode_reward=271.20 +/- 157.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=366000, episode_reward=333.20 +/- 156.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=367000, episode_reward=417.80 +/- 110.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=368000, episode_reward=443.20 +/- 118.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=369000, episode_reward=176.80 +/- 268.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=370000, episode_reward=331.20 +/- 150.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=371000, episode_reward=323.20 +/- 294.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=372000, episode_reward=474.40 +/- 139.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=373000, episode_reward=366.60 +/- 146.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=374000, episode_reward=376.20 +/- 208.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=375000, episode_reward=414.40 +/- 81.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=376000, episode_reward=545.00 +/- 200.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=377000, episode_reward=302.80 +/- 176.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=378000, episode_reward=470.80 +/- 194.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=379000, episode_reward=381.80 +/- 180.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=380000, episode_reward=424.20 +/- 117.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=381000, episode_reward=364.40 +/- 232.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=382000, episode_reward=265.00 +/- 165.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=383000, episode_reward=210.00 +/- 164.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=384000, episode_reward=465.00 +/- 117.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=385000, episode_reward=415.40 +/- 198.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=386000, episode_reward=307.00 +/- 145.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=387000, episode_reward=323.60 +/- 155.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=388000, episode_reward=209.40 +/- 77.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=389000, episode_reward=445.20 +/- 218.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=390000, episode_reward=338.40 +/- 154.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=391000, episode_reward=311.60 +/- 159.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=392000, episode_reward=401.00 +/- 222.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=393000, episode_reward=279.80 +/- 166.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=394000, episode_reward=436.60 +/- 145.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=395000, episode_reward=394.20 +/- 201.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=396000, episode_reward=514.00 +/- 187.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=397000, episode_reward=255.40 +/- 114.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=398000, episode_reward=167.60 +/- 121.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=399000, episode_reward=410.20 +/- 148.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=400000, episode_reward=526.40 +/- 164.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=401000, episode_reward=441.60 +/- 75.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=402000, episode_reward=538.40 +/- 161.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=403000, episode_reward=373.00 +/- 75.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=404000, episode_reward=349.80 +/- 140.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=405000, episode_reward=352.80 +/- 114.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=406000, episode_reward=203.00 +/- 114.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=407000, episode_reward=405.00 +/- 370.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=408000, episode_reward=445.00 +/- 172.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=409000, episode_reward=282.40 +/- 231.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=410000, episode_reward=234.60 +/- 127.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=411000, episode_reward=262.80 +/- 183.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=412000, episode_reward=165.00 +/- 92.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=413000, episode_reward=249.20 +/- 102.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=414000, episode_reward=420.80 +/- 155.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=415000, episode_reward=196.00 +/- 271.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=416000, episode_reward=324.00 +/- 110.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=417000, episode_reward=273.00 +/- 212.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=418000, episode_reward=440.00 +/- 132.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=419000, episode_reward=369.00 +/- 204.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=420000, episode_reward=244.00 +/- 149.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=421000, episode_reward=437.80 +/- 75.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=422000, episode_reward=528.60 +/- 178.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=423000, episode_reward=342.60 +/- 136.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=424000, episode_reward=234.40 +/- 110.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=425000, episode_reward=382.80 +/- 249.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=426000, episode_reward=385.20 +/- 246.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=427000, episode_reward=363.80 +/- 236.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=428000, episode_reward=284.20 +/- 147.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=429000, episode_reward=307.40 +/- 187.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=430000, episode_reward=484.00 +/- 107.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=431000, episode_reward=295.20 +/- 285.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=432000, episode_reward=290.80 +/- 87.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=433000, episode_reward=380.80 +/- 127.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=434000, episode_reward=317.20 +/- 84.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=435000, episode_reward=260.60 +/- 151.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=436000, episode_reward=290.40 +/- 180.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=437000, episode_reward=419.40 +/- 107.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=438000, episode_reward=208.40 +/- 294.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=439000, episode_reward=367.40 +/- 149.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=440000, episode_reward=449.20 +/- 156.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=441000, episode_reward=376.20 +/- 84.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=442000, episode_reward=406.40 +/- 164.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=443000, episode_reward=235.20 +/- 311.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=444000, episode_reward=286.40 +/- 330.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=445000, episode_reward=376.60 +/- 210.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=446000, episode_reward=417.80 +/- 169.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=447000, episode_reward=376.00 +/- 159.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=448000, episode_reward=314.20 +/- 179.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=449000, episode_reward=230.80 +/- 164.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=450000, episode_reward=378.80 +/- 147.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=451000, episode_reward=346.00 +/- 154.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=452000, episode_reward=392.60 +/- 150.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=453000, episode_reward=281.00 +/- 234.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=454000, episode_reward=326.40 +/- 128.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=455000, episode_reward=367.00 +/- 200.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=456000, episode_reward=321.40 +/- 239.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=457000, episode_reward=376.00 +/- 162.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=458000, episode_reward=439.60 +/- 186.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=459000, episode_reward=287.60 +/- 155.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=460000, episode_reward=473.40 +/- 214.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=461000, episode_reward=377.40 +/- 115.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=462000, episode_reward=390.00 +/- 214.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=463000, episode_reward=332.40 +/- 49.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=464000, episode_reward=362.20 +/- 120.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=465000, episode_reward=346.80 +/- 167.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=466000, episode_reward=216.80 +/- 158.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=467000, episode_reward=392.80 +/- 160.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=468000, episode_reward=254.60 +/- 199.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=469000, episode_reward=266.40 +/- 421.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=470000, episode_reward=371.60 +/- 244.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=471000, episode_reward=306.80 +/- 157.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=472000, episode_reward=269.00 +/- 202.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=473000, episode_reward=376.00 +/- 142.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=474000, episode_reward=405.60 +/- 84.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=475000, episode_reward=295.00 +/- 87.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=476000, episode_reward=506.20 +/- 113.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=477000, episode_reward=355.60 +/- 214.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=478000, episode_reward=447.00 +/- 163.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=479000, episode_reward=465.40 +/- 277.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=480000, episode_reward=474.60 +/- 114.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=481000, episode_reward=325.20 +/- 171.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=482000, episode_reward=396.80 +/- 134.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=483000, episode_reward=477.40 +/- 208.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=484000, episode_reward=379.40 +/- 169.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=485000, episode_reward=335.20 +/- 134.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=486000, episode_reward=368.80 +/- 189.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=487000, episode_reward=398.40 +/- 243.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=488000, episode_reward=380.80 +/- 122.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=489000, episode_reward=349.60 +/- 110.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=490000, episode_reward=197.00 +/- 264.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=491000, episode_reward=158.80 +/- 353.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=492000, episode_reward=269.60 +/- 164.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=493000, episode_reward=412.60 +/- 199.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=494000, episode_reward=321.60 +/- 202.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=495000, episode_reward=526.60 +/- 152.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=496000, episode_reward=226.40 +/- 157.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=497000, episode_reward=294.80 +/- 109.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=498000, episode_reward=502.60 +/- 141.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=499000, episode_reward=295.00 +/- 228.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=500000, episode_reward=466.40 +/- 219.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=501000, episode_reward=240.80 +/- 170.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Gamma = 0.8, Learning Rate = 3e-05, Activation Function = ReLU, Neurons = 32, Process noise = 0.1, Measurement noise = 0.0\n",
      "Eval num_timesteps=1000, episode_reward=-1725.80 +/- 237.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=-1468.80 +/- 303.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3000, episode_reward=509.60 +/- 89.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4000, episode_reward=277.00 +/- 316.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=5000, episode_reward=-5853.80 +/- 1268.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=6000, episode_reward=-6617.40 +/- 655.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=7000, episode_reward=-5394.40 +/- 1578.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=8000, episode_reward=-5865.80 +/- 1844.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=9000, episode_reward=-6453.80 +/- 1973.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=10000, episode_reward=-5744.60 +/- 613.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=11000, episode_reward=288.40 +/- 195.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=12000, episode_reward=201.00 +/- 244.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=13000, episode_reward=-147.80 +/- 232.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=14000, episode_reward=-103.00 +/- 183.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=15000, episode_reward=183.00 +/- 276.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=16000, episode_reward=-155.40 +/- 185.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=17000, episode_reward=-214.00 +/- 573.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=18000, episode_reward=-209.00 +/- 147.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=19000, episode_reward=-27.60 +/- 492.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=20000, episode_reward=-235.40 +/- 660.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=21000, episode_reward=-148.20 +/- 394.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=22000, episode_reward=-95.20 +/- 247.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=23000, episode_reward=-305.40 +/- 573.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=24000, episode_reward=-326.00 +/- 632.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=25000, episode_reward=-461.80 +/- 432.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=26000, episode_reward=-68.60 +/- 302.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=27000, episode_reward=260.60 +/- 121.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=28000, episode_reward=-377.40 +/- 475.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=29000, episode_reward=70.60 +/- 367.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=30000, episode_reward=153.80 +/- 329.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=31000, episode_reward=-360.20 +/- 774.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=32000, episode_reward=174.20 +/- 134.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=33000, episode_reward=-55.40 +/- 259.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=34000, episode_reward=-123.60 +/- 408.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=35000, episode_reward=-145.20 +/- 426.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=36000, episode_reward=-162.00 +/- 305.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=37000, episode_reward=63.40 +/- 438.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=38000, episode_reward=-137.60 +/- 323.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=39000, episode_reward=361.20 +/- 158.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=40000, episode_reward=283.80 +/- 128.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=41000, episode_reward=316.00 +/- 168.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=42000, episode_reward=108.40 +/- 138.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=43000, episode_reward=465.80 +/- 87.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=44000, episode_reward=220.80 +/- 89.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=45000, episode_reward=355.00 +/- 160.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=46000, episode_reward=334.40 +/- 129.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=47000, episode_reward=296.80 +/- 210.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=48000, episode_reward=283.80 +/- 99.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=49000, episode_reward=286.40 +/- 88.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=50000, episode_reward=335.40 +/- 102.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=51000, episode_reward=272.40 +/- 173.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=52000, episode_reward=476.60 +/- 216.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=53000, episode_reward=322.80 +/- 152.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=54000, episode_reward=440.00 +/- 143.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=55000, episode_reward=463.80 +/- 147.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=56000, episode_reward=166.20 +/- 204.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=57000, episode_reward=397.60 +/- 156.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=58000, episode_reward=-89.60 +/- 154.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=59000, episode_reward=53.40 +/- 140.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=60000, episode_reward=177.80 +/- 29.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=61000, episode_reward=-10.40 +/- 147.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=62000, episode_reward=179.00 +/- 94.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=63000, episode_reward=246.20 +/- 147.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=64000, episode_reward=296.00 +/- 132.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=65000, episode_reward=239.20 +/- 134.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=66000, episode_reward=201.40 +/- 125.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=67000, episode_reward=205.00 +/- 91.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=68000, episode_reward=188.00 +/- 101.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=69000, episode_reward=1.20 +/- 105.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=70000, episode_reward=237.40 +/- 110.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=71000, episode_reward=16.80 +/- 115.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=72000, episode_reward=64.80 +/- 82.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=73000, episode_reward=19.40 +/- 171.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=74000, episode_reward=128.60 +/- 143.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=75000, episode_reward=123.80 +/- 78.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=76000, episode_reward=184.40 +/- 232.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=77000, episode_reward=336.40 +/- 152.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=78000, episode_reward=37.40 +/- 209.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=79000, episode_reward=27.40 +/- 147.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=80000, episode_reward=308.20 +/- 157.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=81000, episode_reward=248.60 +/- 118.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=82000, episode_reward=143.00 +/- 113.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=83000, episode_reward=63.00 +/- 141.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=84000, episode_reward=252.40 +/- 136.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=85000, episode_reward=237.60 +/- 149.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=86000, episode_reward=453.80 +/- 93.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=87000, episode_reward=47.60 +/- 110.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=88000, episode_reward=-12.00 +/- 186.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=89000, episode_reward=103.80 +/- 104.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=90000, episode_reward=268.40 +/- 221.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=91000, episode_reward=160.80 +/- 76.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=92000, episode_reward=225.40 +/- 61.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=93000, episode_reward=427.40 +/- 111.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=94000, episode_reward=260.80 +/- 121.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=95000, episode_reward=349.00 +/- 163.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=96000, episode_reward=248.20 +/- 89.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=97000, episode_reward=244.40 +/- 186.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=98000, episode_reward=257.00 +/- 141.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=99000, episode_reward=396.60 +/- 145.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=100000, episode_reward=385.20 +/- 150.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=101000, episode_reward=103.00 +/- 134.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=102000, episode_reward=196.40 +/- 111.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=103000, episode_reward=234.80 +/- 285.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=104000, episode_reward=357.40 +/- 139.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=105000, episode_reward=237.60 +/- 205.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=106000, episode_reward=267.00 +/- 135.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=107000, episode_reward=263.80 +/- 162.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=108000, episode_reward=188.00 +/- 189.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=109000, episode_reward=543.60 +/- 182.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=110000, episode_reward=433.20 +/- 98.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=111000, episode_reward=331.00 +/- 198.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=112000, episode_reward=381.40 +/- 131.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=113000, episode_reward=427.60 +/- 240.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=114000, episode_reward=391.80 +/- 133.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=115000, episode_reward=264.00 +/- 130.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=116000, episode_reward=184.80 +/- 203.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=117000, episode_reward=223.60 +/- 193.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=118000, episode_reward=462.40 +/- 188.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=119000, episode_reward=417.00 +/- 158.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=120000, episode_reward=277.20 +/- 171.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=121000, episode_reward=308.00 +/- 157.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=122000, episode_reward=408.60 +/- 101.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=123000, episode_reward=305.80 +/- 224.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=124000, episode_reward=367.20 +/- 108.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=125000, episode_reward=245.00 +/- 215.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=126000, episode_reward=287.60 +/- 173.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=127000, episode_reward=190.60 +/- 109.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=128000, episode_reward=162.60 +/- 212.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=129000, episode_reward=367.80 +/- 86.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=130000, episode_reward=390.00 +/- 167.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=131000, episode_reward=270.20 +/- 84.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=132000, episode_reward=403.20 +/- 181.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=133000, episode_reward=120.20 +/- 131.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=134000, episode_reward=313.20 +/- 179.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=135000, episode_reward=333.20 +/- 131.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=136000, episode_reward=253.00 +/- 208.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=137000, episode_reward=260.80 +/- 105.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=138000, episode_reward=229.80 +/- 114.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=139000, episode_reward=270.20 +/- 137.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=140000, episode_reward=290.80 +/- 188.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=141000, episode_reward=246.60 +/- 32.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=142000, episode_reward=319.80 +/- 173.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=143000, episode_reward=347.20 +/- 136.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=144000, episode_reward=330.40 +/- 74.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=145000, episode_reward=329.40 +/- 113.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=146000, episode_reward=289.00 +/- 213.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=147000, episode_reward=329.80 +/- 88.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=148000, episode_reward=234.60 +/- 183.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=149000, episode_reward=369.60 +/- 149.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=150000, episode_reward=340.20 +/- 142.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=151000, episode_reward=189.40 +/- 202.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=152000, episode_reward=351.60 +/- 155.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=153000, episode_reward=476.00 +/- 64.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=154000, episode_reward=423.60 +/- 149.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=155000, episode_reward=436.40 +/- 224.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=156000, episode_reward=237.60 +/- 67.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=157000, episode_reward=240.60 +/- 101.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=158000, episode_reward=355.40 +/- 137.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=159000, episode_reward=341.40 +/- 218.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=160000, episode_reward=548.00 +/- 41.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=161000, episode_reward=213.20 +/- 248.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=162000, episode_reward=429.40 +/- 111.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=163000, episode_reward=240.40 +/- 214.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=164000, episode_reward=419.00 +/- 201.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=165000, episode_reward=319.40 +/- 80.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=166000, episode_reward=349.80 +/- 162.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=167000, episode_reward=367.20 +/- 90.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=168000, episode_reward=306.00 +/- 145.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=169000, episode_reward=370.20 +/- 112.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=170000, episode_reward=329.80 +/- 169.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=171000, episode_reward=154.80 +/- 99.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=172000, episode_reward=281.20 +/- 137.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=173000, episode_reward=339.60 +/- 207.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=174000, episode_reward=243.80 +/- 157.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=175000, episode_reward=230.00 +/- 249.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=176000, episode_reward=341.00 +/- 228.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=177000, episode_reward=312.20 +/- 142.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=178000, episode_reward=448.40 +/- 191.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=179000, episode_reward=337.00 +/- 68.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=180000, episode_reward=379.80 +/- 166.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=181000, episode_reward=302.80 +/- 44.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=182000, episode_reward=554.00 +/- 182.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=183000, episode_reward=330.60 +/- 182.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=184000, episode_reward=360.00 +/- 141.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=185000, episode_reward=298.80 +/- 90.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=186000, episode_reward=329.20 +/- 182.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=187000, episode_reward=347.20 +/- 197.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=188000, episode_reward=316.80 +/- 168.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=189000, episode_reward=350.60 +/- 159.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=190000, episode_reward=279.80 +/- 218.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=191000, episode_reward=373.60 +/- 158.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=192000, episode_reward=341.40 +/- 161.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=193000, episode_reward=186.60 +/- 306.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=194000, episode_reward=473.80 +/- 224.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=195000, episode_reward=464.40 +/- 101.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=196000, episode_reward=326.80 +/- 137.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=197000, episode_reward=95.00 +/- 163.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=198000, episode_reward=179.00 +/- 147.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=199000, episode_reward=379.40 +/- 125.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=200000, episode_reward=459.40 +/- 111.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=201000, episode_reward=349.80 +/- 151.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=202000, episode_reward=507.20 +/- 179.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=203000, episode_reward=198.20 +/- 151.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=204000, episode_reward=403.40 +/- 172.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=205000, episode_reward=345.80 +/- 212.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=206000, episode_reward=427.20 +/- 91.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=207000, episode_reward=281.60 +/- 285.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=208000, episode_reward=428.40 +/- 284.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=209000, episode_reward=402.40 +/- 157.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=210000, episode_reward=404.20 +/- 184.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=211000, episode_reward=310.80 +/- 136.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=212000, episode_reward=295.40 +/- 196.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=213000, episode_reward=351.20 +/- 276.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=214000, episode_reward=406.80 +/- 97.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=215000, episode_reward=459.80 +/- 186.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=216000, episode_reward=229.80 +/- 163.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=217000, episode_reward=299.40 +/- 216.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=218000, episode_reward=344.80 +/- 188.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=219000, episode_reward=248.60 +/- 227.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=220000, episode_reward=304.40 +/- 138.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=221000, episode_reward=286.00 +/- 186.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=222000, episode_reward=380.80 +/- 80.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=223000, episode_reward=337.80 +/- 176.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=224000, episode_reward=327.80 +/- 215.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=225000, episode_reward=330.60 +/- 134.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=226000, episode_reward=393.80 +/- 280.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=227000, episode_reward=218.00 +/- 174.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=228000, episode_reward=359.80 +/- 187.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=229000, episode_reward=378.60 +/- 201.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=230000, episode_reward=397.40 +/- 145.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=231000, episode_reward=417.60 +/- 117.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=232000, episode_reward=233.40 +/- 299.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=233000, episode_reward=486.60 +/- 90.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=234000, episode_reward=290.40 +/- 146.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=235000, episode_reward=495.60 +/- 150.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=236000, episode_reward=301.20 +/- 122.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=237000, episode_reward=326.00 +/- 219.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=238000, episode_reward=412.80 +/- 109.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=239000, episode_reward=461.20 +/- 158.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=240000, episode_reward=331.20 +/- 199.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=241000, episode_reward=395.80 +/- 138.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=242000, episode_reward=327.20 +/- 208.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=243000, episode_reward=382.00 +/- 181.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=244000, episode_reward=313.20 +/- 183.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=245000, episode_reward=323.80 +/- 85.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=246000, episode_reward=323.60 +/- 211.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=247000, episode_reward=291.60 +/- 108.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=248000, episode_reward=321.00 +/- 85.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=249000, episode_reward=576.60 +/- 225.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=250000, episode_reward=235.00 +/- 60.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=251000, episode_reward=445.40 +/- 108.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=252000, episode_reward=162.80 +/- 144.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=253000, episode_reward=277.60 +/- 194.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=254000, episode_reward=371.60 +/- 175.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=255000, episode_reward=274.20 +/- 245.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=256000, episode_reward=372.40 +/- 136.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=257000, episode_reward=179.00 +/- 376.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=258000, episode_reward=503.40 +/- 160.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=259000, episode_reward=213.20 +/- 255.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=260000, episode_reward=435.60 +/- 177.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=261000, episode_reward=235.60 +/- 166.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=262000, episode_reward=366.20 +/- 83.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=263000, episode_reward=396.40 +/- 184.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=264000, episode_reward=298.40 +/- 53.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=265000, episode_reward=220.60 +/- 145.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=266000, episode_reward=371.20 +/- 182.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=267000, episode_reward=354.00 +/- 254.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=268000, episode_reward=370.20 +/- 176.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=269000, episode_reward=425.60 +/- 208.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=270000, episode_reward=165.00 +/- 157.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=271000, episode_reward=342.60 +/- 123.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=272000, episode_reward=404.40 +/- 170.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=273000, episode_reward=198.00 +/- 130.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=274000, episode_reward=189.40 +/- 147.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=275000, episode_reward=400.60 +/- 180.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=276000, episode_reward=164.80 +/- 199.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=277000, episode_reward=393.00 +/- 154.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=278000, episode_reward=320.20 +/- 150.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=279000, episode_reward=519.40 +/- 161.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=280000, episode_reward=302.80 +/- 378.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=281000, episode_reward=242.40 +/- 194.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=282000, episode_reward=298.80 +/- 99.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=283000, episode_reward=324.40 +/- 190.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=284000, episode_reward=315.20 +/- 82.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=285000, episode_reward=379.40 +/- 117.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=286000, episode_reward=313.20 +/- 213.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=287000, episode_reward=459.20 +/- 177.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=288000, episode_reward=404.00 +/- 253.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=289000, episode_reward=419.40 +/- 158.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=290000, episode_reward=379.60 +/- 54.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=291000, episode_reward=324.00 +/- 157.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=292000, episode_reward=287.60 +/- 133.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=293000, episode_reward=340.60 +/- 214.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=294000, episode_reward=373.20 +/- 113.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=295000, episode_reward=231.00 +/- 189.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=296000, episode_reward=298.60 +/- 178.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=297000, episode_reward=317.00 +/- 184.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=298000, episode_reward=304.40 +/- 66.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=299000, episode_reward=401.80 +/- 93.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=300000, episode_reward=354.00 +/- 255.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=301000, episode_reward=396.20 +/- 143.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=302000, episode_reward=535.20 +/- 238.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=303000, episode_reward=428.20 +/- 130.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=304000, episode_reward=368.40 +/- 124.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=305000, episode_reward=192.20 +/- 120.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=306000, episode_reward=322.00 +/- 69.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=307000, episode_reward=468.40 +/- 136.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=308000, episode_reward=432.20 +/- 183.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=309000, episode_reward=416.60 +/- 85.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=310000, episode_reward=354.80 +/- 225.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=311000, episode_reward=342.80 +/- 254.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=312000, episode_reward=403.80 +/- 139.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=313000, episode_reward=220.20 +/- 230.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=314000, episode_reward=334.00 +/- 35.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=315000, episode_reward=330.20 +/- 54.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=316000, episode_reward=303.40 +/- 144.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=317000, episode_reward=186.80 +/- 262.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=318000, episode_reward=403.00 +/- 152.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=319000, episode_reward=474.60 +/- 253.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=320000, episode_reward=359.20 +/- 159.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=321000, episode_reward=401.00 +/- 112.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=322000, episode_reward=217.20 +/- 151.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=323000, episode_reward=383.60 +/- 128.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=324000, episode_reward=333.40 +/- 82.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=325000, episode_reward=422.20 +/- 129.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=326000, episode_reward=224.80 +/- 170.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=327000, episode_reward=349.20 +/- 220.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=328000, episode_reward=416.20 +/- 83.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=329000, episode_reward=306.00 +/- 66.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=330000, episode_reward=230.40 +/- 194.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=331000, episode_reward=386.60 +/- 98.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=332000, episode_reward=374.60 +/- 219.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=333000, episode_reward=174.20 +/- 194.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=334000, episode_reward=318.60 +/- 112.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=335000, episode_reward=321.00 +/- 127.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=336000, episode_reward=341.20 +/- 184.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=337000, episode_reward=227.80 +/- 99.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=338000, episode_reward=382.60 +/- 128.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=339000, episode_reward=389.60 +/- 124.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=340000, episode_reward=490.40 +/- 68.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=341000, episode_reward=142.00 +/- 136.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=342000, episode_reward=176.40 +/- 185.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=343000, episode_reward=228.00 +/- 141.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=344000, episode_reward=334.00 +/- 56.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=345000, episode_reward=376.80 +/- 201.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=346000, episode_reward=397.20 +/- 58.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=347000, episode_reward=318.00 +/- 336.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=348000, episode_reward=272.00 +/- 50.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=349000, episode_reward=397.60 +/- 141.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=350000, episode_reward=206.40 +/- 157.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=351000, episode_reward=414.80 +/- 162.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=352000, episode_reward=220.60 +/- 117.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=353000, episode_reward=335.80 +/- 144.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=354000, episode_reward=469.00 +/- 122.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=355000, episode_reward=286.60 +/- 228.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=356000, episode_reward=344.00 +/- 156.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=357000, episode_reward=366.20 +/- 188.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=358000, episode_reward=277.60 +/- 154.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=359000, episode_reward=276.60 +/- 204.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=360000, episode_reward=344.60 +/- 159.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=361000, episode_reward=208.80 +/- 126.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=362000, episode_reward=322.80 +/- 237.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=363000, episode_reward=326.80 +/- 203.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=364000, episode_reward=228.60 +/- 213.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=365000, episode_reward=358.20 +/- 322.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=366000, episode_reward=381.40 +/- 100.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=367000, episode_reward=224.00 +/- 46.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=368000, episode_reward=349.20 +/- 246.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=369000, episode_reward=238.20 +/- 120.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=370000, episode_reward=242.20 +/- 148.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=371000, episode_reward=231.80 +/- 95.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=372000, episode_reward=313.40 +/- 100.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=373000, episode_reward=315.40 +/- 237.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=374000, episode_reward=261.60 +/- 162.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=375000, episode_reward=274.80 +/- 218.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=376000, episode_reward=330.60 +/- 193.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=377000, episode_reward=336.80 +/- 135.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=378000, episode_reward=433.40 +/- 96.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=379000, episode_reward=262.80 +/- 233.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=380000, episode_reward=254.20 +/- 59.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=381000, episode_reward=496.20 +/- 137.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=382000, episode_reward=368.40 +/- 123.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=383000, episode_reward=396.00 +/- 173.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=384000, episode_reward=246.20 +/- 86.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=385000, episode_reward=339.80 +/- 90.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=386000, episode_reward=412.20 +/- 182.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=387000, episode_reward=328.40 +/- 179.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=388000, episode_reward=306.60 +/- 165.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=389000, episode_reward=308.20 +/- 259.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=390000, episode_reward=366.60 +/- 83.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=391000, episode_reward=421.40 +/- 88.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=392000, episode_reward=263.40 +/- 222.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=393000, episode_reward=314.20 +/- 86.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=394000, episode_reward=297.20 +/- 241.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=395000, episode_reward=442.80 +/- 71.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=396000, episode_reward=232.00 +/- 121.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=397000, episode_reward=322.40 +/- 269.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=398000, episode_reward=358.40 +/- 198.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=399000, episode_reward=445.60 +/- 143.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=400000, episode_reward=380.60 +/- 163.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=401000, episode_reward=180.00 +/- 269.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=402000, episode_reward=439.00 +/- 119.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=403000, episode_reward=233.00 +/- 149.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=404000, episode_reward=575.40 +/- 116.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=405000, episode_reward=284.00 +/- 76.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=406000, episode_reward=418.20 +/- 209.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=407000, episode_reward=212.00 +/- 276.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=408000, episode_reward=391.40 +/- 168.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=409000, episode_reward=118.20 +/- 172.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=410000, episode_reward=160.20 +/- 162.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=411000, episode_reward=277.20 +/- 142.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=412000, episode_reward=337.80 +/- 222.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=413000, episode_reward=115.40 +/- 216.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=414000, episode_reward=279.60 +/- 182.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=415000, episode_reward=257.80 +/- 126.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=416000, episode_reward=314.80 +/- 127.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=417000, episode_reward=325.00 +/- 175.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=418000, episode_reward=329.80 +/- 79.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=419000, episode_reward=235.40 +/- 336.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=420000, episode_reward=302.00 +/- 181.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=421000, episode_reward=383.80 +/- 88.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=422000, episode_reward=375.80 +/- 77.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=423000, episode_reward=354.40 +/- 311.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=424000, episode_reward=245.60 +/- 133.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=425000, episode_reward=284.40 +/- 142.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=426000, episode_reward=358.20 +/- 127.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=427000, episode_reward=416.40 +/- 124.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=428000, episode_reward=355.40 +/- 235.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=429000, episode_reward=382.60 +/- 120.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=430000, episode_reward=541.00 +/- 140.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=431000, episode_reward=298.20 +/- 115.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=432000, episode_reward=450.60 +/- 164.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=433000, episode_reward=460.80 +/- 159.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=434000, episode_reward=117.80 +/- 293.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=435000, episode_reward=205.00 +/- 233.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=436000, episode_reward=412.80 +/- 124.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=437000, episode_reward=192.60 +/- 125.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=438000, episode_reward=382.00 +/- 197.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=439000, episode_reward=280.80 +/- 38.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=440000, episode_reward=338.60 +/- 147.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=441000, episode_reward=387.20 +/- 48.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=442000, episode_reward=239.80 +/- 97.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=443000, episode_reward=307.40 +/- 217.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=444000, episode_reward=265.00 +/- 151.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=445000, episode_reward=257.40 +/- 206.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=446000, episode_reward=333.40 +/- 207.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=447000, episode_reward=427.60 +/- 145.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=448000, episode_reward=242.00 +/- 132.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=449000, episode_reward=343.20 +/- 186.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=450000, episode_reward=424.60 +/- 134.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=451000, episode_reward=361.20 +/- 218.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=452000, episode_reward=215.40 +/- 224.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=453000, episode_reward=287.40 +/- 70.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=454000, episode_reward=308.80 +/- 136.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=455000, episode_reward=318.00 +/- 153.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=456000, episode_reward=298.40 +/- 102.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=457000, episode_reward=241.00 +/- 194.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=458000, episode_reward=248.60 +/- 65.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=459000, episode_reward=254.00 +/- 116.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=460000, episode_reward=333.60 +/- 166.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=461000, episode_reward=495.60 +/- 175.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=462000, episode_reward=364.00 +/- 110.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=463000, episode_reward=384.80 +/- 232.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=464000, episode_reward=229.00 +/- 250.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=465000, episode_reward=381.20 +/- 130.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=466000, episode_reward=299.00 +/- 151.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=467000, episode_reward=472.00 +/- 180.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=468000, episode_reward=236.60 +/- 221.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=469000, episode_reward=195.60 +/- 187.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=470000, episode_reward=385.60 +/- 152.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=471000, episode_reward=335.20 +/- 71.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=472000, episode_reward=329.20 +/- 239.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=473000, episode_reward=316.00 +/- 111.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=474000, episode_reward=278.60 +/- 118.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=475000, episode_reward=376.60 +/- 174.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=476000, episode_reward=346.80 +/- 101.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=477000, episode_reward=303.20 +/- 101.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=478000, episode_reward=234.60 +/- 85.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=479000, episode_reward=368.80 +/- 100.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=480000, episode_reward=362.20 +/- 119.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=481000, episode_reward=278.00 +/- 65.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=482000, episode_reward=486.00 +/- 130.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=483000, episode_reward=360.80 +/- 194.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=484000, episode_reward=370.60 +/- 64.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=485000, episode_reward=346.60 +/- 174.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=486000, episode_reward=326.20 +/- 192.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=487000, episode_reward=474.00 +/- 152.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=488000, episode_reward=419.60 +/- 225.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=489000, episode_reward=299.20 +/- 120.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=490000, episode_reward=278.40 +/- 209.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=491000, episode_reward=412.00 +/- 118.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=492000, episode_reward=272.00 +/- 194.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=493000, episode_reward=366.40 +/- 181.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=494000, episode_reward=137.80 +/- 72.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=495000, episode_reward=357.80 +/- 119.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=496000, episode_reward=328.20 +/- 96.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=497000, episode_reward=234.80 +/- 101.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=498000, episode_reward=242.00 +/- 181.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=499000, episode_reward=473.40 +/- 187.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=500000, episode_reward=508.20 +/- 130.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=501000, episode_reward=208.20 +/- 108.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Gamma = 0.8, Learning Rate = 3e-05, Activation Function = ReLU, Neurons = 64, Process noise = 0.1, Measurement noise = 0.0\n",
      "Eval num_timesteps=1000, episode_reward=-1446.60 +/- 294.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=-1886.60 +/- 747.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=3000, episode_reward=-5757.20 +/- 1099.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=4000, episode_reward=-4341.80 +/- 1105.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=5000, episode_reward=-6021.40 +/- 1006.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=6000, episode_reward=-5805.60 +/- 1227.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=7000, episode_reward=-197.80 +/- 664.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=8000, episode_reward=-119.00 +/- 632.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=9000, episode_reward=-6694.80 +/- 2264.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=10000, episode_reward=-6702.40 +/- 2349.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=11000, episode_reward=-688.00 +/- 398.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=12000, episode_reward=-881.60 +/- 1491.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=13000, episode_reward=-652.20 +/- 423.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=14000, episode_reward=-355.80 +/- 279.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=15000, episode_reward=41.00 +/- 304.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=16000, episode_reward=129.80 +/- 298.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=17000, episode_reward=-1003.60 +/- 781.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=18000, episode_reward=-499.60 +/- 440.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=19000, episode_reward=-26.60 +/- 166.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=20000, episode_reward=-722.60 +/- 950.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=21000, episode_reward=34.40 +/- 198.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=22000, episode_reward=-7.60 +/- 207.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=23000, episode_reward=-430.80 +/- 726.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=24000, episode_reward=-1062.60 +/- 1453.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=25000, episode_reward=124.80 +/- 331.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=26000, episode_reward=-121.40 +/- 126.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=27000, episode_reward=10.20 +/- 396.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=28000, episode_reward=39.00 +/- 469.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=29000, episode_reward=303.60 +/- 216.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=30000, episode_reward=366.20 +/- 90.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=31000, episode_reward=249.00 +/- 241.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=32000, episode_reward=89.40 +/- 303.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=33000, episode_reward=292.80 +/- 204.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=34000, episode_reward=46.80 +/- 161.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=35000, episode_reward=381.40 +/- 164.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=36000, episode_reward=239.60 +/- 155.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=37000, episode_reward=375.00 +/- 140.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=38000, episode_reward=358.20 +/- 262.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=39000, episode_reward=150.60 +/- 205.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=40000, episode_reward=328.80 +/- 97.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=41000, episode_reward=385.00 +/- 25.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=42000, episode_reward=382.80 +/- 57.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=43000, episode_reward=250.60 +/- 93.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=44000, episode_reward=258.80 +/- 268.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=45000, episode_reward=133.20 +/- 183.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=46000, episode_reward=298.00 +/- 243.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=47000, episode_reward=297.60 +/- 202.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=48000, episode_reward=417.40 +/- 148.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=49000, episode_reward=241.20 +/- 51.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=50000, episode_reward=290.40 +/- 216.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=51000, episode_reward=381.80 +/- 270.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=52000, episode_reward=398.00 +/- 208.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=53000, episode_reward=490.80 +/- 122.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=54000, episode_reward=282.80 +/- 256.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=55000, episode_reward=332.40 +/- 131.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=56000, episode_reward=100.40 +/- 328.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=57000, episode_reward=219.00 +/- 200.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=58000, episode_reward=-120.40 +/- 434.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=59000, episode_reward=-583.60 +/- 1014.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=60000, episode_reward=393.40 +/- 137.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=61000, episode_reward=312.40 +/- 149.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=62000, episode_reward=224.20 +/- 163.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=63000, episode_reward=159.20 +/- 225.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=64000, episode_reward=212.20 +/- 106.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=65000, episode_reward=271.40 +/- 153.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=66000, episode_reward=439.80 +/- 103.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=67000, episode_reward=489.00 +/- 190.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=68000, episode_reward=443.40 +/- 264.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=69000, episode_reward=458.60 +/- 77.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=70000, episode_reward=131.60 +/- 201.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=71000, episode_reward=306.40 +/- 146.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=72000, episode_reward=298.80 +/- 319.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=73000, episode_reward=260.80 +/- 64.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=74000, episode_reward=359.60 +/- 136.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=75000, episode_reward=367.00 +/- 167.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=76000, episode_reward=343.00 +/- 181.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=77000, episode_reward=523.00 +/- 139.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=78000, episode_reward=463.20 +/- 190.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=79000, episode_reward=414.80 +/- 146.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=80000, episode_reward=497.00 +/- 183.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=81000, episode_reward=293.80 +/- 176.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=82000, episode_reward=331.00 +/- 199.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=83000, episode_reward=358.60 +/- 194.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=84000, episode_reward=354.60 +/- 115.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=85000, episode_reward=213.00 +/- 79.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=86000, episode_reward=246.80 +/- 73.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=87000, episode_reward=358.80 +/- 146.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=88000, episode_reward=304.80 +/- 125.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=89000, episode_reward=405.20 +/- 108.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=90000, episode_reward=242.00 +/- 113.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=91000, episode_reward=254.00 +/- 48.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=92000, episode_reward=174.80 +/- 161.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=93000, episode_reward=167.20 +/- 155.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=94000, episode_reward=361.80 +/- 68.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=95000, episode_reward=173.20 +/- 146.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=96000, episode_reward=369.00 +/- 326.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=97000, episode_reward=160.60 +/- 120.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=98000, episode_reward=545.20 +/- 223.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=99000, episode_reward=367.40 +/- 230.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=100000, episode_reward=313.00 +/- 92.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=101000, episode_reward=295.20 +/- 156.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=102000, episode_reward=317.00 +/- 214.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=103000, episode_reward=275.60 +/- 220.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=104000, episode_reward=317.00 +/- 114.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=105000, episode_reward=171.00 +/- 241.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=106000, episode_reward=237.00 +/- 168.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=107000, episode_reward=276.20 +/- 255.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=108000, episode_reward=401.60 +/- 186.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=109000, episode_reward=343.80 +/- 264.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=110000, episode_reward=383.20 +/- 132.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=111000, episode_reward=215.20 +/- 161.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=112000, episode_reward=245.40 +/- 155.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=113000, episode_reward=353.80 +/- 202.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=114000, episode_reward=225.60 +/- 205.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=115000, episode_reward=276.20 +/- 56.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=116000, episode_reward=271.80 +/- 205.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=117000, episode_reward=342.20 +/- 105.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=118000, episode_reward=358.40 +/- 167.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=119000, episode_reward=327.60 +/- 46.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=120000, episode_reward=365.60 +/- 112.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=121000, episode_reward=285.00 +/- 211.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=122000, episode_reward=266.00 +/- 72.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=123000, episode_reward=141.20 +/- 112.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=124000, episode_reward=297.80 +/- 170.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=125000, episode_reward=335.00 +/- 104.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=126000, episode_reward=258.20 +/- 156.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=127000, episode_reward=200.40 +/- 280.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=128000, episode_reward=470.80 +/- 225.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=129000, episode_reward=245.00 +/- 148.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=130000, episode_reward=367.00 +/- 169.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=131000, episode_reward=386.00 +/- 123.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=132000, episode_reward=375.80 +/- 155.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=133000, episode_reward=268.80 +/- 192.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=134000, episode_reward=220.80 +/- 184.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=135000, episode_reward=279.60 +/- 151.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=136000, episode_reward=334.20 +/- 128.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=137000, episode_reward=444.60 +/- 126.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=138000, episode_reward=214.40 +/- 211.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=139000, episode_reward=244.80 +/- 126.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=140000, episode_reward=336.20 +/- 214.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=141000, episode_reward=297.20 +/- 93.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=142000, episode_reward=334.40 +/- 193.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=143000, episode_reward=254.20 +/- 142.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=144000, episode_reward=432.40 +/- 122.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=145000, episode_reward=280.60 +/- 164.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=146000, episode_reward=230.20 +/- 63.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=147000, episode_reward=321.00 +/- 178.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=148000, episode_reward=267.20 +/- 197.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=149000, episode_reward=284.20 +/- 264.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=150000, episode_reward=321.80 +/- 118.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=151000, episode_reward=446.60 +/- 56.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=152000, episode_reward=275.00 +/- 138.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=153000, episode_reward=377.00 +/- 99.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=154000, episode_reward=356.60 +/- 137.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=155000, episode_reward=420.20 +/- 160.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=156000, episode_reward=339.40 +/- 109.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=157000, episode_reward=184.80 +/- 158.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=158000, episode_reward=318.40 +/- 151.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=159000, episode_reward=385.20 +/- 135.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=160000, episode_reward=367.80 +/- 82.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=161000, episode_reward=283.20 +/- 122.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=162000, episode_reward=283.80 +/- 75.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=163000, episode_reward=309.80 +/- 236.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=164000, episode_reward=313.40 +/- 46.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=165000, episode_reward=277.60 +/- 228.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=166000, episode_reward=479.20 +/- 145.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=167000, episode_reward=327.00 +/- 179.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=168000, episode_reward=369.80 +/- 201.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=169000, episode_reward=390.40 +/- 256.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=170000, episode_reward=291.80 +/- 149.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=171000, episode_reward=328.20 +/- 77.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=172000, episode_reward=154.20 +/- 178.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=173000, episode_reward=360.80 +/- 194.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=174000, episode_reward=421.40 +/- 209.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=175000, episode_reward=351.00 +/- 209.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=176000, episode_reward=307.80 +/- 41.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=177000, episode_reward=359.00 +/- 63.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=178000, episode_reward=249.60 +/- 317.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=179000, episode_reward=314.00 +/- 124.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=180000, episode_reward=364.20 +/- 71.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=181000, episode_reward=272.20 +/- 144.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=182000, episode_reward=404.20 +/- 126.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=183000, episode_reward=234.80 +/- 245.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=184000, episode_reward=341.00 +/- 137.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=185000, episode_reward=195.60 +/- 181.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=186000, episode_reward=161.40 +/- 203.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=187000, episode_reward=385.00 +/- 199.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=188000, episode_reward=337.40 +/- 76.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=189000, episode_reward=262.40 +/- 217.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=190000, episode_reward=325.00 +/- 165.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=191000, episode_reward=348.40 +/- 159.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=192000, episode_reward=271.80 +/- 197.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=193000, episode_reward=265.80 +/- 201.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=194000, episode_reward=348.20 +/- 205.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=195000, episode_reward=265.40 +/- 97.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=196000, episode_reward=228.00 +/- 80.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=197000, episode_reward=131.20 +/- 182.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=198000, episode_reward=330.20 +/- 93.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=199000, episode_reward=420.80 +/- 199.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=200000, episode_reward=402.00 +/- 100.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=201000, episode_reward=313.20 +/- 214.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=202000, episode_reward=363.00 +/- 96.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=203000, episode_reward=385.00 +/- 196.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=204000, episode_reward=330.00 +/- 170.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=205000, episode_reward=282.20 +/- 102.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=206000, episode_reward=397.80 +/- 218.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=207000, episode_reward=432.80 +/- 173.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=208000, episode_reward=213.20 +/- 218.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=209000, episode_reward=376.40 +/- 116.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=210000, episode_reward=259.80 +/- 154.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=211000, episode_reward=290.20 +/- 286.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=212000, episode_reward=350.60 +/- 163.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=213000, episode_reward=294.20 +/- 253.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=214000, episode_reward=172.20 +/- 198.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=215000, episode_reward=250.80 +/- 210.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=216000, episode_reward=437.20 +/- 209.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=217000, episode_reward=207.20 +/- 148.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=218000, episode_reward=358.00 +/- 141.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=219000, episode_reward=367.60 +/- 106.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=220000, episode_reward=205.20 +/- 118.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=221000, episode_reward=318.40 +/- 151.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=222000, episode_reward=339.80 +/- 231.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=223000, episode_reward=256.80 +/- 280.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=224000, episode_reward=237.60 +/- 139.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=225000, episode_reward=394.60 +/- 61.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=226000, episode_reward=371.20 +/- 151.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=227000, episode_reward=282.80 +/- 102.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=228000, episode_reward=409.40 +/- 194.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=229000, episode_reward=269.80 +/- 106.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=230000, episode_reward=405.20 +/- 166.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=231000, episode_reward=338.40 +/- 261.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=232000, episode_reward=373.40 +/- 180.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=233000, episode_reward=236.00 +/- 179.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=234000, episode_reward=362.40 +/- 114.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=235000, episode_reward=426.80 +/- 207.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=236000, episode_reward=251.80 +/- 207.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=237000, episode_reward=339.00 +/- 146.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=238000, episode_reward=407.80 +/- 174.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=239000, episode_reward=367.80 +/- 137.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=240000, episode_reward=221.00 +/- 147.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=241000, episode_reward=214.20 +/- 96.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=242000, episode_reward=188.80 +/- 103.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=243000, episode_reward=299.80 +/- 64.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=244000, episode_reward=449.00 +/- 180.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=245000, episode_reward=369.40 +/- 201.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=246000, episode_reward=405.20 +/- 104.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=247000, episode_reward=294.60 +/- 211.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=248000, episode_reward=322.80 +/- 159.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=249000, episode_reward=325.20 +/- 128.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=250000, episode_reward=334.80 +/- 210.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=251000, episode_reward=409.20 +/- 119.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=252000, episode_reward=313.20 +/- 84.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=253000, episode_reward=245.40 +/- 233.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=254000, episode_reward=368.60 +/- 208.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=255000, episode_reward=368.40 +/- 98.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=256000, episode_reward=370.40 +/- 133.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=257000, episode_reward=445.20 +/- 146.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=258000, episode_reward=355.20 +/- 220.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=259000, episode_reward=208.60 +/- 212.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=260000, episode_reward=378.20 +/- 192.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=261000, episode_reward=385.80 +/- 74.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=262000, episode_reward=187.40 +/- 166.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=263000, episode_reward=354.20 +/- 109.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=264000, episode_reward=362.60 +/- 230.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=265000, episode_reward=239.60 +/- 87.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=266000, episode_reward=383.20 +/- 34.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=267000, episode_reward=467.40 +/- 138.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=268000, episode_reward=225.40 +/- 57.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=269000, episode_reward=168.40 +/- 85.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=270000, episode_reward=406.00 +/- 196.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=271000, episode_reward=354.40 +/- 198.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=272000, episode_reward=321.60 +/- 240.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=273000, episode_reward=351.60 +/- 264.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=274000, episode_reward=365.00 +/- 143.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=275000, episode_reward=401.60 +/- 181.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=276000, episode_reward=295.80 +/- 105.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=277000, episode_reward=410.60 +/- 228.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=278000, episode_reward=283.40 +/- 243.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=279000, episode_reward=278.80 +/- 170.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=280000, episode_reward=276.60 +/- 128.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=281000, episode_reward=398.60 +/- 278.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=282000, episode_reward=287.60 +/- 79.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=283000, episode_reward=366.60 +/- 193.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=284000, episode_reward=98.00 +/- 120.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=285000, episode_reward=321.00 +/- 82.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=286000, episode_reward=304.20 +/- 213.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=287000, episode_reward=403.40 +/- 113.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=288000, episode_reward=421.20 +/- 184.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=289000, episode_reward=407.80 +/- 194.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=290000, episode_reward=335.60 +/- 171.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=291000, episode_reward=237.60 +/- 49.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=292000, episode_reward=317.60 +/- 132.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=293000, episode_reward=214.00 +/- 216.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=294000, episode_reward=346.20 +/- 236.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=295000, episode_reward=313.40 +/- 163.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=296000, episode_reward=486.60 +/- 205.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=297000, episode_reward=320.00 +/- 84.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=298000, episode_reward=401.20 +/- 280.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=299000, episode_reward=226.00 +/- 198.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=300000, episode_reward=278.80 +/- 210.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=301000, episode_reward=327.80 +/- 134.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=302000, episode_reward=381.00 +/- 192.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=303000, episode_reward=480.20 +/- 82.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=304000, episode_reward=288.80 +/- 136.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=305000, episode_reward=279.80 +/- 129.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=306000, episode_reward=311.40 +/- 153.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=307000, episode_reward=376.60 +/- 217.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=308000, episode_reward=245.60 +/- 167.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=309000, episode_reward=153.80 +/- 137.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=310000, episode_reward=272.40 +/- 66.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=311000, episode_reward=251.00 +/- 93.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=312000, episode_reward=209.00 +/- 129.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=313000, episode_reward=268.60 +/- 185.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=314000, episode_reward=237.60 +/- 215.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=315000, episode_reward=337.60 +/- 181.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=316000, episode_reward=261.40 +/- 144.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=317000, episode_reward=309.00 +/- 138.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=318000, episode_reward=341.60 +/- 159.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=319000, episode_reward=468.20 +/- 136.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=320000, episode_reward=355.40 +/- 190.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=321000, episode_reward=455.40 +/- 106.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=322000, episode_reward=326.80 +/- 193.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=323000, episode_reward=276.20 +/- 113.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=324000, episode_reward=258.20 +/- 118.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=325000, episode_reward=286.40 +/- 44.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=326000, episode_reward=291.80 +/- 106.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=327000, episode_reward=349.00 +/- 137.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=328000, episode_reward=338.60 +/- 190.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=329000, episode_reward=431.20 +/- 162.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=330000, episode_reward=344.60 +/- 86.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=331000, episode_reward=304.60 +/- 148.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=332000, episode_reward=256.00 +/- 101.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=333000, episode_reward=350.40 +/- 193.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=334000, episode_reward=344.40 +/- 106.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=335000, episode_reward=273.20 +/- 119.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=336000, episode_reward=227.00 +/- 124.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=337000, episode_reward=271.00 +/- 159.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=338000, episode_reward=399.20 +/- 243.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=339000, episode_reward=270.00 +/- 134.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=340000, episode_reward=324.00 +/- 79.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=341000, episode_reward=400.80 +/- 239.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=342000, episode_reward=399.00 +/- 83.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=343000, episode_reward=125.40 +/- 67.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=344000, episode_reward=356.60 +/- 170.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=345000, episode_reward=428.20 +/- 123.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=346000, episode_reward=287.60 +/- 58.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=347000, episode_reward=219.40 +/- 191.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=348000, episode_reward=348.20 +/- 150.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=349000, episode_reward=399.60 +/- 129.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=350000, episode_reward=349.60 +/- 79.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=351000, episode_reward=151.80 +/- 207.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=352000, episode_reward=256.40 +/- 159.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=353000, episode_reward=276.80 +/- 117.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=354000, episode_reward=443.40 +/- 159.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=355000, episode_reward=279.20 +/- 114.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=356000, episode_reward=137.80 +/- 117.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=357000, episode_reward=284.00 +/- 156.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=358000, episode_reward=344.40 +/- 253.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=359000, episode_reward=147.60 +/- 305.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=360000, episode_reward=305.20 +/- 79.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=361000, episode_reward=318.80 +/- 65.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=362000, episode_reward=289.00 +/- 236.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=363000, episode_reward=295.60 +/- 208.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=364000, episode_reward=364.00 +/- 125.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=365000, episode_reward=377.20 +/- 145.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=366000, episode_reward=353.80 +/- 231.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=367000, episode_reward=367.60 +/- 77.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=368000, episode_reward=355.20 +/- 240.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=369000, episode_reward=67.40 +/- 64.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=370000, episode_reward=385.40 +/- 77.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=371000, episode_reward=439.40 +/- 157.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=372000, episode_reward=309.60 +/- 155.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=373000, episode_reward=375.00 +/- 153.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=374000, episode_reward=313.80 +/- 100.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=375000, episode_reward=272.40 +/- 299.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=376000, episode_reward=294.60 +/- 106.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=377000, episode_reward=371.00 +/- 62.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=378000, episode_reward=358.00 +/- 124.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=379000, episode_reward=153.00 +/- 212.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=380000, episode_reward=208.60 +/- 160.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=381000, episode_reward=354.80 +/- 119.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=382000, episode_reward=451.00 +/- 281.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=383000, episode_reward=265.80 +/- 54.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=384000, episode_reward=373.40 +/- 177.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=385000, episode_reward=474.20 +/- 135.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=386000, episode_reward=310.60 +/- 121.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=387000, episode_reward=254.40 +/- 58.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=388000, episode_reward=236.80 +/- 192.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=389000, episode_reward=491.00 +/- 117.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=390000, episode_reward=344.00 +/- 55.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=391000, episode_reward=337.40 +/- 50.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=392000, episode_reward=426.20 +/- 88.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=393000, episode_reward=240.00 +/- 157.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=394000, episode_reward=265.40 +/- 77.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=395000, episode_reward=437.00 +/- 252.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=396000, episode_reward=180.20 +/- 131.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=397000, episode_reward=172.20 +/- 189.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=398000, episode_reward=352.60 +/- 156.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=399000, episode_reward=421.20 +/- 106.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=400000, episode_reward=348.80 +/- 170.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=401000, episode_reward=397.00 +/- 156.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=402000, episode_reward=379.80 +/- 157.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=403000, episode_reward=279.00 +/- 77.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=404000, episode_reward=277.00 +/- 145.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=405000, episode_reward=438.40 +/- 108.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=406000, episode_reward=410.20 +/- 89.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=407000, episode_reward=351.40 +/- 245.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=408000, episode_reward=365.40 +/- 136.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=409000, episode_reward=394.20 +/- 202.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=410000, episode_reward=452.00 +/- 123.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=411000, episode_reward=349.20 +/- 170.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=412000, episode_reward=422.60 +/- 165.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=413000, episode_reward=309.80 +/- 97.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=414000, episode_reward=229.60 +/- 184.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=415000, episode_reward=297.60 +/- 72.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=416000, episode_reward=217.60 +/- 187.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=417000, episode_reward=339.80 +/- 220.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=418000, episode_reward=357.00 +/- 89.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=419000, episode_reward=286.80 +/- 198.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=420000, episode_reward=430.60 +/- 206.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=421000, episode_reward=459.60 +/- 141.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=422000, episode_reward=347.00 +/- 263.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=423000, episode_reward=238.00 +/- 265.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=424000, episode_reward=329.00 +/- 170.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=425000, episode_reward=412.20 +/- 108.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=426000, episode_reward=512.60 +/- 90.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=427000, episode_reward=340.40 +/- 153.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=428000, episode_reward=372.00 +/- 160.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=429000, episode_reward=303.20 +/- 97.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=430000, episode_reward=318.60 +/- 131.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=431000, episode_reward=408.60 +/- 116.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=432000, episode_reward=330.60 +/- 131.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=433000, episode_reward=215.80 +/- 253.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=434000, episode_reward=403.20 +/- 181.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=435000, episode_reward=323.40 +/- 273.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=436000, episode_reward=351.80 +/- 151.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=437000, episode_reward=323.20 +/- 113.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=438000, episode_reward=400.60 +/- 112.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=439000, episode_reward=538.00 +/- 100.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=440000, episode_reward=468.20 +/- 172.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=441000, episode_reward=359.80 +/- 149.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=442000, episode_reward=327.80 +/- 246.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=443000, episode_reward=265.20 +/- 213.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=444000, episode_reward=332.60 +/- 274.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=445000, episode_reward=224.00 +/- 132.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=446000, episode_reward=326.60 +/- 148.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=447000, episode_reward=359.80 +/- 112.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=448000, episode_reward=443.20 +/- 96.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=449000, episode_reward=294.40 +/- 125.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=450000, episode_reward=257.80 +/- 208.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=451000, episode_reward=338.80 +/- 238.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=452000, episode_reward=336.20 +/- 188.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=453000, episode_reward=339.40 +/- 154.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=454000, episode_reward=476.60 +/- 149.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=455000, episode_reward=299.20 +/- 197.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=456000, episode_reward=445.40 +/- 247.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=457000, episode_reward=227.40 +/- 170.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=458000, episode_reward=483.20 +/- 126.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=459000, episode_reward=282.00 +/- 68.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=460000, episode_reward=276.80 +/- 150.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=461000, episode_reward=251.00 +/- 60.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=462000, episode_reward=309.00 +/- 296.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=463000, episode_reward=317.20 +/- 66.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=464000, episode_reward=258.40 +/- 218.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=465000, episode_reward=357.40 +/- 145.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=466000, episode_reward=351.20 +/- 221.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=467000, episode_reward=258.40 +/- 184.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=468000, episode_reward=291.20 +/- 98.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=469000, episode_reward=212.80 +/- 215.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=470000, episode_reward=334.60 +/- 250.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=471000, episode_reward=368.00 +/- 111.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=472000, episode_reward=267.60 +/- 168.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=473000, episode_reward=369.40 +/- 199.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=474000, episode_reward=378.00 +/- 202.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=475000, episode_reward=224.40 +/- 124.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=476000, episode_reward=300.20 +/- 55.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=477000, episode_reward=396.00 +/- 146.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=478000, episode_reward=423.00 +/- 140.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=479000, episode_reward=250.80 +/- 83.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=480000, episode_reward=278.00 +/- 282.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=481000, episode_reward=201.20 +/- 221.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=482000, episode_reward=406.80 +/- 144.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=483000, episode_reward=365.80 +/- 166.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=484000, episode_reward=279.60 +/- 53.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=485000, episode_reward=226.20 +/- 171.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=486000, episode_reward=389.60 +/- 155.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=487000, episode_reward=360.00 +/- 81.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=488000, episode_reward=282.00 +/- 59.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=489000, episode_reward=415.00 +/- 153.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=490000, episode_reward=503.20 +/- 128.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=491000, episode_reward=329.20 +/- 273.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=492000, episode_reward=390.20 +/- 106.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=493000, episode_reward=305.20 +/- 193.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=494000, episode_reward=214.40 +/- 151.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=495000, episode_reward=411.00 +/- 258.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=496000, episode_reward=351.20 +/- 214.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=497000, episode_reward=361.00 +/- 111.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=498000, episode_reward=447.20 +/- 115.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=499000, episode_reward=345.40 +/- 121.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=500000, episode_reward=254.20 +/- 287.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=501000, episode_reward=387.20 +/- 120.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Gamma = 0.9, Learning Rate = 0.003, Activation Function = Tanh, Neurons = 8, Process noise = 0.1, Measurement noise = 0.0\n",
      "Eval num_timesteps=1000, episode_reward=-2823.80 +/- 328.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=-2645.60 +/- 406.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3000, episode_reward=-1059.20 +/- 87.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4000, episode_reward=-831.20 +/- 325.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=5000, episode_reward=445.20 +/- 68.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=6000, episode_reward=470.00 +/- 127.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=7000, episode_reward=420.00 +/- 134.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=8000, episode_reward=246.40 +/- 298.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=9000, episode_reward=375.00 +/- 319.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=10000, episode_reward=262.40 +/- 195.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=11000, episode_reward=308.80 +/- 125.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=12000, episode_reward=399.00 +/- 202.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=13000, episode_reward=696.00 +/- 141.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=14000, episode_reward=553.60 +/- 158.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=15000, episode_reward=740.40 +/- 251.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=16000, episode_reward=254.40 +/- 252.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=17000, episode_reward=327.40 +/- 276.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=18000, episode_reward=428.20 +/- 115.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=19000, episode_reward=387.20 +/- 183.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=20000, episode_reward=307.40 +/- 147.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=21000, episode_reward=354.40 +/- 358.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=22000, episode_reward=678.20 +/- 233.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=23000, episode_reward=1349.40 +/- 277.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=24000, episode_reward=1447.00 +/- 180.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=25000, episode_reward=1383.80 +/- 257.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=26000, episode_reward=1320.60 +/- 201.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=27000, episode_reward=-2446.40 +/- 6899.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=28000, episode_reward=1100.60 +/- 297.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=29000, episode_reward=-1660.20 +/- 6074.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=30000, episode_reward=487.00 +/- 1939.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=31000, episode_reward=1575.40 +/- 86.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=32000, episode_reward=1353.80 +/- 221.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=33000, episode_reward=1304.00 +/- 211.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=34000, episode_reward=1495.00 +/- 133.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=35000, episode_reward=1341.20 +/- 252.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=36000, episode_reward=1420.00 +/- 217.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=37000, episode_reward=1248.40 +/- 309.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=38000, episode_reward=1391.60 +/- 208.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=39000, episode_reward=1426.00 +/- 177.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=40000, episode_reward=1508.00 +/- 122.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=41000, episode_reward=1441.40 +/- 256.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=42000, episode_reward=1158.60 +/- 417.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=43000, episode_reward=1454.80 +/- 269.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=44000, episode_reward=1350.20 +/- 291.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=45000, episode_reward=1372.20 +/- 368.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=46000, episode_reward=1373.40 +/- 179.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=47000, episode_reward=1353.40 +/- 188.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=48000, episode_reward=1260.60 +/- 280.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=49000, episode_reward=1494.00 +/- 244.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=50000, episode_reward=1376.40 +/- 217.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=51000, episode_reward=1405.20 +/- 200.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=52000, episode_reward=1472.40 +/- 221.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=53000, episode_reward=1472.60 +/- 161.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=54000, episode_reward=1496.80 +/- 156.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=55000, episode_reward=1467.00 +/- 229.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=56000, episode_reward=1334.00 +/- 342.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=57000, episode_reward=1548.00 +/- 117.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=58000, episode_reward=1560.80 +/- 220.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=59000, episode_reward=1523.40 +/- 178.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=60000, episode_reward=1416.80 +/- 314.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=61000, episode_reward=1446.60 +/- 285.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=62000, episode_reward=1452.20 +/- 74.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=63000, episode_reward=1551.60 +/- 276.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=64000, episode_reward=1476.80 +/- 217.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=65000, episode_reward=1527.40 +/- 224.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=66000, episode_reward=1734.00 +/- 94.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=67000, episode_reward=1275.00 +/- 379.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=68000, episode_reward=1584.80 +/- 241.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=69000, episode_reward=1627.60 +/- 239.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=70000, episode_reward=1497.80 +/- 230.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=71000, episode_reward=1494.80 +/- 209.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=72000, episode_reward=1594.80 +/- 244.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=73000, episode_reward=1454.40 +/- 272.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=74000, episode_reward=1603.20 +/- 258.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=75000, episode_reward=1562.40 +/- 179.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=76000, episode_reward=1556.00 +/- 225.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=77000, episode_reward=1542.20 +/- 299.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=78000, episode_reward=1676.00 +/- 131.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=79000, episode_reward=1524.20 +/- 216.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=80000, episode_reward=1610.60 +/- 57.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=81000, episode_reward=1625.40 +/- 136.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=82000, episode_reward=1637.20 +/- 235.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=83000, episode_reward=1641.20 +/- 225.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=84000, episode_reward=1530.60 +/- 273.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=85000, episode_reward=1656.60 +/- 182.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=86000, episode_reward=1492.80 +/- 235.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=87000, episode_reward=1628.80 +/- 116.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=88000, episode_reward=1630.40 +/- 161.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=89000, episode_reward=1713.80 +/- 130.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=90000, episode_reward=1742.20 +/- 86.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=91000, episode_reward=1504.60 +/- 241.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=92000, episode_reward=1571.60 +/- 139.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=93000, episode_reward=1384.60 +/- 251.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=94000, episode_reward=1512.20 +/- 142.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=95000, episode_reward=1631.80 +/- 323.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=96000, episode_reward=1478.80 +/- 325.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=97000, episode_reward=1512.80 +/- 241.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=98000, episode_reward=1465.40 +/- 369.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=99000, episode_reward=1728.40 +/- 192.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=100000, episode_reward=1648.40 +/- 273.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=101000, episode_reward=1607.80 +/- 286.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=102000, episode_reward=1686.80 +/- 256.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=103000, episode_reward=1579.20 +/- 322.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=104000, episode_reward=1730.40 +/- 205.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=105000, episode_reward=1729.00 +/- 176.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=106000, episode_reward=1452.20 +/- 62.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=107000, episode_reward=1514.00 +/- 476.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=108000, episode_reward=1527.60 +/- 246.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=109000, episode_reward=1503.40 +/- 256.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=110000, episode_reward=1604.20 +/- 173.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=111000, episode_reward=1511.40 +/- 252.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=112000, episode_reward=1465.60 +/- 294.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=113000, episode_reward=1734.80 +/- 103.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=114000, episode_reward=1624.00 +/- 248.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=115000, episode_reward=1504.00 +/- 235.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=116000, episode_reward=1489.80 +/- 208.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=117000, episode_reward=1349.00 +/- 138.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=118000, episode_reward=1678.40 +/- 134.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=119000, episode_reward=1450.60 +/- 233.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=120000, episode_reward=1586.40 +/- 203.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=121000, episode_reward=1456.20 +/- 338.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=122000, episode_reward=1523.40 +/- 207.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=123000, episode_reward=1502.20 +/- 456.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=124000, episode_reward=1553.20 +/- 186.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=125000, episode_reward=1511.80 +/- 254.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=126000, episode_reward=1616.60 +/- 205.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=127000, episode_reward=1476.80 +/- 244.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=128000, episode_reward=1407.00 +/- 156.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=129000, episode_reward=1768.60 +/- 105.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=130000, episode_reward=1667.40 +/- 143.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=131000, episode_reward=1826.40 +/- 175.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=132000, episode_reward=1640.60 +/- 221.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=133000, episode_reward=1780.00 +/- 77.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=134000, episode_reward=1610.20 +/- 195.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=135000, episode_reward=1706.60 +/- 199.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=136000, episode_reward=1602.40 +/- 201.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=137000, episode_reward=1529.00 +/- 114.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=138000, episode_reward=1428.80 +/- 223.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=139000, episode_reward=1702.40 +/- 88.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=140000, episode_reward=1617.20 +/- 295.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=141000, episode_reward=1591.40 +/- 113.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=142000, episode_reward=1521.40 +/- 345.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=143000, episode_reward=1541.00 +/- 372.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=144000, episode_reward=1725.00 +/- 241.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=145000, episode_reward=1500.80 +/- 389.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=146000, episode_reward=1540.80 +/- 184.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=147000, episode_reward=1639.40 +/- 269.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=148000, episode_reward=1619.80 +/- 200.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=149000, episode_reward=1574.00 +/- 212.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=150000, episode_reward=1645.80 +/- 404.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=151000, episode_reward=1600.20 +/- 177.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=152000, episode_reward=1604.40 +/- 276.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=153000, episode_reward=1794.40 +/- 200.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=154000, episode_reward=1699.60 +/- 335.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=155000, episode_reward=1631.40 +/- 308.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=156000, episode_reward=1541.40 +/- 190.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=157000, episode_reward=1516.60 +/- 258.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=158000, episode_reward=1552.80 +/- 213.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=159000, episode_reward=1652.40 +/- 371.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=160000, episode_reward=1642.40 +/- 240.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=161000, episode_reward=1482.20 +/- 257.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=162000, episode_reward=1348.80 +/- 284.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=163000, episode_reward=1799.20 +/- 346.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=164000, episode_reward=1486.00 +/- 202.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=165000, episode_reward=1429.40 +/- 196.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=166000, episode_reward=1301.40 +/- 338.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=167000, episode_reward=1565.80 +/- 232.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=168000, episode_reward=1538.80 +/- 434.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=169000, episode_reward=1448.80 +/- 137.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=170000, episode_reward=1474.00 +/- 186.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=171000, episode_reward=1644.00 +/- 320.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=172000, episode_reward=1534.40 +/- 385.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=173000, episode_reward=1708.20 +/- 168.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=174000, episode_reward=1618.80 +/- 235.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=175000, episode_reward=1672.80 +/- 161.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=176000, episode_reward=1766.40 +/- 204.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=177000, episode_reward=1849.40 +/- 147.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=178000, episode_reward=1757.00 +/- 226.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=179000, episode_reward=1934.20 +/- 150.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=180000, episode_reward=1610.40 +/- 161.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=181000, episode_reward=1545.60 +/- 350.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=182000, episode_reward=1427.20 +/- 135.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=183000, episode_reward=1465.00 +/- 470.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=184000, episode_reward=1516.40 +/- 243.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=185000, episode_reward=1640.80 +/- 168.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=186000, episode_reward=1686.20 +/- 232.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=187000, episode_reward=1582.00 +/- 342.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=188000, episode_reward=1750.80 +/- 106.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=189000, episode_reward=1709.20 +/- 210.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=190000, episode_reward=1774.60 +/- 241.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=191000, episode_reward=1744.20 +/- 235.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=192000, episode_reward=1778.20 +/- 220.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=193000, episode_reward=1599.80 +/- 312.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=194000, episode_reward=1435.20 +/- 274.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=195000, episode_reward=1314.80 +/- 370.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=196000, episode_reward=1547.20 +/- 338.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=197000, episode_reward=1730.00 +/- 165.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=198000, episode_reward=1697.60 +/- 227.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=199000, episode_reward=1587.40 +/- 181.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=200000, episode_reward=1604.60 +/- 191.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=201000, episode_reward=1656.60 +/- 325.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=202000, episode_reward=1534.00 +/- 185.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=203000, episode_reward=1719.20 +/- 164.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=204000, episode_reward=1614.20 +/- 142.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=205000, episode_reward=1717.00 +/- 100.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=206000, episode_reward=1436.20 +/- 275.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=207000, episode_reward=1638.60 +/- 199.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=208000, episode_reward=1629.40 +/- 232.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=209000, episode_reward=1823.80 +/- 28.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=210000, episode_reward=1551.00 +/- 246.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=211000, episode_reward=1678.80 +/- 54.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=212000, episode_reward=1671.00 +/- 174.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=213000, episode_reward=1772.40 +/- 314.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=214000, episode_reward=1641.40 +/- 309.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=215000, episode_reward=1621.80 +/- 73.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=216000, episode_reward=1733.20 +/- 213.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=217000, episode_reward=1656.00 +/- 366.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=218000, episode_reward=1791.60 +/- 217.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=219000, episode_reward=1352.20 +/- 269.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=220000, episode_reward=1594.40 +/- 207.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=221000, episode_reward=1757.60 +/- 200.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=222000, episode_reward=1594.40 +/- 218.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=223000, episode_reward=1780.60 +/- 192.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=224000, episode_reward=1467.20 +/- 306.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=225000, episode_reward=1536.00 +/- 319.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=226000, episode_reward=1576.40 +/- 271.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=227000, episode_reward=1801.00 +/- 109.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=228000, episode_reward=1649.20 +/- 298.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=229000, episode_reward=1593.20 +/- 287.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=230000, episode_reward=1765.20 +/- 145.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=231000, episode_reward=1662.60 +/- 257.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=232000, episode_reward=1632.80 +/- 206.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=233000, episode_reward=1294.80 +/- 312.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=234000, episode_reward=1573.00 +/- 105.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=235000, episode_reward=1794.00 +/- 91.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=236000, episode_reward=1547.40 +/- 387.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=237000, episode_reward=1491.60 +/- 91.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=238000, episode_reward=1513.00 +/- 424.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=239000, episode_reward=1241.00 +/- 187.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=240000, episode_reward=1659.60 +/- 311.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=241000, episode_reward=1837.40 +/- 135.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=242000, episode_reward=1651.40 +/- 181.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=243000, episode_reward=1607.00 +/- 271.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=244000, episode_reward=1684.40 +/- 135.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=245000, episode_reward=1671.00 +/- 99.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=246000, episode_reward=1627.60 +/- 295.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=247000, episode_reward=1632.60 +/- 253.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=248000, episode_reward=1743.40 +/- 203.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=249000, episode_reward=1629.40 +/- 159.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=250000, episode_reward=1744.40 +/- 192.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=251000, episode_reward=1556.00 +/- 138.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=252000, episode_reward=1576.60 +/- 185.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=253000, episode_reward=1372.40 +/- 125.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=254000, episode_reward=1547.40 +/- 144.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=255000, episode_reward=1784.60 +/- 240.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=256000, episode_reward=1676.40 +/- 167.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=257000, episode_reward=1813.00 +/- 150.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=258000, episode_reward=1269.80 +/- 176.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=259000, episode_reward=1617.00 +/- 284.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=260000, episode_reward=1765.20 +/- 409.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=261000, episode_reward=1836.20 +/- 319.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=262000, episode_reward=1722.60 +/- 132.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=263000, episode_reward=1674.00 +/- 227.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=264000, episode_reward=1648.60 +/- 259.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=265000, episode_reward=1611.20 +/- 264.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=266000, episode_reward=1441.00 +/- 230.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=267000, episode_reward=1560.00 +/- 297.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=268000, episode_reward=1761.20 +/- 257.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=269000, episode_reward=1531.00 +/- 335.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=270000, episode_reward=1668.00 +/- 102.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=271000, episode_reward=1390.00 +/- 401.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=272000, episode_reward=1836.00 +/- 179.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=273000, episode_reward=1600.00 +/- 187.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=274000, episode_reward=1801.20 +/- 258.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=275000, episode_reward=1876.60 +/- 126.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=276000, episode_reward=1715.40 +/- 170.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=277000, episode_reward=1769.80 +/- 249.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=278000, episode_reward=1592.60 +/- 208.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=279000, episode_reward=1763.40 +/- 112.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=280000, episode_reward=1649.80 +/- 193.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=281000, episode_reward=1842.60 +/- 213.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=282000, episode_reward=1473.20 +/- 354.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=283000, episode_reward=1439.40 +/- 196.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=284000, episode_reward=1516.80 +/- 312.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=285000, episode_reward=1702.60 +/- 288.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=286000, episode_reward=1765.60 +/- 151.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=287000, episode_reward=1654.20 +/- 283.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=288000, episode_reward=1305.00 +/- 275.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=289000, episode_reward=1727.20 +/- 288.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=290000, episode_reward=1829.80 +/- 91.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=291000, episode_reward=1713.00 +/- 162.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=292000, episode_reward=1775.20 +/- 224.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=293000, episode_reward=1561.40 +/- 196.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=294000, episode_reward=1549.40 +/- 193.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=295000, episode_reward=1486.60 +/- 357.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=296000, episode_reward=1825.20 +/- 156.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=297000, episode_reward=1699.80 +/- 227.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=298000, episode_reward=1540.60 +/- 334.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=299000, episode_reward=1566.40 +/- 244.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=300000, episode_reward=1709.80 +/- 272.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=301000, episode_reward=1590.20 +/- 195.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=302000, episode_reward=1657.20 +/- 120.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=303000, episode_reward=1668.60 +/- 220.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=304000, episode_reward=1562.00 +/- 221.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=305000, episode_reward=1603.60 +/- 341.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=306000, episode_reward=1570.60 +/- 191.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=307000, episode_reward=1595.40 +/- 213.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=308000, episode_reward=1633.40 +/- 423.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=309000, episode_reward=1683.00 +/- 194.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=310000, episode_reward=1419.80 +/- 144.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=311000, episode_reward=1598.40 +/- 141.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=312000, episode_reward=1677.60 +/- 287.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=313000, episode_reward=1655.00 +/- 160.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=314000, episode_reward=1615.20 +/- 281.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=315000, episode_reward=1612.00 +/- 213.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=316000, episode_reward=1728.20 +/- 249.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=317000, episode_reward=1505.60 +/- 266.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=318000, episode_reward=1749.20 +/- 95.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=319000, episode_reward=1671.40 +/- 203.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=320000, episode_reward=1755.80 +/- 127.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=321000, episode_reward=1604.80 +/- 308.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=322000, episode_reward=1816.40 +/- 214.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=323000, episode_reward=1611.20 +/- 255.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=324000, episode_reward=1570.20 +/- 180.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=325000, episode_reward=1530.60 +/- 139.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=326000, episode_reward=1749.00 +/- 136.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=327000, episode_reward=1562.00 +/- 193.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=328000, episode_reward=1544.00 +/- 290.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=329000, episode_reward=1694.60 +/- 169.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=330000, episode_reward=1835.40 +/- 147.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=331000, episode_reward=1534.60 +/- 200.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=332000, episode_reward=1826.40 +/- 138.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=333000, episode_reward=1692.80 +/- 223.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=334000, episode_reward=1757.00 +/- 252.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=335000, episode_reward=1686.00 +/- 112.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=336000, episode_reward=1731.40 +/- 203.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=337000, episode_reward=1674.40 +/- 134.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=338000, episode_reward=1625.80 +/- 244.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=339000, episode_reward=1723.00 +/- 102.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=340000, episode_reward=1492.60 +/- 151.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=341000, episode_reward=1714.80 +/- 208.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=342000, episode_reward=1697.20 +/- 108.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=343000, episode_reward=1667.60 +/- 159.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=344000, episode_reward=1688.20 +/- 186.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=345000, episode_reward=1721.80 +/- 150.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=346000, episode_reward=1490.00 +/- 198.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=347000, episode_reward=1529.00 +/- 255.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=348000, episode_reward=1528.40 +/- 227.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=349000, episode_reward=1617.60 +/- 95.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=350000, episode_reward=1531.40 +/- 186.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=351000, episode_reward=1532.20 +/- 255.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=352000, episode_reward=1683.40 +/- 174.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=353000, episode_reward=1667.60 +/- 127.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=354000, episode_reward=1789.20 +/- 147.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=355000, episode_reward=1740.40 +/- 341.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=356000, episode_reward=1510.20 +/- 371.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=357000, episode_reward=1711.80 +/- 115.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=358000, episode_reward=1642.80 +/- 215.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=359000, episode_reward=1599.20 +/- 145.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=360000, episode_reward=1642.40 +/- 264.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=361000, episode_reward=1680.60 +/- 181.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=362000, episode_reward=1713.20 +/- 145.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=363000, episode_reward=1617.00 +/- 334.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=364000, episode_reward=1833.20 +/- 84.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=365000, episode_reward=1543.80 +/- 294.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=366000, episode_reward=1530.60 +/- 178.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=367000, episode_reward=1661.00 +/- 91.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=368000, episode_reward=1548.80 +/- 113.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=369000, episode_reward=1549.40 +/- 225.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=370000, episode_reward=1664.80 +/- 137.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=371000, episode_reward=1779.00 +/- 246.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=372000, episode_reward=1799.00 +/- 180.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=373000, episode_reward=1787.40 +/- 148.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=374000, episode_reward=1637.80 +/- 263.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=375000, episode_reward=1766.80 +/- 207.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=376000, episode_reward=1463.00 +/- 309.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=377000, episode_reward=1799.00 +/- 247.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=378000, episode_reward=1660.00 +/- 413.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=379000, episode_reward=1719.60 +/- 271.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=380000, episode_reward=1518.80 +/- 251.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=381000, episode_reward=1703.60 +/- 177.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=382000, episode_reward=1672.60 +/- 145.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=383000, episode_reward=1770.80 +/- 150.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=384000, episode_reward=1733.00 +/- 138.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=385000, episode_reward=1760.80 +/- 187.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=386000, episode_reward=1574.60 +/- 353.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=387000, episode_reward=1594.20 +/- 73.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=388000, episode_reward=1691.80 +/- 309.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=389000, episode_reward=1727.20 +/- 116.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=390000, episode_reward=1750.80 +/- 90.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=391000, episode_reward=1742.60 +/- 133.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=392000, episode_reward=1662.60 +/- 229.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=393000, episode_reward=1807.00 +/- 211.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=394000, episode_reward=1749.20 +/- 245.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=395000, episode_reward=1878.60 +/- 188.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=396000, episode_reward=1566.20 +/- 294.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=397000, episode_reward=1777.80 +/- 112.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=398000, episode_reward=1707.40 +/- 158.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=399000, episode_reward=1657.60 +/- 163.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=400000, episode_reward=1678.40 +/- 110.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=401000, episode_reward=1516.00 +/- 157.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=402000, episode_reward=1621.60 +/- 261.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=403000, episode_reward=1658.80 +/- 193.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=404000, episode_reward=1352.60 +/- 255.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=405000, episode_reward=1696.80 +/- 182.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=406000, episode_reward=1858.40 +/- 219.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=407000, episode_reward=1351.40 +/- 212.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=408000, episode_reward=1570.20 +/- 366.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=409000, episode_reward=1621.60 +/- 285.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=410000, episode_reward=1774.40 +/- 171.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=411000, episode_reward=1624.20 +/- 137.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=412000, episode_reward=1418.20 +/- 334.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=413000, episode_reward=1670.40 +/- 81.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=414000, episode_reward=1630.00 +/- 317.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=415000, episode_reward=1593.60 +/- 350.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=416000, episode_reward=1653.40 +/- 216.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=417000, episode_reward=1738.00 +/- 125.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=418000, episode_reward=1555.40 +/- 209.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=419000, episode_reward=1719.00 +/- 98.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=420000, episode_reward=1665.40 +/- 143.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=421000, episode_reward=1743.00 +/- 215.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=422000, episode_reward=1751.40 +/- 130.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=423000, episode_reward=1706.40 +/- 222.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=424000, episode_reward=1733.20 +/- 194.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=425000, episode_reward=1605.00 +/- 90.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=426000, episode_reward=1557.80 +/- 238.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=427000, episode_reward=1504.80 +/- 207.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=428000, episode_reward=1602.40 +/- 291.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=429000, episode_reward=1637.00 +/- 336.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=430000, episode_reward=1740.40 +/- 236.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=431000, episode_reward=1734.20 +/- 188.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=432000, episode_reward=1698.20 +/- 254.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=433000, episode_reward=1757.60 +/- 297.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=434000, episode_reward=1839.40 +/- 141.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=435000, episode_reward=1778.20 +/- 152.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=436000, episode_reward=1705.60 +/- 273.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=437000, episode_reward=1562.00 +/- 163.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=438000, episode_reward=1615.20 +/- 119.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=439000, episode_reward=1667.60 +/- 161.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=440000, episode_reward=1282.80 +/- 333.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=441000, episode_reward=1710.40 +/- 152.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=442000, episode_reward=1736.60 +/- 212.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=443000, episode_reward=1536.80 +/- 230.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=444000, episode_reward=1680.60 +/- 221.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=445000, episode_reward=1787.40 +/- 226.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=446000, episode_reward=1722.00 +/- 184.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=447000, episode_reward=1620.40 +/- 80.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=448000, episode_reward=1686.60 +/- 193.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=449000, episode_reward=1663.00 +/- 87.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=450000, episode_reward=1631.80 +/- 274.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=451000, episode_reward=1669.00 +/- 165.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=452000, episode_reward=1771.60 +/- 130.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=453000, episode_reward=1689.60 +/- 195.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=454000, episode_reward=1580.60 +/- 140.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=455000, episode_reward=1551.40 +/- 182.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=456000, episode_reward=1692.60 +/- 205.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=457000, episode_reward=1784.60 +/- 109.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=458000, episode_reward=1668.40 +/- 421.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=459000, episode_reward=1563.20 +/- 305.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=460000, episode_reward=1944.80 +/- 119.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=461000, episode_reward=1569.00 +/- 179.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=462000, episode_reward=1803.00 +/- 60.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=463000, episode_reward=1737.40 +/- 315.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=464000, episode_reward=1751.20 +/- 124.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=465000, episode_reward=1660.00 +/- 181.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=466000, episode_reward=1607.80 +/- 262.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=467000, episode_reward=1708.40 +/- 111.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=468000, episode_reward=1883.60 +/- 128.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=469000, episode_reward=1741.20 +/- 240.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=470000, episode_reward=1722.40 +/- 132.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=471000, episode_reward=1530.20 +/- 273.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=472000, episode_reward=1794.20 +/- 243.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=473000, episode_reward=1493.40 +/- 338.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=474000, episode_reward=1785.60 +/- 247.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=475000, episode_reward=1555.40 +/- 272.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=476000, episode_reward=1786.60 +/- 199.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=477000, episode_reward=1572.80 +/- 275.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=478000, episode_reward=1519.40 +/- 242.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=479000, episode_reward=1610.80 +/- 193.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=480000, episode_reward=1705.80 +/- 137.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=481000, episode_reward=1686.60 +/- 239.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=482000, episode_reward=1749.60 +/- 161.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=483000, episode_reward=1679.00 +/- 135.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=484000, episode_reward=1815.00 +/- 203.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=485000, episode_reward=1601.40 +/- 272.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=486000, episode_reward=1542.40 +/- 173.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=487000, episode_reward=1714.60 +/- 273.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=488000, episode_reward=1722.00 +/- 212.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=489000, episode_reward=1617.20 +/- 171.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=490000, episode_reward=1653.20 +/- 221.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=491000, episode_reward=1658.60 +/- 147.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=492000, episode_reward=1664.20 +/- 355.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=493000, episode_reward=1667.60 +/- 165.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=494000, episode_reward=1728.20 +/- 86.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=495000, episode_reward=1676.80 +/- 225.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=496000, episode_reward=1895.60 +/- 181.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=497000, episode_reward=1744.40 +/- 137.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=498000, episode_reward=1792.40 +/- 257.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=499000, episode_reward=1654.00 +/- 156.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=500000, episode_reward=1740.80 +/- 205.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=501000, episode_reward=1789.60 +/- 235.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Gamma = 0.9, Learning Rate = 0.003, Activation Function = Tanh, Neurons = 32, Process noise = 0.1, Measurement noise = 0.0\n",
      "Eval num_timesteps=1000, episode_reward=92.20 +/- 74.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=16.40 +/- 140.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=3000, episode_reward=-252.40 +/- 166.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=4000, episode_reward=-303.40 +/- 157.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=5000, episode_reward=800.60 +/- 81.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=6000, episode_reward=698.60 +/- 268.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=7000, episode_reward=314.60 +/- 170.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=8000, episode_reward=444.60 +/- 235.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=9000, episode_reward=109.00 +/- 201.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=10000, episode_reward=219.60 +/- 279.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=11000, episode_reward=342.00 +/- 109.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=12000, episode_reward=298.80 +/- 225.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=13000, episode_reward=244.80 +/- 129.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=14000, episode_reward=355.80 +/- 211.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=15000, episode_reward=374.00 +/- 171.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=16000, episode_reward=233.60 +/- 229.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=17000, episode_reward=314.40 +/- 199.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=18000, episode_reward=326.20 +/- 243.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=19000, episode_reward=900.60 +/- 392.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=20000, episode_reward=959.20 +/- 156.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=21000, episode_reward=1170.60 +/- 131.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=22000, episode_reward=966.60 +/- 471.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=23000, episode_reward=1131.00 +/- 161.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=24000, episode_reward=1211.60 +/- 351.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=25000, episode_reward=1389.20 +/- 179.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=26000, episode_reward=1165.60 +/- 243.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=27000, episode_reward=1219.60 +/- 281.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=28000, episode_reward=1108.80 +/- 314.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=29000, episode_reward=1420.00 +/- 118.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=30000, episode_reward=523.00 +/- 1583.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=31000, episode_reward=1416.20 +/- 186.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=32000, episode_reward=1339.20 +/- 257.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=33000, episode_reward=280.80 +/- 2410.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=34000, episode_reward=1628.60 +/- 135.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=35000, episode_reward=1453.60 +/- 262.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=36000, episode_reward=1429.80 +/- 180.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=37000, episode_reward=1540.80 +/- 283.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=38000, episode_reward=1583.00 +/- 241.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=39000, episode_reward=1504.20 +/- 207.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=40000, episode_reward=1534.20 +/- 250.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=41000, episode_reward=1517.60 +/- 263.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=42000, episode_reward=1632.00 +/- 353.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=43000, episode_reward=1658.20 +/- 205.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=44000, episode_reward=1427.60 +/- 229.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=45000, episode_reward=1507.40 +/- 150.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=46000, episode_reward=1616.60 +/- 301.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=47000, episode_reward=1739.60 +/- 266.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=48000, episode_reward=1428.00 +/- 129.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=49000, episode_reward=1533.40 +/- 241.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=50000, episode_reward=1553.60 +/- 303.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=51000, episode_reward=1593.60 +/- 209.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=52000, episode_reward=1639.20 +/- 236.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=53000, episode_reward=1824.20 +/- 225.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=54000, episode_reward=1590.00 +/- 275.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=55000, episode_reward=1552.60 +/- 199.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=56000, episode_reward=1705.00 +/- 281.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=57000, episode_reward=1774.80 +/- 220.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=58000, episode_reward=1591.60 +/- 232.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=59000, episode_reward=1773.00 +/- 112.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=60000, episode_reward=1813.40 +/- 143.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=61000, episode_reward=1525.00 +/- 249.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=62000, episode_reward=1518.20 +/- 298.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=63000, episode_reward=1676.40 +/- 222.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=64000, episode_reward=1543.60 +/- 196.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=65000, episode_reward=1768.60 +/- 298.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=66000, episode_reward=1918.80 +/- 110.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=67000, episode_reward=1387.60 +/- 289.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=68000, episode_reward=1528.60 +/- 156.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=69000, episode_reward=1264.80 +/- 215.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=70000, episode_reward=1833.80 +/- 151.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=71000, episode_reward=1696.40 +/- 102.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=72000, episode_reward=1687.80 +/- 173.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=73000, episode_reward=1342.00 +/- 155.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=74000, episode_reward=1688.00 +/- 292.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=75000, episode_reward=1730.60 +/- 209.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=76000, episode_reward=1654.20 +/- 262.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=77000, episode_reward=1416.20 +/- 155.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=78000, episode_reward=1732.60 +/- 113.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=79000, episode_reward=1801.40 +/- 144.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=80000, episode_reward=1741.40 +/- 264.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=81000, episode_reward=1496.00 +/- 182.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=82000, episode_reward=1553.00 +/- 60.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=83000, episode_reward=1560.40 +/- 316.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=84000, episode_reward=1772.60 +/- 204.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=85000, episode_reward=1676.40 +/- 139.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=86000, episode_reward=1685.20 +/- 107.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=87000, episode_reward=1449.60 +/- 276.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=88000, episode_reward=1452.60 +/- 232.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=89000, episode_reward=1904.20 +/- 131.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=90000, episode_reward=1578.00 +/- 370.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=91000, episode_reward=1622.40 +/- 206.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=92000, episode_reward=1622.40 +/- 115.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=93000, episode_reward=1655.60 +/- 156.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=94000, episode_reward=1762.60 +/- 208.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=95000, episode_reward=1729.80 +/- 157.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=96000, episode_reward=1704.40 +/- 69.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=97000, episode_reward=1450.00 +/- 231.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=98000, episode_reward=1746.80 +/- 141.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=99000, episode_reward=1709.20 +/- 149.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=100000, episode_reward=1566.40 +/- 208.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=101000, episode_reward=1535.80 +/- 324.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=102000, episode_reward=1699.80 +/- 227.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=103000, episode_reward=1678.60 +/- 277.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=104000, episode_reward=1601.80 +/- 192.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=105000, episode_reward=1769.00 +/- 168.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=106000, episode_reward=1716.60 +/- 215.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=107000, episode_reward=1722.60 +/- 204.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=108000, episode_reward=1423.80 +/- 78.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=109000, episode_reward=1423.40 +/- 233.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=110000, episode_reward=1650.60 +/- 196.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=111000, episode_reward=1647.20 +/- 277.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=112000, episode_reward=1522.40 +/- 257.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=113000, episode_reward=1540.20 +/- 110.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=114000, episode_reward=1373.00 +/- 249.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=115000, episode_reward=1692.20 +/- 240.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=116000, episode_reward=1635.60 +/- 134.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=117000, episode_reward=1376.40 +/- 270.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=118000, episode_reward=1502.60 +/- 150.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=119000, episode_reward=1564.80 +/- 233.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=120000, episode_reward=1605.20 +/- 418.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=121000, episode_reward=1377.40 +/- 280.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=122000, episode_reward=1298.20 +/- 297.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=123000, episode_reward=1600.00 +/- 188.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=124000, episode_reward=1461.60 +/- 264.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=125000, episode_reward=1628.40 +/- 228.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=126000, episode_reward=1513.80 +/- 240.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=127000, episode_reward=1483.80 +/- 271.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=128000, episode_reward=1363.00 +/- 237.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=129000, episode_reward=1507.40 +/- 190.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=130000, episode_reward=1604.40 +/- 367.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=131000, episode_reward=1432.80 +/- 194.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=132000, episode_reward=1499.20 +/- 277.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=133000, episode_reward=1591.20 +/- 211.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=134000, episode_reward=1501.60 +/- 270.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=135000, episode_reward=1631.40 +/- 258.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=136000, episode_reward=1487.60 +/- 174.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=137000, episode_reward=1504.40 +/- 150.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=138000, episode_reward=1652.80 +/- 327.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=139000, episode_reward=1644.60 +/- 227.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=140000, episode_reward=1681.60 +/- 130.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=141000, episode_reward=1557.80 +/- 209.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=142000, episode_reward=1715.80 +/- 212.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=143000, episode_reward=1620.80 +/- 223.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=144000, episode_reward=1645.40 +/- 281.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=145000, episode_reward=1664.00 +/- 190.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=146000, episode_reward=1452.00 +/- 134.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=147000, episode_reward=1395.60 +/- 305.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=148000, episode_reward=1712.80 +/- 275.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=149000, episode_reward=1656.20 +/- 215.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=150000, episode_reward=1694.40 +/- 104.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=151000, episode_reward=1734.20 +/- 175.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=152000, episode_reward=1455.60 +/- 193.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=153000, episode_reward=1664.80 +/- 213.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=154000, episode_reward=1498.60 +/- 336.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=155000, episode_reward=1341.00 +/- 277.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=156000, episode_reward=1734.80 +/- 190.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=157000, episode_reward=1718.20 +/- 236.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=158000, episode_reward=1714.20 +/- 306.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=159000, episode_reward=1630.00 +/- 323.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=160000, episode_reward=1596.40 +/- 227.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=161000, episode_reward=1690.80 +/- 113.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=162000, episode_reward=1625.00 +/- 102.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=163000, episode_reward=1737.00 +/- 127.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=164000, episode_reward=1693.00 +/- 232.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=165000, episode_reward=1573.60 +/- 325.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=166000, episode_reward=1659.20 +/- 125.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=167000, episode_reward=1652.60 +/- 366.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=168000, episode_reward=1600.80 +/- 133.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=169000, episode_reward=1702.20 +/- 167.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=170000, episode_reward=1842.00 +/- 153.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=171000, episode_reward=1578.20 +/- 228.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=172000, episode_reward=1751.60 +/- 171.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=173000, episode_reward=1686.20 +/- 107.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=174000, episode_reward=1566.20 +/- 116.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=175000, episode_reward=1612.60 +/- 219.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=176000, episode_reward=1676.20 +/- 168.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=177000, episode_reward=1536.60 +/- 292.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=178000, episode_reward=1497.00 +/- 193.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=179000, episode_reward=1506.20 +/- 79.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=180000, episode_reward=1512.80 +/- 179.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=181000, episode_reward=1543.00 +/- 143.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=182000, episode_reward=1648.20 +/- 169.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=183000, episode_reward=1595.60 +/- 126.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=184000, episode_reward=1601.60 +/- 234.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=185000, episode_reward=1635.80 +/- 229.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=186000, episode_reward=1752.80 +/- 195.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=187000, episode_reward=1582.80 +/- 200.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=188000, episode_reward=1689.20 +/- 142.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=189000, episode_reward=1589.00 +/- 209.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=190000, episode_reward=1619.40 +/- 111.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=191000, episode_reward=1696.40 +/- 114.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=192000, episode_reward=1622.40 +/- 166.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=193000, episode_reward=1534.60 +/- 254.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=194000, episode_reward=1710.60 +/- 114.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=195000, episode_reward=1602.60 +/- 161.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=196000, episode_reward=1589.60 +/- 274.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=197000, episode_reward=1583.60 +/- 103.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=198000, episode_reward=1671.00 +/- 219.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=199000, episode_reward=1676.80 +/- 245.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=200000, episode_reward=1632.60 +/- 152.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=201000, episode_reward=1538.80 +/- 293.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=202000, episode_reward=1620.00 +/- 298.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=203000, episode_reward=1747.40 +/- 104.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=204000, episode_reward=1423.80 +/- 199.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=205000, episode_reward=1339.40 +/- 343.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=206000, episode_reward=1539.20 +/- 413.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=207000, episode_reward=1606.40 +/- 158.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=208000, episode_reward=1593.80 +/- 167.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=209000, episode_reward=1489.20 +/- 201.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=210000, episode_reward=1742.80 +/- 190.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=211000, episode_reward=1848.00 +/- 157.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=212000, episode_reward=1670.40 +/- 162.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=213000, episode_reward=1435.40 +/- 245.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=214000, episode_reward=1429.80 +/- 167.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=215000, episode_reward=1805.60 +/- 310.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=216000, episode_reward=1519.20 +/- 320.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=217000, episode_reward=1518.20 +/- 192.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=218000, episode_reward=1463.00 +/- 231.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=219000, episode_reward=1619.00 +/- 280.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=220000, episode_reward=1525.20 +/- 153.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=221000, episode_reward=1485.60 +/- 131.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=222000, episode_reward=1262.60 +/- 362.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=223000, episode_reward=1381.40 +/- 304.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=224000, episode_reward=1637.40 +/- 206.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=225000, episode_reward=1579.80 +/- 288.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=226000, episode_reward=1557.00 +/- 249.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=227000, episode_reward=1519.20 +/- 326.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=228000, episode_reward=1485.00 +/- 306.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=229000, episode_reward=1561.20 +/- 329.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=230000, episode_reward=1749.60 +/- 156.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=231000, episode_reward=1469.60 +/- 275.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=232000, episode_reward=1446.40 +/- 314.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=233000, episode_reward=1610.00 +/- 269.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=234000, episode_reward=1579.80 +/- 164.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=235000, episode_reward=1712.00 +/- 168.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=236000, episode_reward=1629.40 +/- 135.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=237000, episode_reward=1573.40 +/- 377.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=238000, episode_reward=1630.00 +/- 167.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=239000, episode_reward=1521.40 +/- 358.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=240000, episode_reward=1640.80 +/- 241.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=241000, episode_reward=1776.80 +/- 94.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=242000, episode_reward=1640.80 +/- 229.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=243000, episode_reward=1665.20 +/- 335.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=244000, episode_reward=1626.20 +/- 224.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=245000, episode_reward=1761.60 +/- 161.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=246000, episode_reward=1687.00 +/- 240.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=247000, episode_reward=1678.80 +/- 100.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=248000, episode_reward=1699.40 +/- 191.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=249000, episode_reward=1480.60 +/- 198.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=250000, episode_reward=1710.80 +/- 184.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=251000, episode_reward=1571.80 +/- 194.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=252000, episode_reward=1563.60 +/- 262.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=253000, episode_reward=1702.80 +/- 219.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=254000, episode_reward=1786.60 +/- 218.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=255000, episode_reward=1366.60 +/- 170.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=256000, episode_reward=1756.80 +/- 121.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=257000, episode_reward=1816.20 +/- 109.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=258000, episode_reward=1713.60 +/- 232.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=259000, episode_reward=1593.40 +/- 250.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=260000, episode_reward=1567.60 +/- 310.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=261000, episode_reward=1572.20 +/- 162.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=262000, episode_reward=1638.80 +/- 286.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=263000, episode_reward=1841.80 +/- 72.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=264000, episode_reward=1517.80 +/- 246.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=265000, episode_reward=1547.80 +/- 340.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=266000, episode_reward=1824.20 +/- 135.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=267000, episode_reward=1504.80 +/- 376.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=268000, episode_reward=1477.20 +/- 181.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=269000, episode_reward=1538.20 +/- 147.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=270000, episode_reward=1743.40 +/- 115.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=271000, episode_reward=1707.60 +/- 215.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=272000, episode_reward=1533.60 +/- 221.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=273000, episode_reward=1724.80 +/- 182.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=274000, episode_reward=1703.40 +/- 272.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=275000, episode_reward=1781.60 +/- 121.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=276000, episode_reward=1623.60 +/- 161.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=277000, episode_reward=1692.80 +/- 132.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=278000, episode_reward=1710.80 +/- 249.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=279000, episode_reward=1592.60 +/- 287.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=280000, episode_reward=1562.60 +/- 211.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=281000, episode_reward=1624.40 +/- 180.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=282000, episode_reward=1548.00 +/- 186.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=283000, episode_reward=1700.00 +/- 152.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=284000, episode_reward=1598.40 +/- 157.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=285000, episode_reward=1435.00 +/- 436.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=286000, episode_reward=1583.00 +/- 356.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=287000, episode_reward=1549.00 +/- 340.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=288000, episode_reward=1713.00 +/- 251.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=289000, episode_reward=1596.60 +/- 219.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=290000, episode_reward=1588.00 +/- 197.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=291000, episode_reward=1648.20 +/- 122.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=292000, episode_reward=1445.00 +/- 271.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=293000, episode_reward=1518.00 +/- 247.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=294000, episode_reward=1702.60 +/- 180.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=295000, episode_reward=1465.00 +/- 69.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=296000, episode_reward=1532.00 +/- 187.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=297000, episode_reward=1652.40 +/- 259.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=298000, episode_reward=1595.40 +/- 190.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=299000, episode_reward=1499.80 +/- 193.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=300000, episode_reward=1632.20 +/- 183.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=301000, episode_reward=1927.20 +/- 127.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=302000, episode_reward=1592.00 +/- 108.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=303000, episode_reward=1349.60 +/- 268.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=304000, episode_reward=1553.60 +/- 388.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=305000, episode_reward=1587.60 +/- 247.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=306000, episode_reward=1288.60 +/- 484.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=307000, episode_reward=1696.40 +/- 238.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=308000, episode_reward=1520.60 +/- 148.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=309000, episode_reward=1652.00 +/- 313.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=310000, episode_reward=1521.80 +/- 165.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=311000, episode_reward=1616.40 +/- 205.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=312000, episode_reward=1731.80 +/- 171.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=313000, episode_reward=1417.60 +/- 224.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=314000, episode_reward=1627.00 +/- 264.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=315000, episode_reward=1728.40 +/- 227.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=316000, episode_reward=1775.00 +/- 180.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=317000, episode_reward=1810.80 +/- 229.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=318000, episode_reward=1593.20 +/- 86.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=319000, episode_reward=1751.00 +/- 100.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=320000, episode_reward=1680.60 +/- 203.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=321000, episode_reward=1711.40 +/- 339.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=322000, episode_reward=1660.00 +/- 192.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=323000, episode_reward=1573.80 +/- 161.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=324000, episode_reward=1598.00 +/- 350.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=325000, episode_reward=1628.40 +/- 156.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=326000, episode_reward=1634.60 +/- 157.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=327000, episode_reward=1618.60 +/- 198.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=328000, episode_reward=1688.40 +/- 275.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=329000, episode_reward=1615.40 +/- 142.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=330000, episode_reward=1514.20 +/- 365.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=331000, episode_reward=1539.80 +/- 272.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=332000, episode_reward=1567.80 +/- 298.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=333000, episode_reward=1481.60 +/- 215.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=334000, episode_reward=1769.80 +/- 256.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=335000, episode_reward=1793.00 +/- 323.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=336000, episode_reward=1855.40 +/- 141.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=337000, episode_reward=1539.20 +/- 173.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=338000, episode_reward=1834.60 +/- 260.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=339000, episode_reward=1784.00 +/- 312.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=340000, episode_reward=1606.80 +/- 393.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=341000, episode_reward=1699.00 +/- 163.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=342000, episode_reward=1633.40 +/- 265.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=343000, episode_reward=1476.00 +/- 145.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=344000, episode_reward=1669.80 +/- 198.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=345000, episode_reward=1637.80 +/- 353.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=346000, episode_reward=1717.40 +/- 162.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=347000, episode_reward=1690.80 +/- 188.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=348000, episode_reward=1661.80 +/- 310.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=349000, episode_reward=1739.60 +/- 197.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=350000, episode_reward=1365.20 +/- 180.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=351000, episode_reward=1648.00 +/- 392.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=352000, episode_reward=1708.00 +/- 253.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=353000, episode_reward=1636.20 +/- 112.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=354000, episode_reward=1735.20 +/- 263.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=355000, episode_reward=1716.20 +/- 261.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=356000, episode_reward=1715.40 +/- 240.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=357000, episode_reward=1752.40 +/- 198.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=358000, episode_reward=1755.20 +/- 238.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=359000, episode_reward=1764.60 +/- 358.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=360000, episode_reward=1908.60 +/- 165.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=361000, episode_reward=1490.60 +/- 363.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=362000, episode_reward=1769.60 +/- 166.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=363000, episode_reward=1647.20 +/- 165.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=364000, episode_reward=1584.80 +/- 452.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=365000, episode_reward=1732.00 +/- 221.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=366000, episode_reward=1776.20 +/- 82.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=367000, episode_reward=1765.60 +/- 69.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=368000, episode_reward=1670.80 +/- 296.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=369000, episode_reward=1641.40 +/- 186.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=370000, episode_reward=1507.00 +/- 289.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=371000, episode_reward=1692.00 +/- 227.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=372000, episode_reward=1781.40 +/- 368.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=373000, episode_reward=1564.60 +/- 483.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=374000, episode_reward=1786.60 +/- 138.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=375000, episode_reward=1666.20 +/- 179.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=376000, episode_reward=1574.20 +/- 182.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=377000, episode_reward=1908.00 +/- 147.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=378000, episode_reward=1622.00 +/- 219.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=379000, episode_reward=1632.20 +/- 102.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=380000, episode_reward=1720.80 +/- 225.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=381000, episode_reward=1710.40 +/- 258.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=382000, episode_reward=1638.80 +/- 148.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=383000, episode_reward=1786.20 +/- 107.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=384000, episode_reward=1846.60 +/- 58.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=385000, episode_reward=1788.40 +/- 164.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=386000, episode_reward=1855.00 +/- 174.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=387000, episode_reward=1649.00 +/- 149.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=388000, episode_reward=1507.60 +/- 167.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=389000, episode_reward=1600.40 +/- 165.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=390000, episode_reward=1746.80 +/- 264.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=391000, episode_reward=1846.40 +/- 110.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=392000, episode_reward=1838.80 +/- 204.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=393000, episode_reward=1685.20 +/- 80.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=394000, episode_reward=1651.60 +/- 181.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=395000, episode_reward=1668.60 +/- 133.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=396000, episode_reward=1763.00 +/- 131.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=397000, episode_reward=1690.60 +/- 238.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=398000, episode_reward=1657.60 +/- 132.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=399000, episode_reward=1641.20 +/- 381.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=400000, episode_reward=1512.00 +/- 208.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=401000, episode_reward=1660.00 +/- 324.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=402000, episode_reward=1646.40 +/- 292.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=403000, episode_reward=1684.60 +/- 116.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=404000, episode_reward=1740.20 +/- 304.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=405000, episode_reward=1712.20 +/- 183.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=406000, episode_reward=1797.60 +/- 81.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=407000, episode_reward=1709.80 +/- 262.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=408000, episode_reward=1547.20 +/- 157.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=409000, episode_reward=1700.80 +/- 254.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=410000, episode_reward=1521.40 +/- 192.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=411000, episode_reward=1528.40 +/- 186.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=412000, episode_reward=1716.60 +/- 292.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=413000, episode_reward=1650.20 +/- 54.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=414000, episode_reward=1735.00 +/- 224.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=415000, episode_reward=1655.60 +/- 167.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=416000, episode_reward=1732.00 +/- 61.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=417000, episode_reward=1853.00 +/- 172.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=418000, episode_reward=1773.40 +/- 167.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=419000, episode_reward=1979.60 +/- 180.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=420000, episode_reward=1448.60 +/- 139.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=421000, episode_reward=1756.00 +/- 131.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=422000, episode_reward=1756.40 +/- 138.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=423000, episode_reward=1571.60 +/- 380.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=424000, episode_reward=1551.00 +/- 146.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=425000, episode_reward=1609.00 +/- 133.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=426000, episode_reward=1627.80 +/- 177.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=427000, episode_reward=1702.20 +/- 171.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=428000, episode_reward=1536.20 +/- 137.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=429000, episode_reward=1687.80 +/- 176.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=430000, episode_reward=1724.40 +/- 145.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=431000, episode_reward=1570.60 +/- 308.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=432000, episode_reward=1776.00 +/- 186.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=433000, episode_reward=1553.20 +/- 163.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=434000, episode_reward=1846.40 +/- 133.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=435000, episode_reward=1748.60 +/- 83.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=436000, episode_reward=1496.20 +/- 213.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=437000, episode_reward=1699.00 +/- 185.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=438000, episode_reward=1683.60 +/- 366.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=439000, episode_reward=1712.00 +/- 199.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=440000, episode_reward=1592.40 +/- 176.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=441000, episode_reward=1638.40 +/- 176.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=442000, episode_reward=1623.60 +/- 203.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=443000, episode_reward=1688.80 +/- 186.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=444000, episode_reward=1540.60 +/- 288.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=445000, episode_reward=1642.80 +/- 240.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=446000, episode_reward=1518.40 +/- 198.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=447000, episode_reward=1704.80 +/- 254.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=448000, episode_reward=1580.80 +/- 152.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=449000, episode_reward=1761.20 +/- 112.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=450000, episode_reward=1674.80 +/- 342.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=451000, episode_reward=1635.40 +/- 244.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=452000, episode_reward=1816.20 +/- 97.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=453000, episode_reward=1699.40 +/- 298.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=454000, episode_reward=1599.20 +/- 131.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=455000, episode_reward=1814.00 +/- 132.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=456000, episode_reward=1731.60 +/- 230.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=457000, episode_reward=1725.60 +/- 209.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=458000, episode_reward=1556.20 +/- 113.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=459000, episode_reward=1672.00 +/- 176.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=460000, episode_reward=1335.80 +/- 374.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=461000, episode_reward=1515.40 +/- 226.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=462000, episode_reward=1729.80 +/- 223.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=463000, episode_reward=1689.60 +/- 124.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=464000, episode_reward=1784.20 +/- 169.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=465000, episode_reward=1562.20 +/- 344.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=466000, episode_reward=1443.80 +/- 187.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=467000, episode_reward=1674.00 +/- 214.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=468000, episode_reward=1779.40 +/- 233.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=469000, episode_reward=1562.40 +/- 392.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=470000, episode_reward=1787.60 +/- 111.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=471000, episode_reward=1528.00 +/- 264.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=472000, episode_reward=1641.00 +/- 367.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=473000, episode_reward=1754.00 +/- 296.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=474000, episode_reward=1401.40 +/- 232.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=475000, episode_reward=1838.60 +/- 140.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=476000, episode_reward=1429.40 +/- 232.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=477000, episode_reward=1613.80 +/- 190.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=478000, episode_reward=1675.00 +/- 435.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=479000, episode_reward=1832.80 +/- 115.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=480000, episode_reward=1498.00 +/- 302.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=481000, episode_reward=1548.00 +/- 324.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=482000, episode_reward=1676.80 +/- 75.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=483000, episode_reward=1635.60 +/- 125.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=484000, episode_reward=1651.80 +/- 266.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=485000, episode_reward=1574.80 +/- 147.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=486000, episode_reward=1566.20 +/- 211.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=487000, episode_reward=1553.20 +/- 171.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=488000, episode_reward=1510.80 +/- 235.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=489000, episode_reward=1636.00 +/- 210.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=490000, episode_reward=1794.20 +/- 99.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=491000, episode_reward=1606.00 +/- 263.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=492000, episode_reward=1653.40 +/- 155.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=493000, episode_reward=1666.80 +/- 276.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=494000, episode_reward=1739.80 +/- 222.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=495000, episode_reward=1779.00 +/- 124.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=496000, episode_reward=1557.60 +/- 90.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=497000, episode_reward=1625.20 +/- 228.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=498000, episode_reward=1447.80 +/- 271.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=499000, episode_reward=1651.00 +/- 69.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=500000, episode_reward=1589.40 +/- 176.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=501000, episode_reward=1684.00 +/- 210.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Gamma = 0.9, Learning Rate = 0.003, Activation Function = Tanh, Neurons = 64, Process noise = 0.1, Measurement noise = 0.0\n",
      "Eval num_timesteps=1000, episode_reward=-1064.80 +/- 158.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=-1059.40 +/- 107.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3000, episode_reward=-227.60 +/- 119.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4000, episode_reward=-202.40 +/- 108.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=5000, episode_reward=-187.20 +/- 185.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=6000, episode_reward=-338.20 +/- 80.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=7000, episode_reward=59.00 +/- 132.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=8000, episode_reward=128.40 +/- 183.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=9000, episode_reward=126.80 +/- 115.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=10000, episode_reward=89.60 +/- 118.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=11000, episode_reward=104.20 +/- 172.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=12000, episode_reward=1.00 +/- 173.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=13000, episode_reward=110.60 +/- 116.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=14000, episode_reward=152.40 +/- 50.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=15000, episode_reward=49.00 +/- 155.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=16000, episode_reward=90.80 +/- 91.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=17000, episode_reward=635.40 +/- 337.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=18000, episode_reward=511.60 +/- 207.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=19000, episode_reward=767.80 +/- 119.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=20000, episode_reward=484.60 +/- 214.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=21000, episode_reward=764.00 +/- 302.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=22000, episode_reward=920.80 +/- 351.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=23000, episode_reward=-1186.00 +/- 4619.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=24000, episode_reward=556.60 +/- 892.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=25000, episode_reward=184.40 +/- 2050.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=26000, episode_reward=1133.00 +/- 346.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=27000, episode_reward=821.80 +/- 276.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=28000, episode_reward=922.40 +/- 239.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=29000, episode_reward=1422.60 +/- 67.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=30000, episode_reward=1163.60 +/- 189.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=31000, episode_reward=1267.40 +/- 213.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=32000, episode_reward=1421.40 +/- 351.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=33000, episode_reward=-2093.40 +/- 6339.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=34000, episode_reward=1079.20 +/- 55.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=35000, episode_reward=1171.60 +/- 229.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=36000, episode_reward=1362.60 +/- 251.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=37000, episode_reward=1266.40 +/- 133.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=38000, episode_reward=1342.80 +/- 208.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=39000, episode_reward=1313.80 +/- 252.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=40000, episode_reward=1282.60 +/- 198.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=41000, episode_reward=1532.40 +/- 282.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=42000, episode_reward=1578.80 +/- 147.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=43000, episode_reward=1452.40 +/- 148.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=44000, episode_reward=1638.80 +/- 193.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=45000, episode_reward=1540.00 +/- 212.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=46000, episode_reward=1714.40 +/- 121.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=47000, episode_reward=1612.20 +/- 114.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=48000, episode_reward=1521.00 +/- 135.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=49000, episode_reward=1511.00 +/- 74.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=50000, episode_reward=1512.60 +/- 233.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=51000, episode_reward=1468.40 +/- 251.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=52000, episode_reward=1427.00 +/- 271.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=53000, episode_reward=1568.00 +/- 112.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=54000, episode_reward=1586.20 +/- 176.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=55000, episode_reward=1378.40 +/- 295.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=56000, episode_reward=1355.60 +/- 224.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=57000, episode_reward=1515.40 +/- 173.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=58000, episode_reward=1435.80 +/- 176.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=59000, episode_reward=1342.40 +/- 318.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=60000, episode_reward=1529.40 +/- 325.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=61000, episode_reward=1625.80 +/- 154.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=62000, episode_reward=1770.60 +/- 108.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=63000, episode_reward=1444.40 +/- 243.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=64000, episode_reward=1396.80 +/- 256.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=65000, episode_reward=1628.60 +/- 211.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=66000, episode_reward=1736.60 +/- 180.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=67000, episode_reward=1615.00 +/- 183.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=68000, episode_reward=1556.80 +/- 245.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=69000, episode_reward=1553.20 +/- 273.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=70000, episode_reward=1491.40 +/- 86.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=71000, episode_reward=1492.40 +/- 135.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=72000, episode_reward=1577.00 +/- 149.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=73000, episode_reward=1462.20 +/- 281.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=74000, episode_reward=1651.40 +/- 232.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=75000, episode_reward=1413.20 +/- 387.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=76000, episode_reward=1324.60 +/- 231.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=77000, episode_reward=1667.40 +/- 169.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=78000, episode_reward=1680.80 +/- 203.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=79000, episode_reward=1701.80 +/- 190.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=80000, episode_reward=1801.80 +/- 180.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=81000, episode_reward=1547.00 +/- 212.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=82000, episode_reward=1581.20 +/- 153.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=83000, episode_reward=1644.00 +/- 161.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=84000, episode_reward=1572.00 +/- 109.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=85000, episode_reward=1556.20 +/- 121.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=86000, episode_reward=1711.40 +/- 115.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=87000, episode_reward=1670.40 +/- 180.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=88000, episode_reward=1442.80 +/- 260.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=89000, episode_reward=1624.20 +/- 208.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=90000, episode_reward=1695.40 +/- 168.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=91000, episode_reward=1776.40 +/- 138.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=92000, episode_reward=1655.60 +/- 216.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=93000, episode_reward=1526.60 +/- 125.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=94000, episode_reward=1563.00 +/- 238.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=95000, episode_reward=1659.00 +/- 139.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=96000, episode_reward=1401.40 +/- 226.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=97000, episode_reward=1446.60 +/- 290.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=98000, episode_reward=1496.20 +/- 272.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=99000, episode_reward=1606.40 +/- 110.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=100000, episode_reward=1467.60 +/- 249.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=101000, episode_reward=1522.60 +/- 128.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=102000, episode_reward=1653.20 +/- 237.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=103000, episode_reward=1576.80 +/- 353.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=104000, episode_reward=1780.20 +/- 190.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=105000, episode_reward=1493.60 +/- 142.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=106000, episode_reward=1643.00 +/- 213.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=107000, episode_reward=1701.40 +/- 241.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=108000, episode_reward=1556.80 +/- 296.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=109000, episode_reward=1717.60 +/- 186.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=110000, episode_reward=1467.60 +/- 197.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=111000, episode_reward=1488.60 +/- 201.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=112000, episode_reward=1621.20 +/- 112.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=113000, episode_reward=1711.40 +/- 266.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=114000, episode_reward=1759.80 +/- 98.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=115000, episode_reward=1677.40 +/- 61.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=116000, episode_reward=1808.40 +/- 85.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=117000, episode_reward=1596.00 +/- 282.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=118000, episode_reward=1659.20 +/- 147.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=119000, episode_reward=1679.00 +/- 253.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=120000, episode_reward=1628.20 +/- 145.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=121000, episode_reward=1534.20 +/- 269.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=122000, episode_reward=1602.60 +/- 337.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=123000, episode_reward=1572.40 +/- 185.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=124000, episode_reward=1639.60 +/- 174.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=125000, episode_reward=1487.40 +/- 209.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=126000, episode_reward=1490.00 +/- 214.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=127000, episode_reward=1617.00 +/- 211.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=128000, episode_reward=1747.00 +/- 206.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=129000, episode_reward=1651.80 +/- 85.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=130000, episode_reward=1626.60 +/- 120.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=131000, episode_reward=1571.80 +/- 119.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=132000, episode_reward=1726.80 +/- 181.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=133000, episode_reward=1713.40 +/- 93.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=134000, episode_reward=1502.80 +/- 298.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=135000, episode_reward=1461.60 +/- 216.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=136000, episode_reward=1601.20 +/- 335.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=137000, episode_reward=1660.40 +/- 394.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=138000, episode_reward=1541.40 +/- 179.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=139000, episode_reward=1523.40 +/- 333.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=140000, episode_reward=1644.20 +/- 164.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=141000, episode_reward=1796.00 +/- 104.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=142000, episode_reward=1724.80 +/- 204.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=143000, episode_reward=1604.20 +/- 174.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=144000, episode_reward=1718.80 +/- 219.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=145000, episode_reward=1686.40 +/- 210.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=146000, episode_reward=1546.40 +/- 188.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=147000, episode_reward=1590.00 +/- 200.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=148000, episode_reward=1610.00 +/- 137.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=149000, episode_reward=1455.20 +/- 137.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=150000, episode_reward=1742.20 +/- 243.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=151000, episode_reward=1532.00 +/- 148.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=152000, episode_reward=1545.20 +/- 136.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=153000, episode_reward=1623.20 +/- 246.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=154000, episode_reward=1742.00 +/- 266.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=155000, episode_reward=1641.20 +/- 227.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=156000, episode_reward=1456.60 +/- 245.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=157000, episode_reward=1445.20 +/- 138.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=158000, episode_reward=1752.00 +/- 234.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=159000, episode_reward=1541.40 +/- 212.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=160000, episode_reward=1652.40 +/- 248.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=161000, episode_reward=1503.40 +/- 210.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=162000, episode_reward=1705.40 +/- 253.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=163000, episode_reward=1683.00 +/- 128.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=164000, episode_reward=1624.20 +/- 143.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=165000, episode_reward=1671.20 +/- 166.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=166000, episode_reward=1539.20 +/- 212.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=167000, episode_reward=1671.60 +/- 92.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=168000, episode_reward=1619.60 +/- 243.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=169000, episode_reward=1593.60 +/- 253.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=170000, episode_reward=1777.40 +/- 166.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=171000, episode_reward=1632.80 +/- 287.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=172000, episode_reward=1553.80 +/- 204.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=173000, episode_reward=1631.80 +/- 207.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=174000, episode_reward=1640.40 +/- 344.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=175000, episode_reward=1686.40 +/- 287.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=176000, episode_reward=1484.00 +/- 141.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=177000, episode_reward=1559.40 +/- 171.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=178000, episode_reward=1661.40 +/- 81.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=179000, episode_reward=1594.20 +/- 304.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=180000, episode_reward=1723.80 +/- 142.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=181000, episode_reward=1607.40 +/- 357.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=182000, episode_reward=1598.60 +/- 207.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=183000, episode_reward=1417.60 +/- 193.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=184000, episode_reward=1802.40 +/- 140.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=185000, episode_reward=1568.40 +/- 177.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=186000, episode_reward=1472.20 +/- 223.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=187000, episode_reward=1680.00 +/- 204.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=188000, episode_reward=1726.20 +/- 224.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=189000, episode_reward=1663.20 +/- 277.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=190000, episode_reward=1605.20 +/- 151.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=191000, episode_reward=1554.20 +/- 219.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=192000, episode_reward=1651.80 +/- 135.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=193000, episode_reward=1621.40 +/- 175.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=194000, episode_reward=1823.60 +/- 270.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=195000, episode_reward=1709.00 +/- 263.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=196000, episode_reward=1526.00 +/- 298.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=197000, episode_reward=1621.80 +/- 215.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=198000, episode_reward=1564.20 +/- 137.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=199000, episode_reward=1533.00 +/- 131.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=200000, episode_reward=1649.20 +/- 196.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=201000, episode_reward=1528.40 +/- 210.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=202000, episode_reward=1510.80 +/- 242.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=203000, episode_reward=1553.80 +/- 336.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=204000, episode_reward=1556.40 +/- 91.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=205000, episode_reward=1632.20 +/- 189.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=206000, episode_reward=1380.60 +/- 302.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=207000, episode_reward=1637.00 +/- 338.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=208000, episode_reward=1352.60 +/- 244.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=209000, episode_reward=1675.40 +/- 170.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=210000, episode_reward=1445.60 +/- 35.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=211000, episode_reward=1650.40 +/- 276.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=212000, episode_reward=1513.40 +/- 239.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=213000, episode_reward=1623.60 +/- 269.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=214000, episode_reward=1560.60 +/- 203.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=215000, episode_reward=1705.00 +/- 125.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=216000, episode_reward=1678.00 +/- 176.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=217000, episode_reward=1475.60 +/- 193.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=218000, episode_reward=1751.80 +/- 90.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=219000, episode_reward=1764.60 +/- 280.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=220000, episode_reward=1571.60 +/- 86.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=221000, episode_reward=1699.00 +/- 319.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=222000, episode_reward=1632.40 +/- 232.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=223000, episode_reward=1554.40 +/- 142.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=224000, episode_reward=1510.60 +/- 378.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=225000, episode_reward=1379.00 +/- 251.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=226000, episode_reward=1642.40 +/- 280.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=227000, episode_reward=1745.40 +/- 155.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=228000, episode_reward=1725.80 +/- 180.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=229000, episode_reward=1608.20 +/- 268.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=230000, episode_reward=1714.40 +/- 201.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=231000, episode_reward=1673.20 +/- 410.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=232000, episode_reward=1470.60 +/- 243.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=233000, episode_reward=1700.00 +/- 230.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=234000, episode_reward=1519.40 +/- 283.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=235000, episode_reward=1551.20 +/- 302.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=236000, episode_reward=1675.20 +/- 100.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=237000, episode_reward=1847.40 +/- 316.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=238000, episode_reward=1646.80 +/- 159.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=239000, episode_reward=1535.20 +/- 154.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=240000, episode_reward=1439.00 +/- 89.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=241000, episode_reward=1656.80 +/- 235.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=242000, episode_reward=1695.80 +/- 85.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=243000, episode_reward=1502.80 +/- 144.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=244000, episode_reward=1664.20 +/- 259.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=245000, episode_reward=1720.80 +/- 311.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=246000, episode_reward=1582.40 +/- 399.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=247000, episode_reward=1692.40 +/- 233.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=248000, episode_reward=1656.00 +/- 212.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=249000, episode_reward=1666.00 +/- 179.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=250000, episode_reward=1723.60 +/- 236.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=251000, episode_reward=1650.00 +/- 189.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=252000, episode_reward=1572.40 +/- 316.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=253000, episode_reward=1587.00 +/- 269.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=254000, episode_reward=1606.80 +/- 189.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=255000, episode_reward=1788.80 +/- 107.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=256000, episode_reward=1416.80 +/- 185.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=257000, episode_reward=1644.20 +/- 215.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=258000, episode_reward=1634.20 +/- 208.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=259000, episode_reward=1494.80 +/- 389.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=260000, episode_reward=1837.60 +/- 140.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=261000, episode_reward=1677.80 +/- 101.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=262000, episode_reward=1768.80 +/- 195.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=263000, episode_reward=1674.40 +/- 299.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=264000, episode_reward=1636.60 +/- 155.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=265000, episode_reward=1731.60 +/- 209.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=266000, episode_reward=1509.60 +/- 267.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=267000, episode_reward=1688.20 +/- 218.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=268000, episode_reward=1790.00 +/- 204.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=269000, episode_reward=1666.40 +/- 330.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=270000, episode_reward=1700.40 +/- 132.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=271000, episode_reward=1646.60 +/- 201.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=272000, episode_reward=1515.80 +/- 247.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=273000, episode_reward=1689.20 +/- 182.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=274000, episode_reward=1679.40 +/- 182.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=275000, episode_reward=1573.20 +/- 191.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=276000, episode_reward=1638.20 +/- 191.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=277000, episode_reward=1778.40 +/- 105.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=278000, episode_reward=1815.00 +/- 142.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=279000, episode_reward=1589.40 +/- 193.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=280000, episode_reward=1555.60 +/- 171.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=281000, episode_reward=1238.20 +/- 282.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=282000, episode_reward=1493.40 +/- 230.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=283000, episode_reward=1641.80 +/- 188.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=284000, episode_reward=1741.80 +/- 171.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=285000, episode_reward=1508.40 +/- 407.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=286000, episode_reward=1694.80 +/- 44.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=287000, episode_reward=1559.20 +/- 130.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=288000, episode_reward=1669.40 +/- 46.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=289000, episode_reward=1702.80 +/- 129.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=290000, episode_reward=1442.60 +/- 184.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=291000, episode_reward=1582.40 +/- 173.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=292000, episode_reward=1434.60 +/- 333.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=293000, episode_reward=1585.60 +/- 277.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=294000, episode_reward=1655.00 +/- 171.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=295000, episode_reward=1579.00 +/- 233.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=296000, episode_reward=1659.80 +/- 88.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=297000, episode_reward=1590.60 +/- 148.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=298000, episode_reward=1632.40 +/- 183.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=299000, episode_reward=1701.60 +/- 103.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=300000, episode_reward=1793.20 +/- 226.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=301000, episode_reward=1637.40 +/- 467.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=302000, episode_reward=1606.60 +/- 124.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=303000, episode_reward=1596.40 +/- 428.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=304000, episode_reward=1642.60 +/- 130.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=305000, episode_reward=1695.80 +/- 185.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=306000, episode_reward=1599.00 +/- 151.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=307000, episode_reward=1293.20 +/- 125.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=308000, episode_reward=1597.20 +/- 346.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=309000, episode_reward=1637.20 +/- 220.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=310000, episode_reward=1723.40 +/- 171.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=311000, episode_reward=1620.20 +/- 311.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=312000, episode_reward=1507.20 +/- 105.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=313000, episode_reward=1825.00 +/- 165.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=314000, episode_reward=1625.20 +/- 288.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=315000, episode_reward=1609.40 +/- 247.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=316000, episode_reward=1669.40 +/- 277.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=317000, episode_reward=1726.60 +/- 82.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=318000, episode_reward=1619.80 +/- 361.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=319000, episode_reward=1773.40 +/- 134.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=320000, episode_reward=1748.00 +/- 146.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=321000, episode_reward=1757.20 +/- 137.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=322000, episode_reward=1762.80 +/- 63.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=323000, episode_reward=1616.40 +/- 215.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=324000, episode_reward=1712.20 +/- 160.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=325000, episode_reward=1752.60 +/- 168.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=326000, episode_reward=1578.00 +/- 316.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=327000, episode_reward=1697.60 +/- 241.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=328000, episode_reward=1439.60 +/- 352.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=329000, episode_reward=1666.40 +/- 168.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=330000, episode_reward=1513.00 +/- 257.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=331000, episode_reward=1749.60 +/- 149.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=332000, episode_reward=1493.00 +/- 239.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=333000, episode_reward=1557.00 +/- 211.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=334000, episode_reward=1663.60 +/- 154.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=335000, episode_reward=1720.40 +/- 111.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=336000, episode_reward=1715.40 +/- 86.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=337000, episode_reward=1741.40 +/- 131.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=338000, episode_reward=1627.20 +/- 309.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=339000, episode_reward=1561.80 +/- 165.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=340000, episode_reward=1729.80 +/- 163.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=341000, episode_reward=1400.60 +/- 326.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=342000, episode_reward=1757.40 +/- 187.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=343000, episode_reward=1681.60 +/- 186.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=344000, episode_reward=1598.60 +/- 72.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=345000, episode_reward=1852.60 +/- 207.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=346000, episode_reward=1771.40 +/- 206.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=347000, episode_reward=1662.80 +/- 224.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=348000, episode_reward=1794.60 +/- 120.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=349000, episode_reward=1594.80 +/- 378.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=350000, episode_reward=1606.20 +/- 240.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=351000, episode_reward=1604.20 +/- 272.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=352000, episode_reward=1600.00 +/- 252.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=353000, episode_reward=1776.80 +/- 128.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=354000, episode_reward=1594.00 +/- 184.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=355000, episode_reward=1578.20 +/- 179.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=356000, episode_reward=1533.40 +/- 257.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=357000, episode_reward=1861.60 +/- 177.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=358000, episode_reward=1612.20 +/- 135.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=359000, episode_reward=1692.60 +/- 177.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=360000, episode_reward=1622.00 +/- 229.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=361000, episode_reward=1744.80 +/- 102.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=362000, episode_reward=1833.40 +/- 97.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=363000, episode_reward=1599.00 +/- 136.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=364000, episode_reward=1533.00 +/- 245.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=365000, episode_reward=1544.80 +/- 390.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=366000, episode_reward=1696.20 +/- 108.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=367000, episode_reward=1689.00 +/- 167.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=368000, episode_reward=1684.20 +/- 93.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=369000, episode_reward=1639.60 +/- 324.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=370000, episode_reward=1752.40 +/- 183.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=371000, episode_reward=1650.20 +/- 170.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=372000, episode_reward=1795.80 +/- 134.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=373000, episode_reward=1685.00 +/- 173.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=374000, episode_reward=1703.20 +/- 190.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=375000, episode_reward=1594.20 +/- 124.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=376000, episode_reward=1591.60 +/- 283.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=377000, episode_reward=1822.80 +/- 175.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=378000, episode_reward=1611.00 +/- 161.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=379000, episode_reward=1532.40 +/- 204.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=380000, episode_reward=1601.80 +/- 207.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=381000, episode_reward=1705.40 +/- 21.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=382000, episode_reward=1516.60 +/- 389.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=383000, episode_reward=1539.00 +/- 362.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=384000, episode_reward=1655.60 +/- 238.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=385000, episode_reward=1646.20 +/- 203.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=386000, episode_reward=1498.60 +/- 228.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=387000, episode_reward=1544.20 +/- 414.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=388000, episode_reward=1682.80 +/- 175.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=389000, episode_reward=1647.00 +/- 325.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=390000, episode_reward=1466.40 +/- 144.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=391000, episode_reward=1569.40 +/- 167.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=392000, episode_reward=1557.20 +/- 235.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=393000, episode_reward=1777.40 +/- 266.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=394000, episode_reward=1661.20 +/- 302.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=395000, episode_reward=1552.40 +/- 213.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=396000, episode_reward=1640.00 +/- 147.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=397000, episode_reward=1753.80 +/- 151.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=398000, episode_reward=1677.00 +/- 302.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=399000, episode_reward=1679.20 +/- 179.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=400000, episode_reward=1746.80 +/- 162.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=401000, episode_reward=1625.80 +/- 230.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=402000, episode_reward=1688.40 +/- 285.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=403000, episode_reward=1680.40 +/- 333.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=404000, episode_reward=1748.60 +/- 377.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=405000, episode_reward=1649.60 +/- 120.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=406000, episode_reward=1531.60 +/- 182.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=407000, episode_reward=1639.60 +/- 151.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=408000, episode_reward=1693.80 +/- 275.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=409000, episode_reward=1781.40 +/- 109.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=410000, episode_reward=1589.60 +/- 273.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=411000, episode_reward=1806.80 +/- 146.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=412000, episode_reward=1646.20 +/- 466.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=413000, episode_reward=1847.40 +/- 267.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=414000, episode_reward=1670.80 +/- 252.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=415000, episode_reward=1664.00 +/- 226.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=416000, episode_reward=1646.20 +/- 151.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=417000, episode_reward=1701.60 +/- 154.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=418000, episode_reward=1646.20 +/- 207.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=419000, episode_reward=1693.60 +/- 154.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=420000, episode_reward=1742.20 +/- 78.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=421000, episode_reward=1587.60 +/- 157.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=422000, episode_reward=1640.20 +/- 217.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=423000, episode_reward=1837.60 +/- 146.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=424000, episode_reward=1613.80 +/- 190.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=425000, episode_reward=1537.60 +/- 377.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=426000, episode_reward=1622.60 +/- 222.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=427000, episode_reward=1788.80 +/- 266.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=428000, episode_reward=1548.40 +/- 202.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=429000, episode_reward=1707.60 +/- 89.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=430000, episode_reward=1725.60 +/- 237.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=431000, episode_reward=1769.80 +/- 188.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=432000, episode_reward=1616.60 +/- 265.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=433000, episode_reward=1712.60 +/- 190.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=434000, episode_reward=1508.60 +/- 219.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=435000, episode_reward=1670.60 +/- 240.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=436000, episode_reward=1756.20 +/- 78.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=437000, episode_reward=1722.60 +/- 167.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=438000, episode_reward=1780.20 +/- 78.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=439000, episode_reward=1621.40 +/- 327.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=440000, episode_reward=1603.40 +/- 322.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=441000, episode_reward=1723.20 +/- 225.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=442000, episode_reward=1712.80 +/- 151.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=443000, episode_reward=1520.40 +/- 228.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=444000, episode_reward=1570.40 +/- 239.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=445000, episode_reward=1731.80 +/- 88.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=446000, episode_reward=1695.60 +/- 190.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=447000, episode_reward=1671.60 +/- 184.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=448000, episode_reward=1671.60 +/- 265.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=449000, episode_reward=1598.20 +/- 201.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=450000, episode_reward=1529.20 +/- 122.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=451000, episode_reward=1533.00 +/- 161.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=452000, episode_reward=1834.80 +/- 172.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=453000, episode_reward=1448.20 +/- 300.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=454000, episode_reward=1549.40 +/- 227.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=455000, episode_reward=1763.80 +/- 175.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=456000, episode_reward=1738.00 +/- 198.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=457000, episode_reward=1541.80 +/- 287.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=458000, episode_reward=1781.20 +/- 261.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=459000, episode_reward=1579.80 +/- 277.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=460000, episode_reward=1702.60 +/- 130.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=461000, episode_reward=1707.80 +/- 155.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=462000, episode_reward=1647.60 +/- 271.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=463000, episode_reward=1666.20 +/- 321.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=464000, episode_reward=1648.80 +/- 200.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=465000, episode_reward=1519.20 +/- 348.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=466000, episode_reward=1710.40 +/- 326.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=467000, episode_reward=1672.80 +/- 274.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=468000, episode_reward=1685.80 +/- 135.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=469000, episode_reward=1756.60 +/- 144.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=470000, episode_reward=1417.40 +/- 173.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=471000, episode_reward=1711.80 +/- 198.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=472000, episode_reward=1808.00 +/- 109.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=473000, episode_reward=1778.60 +/- 240.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=474000, episode_reward=1397.60 +/- 177.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=475000, episode_reward=1381.20 +/- 260.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=476000, episode_reward=1664.40 +/- 168.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=477000, episode_reward=1437.20 +/- 242.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=478000, episode_reward=1710.80 +/- 285.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=479000, episode_reward=1608.00 +/- 139.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=480000, episode_reward=1670.20 +/- 157.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=481000, episode_reward=1652.60 +/- 297.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=482000, episode_reward=1739.20 +/- 104.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=483000, episode_reward=1702.40 +/- 335.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=484000, episode_reward=1787.60 +/- 247.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=485000, episode_reward=1727.80 +/- 317.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=486000, episode_reward=1773.60 +/- 224.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=487000, episode_reward=1529.00 +/- 247.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=488000, episode_reward=1620.20 +/- 160.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=489000, episode_reward=1792.00 +/- 128.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=490000, episode_reward=1596.20 +/- 329.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=491000, episode_reward=1692.40 +/- 285.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=492000, episode_reward=1511.00 +/- 299.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=493000, episode_reward=1540.80 +/- 225.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=494000, episode_reward=1769.00 +/- 262.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=495000, episode_reward=1869.60 +/- 255.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=496000, episode_reward=1673.40 +/- 310.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=497000, episode_reward=1701.60 +/- 143.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=498000, episode_reward=1764.60 +/- 223.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=499000, episode_reward=1592.20 +/- 192.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=500000, episode_reward=1741.60 +/- 262.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=501000, episode_reward=1447.20 +/- 245.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Gamma = 0.9, Learning Rate = 0.003, Activation Function = ReLU, Neurons = 8, Process noise = 0.1, Measurement noise = 0.0\n",
      "Eval num_timesteps=1000, episode_reward=-354.20 +/- 268.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=-504.80 +/- 78.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=3000, episode_reward=-102.60 +/- 130.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4000, episode_reward=-214.60 +/- 185.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=5000, episode_reward=-161.20 +/- 173.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=6000, episode_reward=-219.00 +/- 260.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=7000, episode_reward=-194.40 +/- 246.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=8000, episode_reward=-0.20 +/- 466.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=9000, episode_reward=-154.00 +/- 185.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=10000, episode_reward=2.60 +/- 120.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=11000, episode_reward=624.20 +/- 276.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=12000, episode_reward=649.00 +/- 174.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=13000, episode_reward=-38.40 +/- 107.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=14000, episode_reward=52.20 +/- 142.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=15000, episode_reward=67.00 +/- 157.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=16000, episode_reward=-77.60 +/- 231.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=17000, episode_reward=257.80 +/- 171.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=18000, episode_reward=148.80 +/- 183.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=19000, episode_reward=47.40 +/- 134.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=20000, episode_reward=299.40 +/- 161.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=21000, episode_reward=251.20 +/- 141.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=22000, episode_reward=297.20 +/- 157.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=23000, episode_reward=243.20 +/- 80.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=24000, episode_reward=358.40 +/- 184.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=25000, episode_reward=678.40 +/- 339.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=26000, episode_reward=309.60 +/- 366.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=27000, episode_reward=714.60 +/- 497.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=28000, episode_reward=-1448.20 +/- 4985.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=29000, episode_reward=1168.60 +/- 136.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=30000, episode_reward=907.60 +/- 231.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=31000, episode_reward=1016.00 +/- 188.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=32000, episode_reward=1215.20 +/- 279.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=33000, episode_reward=1206.80 +/- 320.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=34000, episode_reward=1224.40 +/- 163.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=35000, episode_reward=1202.00 +/- 336.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=36000, episode_reward=1063.20 +/- 218.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=37000, episode_reward=1436.60 +/- 137.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=38000, episode_reward=995.60 +/- 678.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=39000, episode_reward=1350.40 +/- 527.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=40000, episode_reward=1084.40 +/- 416.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=41000, episode_reward=1336.00 +/- 174.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=42000, episode_reward=1431.60 +/- 168.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=43000, episode_reward=1377.60 +/- 300.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=44000, episode_reward=1508.60 +/- 250.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=45000, episode_reward=1357.80 +/- 160.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=46000, episode_reward=1173.00 +/- 374.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=47000, episode_reward=-1030.60 +/- 4373.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=48000, episode_reward=1399.60 +/- 351.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=49000, episode_reward=1320.40 +/- 340.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=50000, episode_reward=1547.80 +/- 138.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=51000, episode_reward=1528.20 +/- 301.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=52000, episode_reward=1384.40 +/- 160.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=53000, episode_reward=1361.00 +/- 225.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=54000, episode_reward=1552.20 +/- 251.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=55000, episode_reward=1568.80 +/- 251.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=56000, episode_reward=1569.40 +/- 318.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=57000, episode_reward=1578.40 +/- 125.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=58000, episode_reward=1486.60 +/- 245.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=59000, episode_reward=1613.40 +/- 236.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=60000, episode_reward=1482.40 +/- 101.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=61000, episode_reward=1458.80 +/- 190.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=62000, episode_reward=1577.80 +/- 106.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=63000, episode_reward=1694.20 +/- 253.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=64000, episode_reward=1321.60 +/- 293.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=65000, episode_reward=1502.60 +/- 180.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=66000, episode_reward=-1296.20 +/- 5876.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=67000, episode_reward=1090.60 +/- 1080.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=68000, episode_reward=1674.00 +/- 105.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=69000, episode_reward=1684.00 +/- 126.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=70000, episode_reward=1567.40 +/- 262.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=71000, episode_reward=1534.80 +/- 405.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=72000, episode_reward=1368.40 +/- 191.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=73000, episode_reward=1533.40 +/- 131.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=74000, episode_reward=1534.20 +/- 129.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=75000, episode_reward=1757.00 +/- 214.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=76000, episode_reward=1711.60 +/- 99.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=77000, episode_reward=1691.20 +/- 259.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=78000, episode_reward=-1808.60 +/- 6877.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=79000, episode_reward=1622.80 +/- 231.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=80000, episode_reward=-3842.20 +/- 6660.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=81000, episode_reward=1742.00 +/- 172.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=82000, episode_reward=1612.20 +/- 119.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=83000, episode_reward=1499.40 +/- 104.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=84000, episode_reward=1622.20 +/- 317.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=85000, episode_reward=1830.20 +/- 239.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=86000, episode_reward=1478.80 +/- 155.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=87000, episode_reward=1779.60 +/- 144.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=88000, episode_reward=1422.00 +/- 257.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=89000, episode_reward=1554.00 +/- 135.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=90000, episode_reward=1563.80 +/- 239.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=91000, episode_reward=1611.00 +/- 301.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=92000, episode_reward=1518.00 +/- 226.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=93000, episode_reward=1515.60 +/- 241.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=94000, episode_reward=1683.40 +/- 118.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=95000, episode_reward=1382.40 +/- 105.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=96000, episode_reward=1672.80 +/- 182.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=97000, episode_reward=1837.20 +/- 98.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=98000, episode_reward=1656.60 +/- 265.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=99000, episode_reward=1760.20 +/- 82.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=100000, episode_reward=1570.80 +/- 96.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=101000, episode_reward=1572.60 +/- 131.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=102000, episode_reward=1729.00 +/- 94.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=103000, episode_reward=1660.80 +/- 181.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=104000, episode_reward=1766.40 +/- 168.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=105000, episode_reward=1650.40 +/- 79.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=106000, episode_reward=1804.20 +/- 154.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=107000, episode_reward=1547.20 +/- 204.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=108000, episode_reward=1699.00 +/- 72.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=109000, episode_reward=1763.40 +/- 171.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=110000, episode_reward=1670.00 +/- 93.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=111000, episode_reward=1649.60 +/- 183.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=112000, episode_reward=1646.60 +/- 261.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=113000, episode_reward=1766.40 +/- 218.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=114000, episode_reward=1870.20 +/- 93.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=115000, episode_reward=1612.20 +/- 126.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=116000, episode_reward=1654.00 +/- 188.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=117000, episode_reward=1638.20 +/- 198.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=118000, episode_reward=1603.60 +/- 252.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=119000, episode_reward=1714.20 +/- 144.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=120000, episode_reward=1657.00 +/- 186.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=121000, episode_reward=1483.40 +/- 266.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=122000, episode_reward=1562.60 +/- 272.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=123000, episode_reward=1413.00 +/- 198.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=124000, episode_reward=1624.40 +/- 276.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=125000, episode_reward=1626.00 +/- 232.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=126000, episode_reward=1498.20 +/- 254.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=127000, episode_reward=1649.60 +/- 70.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=128000, episode_reward=1439.00 +/- 304.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=129000, episode_reward=1667.80 +/- 132.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=130000, episode_reward=1780.40 +/- 101.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=131000, episode_reward=1887.60 +/- 197.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=132000, episode_reward=1484.00 +/- 278.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=133000, episode_reward=1496.00 +/- 165.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=134000, episode_reward=1599.40 +/- 232.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=135000, episode_reward=1743.00 +/- 347.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=136000, episode_reward=1769.00 +/- 84.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=137000, episode_reward=1451.00 +/- 194.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=138000, episode_reward=1565.80 +/- 214.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=139000, episode_reward=1422.40 +/- 271.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=140000, episode_reward=1550.00 +/- 185.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=141000, episode_reward=1616.20 +/- 186.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=142000, episode_reward=1565.60 +/- 65.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=143000, episode_reward=1698.80 +/- 180.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=144000, episode_reward=1809.60 +/- 186.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=145000, episode_reward=1590.20 +/- 240.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=146000, episode_reward=1683.20 +/- 222.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=147000, episode_reward=1399.40 +/- 134.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=148000, episode_reward=1644.80 +/- 235.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=149000, episode_reward=1776.40 +/- 144.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=150000, episode_reward=1602.00 +/- 237.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=151000, episode_reward=1443.40 +/- 219.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=152000, episode_reward=1751.60 +/- 222.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=153000, episode_reward=1767.60 +/- 100.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=154000, episode_reward=1546.40 +/- 273.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=155000, episode_reward=1516.40 +/- 180.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=156000, episode_reward=1776.00 +/- 202.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=157000, episode_reward=1661.80 +/- 336.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=158000, episode_reward=1554.60 +/- 170.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=159000, episode_reward=1594.80 +/- 307.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=160000, episode_reward=1492.20 +/- 274.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=161000, episode_reward=1747.60 +/- 205.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=162000, episode_reward=1715.40 +/- 226.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=163000, episode_reward=1657.80 +/- 163.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=164000, episode_reward=1629.20 +/- 230.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=165000, episode_reward=1694.40 +/- 238.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=166000, episode_reward=1595.20 +/- 339.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=167000, episode_reward=1777.60 +/- 162.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=168000, episode_reward=1629.20 +/- 331.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=169000, episode_reward=1514.20 +/- 131.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=170000, episode_reward=1638.40 +/- 159.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=171000, episode_reward=1581.80 +/- 287.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=172000, episode_reward=1471.20 +/- 120.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=173000, episode_reward=1619.40 +/- 245.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=174000, episode_reward=1540.60 +/- 178.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=175000, episode_reward=1646.60 +/- 144.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=176000, episode_reward=1611.80 +/- 286.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=177000, episode_reward=1605.80 +/- 181.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=178000, episode_reward=1650.60 +/- 130.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=179000, episode_reward=1454.80 +/- 124.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=180000, episode_reward=1576.80 +/- 222.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=181000, episode_reward=1347.80 +/- 355.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=182000, episode_reward=1796.80 +/- 218.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=183000, episode_reward=1672.60 +/- 136.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=184000, episode_reward=1768.20 +/- 173.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=185000, episode_reward=1636.20 +/- 249.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=186000, episode_reward=1828.80 +/- 181.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=187000, episode_reward=1708.00 +/- 249.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=188000, episode_reward=1658.60 +/- 276.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=189000, episode_reward=1624.60 +/- 210.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=190000, episode_reward=1595.20 +/- 109.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=191000, episode_reward=1722.60 +/- 263.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=192000, episode_reward=1502.80 +/- 167.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=193000, episode_reward=1869.60 +/- 146.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=194000, episode_reward=1615.60 +/- 247.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=195000, episode_reward=1457.60 +/- 186.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=196000, episode_reward=1684.20 +/- 195.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=197000, episode_reward=1649.00 +/- 175.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=198000, episode_reward=1641.60 +/- 198.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=199000, episode_reward=1722.00 +/- 341.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=200000, episode_reward=1858.40 +/- 177.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=201000, episode_reward=1734.20 +/- 191.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=202000, episode_reward=1656.80 +/- 234.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=203000, episode_reward=1685.80 +/- 96.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=204000, episode_reward=1589.20 +/- 208.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=205000, episode_reward=1645.60 +/- 132.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=206000, episode_reward=1678.20 +/- 129.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=207000, episode_reward=1654.60 +/- 195.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=208000, episode_reward=1748.60 +/- 118.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=209000, episode_reward=1482.00 +/- 121.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=210000, episode_reward=1797.80 +/- 163.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=211000, episode_reward=1750.20 +/- 75.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=212000, episode_reward=1480.40 +/- 308.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=213000, episode_reward=1765.60 +/- 119.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=214000, episode_reward=1698.40 +/- 253.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=215000, episode_reward=1528.80 +/- 336.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=216000, episode_reward=1773.80 +/- 283.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=217000, episode_reward=1606.00 +/- 136.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=218000, episode_reward=1889.20 +/- 136.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=219000, episode_reward=1686.00 +/- 149.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=220000, episode_reward=1575.80 +/- 201.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=221000, episode_reward=1577.00 +/- 173.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=222000, episode_reward=1336.40 +/- 256.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=223000, episode_reward=1560.40 +/- 84.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=224000, episode_reward=1636.20 +/- 223.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=225000, episode_reward=1530.80 +/- 180.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=226000, episode_reward=1887.80 +/- 189.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=227000, episode_reward=1740.60 +/- 84.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=228000, episode_reward=1893.00 +/- 194.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=229000, episode_reward=1399.00 +/- 330.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=230000, episode_reward=1779.40 +/- 154.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=231000, episode_reward=1477.00 +/- 429.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=232000, episode_reward=1610.00 +/- 370.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=233000, episode_reward=1565.00 +/- 112.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=234000, episode_reward=1489.80 +/- 280.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=235000, episode_reward=1513.40 +/- 287.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=236000, episode_reward=1729.60 +/- 92.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=237000, episode_reward=1676.60 +/- 106.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=238000, episode_reward=1474.60 +/- 203.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=239000, episode_reward=1864.40 +/- 180.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=240000, episode_reward=1475.00 +/- 182.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=241000, episode_reward=1737.20 +/- 199.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=242000, episode_reward=1501.40 +/- 161.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=243000, episode_reward=1552.80 +/- 237.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=244000, episode_reward=1771.60 +/- 89.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=245000, episode_reward=1738.00 +/- 219.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=246000, episode_reward=1696.00 +/- 230.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=247000, episode_reward=1608.80 +/- 216.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=248000, episode_reward=1613.00 +/- 156.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=249000, episode_reward=1750.80 +/- 266.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=250000, episode_reward=1599.00 +/- 293.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=251000, episode_reward=1749.60 +/- 162.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=252000, episode_reward=1897.40 +/- 102.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=253000, episode_reward=1680.60 +/- 128.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=254000, episode_reward=1623.20 +/- 125.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=255000, episode_reward=1660.20 +/- 203.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=256000, episode_reward=1752.00 +/- 39.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=257000, episode_reward=1913.60 +/- 124.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=258000, episode_reward=1527.60 +/- 321.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=259000, episode_reward=1819.20 +/- 213.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=260000, episode_reward=1754.40 +/- 117.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=261000, episode_reward=1920.60 +/- 185.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=262000, episode_reward=1671.80 +/- 156.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=263000, episode_reward=1688.00 +/- 271.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=264000, episode_reward=1614.00 +/- 210.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=265000, episode_reward=1604.80 +/- 330.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=266000, episode_reward=1598.00 +/- 348.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=267000, episode_reward=1635.40 +/- 165.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=268000, episode_reward=1786.20 +/- 152.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=269000, episode_reward=1570.00 +/- 210.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=270000, episode_reward=1684.40 +/- 265.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=271000, episode_reward=1543.80 +/- 151.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=272000, episode_reward=1513.40 +/- 382.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=273000, episode_reward=1615.20 +/- 71.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=274000, episode_reward=1716.40 +/- 289.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=275000, episode_reward=1567.80 +/- 121.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=276000, episode_reward=1615.80 +/- 233.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=277000, episode_reward=1519.40 +/- 343.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=278000, episode_reward=1537.60 +/- 260.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=279000, episode_reward=1715.00 +/- 309.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=280000, episode_reward=1539.40 +/- 150.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=281000, episode_reward=1796.20 +/- 95.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=282000, episode_reward=1696.80 +/- 228.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=283000, episode_reward=1637.00 +/- 318.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=284000, episode_reward=1613.80 +/- 127.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=285000, episode_reward=1649.40 +/- 231.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=286000, episode_reward=1648.40 +/- 295.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=287000, episode_reward=1687.00 +/- 242.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=288000, episode_reward=1633.40 +/- 282.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=289000, episode_reward=1678.00 +/- 182.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=290000, episode_reward=1795.80 +/- 201.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=291000, episode_reward=1350.80 +/- 247.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=292000, episode_reward=1792.00 +/- 210.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=293000, episode_reward=1839.00 +/- 167.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=294000, episode_reward=1508.20 +/- 195.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=295000, episode_reward=1788.60 +/- 96.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=296000, episode_reward=1612.60 +/- 124.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=297000, episode_reward=1545.20 +/- 259.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=298000, episode_reward=1683.20 +/- 196.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=299000, episode_reward=1842.20 +/- 94.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=300000, episode_reward=1678.40 +/- 85.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=301000, episode_reward=1640.00 +/- 89.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=302000, episode_reward=1770.80 +/- 117.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=303000, episode_reward=1773.20 +/- 204.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=304000, episode_reward=1653.20 +/- 189.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=305000, episode_reward=1910.80 +/- 90.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=306000, episode_reward=1509.40 +/- 374.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=307000, episode_reward=1574.00 +/- 156.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=308000, episode_reward=1679.60 +/- 327.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=309000, episode_reward=1825.00 +/- 122.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=310000, episode_reward=1575.40 +/- 228.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=311000, episode_reward=1609.20 +/- 288.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=312000, episode_reward=1766.60 +/- 220.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=313000, episode_reward=1826.60 +/- 222.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=314000, episode_reward=1596.20 +/- 174.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=315000, episode_reward=1633.40 +/- 172.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=316000, episode_reward=1658.20 +/- 87.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=317000, episode_reward=1659.60 +/- 188.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=318000, episode_reward=1623.40 +/- 122.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=319000, episode_reward=1495.60 +/- 139.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=320000, episode_reward=1802.60 +/- 118.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=321000, episode_reward=1740.00 +/- 254.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=322000, episode_reward=1772.80 +/- 194.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=323000, episode_reward=1754.60 +/- 187.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=324000, episode_reward=1739.20 +/- 122.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=325000, episode_reward=1536.60 +/- 284.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=326000, episode_reward=1475.80 +/- 274.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=327000, episode_reward=1579.80 +/- 263.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=328000, episode_reward=1499.80 +/- 375.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=329000, episode_reward=1721.40 +/- 223.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=330000, episode_reward=1591.40 +/- 62.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=331000, episode_reward=1783.20 +/- 182.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=332000, episode_reward=1536.80 +/- 242.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=333000, episode_reward=1692.20 +/- 116.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=334000, episode_reward=1785.80 +/- 94.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=335000, episode_reward=1691.20 +/- 187.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=336000, episode_reward=1535.20 +/- 227.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=337000, episode_reward=1539.80 +/- 200.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=338000, episode_reward=1694.80 +/- 240.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=339000, episode_reward=1735.20 +/- 185.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=340000, episode_reward=1685.40 +/- 141.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=341000, episode_reward=1756.40 +/- 109.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=342000, episode_reward=1628.80 +/- 224.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=343000, episode_reward=1696.40 +/- 184.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=344000, episode_reward=1565.40 +/- 170.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=345000, episode_reward=1595.00 +/- 177.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=346000, episode_reward=1685.60 +/- 165.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=347000, episode_reward=1758.80 +/- 159.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=348000, episode_reward=1700.60 +/- 170.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=349000, episode_reward=1616.60 +/- 235.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=350000, episode_reward=1852.60 +/- 77.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=351000, episode_reward=1631.60 +/- 219.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=352000, episode_reward=1686.60 +/- 198.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=353000, episode_reward=1593.60 +/- 224.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=354000, episode_reward=1741.00 +/- 143.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=355000, episode_reward=1790.40 +/- 150.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=356000, episode_reward=1562.60 +/- 362.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=357000, episode_reward=1631.80 +/- 136.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=358000, episode_reward=1608.20 +/- 120.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=359000, episode_reward=1736.40 +/- 268.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=360000, episode_reward=1552.20 +/- 280.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=361000, episode_reward=1628.20 +/- 280.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=362000, episode_reward=1621.80 +/- 302.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=363000, episode_reward=1590.00 +/- 465.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=364000, episode_reward=1773.40 +/- 195.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=365000, episode_reward=1683.60 +/- 191.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=366000, episode_reward=1638.20 +/- 207.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=367000, episode_reward=1452.00 +/- 267.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=368000, episode_reward=1417.20 +/- 297.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=369000, episode_reward=1752.80 +/- 123.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=370000, episode_reward=1597.40 +/- 182.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=371000, episode_reward=1588.20 +/- 425.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=372000, episode_reward=1572.20 +/- 175.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=373000, episode_reward=1740.00 +/- 239.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=374000, episode_reward=1764.60 +/- 301.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=375000, episode_reward=1577.60 +/- 232.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=376000, episode_reward=1718.80 +/- 235.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=377000, episode_reward=1692.80 +/- 253.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=378000, episode_reward=1734.40 +/- 320.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=379000, episode_reward=1807.40 +/- 65.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=380000, episode_reward=1458.40 +/- 263.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=381000, episode_reward=1719.80 +/- 274.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=382000, episode_reward=1729.40 +/- 227.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=383000, episode_reward=1629.60 +/- 253.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=384000, episode_reward=1473.60 +/- 271.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=385000, episode_reward=1853.40 +/- 254.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=386000, episode_reward=1468.80 +/- 141.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=387000, episode_reward=1604.40 +/- 315.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=388000, episode_reward=1757.00 +/- 154.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=389000, episode_reward=1549.20 +/- 314.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=390000, episode_reward=1576.20 +/- 178.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=391000, episode_reward=1467.00 +/- 207.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=392000, episode_reward=1490.80 +/- 208.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=393000, episode_reward=1588.20 +/- 296.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=394000, episode_reward=1662.60 +/- 207.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=395000, episode_reward=1712.80 +/- 242.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=396000, episode_reward=1644.80 +/- 306.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=397000, episode_reward=1637.20 +/- 363.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=398000, episode_reward=1603.40 +/- 81.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=399000, episode_reward=1624.40 +/- 247.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=400000, episode_reward=1642.60 +/- 185.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=401000, episode_reward=1614.60 +/- 238.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=402000, episode_reward=1845.60 +/- 123.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=403000, episode_reward=1334.40 +/- 283.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=404000, episode_reward=1518.00 +/- 351.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=405000, episode_reward=1689.60 +/- 194.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=406000, episode_reward=1628.00 +/- 247.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=407000, episode_reward=1731.00 +/- 162.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=408000, episode_reward=1775.60 +/- 128.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=409000, episode_reward=1690.60 +/- 118.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=410000, episode_reward=1405.80 +/- 341.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=411000, episode_reward=1583.20 +/- 265.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=412000, episode_reward=1704.80 +/- 151.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=413000, episode_reward=1707.40 +/- 241.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=414000, episode_reward=1671.80 +/- 142.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=415000, episode_reward=1614.40 +/- 133.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=416000, episode_reward=1803.20 +/- 157.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=417000, episode_reward=1788.80 +/- 198.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=418000, episode_reward=1455.60 +/- 231.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=419000, episode_reward=1613.40 +/- 82.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=420000, episode_reward=1708.40 +/- 210.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=421000, episode_reward=1659.60 +/- 204.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=422000, episode_reward=1526.20 +/- 283.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=423000, episode_reward=1724.00 +/- 144.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=424000, episode_reward=1636.20 +/- 160.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=425000, episode_reward=1679.40 +/- 335.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=426000, episode_reward=1686.60 +/- 208.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=427000, episode_reward=1761.60 +/- 133.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=428000, episode_reward=1703.00 +/- 107.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=429000, episode_reward=1713.40 +/- 246.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=430000, episode_reward=1815.60 +/- 45.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=431000, episode_reward=1526.00 +/- 147.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=432000, episode_reward=1815.20 +/- 292.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=433000, episode_reward=1874.40 +/- 154.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=434000, episode_reward=1772.80 +/- 37.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=435000, episode_reward=1460.40 +/- 381.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=436000, episode_reward=1487.80 +/- 291.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=437000, episode_reward=1711.60 +/- 263.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=438000, episode_reward=1564.00 +/- 237.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=439000, episode_reward=1786.00 +/- 164.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=440000, episode_reward=1719.20 +/- 266.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=441000, episode_reward=1457.80 +/- 191.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=442000, episode_reward=1703.00 +/- 198.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=443000, episode_reward=1855.20 +/- 198.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=444000, episode_reward=1615.00 +/- 259.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=445000, episode_reward=1581.40 +/- 279.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=446000, episode_reward=1463.40 +/- 142.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=447000, episode_reward=1475.00 +/- 220.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=448000, episode_reward=1830.80 +/- 115.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=449000, episode_reward=1645.20 +/- 338.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=450000, episode_reward=1393.60 +/- 293.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=451000, episode_reward=1650.00 +/- 215.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=452000, episode_reward=1533.40 +/- 239.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=453000, episode_reward=1583.00 +/- 145.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=454000, episode_reward=1407.40 +/- 424.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=455000, episode_reward=1604.40 +/- 378.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=456000, episode_reward=1624.00 +/- 258.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=457000, episode_reward=1680.00 +/- 144.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=458000, episode_reward=1603.80 +/- 295.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=459000, episode_reward=1745.40 +/- 207.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=460000, episode_reward=1762.80 +/- 268.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=461000, episode_reward=1649.40 +/- 176.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=462000, episode_reward=1492.00 +/- 180.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=463000, episode_reward=1569.80 +/- 301.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=464000, episode_reward=1543.60 +/- 188.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=465000, episode_reward=1605.00 +/- 269.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=466000, episode_reward=1566.20 +/- 73.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=467000, episode_reward=1720.40 +/- 317.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=468000, episode_reward=1488.80 +/- 253.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=469000, episode_reward=1674.80 +/- 168.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=470000, episode_reward=1755.00 +/- 363.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=471000, episode_reward=1397.40 +/- 314.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=472000, episode_reward=1649.00 +/- 133.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=473000, episode_reward=1518.40 +/- 344.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=474000, episode_reward=1680.80 +/- 277.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=475000, episode_reward=1633.60 +/- 203.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=476000, episode_reward=1764.60 +/- 156.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=477000, episode_reward=1564.80 +/- 455.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=478000, episode_reward=1712.20 +/- 193.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=479000, episode_reward=1618.20 +/- 206.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=480000, episode_reward=1670.20 +/- 301.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=481000, episode_reward=1533.00 +/- 168.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=482000, episode_reward=1680.40 +/- 272.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=483000, episode_reward=1770.40 +/- 199.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=484000, episode_reward=1627.20 +/- 257.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=485000, episode_reward=2038.20 +/- 148.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=486000, episode_reward=1590.00 +/- 131.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=487000, episode_reward=1554.20 +/- 177.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=488000, episode_reward=1671.00 +/- 184.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=489000, episode_reward=1608.00 +/- 101.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=490000, episode_reward=1757.60 +/- 311.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=491000, episode_reward=1759.40 +/- 254.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=492000, episode_reward=1751.40 +/- 212.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=493000, episode_reward=1561.20 +/- 230.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=494000, episode_reward=1740.00 +/- 150.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=495000, episode_reward=1476.60 +/- 158.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=496000, episode_reward=1804.20 +/- 231.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=497000, episode_reward=1507.20 +/- 250.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=498000, episode_reward=1670.80 +/- 172.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=499000, episode_reward=1778.00 +/- 409.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=500000, episode_reward=1702.00 +/- 120.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=501000, episode_reward=1586.00 +/- 174.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Gamma = 0.9, Learning Rate = 0.003, Activation Function = ReLU, Neurons = 32, Process noise = 0.1, Measurement noise = 0.0\n",
      "Eval num_timesteps=1000, episode_reward=-367.20 +/- 52.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=-226.40 +/- 191.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3000, episode_reward=-203.40 +/- 168.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4000, episode_reward=-132.60 +/- 223.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=5000, episode_reward=283.00 +/- 208.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=6000, episode_reward=488.60 +/- 58.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=7000, episode_reward=360.80 +/- 364.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=8000, episode_reward=508.00 +/- 318.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=9000, episode_reward=312.60 +/- 110.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=10000, episode_reward=176.40 +/- 151.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=11000, episode_reward=420.60 +/- 192.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=12000, episode_reward=377.20 +/- 157.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=13000, episode_reward=254.60 +/- 208.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=14000, episode_reward=271.00 +/- 229.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=15000, episode_reward=185.60 +/- 224.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=16000, episode_reward=196.80 +/- 133.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=17000, episode_reward=405.60 +/- 134.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=18000, episode_reward=336.00 +/- 347.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=19000, episode_reward=406.20 +/- 252.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=20000, episode_reward=304.20 +/- 111.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=21000, episode_reward=699.20 +/- 348.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=22000, episode_reward=746.60 +/- 490.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=23000, episode_reward=569.20 +/- 415.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=24000, episode_reward=982.40 +/- 245.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=25000, episode_reward=788.60 +/- 284.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=26000, episode_reward=974.80 +/- 249.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=27000, episode_reward=926.60 +/- 311.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=28000, episode_reward=859.80 +/- 143.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=29000, episode_reward=1112.80 +/- 405.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=30000, episode_reward=1166.80 +/- 360.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=31000, episode_reward=-6194.00 +/- 8865.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=32000, episode_reward=1138.00 +/- 189.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=33000, episode_reward=1273.60 +/- 226.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=34000, episode_reward=1336.80 +/- 355.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=35000, episode_reward=1223.40 +/- 82.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=36000, episode_reward=1103.40 +/- 191.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=37000, episode_reward=1211.40 +/- 304.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=38000, episode_reward=1183.60 +/- 243.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=39000, episode_reward=1375.20 +/- 143.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=921.20 +/- 225.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=41000, episode_reward=1602.60 +/- 226.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=42000, episode_reward=1410.00 +/- 265.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=43000, episode_reward=1352.60 +/- 212.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=44000, episode_reward=1390.20 +/- 163.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=45000, episode_reward=1130.60 +/- 251.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=46000, episode_reward=1167.80 +/- 402.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=47000, episode_reward=1471.00 +/- 230.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=48000, episode_reward=1570.80 +/- 277.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=49000, episode_reward=1409.00 +/- 419.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=50000, episode_reward=1559.00 +/- 126.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=51000, episode_reward=1460.20 +/- 261.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=52000, episode_reward=1466.40 +/- 241.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=53000, episode_reward=1337.00 +/- 275.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=54000, episode_reward=1445.20 +/- 153.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=55000, episode_reward=1744.60 +/- 111.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=56000, episode_reward=1554.40 +/- 171.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=57000, episode_reward=1434.80 +/- 128.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=58000, episode_reward=1666.80 +/- 133.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=59000, episode_reward=1466.20 +/- 295.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=60000, episode_reward=1368.40 +/- 148.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=61000, episode_reward=1502.20 +/- 421.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=62000, episode_reward=1718.80 +/- 181.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=63000, episode_reward=1621.00 +/- 227.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=64000, episode_reward=1573.60 +/- 184.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=65000, episode_reward=1239.80 +/- 148.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=66000, episode_reward=1781.40 +/- 94.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=67000, episode_reward=1492.00 +/- 215.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=68000, episode_reward=1422.00 +/- 292.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=69000, episode_reward=1640.00 +/- 183.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=70000, episode_reward=1664.00 +/- 126.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=71000, episode_reward=1302.00 +/- 266.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=72000, episode_reward=1609.40 +/- 182.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=73000, episode_reward=1580.40 +/- 272.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=74000, episode_reward=1486.20 +/- 81.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=75000, episode_reward=1389.40 +/- 133.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=76000, episode_reward=1710.00 +/- 263.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=77000, episode_reward=1661.00 +/- 83.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=78000, episode_reward=1593.20 +/- 112.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=79000, episode_reward=1466.60 +/- 229.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=80000, episode_reward=1526.20 +/- 205.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=81000, episode_reward=1658.40 +/- 206.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=82000, episode_reward=1496.00 +/- 178.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=83000, episode_reward=1562.20 +/- 136.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=84000, episode_reward=1555.60 +/- 279.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=85000, episode_reward=1638.00 +/- 226.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=86000, episode_reward=1424.60 +/- 316.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=87000, episode_reward=1501.20 +/- 218.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=88000, episode_reward=1385.00 +/- 96.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=89000, episode_reward=1473.00 +/- 253.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=90000, episode_reward=1565.20 +/- 151.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=91000, episode_reward=1545.40 +/- 279.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=92000, episode_reward=1590.60 +/- 263.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=93000, episode_reward=1757.40 +/- 325.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=94000, episode_reward=1501.00 +/- 209.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=95000, episode_reward=1483.80 +/- 345.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=96000, episode_reward=1531.60 +/- 224.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=97000, episode_reward=1560.00 +/- 291.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=98000, episode_reward=1754.80 +/- 82.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=99000, episode_reward=1571.80 +/- 309.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=100000, episode_reward=1441.80 +/- 227.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=101000, episode_reward=1666.80 +/- 315.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=102000, episode_reward=1558.00 +/- 228.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=103000, episode_reward=1242.60 +/- 66.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=104000, episode_reward=1576.40 +/- 135.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=105000, episode_reward=1814.40 +/- 161.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=106000, episode_reward=1542.20 +/- 266.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=107000, episode_reward=1703.40 +/- 233.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=108000, episode_reward=1618.60 +/- 278.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=109000, episode_reward=1573.40 +/- 262.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=110000, episode_reward=1588.00 +/- 283.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=111000, episode_reward=1467.80 +/- 192.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=112000, episode_reward=1629.00 +/- 223.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=113000, episode_reward=1485.60 +/- 104.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=114000, episode_reward=1531.20 +/- 275.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=115000, episode_reward=1871.60 +/- 98.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=116000, episode_reward=1540.00 +/- 54.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=117000, episode_reward=1586.20 +/- 263.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=118000, episode_reward=1486.60 +/- 126.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=119000, episode_reward=1548.20 +/- 416.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=120000, episode_reward=1487.00 +/- 189.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=121000, episode_reward=1653.60 +/- 117.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=122000, episode_reward=1711.60 +/- 183.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=123000, episode_reward=1647.40 +/- 192.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=124000, episode_reward=1541.60 +/- 170.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=125000, episode_reward=1657.40 +/- 139.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=126000, episode_reward=1810.80 +/- 115.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=127000, episode_reward=1758.20 +/- 71.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=128000, episode_reward=1578.80 +/- 210.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=129000, episode_reward=1510.20 +/- 102.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=130000, episode_reward=1505.60 +/- 225.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=131000, episode_reward=1555.20 +/- 226.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=132000, episode_reward=1526.60 +/- 90.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=133000, episode_reward=1503.00 +/- 179.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=134000, episode_reward=1389.20 +/- 312.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=135000, episode_reward=1755.40 +/- 197.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=136000, episode_reward=1736.00 +/- 157.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=137000, episode_reward=1714.00 +/- 150.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=138000, episode_reward=1463.60 +/- 183.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=139000, episode_reward=1790.40 +/- 303.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=140000, episode_reward=1677.00 +/- 186.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=141000, episode_reward=1779.40 +/- 112.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=142000, episode_reward=1631.80 +/- 271.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=143000, episode_reward=1786.80 +/- 108.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=144000, episode_reward=1550.20 +/- 198.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=145000, episode_reward=1410.80 +/- 218.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=146000, episode_reward=1792.00 +/- 86.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=147000, episode_reward=1674.20 +/- 208.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=148000, episode_reward=1606.40 +/- 150.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=149000, episode_reward=1530.20 +/- 141.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=150000, episode_reward=1616.40 +/- 251.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=151000, episode_reward=1637.60 +/- 228.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=152000, episode_reward=1727.00 +/- 131.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=153000, episode_reward=1460.80 +/- 161.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=154000, episode_reward=1575.20 +/- 115.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=155000, episode_reward=1719.40 +/- 143.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=156000, episode_reward=1707.80 +/- 237.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=157000, episode_reward=1523.20 +/- 141.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=158000, episode_reward=1636.00 +/- 170.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=159000, episode_reward=1661.00 +/- 228.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=160000, episode_reward=1717.60 +/- 203.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=161000, episode_reward=1691.40 +/- 113.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=162000, episode_reward=1575.80 +/- 216.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=163000, episode_reward=1877.00 +/- 130.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=164000, episode_reward=1689.80 +/- 192.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=165000, episode_reward=1560.80 +/- 87.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=166000, episode_reward=1668.20 +/- 145.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=167000, episode_reward=1679.20 +/- 183.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=168000, episode_reward=1843.60 +/- 100.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=169000, episode_reward=1722.60 +/- 209.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=170000, episode_reward=1779.80 +/- 60.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=171000, episode_reward=1716.20 +/- 219.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=172000, episode_reward=1695.80 +/- 342.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=173000, episode_reward=1683.80 +/- 276.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=174000, episode_reward=1665.40 +/- 251.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=175000, episode_reward=1672.60 +/- 189.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=176000, episode_reward=1465.20 +/- 188.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=177000, episode_reward=1692.20 +/- 258.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=178000, episode_reward=1786.40 +/- 84.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=179000, episode_reward=1484.80 +/- 139.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=180000, episode_reward=1576.60 +/- 161.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=181000, episode_reward=1906.00 +/- 51.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=182000, episode_reward=1536.00 +/- 149.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=183000, episode_reward=1681.40 +/- 239.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=184000, episode_reward=1785.20 +/- 132.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=185000, episode_reward=1758.40 +/- 125.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=186000, episode_reward=1606.60 +/- 235.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=187000, episode_reward=1474.00 +/- 133.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=188000, episode_reward=1617.60 +/- 58.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=189000, episode_reward=1609.80 +/- 245.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=190000, episode_reward=1600.20 +/- 316.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=191000, episode_reward=1700.00 +/- 210.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=192000, episode_reward=1744.20 +/- 160.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=193000, episode_reward=1606.00 +/- 250.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=194000, episode_reward=1586.20 +/- 365.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=195000, episode_reward=1539.00 +/- 124.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=196000, episode_reward=1786.60 +/- 126.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=197000, episode_reward=1830.60 +/- 87.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=198000, episode_reward=1724.80 +/- 140.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=199000, episode_reward=1591.80 +/- 223.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=200000, episode_reward=1661.00 +/- 212.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=201000, episode_reward=1612.60 +/- 265.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=202000, episode_reward=1676.20 +/- 174.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=203000, episode_reward=1678.00 +/- 169.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=204000, episode_reward=1692.80 +/- 233.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=205000, episode_reward=1700.20 +/- 243.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=206000, episode_reward=1717.80 +/- 78.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=207000, episode_reward=1546.80 +/- 151.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=208000, episode_reward=1682.60 +/- 238.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=209000, episode_reward=1586.80 +/- 118.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=210000, episode_reward=1609.40 +/- 286.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=211000, episode_reward=1763.60 +/- 248.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=212000, episode_reward=1507.80 +/- 141.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=213000, episode_reward=1890.20 +/- 169.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=214000, episode_reward=1728.60 +/- 161.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=215000, episode_reward=1794.60 +/- 114.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=216000, episode_reward=1811.00 +/- 64.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=217000, episode_reward=1513.80 +/- 189.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=218000, episode_reward=1831.20 +/- 89.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=219000, episode_reward=1549.80 +/- 209.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=220000, episode_reward=1789.60 +/- 127.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=221000, episode_reward=1740.20 +/- 238.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=222000, episode_reward=1677.80 +/- 211.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=223000, episode_reward=1444.20 +/- 216.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=224000, episode_reward=1720.40 +/- 264.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=225000, episode_reward=1534.80 +/- 168.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=226000, episode_reward=1604.00 +/- 169.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=227000, episode_reward=1616.80 +/- 206.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=228000, episode_reward=1674.80 +/- 260.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=229000, episode_reward=1656.00 +/- 262.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=230000, episode_reward=1490.20 +/- 268.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=231000, episode_reward=1753.60 +/- 181.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=232000, episode_reward=1560.40 +/- 163.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=233000, episode_reward=1804.20 +/- 176.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=234000, episode_reward=1661.60 +/- 297.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=235000, episode_reward=1707.60 +/- 311.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=236000, episode_reward=1803.00 +/- 225.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=237000, episode_reward=1625.00 +/- 131.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=238000, episode_reward=1784.80 +/- 69.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=239000, episode_reward=1718.80 +/- 300.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=240000, episode_reward=1826.00 +/- 137.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=241000, episode_reward=1581.80 +/- 192.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=242000, episode_reward=1581.00 +/- 117.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=243000, episode_reward=1674.40 +/- 119.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=244000, episode_reward=1501.60 +/- 284.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=245000, episode_reward=1543.00 +/- 293.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=246000, episode_reward=1620.80 +/- 372.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=247000, episode_reward=1649.40 +/- 152.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=248000, episode_reward=1710.40 +/- 124.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=249000, episode_reward=1671.40 +/- 205.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=250000, episode_reward=1632.00 +/- 96.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=251000, episode_reward=1686.40 +/- 268.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=252000, episode_reward=1711.60 +/- 217.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=253000, episode_reward=1559.00 +/- 374.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=254000, episode_reward=1711.60 +/- 151.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=255000, episode_reward=1629.60 +/- 104.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=256000, episode_reward=1657.60 +/- 217.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=257000, episode_reward=1736.20 +/- 77.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=258000, episode_reward=1577.20 +/- 91.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=259000, episode_reward=1769.20 +/- 178.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=260000, episode_reward=1602.60 +/- 116.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=261000, episode_reward=1326.20 +/- 103.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=262000, episode_reward=1644.40 +/- 127.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=263000, episode_reward=1558.20 +/- 238.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=264000, episode_reward=1507.20 +/- 163.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=265000, episode_reward=1516.20 +/- 226.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=266000, episode_reward=1728.20 +/- 112.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=267000, episode_reward=1739.00 +/- 188.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=268000, episode_reward=1771.00 +/- 176.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=269000, episode_reward=1588.80 +/- 179.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=270000, episode_reward=1695.20 +/- 113.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=271000, episode_reward=1493.00 +/- 161.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=272000, episode_reward=1756.00 +/- 104.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=273000, episode_reward=1755.60 +/- 205.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=274000, episode_reward=1845.40 +/- 172.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=275000, episode_reward=1635.60 +/- 180.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=276000, episode_reward=1410.00 +/- 129.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=277000, episode_reward=1586.00 +/- 220.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=278000, episode_reward=1398.80 +/- 368.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=279000, episode_reward=1644.20 +/- 78.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=280000, episode_reward=1776.40 +/- 130.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=281000, episode_reward=1476.40 +/- 270.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=282000, episode_reward=1602.60 +/- 226.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=283000, episode_reward=1794.40 +/- 161.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=284000, episode_reward=1572.40 +/- 138.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=285000, episode_reward=1746.60 +/- 206.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=286000, episode_reward=1848.20 +/- 87.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=287000, episode_reward=1633.20 +/- 255.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=288000, episode_reward=1645.20 +/- 140.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=289000, episode_reward=1814.60 +/- 206.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=290000, episode_reward=1729.20 +/- 91.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=291000, episode_reward=1521.20 +/- 108.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=292000, episode_reward=1543.00 +/- 281.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=293000, episode_reward=1465.60 +/- 410.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=294000, episode_reward=1719.20 +/- 240.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=295000, episode_reward=1690.80 +/- 204.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=296000, episode_reward=1679.80 +/- 136.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=297000, episode_reward=1341.20 +/- 117.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=298000, episode_reward=1811.80 +/- 157.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=299000, episode_reward=1532.60 +/- 141.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=300000, episode_reward=1525.00 +/- 214.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=301000, episode_reward=1521.20 +/- 268.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=302000, episode_reward=1789.60 +/- 139.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=303000, episode_reward=1670.00 +/- 197.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=304000, episode_reward=1380.40 +/- 394.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=305000, episode_reward=1684.80 +/- 150.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=306000, episode_reward=1591.60 +/- 174.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=307000, episode_reward=1827.40 +/- 79.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=308000, episode_reward=1565.60 +/- 176.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=309000, episode_reward=1615.80 +/- 247.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=310000, episode_reward=1555.00 +/- 193.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=311000, episode_reward=1531.00 +/- 202.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=312000, episode_reward=1597.40 +/- 235.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=313000, episode_reward=1687.00 +/- 221.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=314000, episode_reward=1808.80 +/- 105.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=315000, episode_reward=1562.20 +/- 371.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=316000, episode_reward=1507.00 +/- 170.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=317000, episode_reward=1702.80 +/- 204.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=318000, episode_reward=1845.20 +/- 258.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=319000, episode_reward=1722.00 +/- 284.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=320000, episode_reward=1807.60 +/- 214.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=321000, episode_reward=1828.40 +/- 126.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=322000, episode_reward=1673.40 +/- 87.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=323000, episode_reward=1637.00 +/- 156.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=324000, episode_reward=1746.80 +/- 105.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=325000, episode_reward=1686.20 +/- 148.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=326000, episode_reward=1674.60 +/- 87.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=327000, episode_reward=1732.40 +/- 118.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=328000, episode_reward=1615.20 +/- 207.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=329000, episode_reward=1747.80 +/- 100.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=330000, episode_reward=1572.80 +/- 109.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=331000, episode_reward=1626.20 +/- 138.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=332000, episode_reward=1813.60 +/- 104.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=333000, episode_reward=1613.00 +/- 176.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=334000, episode_reward=1732.60 +/- 61.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=335000, episode_reward=1729.60 +/- 172.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=336000, episode_reward=1835.60 +/- 132.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=337000, episode_reward=1681.00 +/- 130.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=338000, episode_reward=1750.60 +/- 177.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=339000, episode_reward=1697.40 +/- 137.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=340000, episode_reward=1741.80 +/- 185.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=341000, episode_reward=1871.40 +/- 120.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=342000, episode_reward=1812.40 +/- 178.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=343000, episode_reward=1713.40 +/- 162.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=344000, episode_reward=1633.40 +/- 189.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=345000, episode_reward=1727.60 +/- 131.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=346000, episode_reward=1626.60 +/- 67.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=347000, episode_reward=1585.40 +/- 210.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=348000, episode_reward=1822.60 +/- 168.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=349000, episode_reward=1729.00 +/- 139.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=350000, episode_reward=1613.00 +/- 254.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=351000, episode_reward=1736.20 +/- 176.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=352000, episode_reward=1530.60 +/- 84.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=353000, episode_reward=1695.00 +/- 47.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=354000, episode_reward=1612.20 +/- 234.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=355000, episode_reward=1565.00 +/- 200.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=356000, episode_reward=1543.20 +/- 366.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=357000, episode_reward=1690.00 +/- 165.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=358000, episode_reward=1417.60 +/- 298.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=359000, episode_reward=1504.20 +/- 317.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=360000, episode_reward=1693.40 +/- 110.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=361000, episode_reward=1717.80 +/- 144.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=362000, episode_reward=1807.40 +/- 98.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=363000, episode_reward=1704.60 +/- 164.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=364000, episode_reward=1696.80 +/- 234.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=365000, episode_reward=1682.40 +/- 168.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=366000, episode_reward=1663.80 +/- 83.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=367000, episode_reward=1445.20 +/- 360.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=368000, episode_reward=1729.80 +/- 187.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=369000, episode_reward=1741.60 +/- 124.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=370000, episode_reward=1829.40 +/- 137.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=371000, episode_reward=1757.00 +/- 202.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=372000, episode_reward=1650.40 +/- 221.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=373000, episode_reward=1829.40 +/- 158.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=374000, episode_reward=1595.20 +/- 181.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=375000, episode_reward=1562.80 +/- 285.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=376000, episode_reward=1649.20 +/- 194.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=377000, episode_reward=1705.00 +/- 79.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=378000, episode_reward=1672.60 +/- 139.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=379000, episode_reward=1770.40 +/- 210.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=380000, episode_reward=1703.80 +/- 250.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=381000, episode_reward=1826.40 +/- 59.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=382000, episode_reward=1467.80 +/- 222.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=383000, episode_reward=1721.20 +/- 97.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=384000, episode_reward=1594.40 +/- 183.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=385000, episode_reward=1588.40 +/- 167.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=386000, episode_reward=1600.60 +/- 160.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=387000, episode_reward=1830.60 +/- 137.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=388000, episode_reward=1600.40 +/- 221.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=389000, episode_reward=1708.80 +/- 137.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=390000, episode_reward=1745.60 +/- 193.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=391000, episode_reward=1650.40 +/- 169.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=392000, episode_reward=1643.00 +/- 162.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=393000, episode_reward=1620.80 +/- 152.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=394000, episode_reward=1586.40 +/- 206.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=395000, episode_reward=1717.00 +/- 174.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=396000, episode_reward=1653.00 +/- 148.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=397000, episode_reward=1632.20 +/- 324.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=398000, episode_reward=1655.40 +/- 131.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=399000, episode_reward=1761.60 +/- 135.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=400000, episode_reward=1575.40 +/- 177.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=401000, episode_reward=1615.60 +/- 166.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=402000, episode_reward=1614.40 +/- 233.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=403000, episode_reward=1746.40 +/- 148.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=404000, episode_reward=1679.60 +/- 260.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=405000, episode_reward=1800.60 +/- 159.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=406000, episode_reward=1802.20 +/- 100.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=407000, episode_reward=1662.80 +/- 241.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=408000, episode_reward=1822.40 +/- 113.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=409000, episode_reward=1643.80 +/- 315.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=410000, episode_reward=1647.40 +/- 151.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=411000, episode_reward=1533.60 +/- 167.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=412000, episode_reward=1600.40 +/- 299.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=413000, episode_reward=1724.20 +/- 205.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=414000, episode_reward=1742.80 +/- 87.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=415000, episode_reward=1683.60 +/- 279.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=416000, episode_reward=1653.40 +/- 191.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=417000, episode_reward=1671.60 +/- 200.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=418000, episode_reward=1646.80 +/- 210.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=419000, episode_reward=1679.40 +/- 174.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=420000, episode_reward=1642.80 +/- 160.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=421000, episode_reward=1713.00 +/- 79.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=422000, episode_reward=1664.60 +/- 210.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=423000, episode_reward=1563.20 +/- 308.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=424000, episode_reward=1713.00 +/- 247.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=425000, episode_reward=1707.40 +/- 236.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=426000, episode_reward=1700.40 +/- 178.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=427000, episode_reward=1601.20 +/- 251.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=428000, episode_reward=1705.40 +/- 183.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=429000, episode_reward=1591.60 +/- 164.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=430000, episode_reward=1727.00 +/- 290.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=431000, episode_reward=1572.40 +/- 214.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=432000, episode_reward=1658.00 +/- 172.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=433000, episode_reward=1688.40 +/- 383.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=434000, episode_reward=1702.60 +/- 179.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=435000, episode_reward=1776.40 +/- 198.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=436000, episode_reward=1691.40 +/- 178.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=437000, episode_reward=1622.20 +/- 204.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=438000, episode_reward=1438.80 +/- 221.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=439000, episode_reward=1569.20 +/- 213.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=440000, episode_reward=1566.00 +/- 146.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=441000, episode_reward=1695.00 +/- 224.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=442000, episode_reward=1600.20 +/- 376.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=443000, episode_reward=1572.80 +/- 210.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=444000, episode_reward=1648.20 +/- 126.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=445000, episode_reward=1631.80 +/- 168.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=446000, episode_reward=1745.60 +/- 230.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=447000, episode_reward=1664.80 +/- 237.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=448000, episode_reward=1798.60 +/- 223.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=449000, episode_reward=1618.40 +/- 127.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=450000, episode_reward=1605.20 +/- 181.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=451000, episode_reward=1495.40 +/- 186.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=452000, episode_reward=1586.60 +/- 197.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=453000, episode_reward=1699.40 +/- 263.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=454000, episode_reward=1654.80 +/- 237.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=455000, episode_reward=1543.60 +/- 205.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=456000, episode_reward=1595.00 +/- 201.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=457000, episode_reward=1807.20 +/- 107.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=458000, episode_reward=1721.40 +/- 190.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=459000, episode_reward=1492.80 +/- 241.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=460000, episode_reward=1676.00 +/- 94.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=461000, episode_reward=1496.20 +/- 216.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=462000, episode_reward=1674.60 +/- 349.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=463000, episode_reward=1751.20 +/- 191.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=464000, episode_reward=1587.60 +/- 167.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=465000, episode_reward=1552.60 +/- 215.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=466000, episode_reward=1568.00 +/- 247.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=467000, episode_reward=1621.20 +/- 233.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=468000, episode_reward=1604.00 +/- 172.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=469000, episode_reward=1803.20 +/- 160.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=470000, episode_reward=1773.60 +/- 131.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=471000, episode_reward=1672.40 +/- 244.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=472000, episode_reward=1576.00 +/- 203.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=473000, episode_reward=1514.60 +/- 137.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=474000, episode_reward=1682.20 +/- 163.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=475000, episode_reward=1771.40 +/- 210.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=476000, episode_reward=1608.20 +/- 245.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=477000, episode_reward=1542.00 +/- 261.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=478000, episode_reward=1790.20 +/- 237.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=479000, episode_reward=1507.40 +/- 172.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=480000, episode_reward=1663.40 +/- 260.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=481000, episode_reward=1778.20 +/- 183.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=482000, episode_reward=1718.80 +/- 105.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=483000, episode_reward=1635.00 +/- 231.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=484000, episode_reward=1724.60 +/- 170.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=485000, episode_reward=1593.80 +/- 204.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=486000, episode_reward=1626.60 +/- 238.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=487000, episode_reward=1670.00 +/- 270.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=488000, episode_reward=1791.40 +/- 134.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=489000, episode_reward=1594.00 +/- 217.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=490000, episode_reward=1843.60 +/- 180.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=491000, episode_reward=1764.00 +/- 136.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=492000, episode_reward=1875.40 +/- 84.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=493000, episode_reward=1591.80 +/- 68.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=494000, episode_reward=1460.20 +/- 278.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=495000, episode_reward=1789.80 +/- 164.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=496000, episode_reward=1831.80 +/- 108.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=497000, episode_reward=1574.20 +/- 279.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=498000, episode_reward=1361.60 +/- 377.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=499000, episode_reward=1830.60 +/- 165.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=500000, episode_reward=1684.20 +/- 302.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=501000, episode_reward=1539.40 +/- 231.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Gamma = 0.9, Learning Rate = 0.003, Activation Function = ReLU, Neurons = 64, Process noise = 0.1, Measurement noise = 0.0\n",
      "Eval num_timesteps=1000, episode_reward=-1287.80 +/- 381.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=-1836.00 +/- 610.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=3000, episode_reward=344.20 +/- 135.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4000, episode_reward=184.60 +/- 238.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=5000, episode_reward=-648.20 +/- 159.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=6000, episode_reward=-571.20 +/- 232.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=7000, episode_reward=371.40 +/- 276.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=8000, episode_reward=338.20 +/- 323.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=9000, episode_reward=123.60 +/- 227.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=10000, episode_reward=273.20 +/- 213.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=11000, episode_reward=427.80 +/- 269.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=12000, episode_reward=511.20 +/- 232.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=13000, episode_reward=268.60 +/- 232.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=14000, episode_reward=556.80 +/- 164.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=15000, episode_reward=477.80 +/- 166.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=16000, episode_reward=446.00 +/- 258.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=17000, episode_reward=507.40 +/- 237.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=18000, episode_reward=492.80 +/- 101.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=19000, episode_reward=439.00 +/- 168.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=20000, episode_reward=513.00 +/- 252.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=21000, episode_reward=912.00 +/- 313.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=22000, episode_reward=1063.00 +/- 332.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=23000, episode_reward=1058.60 +/- 268.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=24000, episode_reward=1406.60 +/- 180.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=25000, episode_reward=1377.00 +/- 160.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=26000, episode_reward=1360.40 +/- 157.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=27000, episode_reward=-1221.80 +/- 4804.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=28000, episode_reward=1178.40 +/- 223.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=29000, episode_reward=1211.00 +/- 313.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=30000, episode_reward=1336.00 +/- 218.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=31000, episode_reward=1097.00 +/- 166.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=32000, episode_reward=1185.00 +/- 208.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=33000, episode_reward=1238.20 +/- 298.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=34000, episode_reward=1358.00 +/- 203.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=35000, episode_reward=1370.40 +/- 252.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=36000, episode_reward=1264.80 +/- 315.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=37000, episode_reward=1143.20 +/- 74.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=38000, episode_reward=1370.80 +/- 150.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=39000, episode_reward=1537.40 +/- 150.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=1503.20 +/- 382.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=41000, episode_reward=1408.00 +/- 187.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=42000, episode_reward=1531.00 +/- 186.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=43000, episode_reward=1565.40 +/- 189.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=44000, episode_reward=1583.00 +/- 99.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=45000, episode_reward=826.80 +/- 1312.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=46000, episode_reward=1442.20 +/- 313.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=47000, episode_reward=1588.60 +/- 194.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=48000, episode_reward=1660.60 +/- 266.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=49000, episode_reward=1696.40 +/- 195.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=50000, episode_reward=1642.00 +/- 131.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=51000, episode_reward=1641.60 +/- 212.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=52000, episode_reward=1646.00 +/- 102.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=53000, episode_reward=1572.60 +/- 299.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=54000, episode_reward=1564.40 +/- 16.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=55000, episode_reward=1634.80 +/- 258.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=56000, episode_reward=1419.80 +/- 166.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=57000, episode_reward=1655.40 +/- 183.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=58000, episode_reward=1565.20 +/- 135.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=59000, episode_reward=1208.80 +/- 301.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=60000, episode_reward=1557.20 +/- 286.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=61000, episode_reward=1438.60 +/- 116.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=62000, episode_reward=1637.20 +/- 295.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=63000, episode_reward=1595.60 +/- 263.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=64000, episode_reward=1597.80 +/- 125.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=65000, episode_reward=1518.00 +/- 63.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=66000, episode_reward=1558.00 +/- 153.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=67000, episode_reward=1545.40 +/- 329.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=68000, episode_reward=1645.40 +/- 134.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=69000, episode_reward=1656.00 +/- 181.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=70000, episode_reward=1499.80 +/- 194.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=71000, episode_reward=1606.60 +/- 109.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=72000, episode_reward=1692.40 +/- 224.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=73000, episode_reward=1326.60 +/- 223.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=74000, episode_reward=1525.80 +/- 177.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=75000, episode_reward=1575.20 +/- 155.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=76000, episode_reward=1577.20 +/- 309.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=77000, episode_reward=1620.40 +/- 270.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=78000, episode_reward=1756.40 +/- 252.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=79000, episode_reward=1355.60 +/- 195.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=80000, episode_reward=1580.00 +/- 53.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=81000, episode_reward=1897.60 +/- 130.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=82000, episode_reward=1658.60 +/- 280.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=83000, episode_reward=1794.60 +/- 149.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=84000, episode_reward=1661.80 +/- 271.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=85000, episode_reward=1480.60 +/- 197.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=86000, episode_reward=1649.40 +/- 210.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=87000, episode_reward=1657.80 +/- 309.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=88000, episode_reward=1619.60 +/- 195.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=89000, episode_reward=1432.80 +/- 292.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=90000, episode_reward=1510.60 +/- 321.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=91000, episode_reward=1485.20 +/- 211.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=92000, episode_reward=1738.80 +/- 228.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=93000, episode_reward=1603.40 +/- 231.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=94000, episode_reward=1624.00 +/- 169.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=95000, episode_reward=1423.80 +/- 374.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=96000, episode_reward=1730.40 +/- 122.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=97000, episode_reward=1408.60 +/- 600.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=98000, episode_reward=1582.80 +/- 335.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=99000, episode_reward=1746.60 +/- 172.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=100000, episode_reward=1701.20 +/- 357.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=101000, episode_reward=1741.60 +/- 215.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=102000, episode_reward=1683.80 +/- 225.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=103000, episode_reward=1460.80 +/- 160.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=104000, episode_reward=1549.60 +/- 295.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=105000, episode_reward=1396.20 +/- 240.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=106000, episode_reward=1825.20 +/- 114.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=107000, episode_reward=1600.20 +/- 229.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=108000, episode_reward=1629.60 +/- 126.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=109000, episode_reward=1615.80 +/- 126.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=110000, episode_reward=1588.20 +/- 149.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=111000, episode_reward=1623.80 +/- 274.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=112000, episode_reward=1453.60 +/- 290.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=113000, episode_reward=1716.80 +/- 313.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=114000, episode_reward=1743.40 +/- 96.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=115000, episode_reward=1575.80 +/- 318.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=116000, episode_reward=1541.40 +/- 182.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=117000, episode_reward=1690.00 +/- 184.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=118000, episode_reward=1566.00 +/- 201.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=119000, episode_reward=1473.60 +/- 328.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=120000, episode_reward=1763.20 +/- 101.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=121000, episode_reward=1781.40 +/- 313.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=122000, episode_reward=1606.80 +/- 150.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=123000, episode_reward=1598.60 +/- 347.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=124000, episode_reward=1573.20 +/- 333.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=125000, episode_reward=1640.60 +/- 148.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=126000, episode_reward=1529.60 +/- 170.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=127000, episode_reward=1749.40 +/- 210.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=128000, episode_reward=1569.00 +/- 335.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=129000, episode_reward=1537.00 +/- 238.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=130000, episode_reward=1668.40 +/- 48.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=131000, episode_reward=1524.20 +/- 136.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=132000, episode_reward=1695.80 +/- 103.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=133000, episode_reward=1287.20 +/- 83.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=134000, episode_reward=1605.80 +/- 205.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=135000, episode_reward=1795.60 +/- 118.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=136000, episode_reward=1667.00 +/- 171.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=137000, episode_reward=1568.40 +/- 202.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=138000, episode_reward=1627.00 +/- 200.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=139000, episode_reward=1892.40 +/- 197.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=140000, episode_reward=1749.20 +/- 77.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=141000, episode_reward=1685.60 +/- 174.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=142000, episode_reward=1654.60 +/- 129.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=143000, episode_reward=1577.40 +/- 124.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=144000, episode_reward=1594.00 +/- 336.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=145000, episode_reward=1484.20 +/- 242.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=146000, episode_reward=1681.60 +/- 125.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=147000, episode_reward=1829.00 +/- 124.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=148000, episode_reward=1634.60 +/- 311.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=149000, episode_reward=1816.80 +/- 75.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=150000, episode_reward=1785.40 +/- 224.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=151000, episode_reward=1645.00 +/- 204.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=152000, episode_reward=1801.80 +/- 245.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=153000, episode_reward=1720.60 +/- 206.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=154000, episode_reward=1658.40 +/- 231.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=155000, episode_reward=1786.20 +/- 118.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=156000, episode_reward=1672.20 +/- 270.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=157000, episode_reward=1550.20 +/- 140.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=158000, episode_reward=1853.80 +/- 186.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=159000, episode_reward=1411.80 +/- 289.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=160000, episode_reward=1567.00 +/- 345.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=161000, episode_reward=1555.00 +/- 170.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=162000, episode_reward=1650.00 +/- 260.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=163000, episode_reward=1772.40 +/- 193.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=164000, episode_reward=1535.60 +/- 318.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=165000, episode_reward=1440.40 +/- 324.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=166000, episode_reward=1568.40 +/- 400.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=167000, episode_reward=1890.60 +/- 138.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=168000, episode_reward=1663.60 +/- 194.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=169000, episode_reward=1707.40 +/- 192.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=170000, episode_reward=1693.80 +/- 253.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=171000, episode_reward=1740.60 +/- 117.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=172000, episode_reward=1697.00 +/- 234.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=173000, episode_reward=1729.00 +/- 212.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=174000, episode_reward=1762.40 +/- 132.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=175000, episode_reward=1710.80 +/- 176.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=176000, episode_reward=1777.40 +/- 221.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=177000, episode_reward=1547.80 +/- 148.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=178000, episode_reward=1699.00 +/- 143.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=179000, episode_reward=1606.40 +/- 105.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=180000, episode_reward=1707.20 +/- 171.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=181000, episode_reward=1664.60 +/- 228.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=182000, episode_reward=1567.60 +/- 251.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=183000, episode_reward=1564.40 +/- 176.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=184000, episode_reward=1700.60 +/- 235.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=185000, episode_reward=1651.60 +/- 350.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=186000, episode_reward=1590.40 +/- 188.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=187000, episode_reward=1666.00 +/- 270.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=188000, episode_reward=1570.20 +/- 68.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=189000, episode_reward=1602.60 +/- 248.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=190000, episode_reward=1602.00 +/- 211.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=191000, episode_reward=1736.40 +/- 136.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=192000, episode_reward=1653.00 +/- 288.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=193000, episode_reward=1685.20 +/- 201.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=194000, episode_reward=1510.60 +/- 267.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=195000, episode_reward=1785.00 +/- 277.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=196000, episode_reward=1477.80 +/- 285.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=197000, episode_reward=1773.40 +/- 210.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=198000, episode_reward=1428.60 +/- 348.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=199000, episode_reward=1534.20 +/- 235.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=200000, episode_reward=1540.20 +/- 272.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=201000, episode_reward=1605.00 +/- 224.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=202000, episode_reward=1530.40 +/- 325.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=203000, episode_reward=1418.20 +/- 413.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=204000, episode_reward=1837.80 +/- 224.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=205000, episode_reward=1679.60 +/- 218.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=206000, episode_reward=1738.80 +/- 243.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=207000, episode_reward=1695.00 +/- 193.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=208000, episode_reward=1768.00 +/- 156.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=209000, episode_reward=1543.20 +/- 231.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=210000, episode_reward=1763.40 +/- 127.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=211000, episode_reward=1472.80 +/- 361.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=212000, episode_reward=1673.60 +/- 286.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=213000, episode_reward=1630.80 +/- 271.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=214000, episode_reward=1675.00 +/- 335.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=215000, episode_reward=1637.60 +/- 250.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=216000, episode_reward=1725.60 +/- 280.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=217000, episode_reward=1492.00 +/- 206.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=218000, episode_reward=1729.00 +/- 270.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=219000, episode_reward=1661.60 +/- 208.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=220000, episode_reward=1803.40 +/- 180.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=221000, episode_reward=1586.00 +/- 269.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=222000, episode_reward=1623.60 +/- 185.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=223000, episode_reward=1632.20 +/- 187.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=224000, episode_reward=1633.00 +/- 187.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=225000, episode_reward=1693.20 +/- 116.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=226000, episode_reward=1466.80 +/- 323.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=227000, episode_reward=1611.80 +/- 147.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=228000, episode_reward=1384.20 +/- 390.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=229000, episode_reward=1562.80 +/- 320.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=230000, episode_reward=1592.40 +/- 300.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=231000, episode_reward=1648.00 +/- 82.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=232000, episode_reward=1748.20 +/- 122.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=233000, episode_reward=1658.20 +/- 130.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=234000, episode_reward=1441.60 +/- 298.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=235000, episode_reward=1758.80 +/- 150.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=236000, episode_reward=1570.20 +/- 85.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=237000, episode_reward=1601.00 +/- 367.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=238000, episode_reward=1403.20 +/- 192.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=239000, episode_reward=1608.00 +/- 244.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=240000, episode_reward=1508.60 +/- 324.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=241000, episode_reward=1543.60 +/- 405.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=242000, episode_reward=1670.80 +/- 140.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=243000, episode_reward=1757.20 +/- 307.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=244000, episode_reward=1728.20 +/- 174.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=245000, episode_reward=1699.60 +/- 283.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=246000, episode_reward=1407.40 +/- 198.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=247000, episode_reward=1604.00 +/- 230.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=248000, episode_reward=1651.00 +/- 317.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=249000, episode_reward=1662.40 +/- 236.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=250000, episode_reward=1592.40 +/- 332.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=251000, episode_reward=1622.60 +/- 191.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=252000, episode_reward=1949.20 +/- 129.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=253000, episode_reward=1500.20 +/- 194.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=254000, episode_reward=1567.00 +/- 192.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=255000, episode_reward=1468.60 +/- 463.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=256000, episode_reward=1635.20 +/- 87.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=257000, episode_reward=1918.80 +/- 196.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=258000, episode_reward=1529.80 +/- 244.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=259000, episode_reward=1497.60 +/- 256.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=260000, episode_reward=1804.20 +/- 113.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=261000, episode_reward=1554.80 +/- 194.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=262000, episode_reward=1663.80 +/- 289.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=263000, episode_reward=1599.00 +/- 169.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=264000, episode_reward=1663.00 +/- 333.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=265000, episode_reward=1624.40 +/- 152.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=266000, episode_reward=1602.60 +/- 179.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=267000, episode_reward=1558.80 +/- 480.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=268000, episode_reward=1365.20 +/- 438.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=269000, episode_reward=1525.60 +/- 143.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=270000, episode_reward=1767.80 +/- 164.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=271000, episode_reward=1669.80 +/- 140.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=272000, episode_reward=1440.20 +/- 40.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=273000, episode_reward=1529.00 +/- 304.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=274000, episode_reward=1512.40 +/- 216.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=275000, episode_reward=1392.80 +/- 273.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=276000, episode_reward=1606.20 +/- 222.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=277000, episode_reward=1638.60 +/- 233.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=278000, episode_reward=1770.00 +/- 166.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=279000, episode_reward=1437.20 +/- 312.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=280000, episode_reward=1644.20 +/- 164.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=281000, episode_reward=1586.40 +/- 117.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=282000, episode_reward=1620.60 +/- 160.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=283000, episode_reward=1786.20 +/- 222.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=284000, episode_reward=1596.60 +/- 154.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=285000, episode_reward=1706.20 +/- 194.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=286000, episode_reward=1767.60 +/- 110.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=287000, episode_reward=1705.20 +/- 152.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=288000, episode_reward=1505.00 +/- 115.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=289000, episode_reward=1603.60 +/- 168.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=290000, episode_reward=1889.60 +/- 112.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=291000, episode_reward=1578.60 +/- 56.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=292000, episode_reward=1814.20 +/- 159.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=293000, episode_reward=1799.80 +/- 152.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=294000, episode_reward=1627.00 +/- 245.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=295000, episode_reward=1654.20 +/- 363.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=296000, episode_reward=1729.20 +/- 219.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=297000, episode_reward=1685.40 +/- 59.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=298000, episode_reward=1564.80 +/- 273.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=299000, episode_reward=1491.40 +/- 264.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=300000, episode_reward=1607.20 +/- 221.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=301000, episode_reward=1709.80 +/- 243.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=302000, episode_reward=1614.40 +/- 232.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=303000, episode_reward=1654.20 +/- 198.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=304000, episode_reward=1641.40 +/- 147.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=305000, episode_reward=1541.80 +/- 263.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=306000, episode_reward=1736.80 +/- 150.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=307000, episode_reward=1570.40 +/- 243.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=308000, episode_reward=1493.20 +/- 238.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=309000, episode_reward=1713.60 +/- 119.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=310000, episode_reward=1844.40 +/- 93.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=311000, episode_reward=1674.40 +/- 285.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=312000, episode_reward=1791.60 +/- 218.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=313000, episode_reward=1497.00 +/- 247.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=314000, episode_reward=1596.20 +/- 149.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=315000, episode_reward=1704.00 +/- 113.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=316000, episode_reward=1397.00 +/- 323.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=317000, episode_reward=1461.80 +/- 309.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=318000, episode_reward=1739.40 +/- 201.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=319000, episode_reward=1539.60 +/- 389.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=320000, episode_reward=1541.40 +/- 292.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=321000, episode_reward=1579.20 +/- 127.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=322000, episode_reward=1626.40 +/- 247.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=323000, episode_reward=1642.40 +/- 317.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=324000, episode_reward=1813.80 +/- 163.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=325000, episode_reward=1753.80 +/- 137.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=326000, episode_reward=1654.00 +/- 202.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=327000, episode_reward=1572.80 +/- 293.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=328000, episode_reward=1767.60 +/- 393.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=329000, episode_reward=1727.00 +/- 166.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=330000, episode_reward=1740.20 +/- 251.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=331000, episode_reward=1379.20 +/- 311.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=332000, episode_reward=1523.00 +/- 148.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=333000, episode_reward=1918.20 +/- 227.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=334000, episode_reward=1658.40 +/- 281.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=335000, episode_reward=1595.60 +/- 223.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=336000, episode_reward=1512.00 +/- 192.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=337000, episode_reward=1674.20 +/- 200.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=338000, episode_reward=1429.00 +/- 364.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=339000, episode_reward=1550.40 +/- 69.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=340000, episode_reward=1594.80 +/- 312.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=341000, episode_reward=1640.20 +/- 192.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=342000, episode_reward=1576.00 +/- 198.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=343000, episode_reward=1803.40 +/- 147.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=344000, episode_reward=1622.20 +/- 371.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=345000, episode_reward=1644.00 +/- 224.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=346000, episode_reward=1583.40 +/- 64.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=347000, episode_reward=1483.00 +/- 238.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=348000, episode_reward=1720.40 +/- 294.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=349000, episode_reward=1591.60 +/- 258.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=350000, episode_reward=1562.80 +/- 353.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=351000, episode_reward=1774.00 +/- 118.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=352000, episode_reward=1722.80 +/- 189.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=353000, episode_reward=1619.20 +/- 227.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=354000, episode_reward=1577.60 +/- 124.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=355000, episode_reward=1393.00 +/- 250.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=356000, episode_reward=1558.80 +/- 191.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=357000, episode_reward=1529.00 +/- 124.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=358000, episode_reward=1504.40 +/- 272.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=359000, episode_reward=1577.60 +/- 102.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=360000, episode_reward=1596.60 +/- 267.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=361000, episode_reward=1759.60 +/- 161.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=362000, episode_reward=1674.40 +/- 279.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=363000, episode_reward=1704.20 +/- 302.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=364000, episode_reward=1588.40 +/- 193.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=365000, episode_reward=1583.80 +/- 300.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=366000, episode_reward=1558.40 +/- 193.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=367000, episode_reward=1704.20 +/- 311.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=368000, episode_reward=1513.20 +/- 310.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=369000, episode_reward=1743.60 +/- 198.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=370000, episode_reward=1455.80 +/- 265.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=371000, episode_reward=1539.00 +/- 338.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=372000, episode_reward=1422.20 +/- 414.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=373000, episode_reward=1720.00 +/- 286.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=374000, episode_reward=1620.40 +/- 142.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=375000, episode_reward=1634.00 +/- 270.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=376000, episode_reward=1722.20 +/- 31.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=377000, episode_reward=1614.80 +/- 301.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=378000, episode_reward=1347.20 +/- 68.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=379000, episode_reward=1571.40 +/- 313.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=380000, episode_reward=1596.80 +/- 244.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=381000, episode_reward=1525.40 +/- 131.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=382000, episode_reward=1610.00 +/- 323.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=383000, episode_reward=1598.20 +/- 233.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=384000, episode_reward=1599.80 +/- 196.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=385000, episode_reward=1717.80 +/- 97.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=386000, episode_reward=1627.80 +/- 126.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=387000, episode_reward=1804.80 +/- 175.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=388000, episode_reward=1619.20 +/- 296.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=389000, episode_reward=1634.60 +/- 301.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=390000, episode_reward=1539.20 +/- 312.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=391000, episode_reward=1755.00 +/- 225.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=392000, episode_reward=1564.20 +/- 211.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=393000, episode_reward=1436.20 +/- 196.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=394000, episode_reward=1429.80 +/- 267.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=395000, episode_reward=1771.20 +/- 251.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=396000, episode_reward=1487.60 +/- 275.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=397000, episode_reward=1770.80 +/- 210.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=398000, episode_reward=1773.00 +/- 252.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=399000, episode_reward=1694.00 +/- 259.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=400000, episode_reward=1667.20 +/- 176.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=401000, episode_reward=1552.40 +/- 257.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=402000, episode_reward=1669.20 +/- 200.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=403000, episode_reward=1722.00 +/- 277.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=404000, episode_reward=1593.60 +/- 252.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=405000, episode_reward=1595.20 +/- 239.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=406000, episode_reward=1706.80 +/- 105.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=407000, episode_reward=1705.20 +/- 140.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=408000, episode_reward=1687.80 +/- 282.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=409000, episode_reward=1586.20 +/- 178.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=410000, episode_reward=1749.40 +/- 162.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=411000, episode_reward=1631.20 +/- 129.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=412000, episode_reward=1603.60 +/- 179.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=413000, episode_reward=1623.00 +/- 143.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=414000, episode_reward=1655.60 +/- 131.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=415000, episode_reward=1873.60 +/- 222.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=416000, episode_reward=1557.20 +/- 247.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=417000, episode_reward=1609.00 +/- 103.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=418000, episode_reward=1598.40 +/- 382.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=419000, episode_reward=1631.00 +/- 149.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=420000, episode_reward=1567.60 +/- 242.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=421000, episode_reward=1665.20 +/- 198.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=422000, episode_reward=1582.80 +/- 67.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=423000, episode_reward=1596.20 +/- 238.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=424000, episode_reward=1818.40 +/- 172.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=425000, episode_reward=1681.60 +/- 274.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=426000, episode_reward=1668.40 +/- 188.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=427000, episode_reward=1592.60 +/- 221.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=428000, episode_reward=1673.60 +/- 258.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=429000, episode_reward=1644.40 +/- 195.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=430000, episode_reward=1529.20 +/- 252.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=431000, episode_reward=1688.40 +/- 215.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=432000, episode_reward=1703.80 +/- 271.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=433000, episode_reward=1733.80 +/- 124.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=434000, episode_reward=1786.60 +/- 219.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=435000, episode_reward=1561.80 +/- 265.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=436000, episode_reward=1594.00 +/- 306.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=437000, episode_reward=1708.60 +/- 286.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=438000, episode_reward=1626.00 +/- 252.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=439000, episode_reward=1655.40 +/- 251.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=440000, episode_reward=1901.40 +/- 261.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=441000, episode_reward=1682.40 +/- 198.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=442000, episode_reward=1773.60 +/- 180.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=443000, episode_reward=1707.00 +/- 112.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=444000, episode_reward=1554.00 +/- 201.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=445000, episode_reward=1723.00 +/- 175.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=446000, episode_reward=1757.80 +/- 174.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=447000, episode_reward=1680.20 +/- 175.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=448000, episode_reward=1752.20 +/- 157.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=449000, episode_reward=1584.00 +/- 203.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=450000, episode_reward=1646.20 +/- 383.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=451000, episode_reward=1581.60 +/- 209.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=452000, episode_reward=1671.20 +/- 140.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=453000, episode_reward=1702.80 +/- 120.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=454000, episode_reward=1679.40 +/- 150.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=455000, episode_reward=1732.60 +/- 87.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=456000, episode_reward=1771.60 +/- 165.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=457000, episode_reward=1717.00 +/- 233.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=458000, episode_reward=1609.00 +/- 207.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=459000, episode_reward=1842.20 +/- 326.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=460000, episode_reward=1690.40 +/- 211.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=461000, episode_reward=1658.40 +/- 236.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=462000, episode_reward=1410.40 +/- 233.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=463000, episode_reward=1784.00 +/- 116.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=464000, episode_reward=1577.60 +/- 426.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=465000, episode_reward=1673.60 +/- 166.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=466000, episode_reward=1673.00 +/- 195.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=467000, episode_reward=1638.40 +/- 153.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=468000, episode_reward=1546.60 +/- 246.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=469000, episode_reward=1572.20 +/- 318.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=470000, episode_reward=1758.20 +/- 117.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=471000, episode_reward=1680.00 +/- 123.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=472000, episode_reward=1777.60 +/- 124.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=473000, episode_reward=1653.00 +/- 168.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=474000, episode_reward=1606.40 +/- 72.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=475000, episode_reward=1564.80 +/- 331.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=476000, episode_reward=1721.60 +/- 196.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=477000, episode_reward=1687.00 +/- 209.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=478000, episode_reward=1922.00 +/- 199.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=479000, episode_reward=1806.80 +/- 169.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=480000, episode_reward=1696.60 +/- 152.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=481000, episode_reward=1720.40 +/- 286.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=482000, episode_reward=1823.60 +/- 155.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=483000, episode_reward=1642.60 +/- 228.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=484000, episode_reward=1711.60 +/- 253.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=485000, episode_reward=1784.00 +/- 284.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=486000, episode_reward=1666.00 +/- 142.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=487000, episode_reward=1723.80 +/- 121.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=488000, episode_reward=1564.40 +/- 196.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=489000, episode_reward=1622.60 +/- 93.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=490000, episode_reward=1731.00 +/- 290.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=491000, episode_reward=1713.60 +/- 76.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=492000, episode_reward=1781.00 +/- 175.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=493000, episode_reward=1564.40 +/- 254.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=494000, episode_reward=1690.40 +/- 180.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=495000, episode_reward=1696.60 +/- 140.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=496000, episode_reward=1611.60 +/- 237.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=497000, episode_reward=1717.60 +/- 149.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=498000, episode_reward=1760.00 +/- 144.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=499000, episode_reward=1695.80 +/- 260.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=500000, episode_reward=1618.60 +/- 278.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=501000, episode_reward=1671.40 +/- 223.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Gamma = 0.9, Learning Rate = 0.0003, Activation Function = Tanh, Neurons = 8, Process noise = 0.1, Measurement noise = 0.0\n",
      "Eval num_timesteps=1000, episode_reward=-1687.00 +/- 144.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=-1707.00 +/- 250.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=3000, episode_reward=-5816.40 +/- 1555.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=4000, episode_reward=-7020.60 +/- 1772.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=5000, episode_reward=-8922.80 +/- 2502.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=6000, episode_reward=-5654.20 +/- 2332.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=7000, episode_reward=-6098.80 +/- 1178.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=8000, episode_reward=-5302.60 +/- 1527.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=9000, episode_reward=-5726.20 +/- 1766.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=10000, episode_reward=-4875.20 +/- 1669.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=11000, episode_reward=4.00 +/- 192.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=12000, episode_reward=43.00 +/- 254.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=13000, episode_reward=-71.80 +/- 178.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=14000, episode_reward=-22.60 +/- 249.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=15000, episode_reward=300.60 +/- 146.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=16000, episode_reward=259.00 +/- 168.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=17000, episode_reward=286.80 +/- 191.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=18000, episode_reward=294.40 +/- 219.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=19000, episode_reward=296.20 +/- 91.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=20000, episode_reward=189.80 +/- 213.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=21000, episode_reward=9.20 +/- 231.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=22000, episode_reward=84.40 +/- 120.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=23000, episode_reward=225.00 +/- 239.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=24000, episode_reward=111.00 +/- 332.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=25000, episode_reward=-61.20 +/- 272.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=26000, episode_reward=-152.80 +/- 609.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=27000, episode_reward=-117.20 +/- 332.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=28000, episode_reward=164.80 +/- 106.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=29000, episode_reward=-787.80 +/- 852.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=30000, episode_reward=-383.80 +/- 667.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=31000, episode_reward=140.40 +/- 547.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=32000, episode_reward=-314.80 +/- 212.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=33000, episode_reward=184.00 +/- 215.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=34000, episode_reward=-141.60 +/- 409.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=35000, episode_reward=37.80 +/- 97.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=36000, episode_reward=-128.00 +/- 274.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=37000, episode_reward=-49.00 +/- 235.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=38000, episode_reward=-244.20 +/- 465.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=39000, episode_reward=-135.00 +/- 586.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=40000, episode_reward=-112.20 +/- 236.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=41000, episode_reward=-296.20 +/- 723.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=42000, episode_reward=105.20 +/- 237.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=43000, episode_reward=-73.20 +/- 415.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=44000, episode_reward=-134.60 +/- 557.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=45000, episode_reward=-293.40 +/- 448.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=46000, episode_reward=-65.80 +/- 173.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=47000, episode_reward=-626.40 +/- 904.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=48000, episode_reward=-417.60 +/- 716.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=49000, episode_reward=-448.20 +/- 840.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=50000, episode_reward=195.40 +/- 543.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=51000, episode_reward=-132.80 +/- 355.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=52000, episode_reward=-258.80 +/- 892.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=53000, episode_reward=96.40 +/- 335.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=54000, episode_reward=145.00 +/- 206.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=55000, episode_reward=18.00 +/- 183.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=56000, episode_reward=285.80 +/- 102.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=57000, episode_reward=237.80 +/- 139.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=58000, episode_reward=200.40 +/- 132.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=59000, episode_reward=197.00 +/- 171.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=60000, episode_reward=-106.40 +/- 561.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=61000, episode_reward=-226.40 +/- 417.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=62000, episode_reward=178.00 +/- 178.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=63000, episode_reward=-288.60 +/- 563.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=64000, episode_reward=-114.00 +/- 376.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=65000, episode_reward=104.40 +/- 327.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=66000, episode_reward=347.20 +/- 268.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=67000, episode_reward=317.00 +/- 128.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=68000, episode_reward=255.80 +/- 150.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=69000, episode_reward=300.20 +/- 244.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=70000, episode_reward=166.20 +/- 171.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=71000, episode_reward=191.40 +/- 141.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=72000, episode_reward=243.60 +/- 156.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=73000, episode_reward=122.40 +/- 140.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=74000, episode_reward=300.00 +/- 268.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=75000, episode_reward=359.80 +/- 157.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=76000, episode_reward=242.00 +/- 99.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=77000, episode_reward=468.40 +/- 133.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=78000, episode_reward=289.20 +/- 162.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=79000, episode_reward=346.80 +/- 143.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=80000, episode_reward=209.40 +/- 117.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=81000, episode_reward=238.00 +/- 362.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=82000, episode_reward=96.00 +/- 92.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=83000, episode_reward=96.00 +/- 118.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=84000, episode_reward=273.00 +/- 125.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=85000, episode_reward=391.40 +/- 137.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=86000, episode_reward=218.20 +/- 253.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=87000, episode_reward=140.80 +/- 126.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=88000, episode_reward=375.80 +/- 180.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=89000, episode_reward=267.60 +/- 88.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=90000, episode_reward=333.00 +/- 179.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=91000, episode_reward=147.20 +/- 87.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=92000, episode_reward=402.20 +/- 123.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=93000, episode_reward=221.80 +/- 233.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=94000, episode_reward=336.40 +/- 156.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=95000, episode_reward=237.20 +/- 63.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=96000, episode_reward=382.80 +/- 131.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=97000, episode_reward=322.80 +/- 173.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=98000, episode_reward=330.00 +/- 262.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=99000, episode_reward=434.40 +/- 128.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=100000, episode_reward=233.60 +/- 161.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=101000, episode_reward=274.80 +/- 180.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=102000, episode_reward=355.00 +/- 200.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=103000, episode_reward=473.40 +/- 344.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=104000, episode_reward=354.40 +/- 213.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=105000, episode_reward=538.80 +/- 382.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=106000, episode_reward=339.00 +/- 169.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=107000, episode_reward=590.20 +/- 380.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=108000, episode_reward=701.60 +/- 356.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=109000, episode_reward=812.40 +/- 238.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=110000, episode_reward=579.40 +/- 146.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=111000, episode_reward=744.20 +/- 200.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=112000, episode_reward=639.40 +/- 252.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=113000, episode_reward=848.00 +/- 358.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=114000, episode_reward=1047.80 +/- 129.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=115000, episode_reward=849.60 +/- 313.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=116000, episode_reward=940.40 +/- 175.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=117000, episode_reward=986.00 +/- 197.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=118000, episode_reward=1054.20 +/- 196.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=119000, episode_reward=1119.80 +/- 254.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=120000, episode_reward=994.40 +/- 283.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=121000, episode_reward=1022.40 +/- 141.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=122000, episode_reward=984.40 +/- 253.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=123000, episode_reward=1090.20 +/- 158.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=124000, episode_reward=1221.60 +/- 202.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=125000, episode_reward=1233.80 +/- 257.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=126000, episode_reward=1116.20 +/- 191.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=127000, episode_reward=1075.80 +/- 238.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=128000, episode_reward=1008.40 +/- 279.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=129000, episode_reward=1188.40 +/- 118.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=130000, episode_reward=1032.20 +/- 160.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=131000, episode_reward=1067.80 +/- 103.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=132000, episode_reward=1070.80 +/- 168.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=133000, episode_reward=1070.40 +/- 161.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=134000, episode_reward=1148.80 +/- 301.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=135000, episode_reward=1043.80 +/- 90.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=136000, episode_reward=1129.20 +/- 218.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=137000, episode_reward=1346.40 +/- 206.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=138000, episode_reward=1011.40 +/- 160.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=139000, episode_reward=921.80 +/- 249.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=140000, episode_reward=1055.80 +/- 409.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=141000, episode_reward=984.80 +/- 312.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=142000, episode_reward=1293.20 +/- 294.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=143000, episode_reward=960.00 +/- 129.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=144000, episode_reward=1225.20 +/- 91.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=145000, episode_reward=-193.20 +/- 1996.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=146000, episode_reward=1076.40 +/- 164.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=147000, episode_reward=1137.40 +/- 195.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=148000, episode_reward=1149.80 +/- 218.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=149000, episode_reward=1021.20 +/- 263.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=150000, episode_reward=815.60 +/- 140.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=151000, episode_reward=1055.80 +/- 265.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=152000, episode_reward=983.20 +/- 97.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=153000, episode_reward=1120.40 +/- 180.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=154000, episode_reward=998.20 +/- 339.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=155000, episode_reward=877.00 +/- 164.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=156000, episode_reward=1057.60 +/- 92.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=157000, episode_reward=-2323.40 +/- 6594.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=158000, episode_reward=988.60 +/- 215.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=159000, episode_reward=1135.00 +/- 170.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=160000, episode_reward=1149.40 +/- 189.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=161000, episode_reward=1178.60 +/- 252.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=162000, episode_reward=972.60 +/- 207.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=163000, episode_reward=1082.40 +/- 308.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=164000, episode_reward=1009.00 +/- 73.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=165000, episode_reward=1084.60 +/- 206.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=166000, episode_reward=1106.00 +/- 201.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=167000, episode_reward=-1624.60 +/- 5295.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=168000, episode_reward=1293.40 +/- 229.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=169000, episode_reward=1137.60 +/- 97.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=170000, episode_reward=1147.00 +/- 146.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=171000, episode_reward=1045.80 +/- 212.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=172000, episode_reward=1081.60 +/- 220.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=173000, episode_reward=1212.80 +/- 222.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=174000, episode_reward=1078.20 +/- 168.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=175000, episode_reward=1138.80 +/- 201.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=176000, episode_reward=991.80 +/- 303.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=177000, episode_reward=1104.60 +/- 182.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=178000, episode_reward=997.00 +/- 153.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=179000, episode_reward=1089.00 +/- 259.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=180000, episode_reward=1134.60 +/- 242.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=181000, episode_reward=1158.60 +/- 137.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=182000, episode_reward=1154.20 +/- 259.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=183000, episode_reward=1069.40 +/- 159.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=184000, episode_reward=1107.40 +/- 190.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=185000, episode_reward=1100.40 +/- 281.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=186000, episode_reward=969.20 +/- 141.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=187000, episode_reward=1116.60 +/- 336.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=188000, episode_reward=903.40 +/- 238.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=189000, episode_reward=904.80 +/- 219.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=190000, episode_reward=1231.20 +/- 218.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=191000, episode_reward=954.60 +/- 71.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=192000, episode_reward=909.40 +/- 276.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=193000, episode_reward=878.40 +/- 133.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=194000, episode_reward=1092.40 +/- 184.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=195000, episode_reward=974.40 +/- 240.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=196000, episode_reward=1042.60 +/- 169.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=197000, episode_reward=1089.60 +/- 270.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=198000, episode_reward=1157.40 +/- 316.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=199000, episode_reward=1085.20 +/- 148.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=200000, episode_reward=1134.20 +/- 360.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=201000, episode_reward=1106.80 +/- 321.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=202000, episode_reward=1270.00 +/- 110.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=203000, episode_reward=1249.00 +/- 207.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=204000, episode_reward=968.80 +/- 270.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=205000, episode_reward=1178.20 +/- 210.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=206000, episode_reward=1062.80 +/- 247.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=207000, episode_reward=1138.20 +/- 348.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=208000, episode_reward=1095.20 +/- 204.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=209000, episode_reward=1251.20 +/- 139.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=210000, episode_reward=1065.20 +/- 242.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=211000, episode_reward=1250.20 +/- 387.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=212000, episode_reward=986.60 +/- 237.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=213000, episode_reward=1166.00 +/- 124.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=214000, episode_reward=-2171.20 +/- 6580.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=215000, episode_reward=1269.00 +/- 270.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=216000, episode_reward=1268.00 +/- 219.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=217000, episode_reward=1211.20 +/- 340.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=218000, episode_reward=1143.20 +/- 160.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=219000, episode_reward=1140.60 +/- 236.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=220000, episode_reward=1377.40 +/- 218.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=221000, episode_reward=1153.00 +/- 385.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=222000, episode_reward=1015.60 +/- 238.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=223000, episode_reward=1230.40 +/- 111.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=224000, episode_reward=1382.00 +/- 102.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=225000, episode_reward=1215.80 +/- 224.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=226000, episode_reward=1310.00 +/- 153.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=227000, episode_reward=1386.60 +/- 110.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=228000, episode_reward=1251.80 +/- 204.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=229000, episode_reward=1166.00 +/- 223.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=230000, episode_reward=1271.60 +/- 94.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=231000, episode_reward=1155.80 +/- 212.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=232000, episode_reward=1462.00 +/- 206.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=233000, episode_reward=1171.20 +/- 312.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=234000, episode_reward=1255.40 +/- 105.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=235000, episode_reward=1414.40 +/- 230.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=236000, episode_reward=1314.40 +/- 289.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=237000, episode_reward=1312.00 +/- 390.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=238000, episode_reward=1476.80 +/- 266.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=239000, episode_reward=1303.00 +/- 187.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=240000, episode_reward=1427.20 +/- 242.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=241000, episode_reward=1380.20 +/- 123.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=242000, episode_reward=1370.80 +/- 113.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=243000, episode_reward=1322.40 +/- 266.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=244000, episode_reward=1436.20 +/- 380.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=245000, episode_reward=1299.00 +/- 224.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=246000, episode_reward=1378.00 +/- 228.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=247000, episode_reward=1362.60 +/- 189.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=248000, episode_reward=1068.60 +/- 46.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=249000, episode_reward=1243.20 +/- 76.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=250000, episode_reward=1408.60 +/- 198.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=251000, episode_reward=1376.80 +/- 213.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=252000, episode_reward=1397.20 +/- 77.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=253000, episode_reward=1444.00 +/- 82.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=254000, episode_reward=1343.40 +/- 125.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=255000, episode_reward=1518.00 +/- 149.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=256000, episode_reward=-198.40 +/- 3273.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=257000, episode_reward=1358.60 +/- 208.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=258000, episode_reward=1531.40 +/- 262.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=259000, episode_reward=1201.60 +/- 214.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=260000, episode_reward=1378.40 +/- 138.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=261000, episode_reward=1384.00 +/- 261.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=262000, episode_reward=1528.60 +/- 137.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=263000, episode_reward=1628.20 +/- 177.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=264000, episode_reward=1448.40 +/- 193.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=265000, episode_reward=1378.40 +/- 482.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=266000, episode_reward=1348.60 +/- 297.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=267000, episode_reward=1457.20 +/- 260.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=268000, episode_reward=1229.40 +/- 218.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=269000, episode_reward=1466.60 +/- 248.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=270000, episode_reward=1500.20 +/- 153.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=271000, episode_reward=1520.20 +/- 148.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=272000, episode_reward=1274.00 +/- 239.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=273000, episode_reward=1464.80 +/- 224.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=274000, episode_reward=1176.00 +/- 292.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=275000, episode_reward=1358.40 +/- 224.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=276000, episode_reward=1474.80 +/- 230.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=277000, episode_reward=1345.20 +/- 115.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=278000, episode_reward=1329.40 +/- 154.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=279000, episode_reward=1413.00 +/- 297.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=280000, episode_reward=1357.40 +/- 222.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=281000, episode_reward=1410.60 +/- 226.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=282000, episode_reward=1335.00 +/- 262.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=283000, episode_reward=1311.40 +/- 202.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=284000, episode_reward=1311.20 +/- 144.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=285000, episode_reward=1345.00 +/- 279.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=286000, episode_reward=1428.00 +/- 199.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=287000, episode_reward=1472.80 +/- 192.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=288000, episode_reward=1336.80 +/- 141.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=289000, episode_reward=1380.20 +/- 234.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=290000, episode_reward=1418.80 +/- 195.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=291000, episode_reward=1345.60 +/- 157.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=292000, episode_reward=1386.20 +/- 178.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=293000, episode_reward=1457.00 +/- 43.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=294000, episode_reward=1343.80 +/- 301.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=295000, episode_reward=1334.80 +/- 149.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=296000, episode_reward=1367.20 +/- 133.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=297000, episode_reward=1522.20 +/- 226.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=298000, episode_reward=1444.00 +/- 199.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=299000, episode_reward=1492.80 +/- 218.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=300000, episode_reward=1403.80 +/- 295.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=301000, episode_reward=1259.20 +/- 187.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=302000, episode_reward=1481.60 +/- 202.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=303000, episode_reward=1463.60 +/- 118.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=304000, episode_reward=1337.60 +/- 259.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=305000, episode_reward=1440.00 +/- 196.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=306000, episode_reward=1433.80 +/- 82.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=307000, episode_reward=1317.20 +/- 278.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=308000, episode_reward=1356.80 +/- 225.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=309000, episode_reward=1490.60 +/- 189.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=310000, episode_reward=1486.40 +/- 113.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=311000, episode_reward=1439.60 +/- 182.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=312000, episode_reward=1452.00 +/- 276.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=313000, episode_reward=1390.40 +/- 278.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=314000, episode_reward=1350.60 +/- 197.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=315000, episode_reward=1375.80 +/- 268.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=316000, episode_reward=1314.60 +/- 274.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=317000, episode_reward=1327.00 +/- 191.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=318000, episode_reward=1606.80 +/- 81.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=319000, episode_reward=1311.60 +/- 158.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=320000, episode_reward=1279.60 +/- 254.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=321000, episode_reward=1410.20 +/- 141.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=322000, episode_reward=1437.00 +/- 88.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=323000, episode_reward=1266.20 +/- 213.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=324000, episode_reward=1341.60 +/- 255.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=325000, episode_reward=1557.80 +/- 353.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=326000, episode_reward=1570.80 +/- 184.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=327000, episode_reward=1348.00 +/- 171.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=328000, episode_reward=1326.20 +/- 102.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=329000, episode_reward=1288.80 +/- 339.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=330000, episode_reward=1530.20 +/- 202.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=331000, episode_reward=1442.40 +/- 220.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=332000, episode_reward=1379.40 +/- 187.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=333000, episode_reward=1505.40 +/- 169.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=334000, episode_reward=1447.20 +/- 178.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=335000, episode_reward=1321.00 +/- 193.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=336000, episode_reward=1574.40 +/- 142.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=337000, episode_reward=1374.20 +/- 177.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=338000, episode_reward=1316.80 +/- 95.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=339000, episode_reward=1537.20 +/- 79.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=340000, episode_reward=1313.80 +/- 188.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=341000, episode_reward=1297.00 +/- 111.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=342000, episode_reward=1381.40 +/- 388.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=343000, episode_reward=1493.40 +/- 202.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=344000, episode_reward=1378.00 +/- 306.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=345000, episode_reward=1460.20 +/- 227.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=346000, episode_reward=1523.00 +/- 101.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=347000, episode_reward=1482.20 +/- 305.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=348000, episode_reward=1434.40 +/- 61.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=349000, episode_reward=1406.60 +/- 221.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=350000, episode_reward=1229.20 +/- 127.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=351000, episode_reward=1463.20 +/- 230.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=352000, episode_reward=1460.60 +/- 111.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=353000, episode_reward=1550.80 +/- 110.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=354000, episode_reward=1538.00 +/- 173.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=355000, episode_reward=1301.60 +/- 336.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=356000, episode_reward=1518.60 +/- 226.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=357000, episode_reward=1498.20 +/- 175.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=358000, episode_reward=1463.80 +/- 80.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=359000, episode_reward=1342.40 +/- 198.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=360000, episode_reward=1281.60 +/- 208.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=361000, episode_reward=1469.20 +/- 170.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=362000, episode_reward=1446.00 +/- 252.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=363000, episode_reward=1488.40 +/- 184.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=364000, episode_reward=1568.80 +/- 82.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=365000, episode_reward=1476.40 +/- 153.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=366000, episode_reward=1663.00 +/- 47.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=367000, episode_reward=1385.60 +/- 195.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=368000, episode_reward=1433.00 +/- 225.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=369000, episode_reward=1520.60 +/- 114.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=370000, episode_reward=1455.80 +/- 191.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=371000, episode_reward=1400.20 +/- 287.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=372000, episode_reward=1437.40 +/- 189.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=373000, episode_reward=1519.60 +/- 190.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=374000, episode_reward=1572.80 +/- 177.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=375000, episode_reward=1395.80 +/- 213.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=376000, episode_reward=1364.60 +/- 204.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=377000, episode_reward=1316.40 +/- 330.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=378000, episode_reward=1316.40 +/- 225.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=379000, episode_reward=1488.00 +/- 146.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=380000, episode_reward=1386.00 +/- 170.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=381000, episode_reward=1438.20 +/- 167.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=382000, episode_reward=1377.80 +/- 105.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=383000, episode_reward=1355.60 +/- 195.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=384000, episode_reward=1439.20 +/- 246.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=385000, episode_reward=1337.80 +/- 374.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=386000, episode_reward=1365.80 +/- 134.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=387000, episode_reward=1442.40 +/- 285.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=388000, episode_reward=1380.60 +/- 119.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=389000, episode_reward=1536.60 +/- 186.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=390000, episode_reward=1694.40 +/- 106.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=391000, episode_reward=1429.20 +/- 101.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=392000, episode_reward=1332.00 +/- 162.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=393000, episode_reward=1439.40 +/- 104.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=394000, episode_reward=1351.40 +/- 161.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=395000, episode_reward=1350.80 +/- 125.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=396000, episode_reward=1411.20 +/- 148.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=397000, episode_reward=1480.20 +/- 126.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=398000, episode_reward=1367.20 +/- 183.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=399000, episode_reward=1499.40 +/- 128.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=400000, episode_reward=1473.60 +/- 136.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=401000, episode_reward=1495.60 +/- 93.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=402000, episode_reward=1530.60 +/- 137.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=403000, episode_reward=1315.80 +/- 128.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=404000, episode_reward=1453.00 +/- 179.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=405000, episode_reward=1375.20 +/- 262.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=406000, episode_reward=1505.60 +/- 95.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=407000, episode_reward=1359.80 +/- 240.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=408000, episode_reward=1356.00 +/- 336.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=409000, episode_reward=1353.00 +/- 297.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=410000, episode_reward=1472.60 +/- 268.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=411000, episode_reward=1349.60 +/- 188.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=412000, episode_reward=1374.40 +/- 134.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=413000, episode_reward=1445.80 +/- 187.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=414000, episode_reward=1388.60 +/- 117.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=415000, episode_reward=1232.80 +/- 295.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=416000, episode_reward=1597.00 +/- 95.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=417000, episode_reward=1410.40 +/- 125.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=418000, episode_reward=1272.40 +/- 119.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=419000, episode_reward=1497.00 +/- 129.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=420000, episode_reward=1445.80 +/- 155.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=421000, episode_reward=1271.40 +/- 375.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=422000, episode_reward=1490.40 +/- 112.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=423000, episode_reward=1444.40 +/- 223.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=424000, episode_reward=1425.80 +/- 104.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=425000, episode_reward=1506.20 +/- 165.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=426000, episode_reward=1451.40 +/- 161.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=427000, episode_reward=1480.40 +/- 158.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=428000, episode_reward=1290.80 +/- 261.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=429000, episode_reward=1249.40 +/- 256.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=430000, episode_reward=1380.00 +/- 216.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=431000, episode_reward=1416.20 +/- 224.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=432000, episode_reward=1408.20 +/- 163.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=433000, episode_reward=1346.20 +/- 253.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=434000, episode_reward=1427.20 +/- 261.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=435000, episode_reward=1554.60 +/- 150.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=436000, episode_reward=1329.80 +/- 177.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=437000, episode_reward=1322.80 +/- 304.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=438000, episode_reward=1401.40 +/- 143.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=439000, episode_reward=1468.60 +/- 178.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=440000, episode_reward=1424.80 +/- 188.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=441000, episode_reward=1324.00 +/- 279.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=442000, episode_reward=1372.40 +/- 205.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=443000, episode_reward=1159.60 +/- 191.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=444000, episode_reward=1428.60 +/- 194.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=445000, episode_reward=1430.00 +/- 212.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=446000, episode_reward=1233.20 +/- 307.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=447000, episode_reward=1481.20 +/- 219.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=448000, episode_reward=1284.60 +/- 106.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=449000, episode_reward=1551.20 +/- 111.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=450000, episode_reward=1499.20 +/- 167.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=451000, episode_reward=1634.20 +/- 29.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=452000, episode_reward=1392.20 +/- 314.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=453000, episode_reward=1282.20 +/- 277.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=454000, episode_reward=1452.20 +/- 146.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=455000, episode_reward=1400.00 +/- 168.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=456000, episode_reward=1470.60 +/- 197.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=457000, episode_reward=1550.60 +/- 130.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=458000, episode_reward=1533.80 +/- 74.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=459000, episode_reward=1335.80 +/- 276.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=460000, episode_reward=1482.20 +/- 124.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=461000, episode_reward=1564.20 +/- 155.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=462000, episode_reward=1347.40 +/- 293.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=463000, episode_reward=1423.00 +/- 115.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=464000, episode_reward=1179.40 +/- 266.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=465000, episode_reward=1213.00 +/- 92.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=466000, episode_reward=1552.40 +/- 216.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=467000, episode_reward=1370.80 +/- 202.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=468000, episode_reward=1280.20 +/- 214.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=469000, episode_reward=1592.20 +/- 151.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=470000, episode_reward=1374.80 +/- 202.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=471000, episode_reward=1417.60 +/- 236.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=472000, episode_reward=1467.80 +/- 116.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=473000, episode_reward=1395.80 +/- 185.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=474000, episode_reward=1413.80 +/- 226.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=475000, episode_reward=1348.40 +/- 183.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=476000, episode_reward=1280.20 +/- 240.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=477000, episode_reward=1347.00 +/- 255.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=478000, episode_reward=1274.60 +/- 108.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=479000, episode_reward=1519.80 +/- 154.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=480000, episode_reward=1431.20 +/- 118.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=481000, episode_reward=1501.40 +/- 173.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=482000, episode_reward=1565.00 +/- 154.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=483000, episode_reward=1199.20 +/- 288.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=484000, episode_reward=1270.00 +/- 282.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=485000, episode_reward=1330.40 +/- 283.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=486000, episode_reward=1268.40 +/- 35.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=487000, episode_reward=1434.60 +/- 86.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=488000, episode_reward=1366.00 +/- 155.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=489000, episode_reward=1443.60 +/- 139.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=490000, episode_reward=1280.40 +/- 113.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=491000, episode_reward=1463.20 +/- 207.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=492000, episode_reward=1354.40 +/- 283.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=493000, episode_reward=1383.00 +/- 204.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=494000, episode_reward=1279.40 +/- 269.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=495000, episode_reward=1414.00 +/- 242.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=496000, episode_reward=1398.80 +/- 374.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=497000, episode_reward=1410.40 +/- 258.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=498000, episode_reward=1258.20 +/- 99.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=499000, episode_reward=1430.80 +/- 249.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=500000, episode_reward=1558.60 +/- 53.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=501000, episode_reward=1515.80 +/- 89.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Gamma = 0.9, Learning Rate = 0.0003, Activation Function = Tanh, Neurons = 32, Process noise = 0.1, Measurement noise = 0.0\n",
      "Eval num_timesteps=1000, episode_reward=-599.80 +/- 72.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=-530.20 +/- 78.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3000, episode_reward=222.40 +/- 200.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4000, episode_reward=313.60 +/- 226.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=5000, episode_reward=322.80 +/- 108.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=6000, episode_reward=297.80 +/- 248.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=7000, episode_reward=285.40 +/- 245.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=8000, episode_reward=408.20 +/- 76.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=9000, episode_reward=325.60 +/- 164.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=10000, episode_reward=306.80 +/- 119.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=11000, episode_reward=217.20 +/- 43.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=12000, episode_reward=283.60 +/- 153.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=13000, episode_reward=257.00 +/- 306.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=14000, episode_reward=467.00 +/- 145.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=15000, episode_reward=132.00 +/- 141.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=16000, episode_reward=282.80 +/- 287.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=17000, episode_reward=282.00 +/- 219.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=18000, episode_reward=276.20 +/- 120.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=19000, episode_reward=334.60 +/- 172.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=20000, episode_reward=168.00 +/- 165.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=21000, episode_reward=308.40 +/- 195.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=22000, episode_reward=404.80 +/- 152.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=23000, episode_reward=249.60 +/- 231.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=24000, episode_reward=254.40 +/- 110.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=25000, episode_reward=334.80 +/- 193.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=26000, episode_reward=160.00 +/- 183.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=27000, episode_reward=64.00 +/- 281.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=28000, episode_reward=309.20 +/- 268.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=29000, episode_reward=277.20 +/- 197.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=30000, episode_reward=190.40 +/- 91.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=31000, episode_reward=463.00 +/- 236.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=32000, episode_reward=424.00 +/- 260.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=33000, episode_reward=486.20 +/- 268.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=34000, episode_reward=570.60 +/- 259.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=35000, episode_reward=767.80 +/- 309.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=36000, episode_reward=805.00 +/- 244.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=37000, episode_reward=1097.00 +/- 198.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=38000, episode_reward=1188.60 +/- 221.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=39000, episode_reward=935.80 +/- 318.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=40000, episode_reward=875.80 +/- 237.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=41000, episode_reward=1111.20 +/- 396.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=42000, episode_reward=990.40 +/- 196.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=43000, episode_reward=1086.20 +/- 157.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=44000, episode_reward=950.00 +/- 191.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=45000, episode_reward=1115.00 +/- 394.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=46000, episode_reward=1003.20 +/- 203.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=47000, episode_reward=805.20 +/- 250.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=48000, episode_reward=506.20 +/- 323.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=49000, episode_reward=701.80 +/- 252.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=50000, episode_reward=960.40 +/- 451.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=51000, episode_reward=1085.80 +/- 299.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=52000, episode_reward=-1166.00 +/- 4210.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=53000, episode_reward=1114.80 +/- 306.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=54000, episode_reward=-359.40 +/- 2991.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=55000, episode_reward=1079.40 +/- 445.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=56000, episode_reward=-2014.00 +/- 6449.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=57000, episode_reward=1248.80 +/- 123.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=58000, episode_reward=-1734.00 +/- 5859.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=59000, episode_reward=880.80 +/- 307.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=60000, episode_reward=-2427.40 +/- 6337.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=61000, episode_reward=1489.00 +/- 359.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=62000, episode_reward=1270.80 +/- 359.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=63000, episode_reward=1107.60 +/- 146.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=64000, episode_reward=1212.00 +/- 232.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=65000, episode_reward=1131.00 +/- 238.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=66000, episode_reward=619.40 +/- 1360.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=67000, episode_reward=1393.00 +/- 233.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=68000, episode_reward=1312.20 +/- 127.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=69000, episode_reward=1157.40 +/- 155.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=70000, episode_reward=811.60 +/- 986.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=71000, episode_reward=1423.00 +/- 343.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=72000, episode_reward=1329.00 +/- 155.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=73000, episode_reward=1238.20 +/- 235.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=74000, episode_reward=1321.00 +/- 362.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=75000, episode_reward=1453.80 +/- 214.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=76000, episode_reward=1285.60 +/- 510.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=77000, episode_reward=1379.20 +/- 145.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=78000, episode_reward=-111.00 +/- 3309.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=79000, episode_reward=1004.60 +/- 282.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=80000, episode_reward=843.20 +/- 663.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=81000, episode_reward=1505.00 +/- 220.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=82000, episode_reward=1405.00 +/- 222.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=83000, episode_reward=-1488.60 +/- 5978.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=84000, episode_reward=1257.80 +/- 237.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=85000, episode_reward=1460.20 +/- 168.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=86000, episode_reward=1310.40 +/- 308.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=87000, episode_reward=1413.40 +/- 185.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=88000, episode_reward=1297.40 +/- 312.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=89000, episode_reward=-1750.80 +/- 6620.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=90000, episode_reward=1145.20 +/- 936.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=91000, episode_reward=1624.80 +/- 266.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=92000, episode_reward=1881.80 +/- 236.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=93000, episode_reward=1451.40 +/- 388.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=94000, episode_reward=-578.20 +/- 4221.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=95000, episode_reward=1353.80 +/- 199.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=96000, episode_reward=1473.20 +/- 234.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=97000, episode_reward=1652.80 +/- 87.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=98000, episode_reward=1545.40 +/- 109.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=99000, episode_reward=1529.00 +/- 140.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=100000, episode_reward=1563.00 +/- 100.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=101000, episode_reward=1379.40 +/- 109.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=102000, episode_reward=-3704.20 +/- 6572.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=103000, episode_reward=-1316.80 +/- 5652.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=104000, episode_reward=-75.60 +/- 3484.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=105000, episode_reward=932.00 +/- 1050.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=106000, episode_reward=1682.20 +/- 238.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=107000, episode_reward=1512.20 +/- 107.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=108000, episode_reward=1617.60 +/- 126.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=109000, episode_reward=-2605.00 +/- 5484.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=110000, episode_reward=1530.80 +/- 169.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=111000, episode_reward=1517.00 +/- 148.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=112000, episode_reward=1493.20 +/- 371.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=113000, episode_reward=1647.20 +/- 204.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=114000, episode_reward=-924.60 +/- 5213.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=115000, episode_reward=1629.20 +/- 236.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=116000, episode_reward=1709.40 +/- 213.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=117000, episode_reward=1521.60 +/- 315.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=118000, episode_reward=1624.20 +/- 265.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=119000, episode_reward=1349.80 +/- 313.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=120000, episode_reward=1555.80 +/- 184.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=121000, episode_reward=1440.40 +/- 314.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=122000, episode_reward=1619.40 +/- 305.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=123000, episode_reward=1444.20 +/- 237.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=124000, episode_reward=1752.80 +/- 153.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=125000, episode_reward=1653.60 +/- 270.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=126000, episode_reward=1599.20 +/- 162.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=127000, episode_reward=1361.20 +/- 169.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=128000, episode_reward=1746.00 +/- 160.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=129000, episode_reward=1811.00 +/- 251.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=130000, episode_reward=1440.40 +/- 517.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=131000, episode_reward=1794.40 +/- 267.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=132000, episode_reward=1565.40 +/- 165.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=133000, episode_reward=1679.40 +/- 153.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=134000, episode_reward=1547.00 +/- 176.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=135000, episode_reward=1436.40 +/- 203.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=136000, episode_reward=1578.60 +/- 168.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=137000, episode_reward=1488.40 +/- 141.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=138000, episode_reward=1508.60 +/- 162.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=139000, episode_reward=1502.40 +/- 460.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=140000, episode_reward=1643.60 +/- 163.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=141000, episode_reward=1512.80 +/- 236.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=142000, episode_reward=1674.60 +/- 366.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=143000, episode_reward=1563.80 +/- 174.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=144000, episode_reward=1688.40 +/- 254.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=145000, episode_reward=1500.80 +/- 144.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=146000, episode_reward=1463.60 +/- 111.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=147000, episode_reward=1715.60 +/- 329.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=148000, episode_reward=1712.20 +/- 144.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=149000, episode_reward=1675.20 +/- 95.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=150000, episode_reward=1823.80 +/- 114.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=151000, episode_reward=1695.00 +/- 170.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=152000, episode_reward=1695.60 +/- 222.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=153000, episode_reward=1799.60 +/- 313.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=154000, episode_reward=1682.60 +/- 95.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=155000, episode_reward=1886.80 +/- 204.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=156000, episode_reward=1414.40 +/- 271.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=157000, episode_reward=1434.60 +/- 183.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=158000, episode_reward=1631.40 +/- 157.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=159000, episode_reward=1559.80 +/- 181.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=160000, episode_reward=1699.80 +/- 186.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=161000, episode_reward=1553.00 +/- 314.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=162000, episode_reward=1555.40 +/- 78.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=163000, episode_reward=1633.40 +/- 209.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=164000, episode_reward=1699.40 +/- 228.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=165000, episode_reward=1681.20 +/- 70.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=166000, episode_reward=1494.40 +/- 308.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=167000, episode_reward=1752.80 +/- 234.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=168000, episode_reward=1483.00 +/- 253.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=169000, episode_reward=1647.00 +/- 306.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=170000, episode_reward=1538.40 +/- 395.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=171000, episode_reward=1667.00 +/- 302.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=172000, episode_reward=1662.20 +/- 107.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=173000, episode_reward=1768.00 +/- 169.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=174000, episode_reward=1608.40 +/- 253.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=175000, episode_reward=1828.00 +/- 139.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=176000, episode_reward=1493.40 +/- 313.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=177000, episode_reward=1629.00 +/- 295.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=178000, episode_reward=1482.40 +/- 229.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=179000, episode_reward=1434.60 +/- 373.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=180000, episode_reward=1567.80 +/- 282.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=181000, episode_reward=1529.60 +/- 281.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=182000, episode_reward=1733.80 +/- 146.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=183000, episode_reward=1682.60 +/- 238.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=184000, episode_reward=1650.00 +/- 137.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=185000, episode_reward=1518.80 +/- 178.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=186000, episode_reward=1704.60 +/- 295.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=187000, episode_reward=1741.40 +/- 193.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=188000, episode_reward=1645.00 +/- 150.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=189000, episode_reward=1786.80 +/- 138.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=190000, episode_reward=1710.20 +/- 390.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=191000, episode_reward=1681.60 +/- 153.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=192000, episode_reward=1453.80 +/- 297.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=193000, episode_reward=1601.80 +/- 261.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=194000, episode_reward=1705.60 +/- 298.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=195000, episode_reward=1741.60 +/- 144.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=196000, episode_reward=1735.80 +/- 177.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=197000, episode_reward=1617.20 +/- 33.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=198000, episode_reward=1543.80 +/- 138.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=199000, episode_reward=1656.20 +/- 118.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=200000, episode_reward=1565.40 +/- 117.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=201000, episode_reward=1731.00 +/- 169.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=202000, episode_reward=1754.00 +/- 130.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=203000, episode_reward=1585.80 +/- 167.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=204000, episode_reward=1623.40 +/- 236.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=205000, episode_reward=1466.40 +/- 187.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=206000, episode_reward=1703.80 +/- 128.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=207000, episode_reward=1508.60 +/- 264.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=208000, episode_reward=1792.60 +/- 209.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=209000, episode_reward=1489.00 +/- 267.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=210000, episode_reward=1719.60 +/- 120.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=211000, episode_reward=1560.60 +/- 249.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=212000, episode_reward=1374.40 +/- 269.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=213000, episode_reward=1593.80 +/- 214.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=214000, episode_reward=1661.60 +/- 171.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=215000, episode_reward=1696.80 +/- 147.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=216000, episode_reward=1547.80 +/- 148.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=217000, episode_reward=1664.00 +/- 176.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=218000, episode_reward=1601.20 +/- 260.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=219000, episode_reward=1490.60 +/- 279.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=220000, episode_reward=1632.40 +/- 100.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=221000, episode_reward=1577.80 +/- 138.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=222000, episode_reward=1558.00 +/- 151.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=223000, episode_reward=1603.40 +/- 123.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=224000, episode_reward=1556.20 +/- 220.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=225000, episode_reward=1478.40 +/- 359.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=226000, episode_reward=1727.20 +/- 238.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=227000, episode_reward=1679.60 +/- 46.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=228000, episode_reward=1495.40 +/- 141.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=229000, episode_reward=1617.60 +/- 243.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=230000, episode_reward=1487.00 +/- 416.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=231000, episode_reward=1613.80 +/- 133.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=232000, episode_reward=1638.80 +/- 185.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=233000, episode_reward=1572.00 +/- 229.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=234000, episode_reward=1514.20 +/- 258.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=235000, episode_reward=1426.20 +/- 297.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=236000, episode_reward=1719.40 +/- 200.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=237000, episode_reward=1652.20 +/- 225.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=238000, episode_reward=1351.60 +/- 173.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=239000, episode_reward=1440.00 +/- 146.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=240000, episode_reward=1722.00 +/- 207.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=241000, episode_reward=1675.40 +/- 283.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=242000, episode_reward=1584.60 +/- 209.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=243000, episode_reward=1722.20 +/- 211.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=244000, episode_reward=1583.20 +/- 111.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=245000, episode_reward=1620.80 +/- 146.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=246000, episode_reward=1526.40 +/- 263.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=247000, episode_reward=1535.40 +/- 272.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=248000, episode_reward=1709.60 +/- 199.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=249000, episode_reward=1470.00 +/- 171.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=250000, episode_reward=1693.60 +/- 94.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=251000, episode_reward=1665.20 +/- 114.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=252000, episode_reward=1783.00 +/- 92.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=253000, episode_reward=1703.60 +/- 187.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=254000, episode_reward=1809.60 +/- 234.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=255000, episode_reward=1691.80 +/- 185.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=256000, episode_reward=1599.60 +/- 226.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=257000, episode_reward=1487.00 +/- 304.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=258000, episode_reward=1561.80 +/- 265.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=259000, episode_reward=1723.80 +/- 215.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=260000, episode_reward=1797.80 +/- 175.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=261000, episode_reward=1618.60 +/- 158.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=262000, episode_reward=1645.60 +/- 149.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=263000, episode_reward=1787.60 +/- 169.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=264000, episode_reward=1656.40 +/- 218.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=265000, episode_reward=1645.40 +/- 184.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=266000, episode_reward=1589.60 +/- 209.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=267000, episode_reward=1490.40 +/- 157.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=268000, episode_reward=1656.60 +/- 86.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=269000, episode_reward=1722.80 +/- 87.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=270000, episode_reward=1755.60 +/- 138.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=271000, episode_reward=1703.20 +/- 79.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=272000, episode_reward=1646.60 +/- 58.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=273000, episode_reward=1505.00 +/- 186.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=274000, episode_reward=1632.80 +/- 287.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=275000, episode_reward=1679.00 +/- 143.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=276000, episode_reward=1540.40 +/- 323.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=277000, episode_reward=1575.60 +/- 197.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=278000, episode_reward=1645.40 +/- 299.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=279000, episode_reward=1493.20 +/- 69.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=280000, episode_reward=1576.40 +/- 139.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=281000, episode_reward=1597.00 +/- 139.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=282000, episode_reward=1496.40 +/- 116.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=283000, episode_reward=1628.40 +/- 425.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=284000, episode_reward=1721.40 +/- 175.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=285000, episode_reward=1721.80 +/- 182.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=286000, episode_reward=1504.60 +/- 310.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=287000, episode_reward=1601.00 +/- 129.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=288000, episode_reward=1549.40 +/- 247.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=289000, episode_reward=1545.40 +/- 296.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=290000, episode_reward=1513.80 +/- 148.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=291000, episode_reward=1641.80 +/- 246.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=292000, episode_reward=1764.80 +/- 247.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=293000, episode_reward=1742.80 +/- 133.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=294000, episode_reward=1709.40 +/- 56.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=295000, episode_reward=1600.80 +/- 122.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=296000, episode_reward=1636.80 +/- 281.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=297000, episode_reward=1747.00 +/- 175.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=298000, episode_reward=1640.00 +/- 233.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=299000, episode_reward=1500.80 +/- 268.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=300000, episode_reward=1624.60 +/- 109.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=301000, episode_reward=1338.60 +/- 176.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=302000, episode_reward=1721.20 +/- 129.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=303000, episode_reward=1542.20 +/- 274.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=304000, episode_reward=1788.80 +/- 204.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=305000, episode_reward=1722.00 +/- 239.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=306000, episode_reward=1628.20 +/- 225.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=307000, episode_reward=1416.60 +/- 113.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=308000, episode_reward=1465.80 +/- 267.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=309000, episode_reward=1728.20 +/- 92.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=310000, episode_reward=1557.00 +/- 188.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=311000, episode_reward=1725.80 +/- 189.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=312000, episode_reward=1691.60 +/- 188.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=313000, episode_reward=1824.20 +/- 215.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=314000, episode_reward=1746.20 +/- 160.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=315000, episode_reward=1775.20 +/- 265.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=316000, episode_reward=1695.60 +/- 241.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=317000, episode_reward=1506.40 +/- 149.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=318000, episode_reward=1296.40 +/- 168.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=319000, episode_reward=1618.20 +/- 196.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=320000, episode_reward=1502.80 +/- 326.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=321000, episode_reward=1753.40 +/- 227.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=322000, episode_reward=1623.00 +/- 216.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=323000, episode_reward=1641.40 +/- 344.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=324000, episode_reward=1692.40 +/- 338.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=325000, episode_reward=1725.60 +/- 332.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=326000, episode_reward=1537.40 +/- 169.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=327000, episode_reward=1700.20 +/- 245.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=328000, episode_reward=1753.00 +/- 110.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=329000, episode_reward=1822.20 +/- 311.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=330000, episode_reward=1687.80 +/- 321.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=331000, episode_reward=1428.60 +/- 188.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=332000, episode_reward=1517.40 +/- 281.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=333000, episode_reward=1707.00 +/- 265.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=334000, episode_reward=1576.20 +/- 357.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=335000, episode_reward=1640.60 +/- 290.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=336000, episode_reward=1487.80 +/- 195.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=337000, episode_reward=1684.60 +/- 110.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=338000, episode_reward=1750.00 +/- 171.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=339000, episode_reward=1676.60 +/- 362.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=340000, episode_reward=1658.60 +/- 293.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=341000, episode_reward=1675.00 +/- 207.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=342000, episode_reward=1746.00 +/- 99.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=343000, episode_reward=1656.80 +/- 404.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=344000, episode_reward=1577.00 +/- 208.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=345000, episode_reward=1687.60 +/- 192.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=346000, episode_reward=1572.60 +/- 373.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=347000, episode_reward=1520.80 +/- 179.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=348000, episode_reward=1578.60 +/- 163.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=349000, episode_reward=1557.60 +/- 142.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=350000, episode_reward=1608.20 +/- 293.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=351000, episode_reward=1484.00 +/- 177.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=352000, episode_reward=1668.00 +/- 94.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=353000, episode_reward=1705.20 +/- 324.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=354000, episode_reward=1520.40 +/- 113.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=355000, episode_reward=1530.00 +/- 49.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=356000, episode_reward=1756.00 +/- 79.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=357000, episode_reward=1617.60 +/- 171.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=358000, episode_reward=1643.80 +/- 60.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=359000, episode_reward=1449.80 +/- 148.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=360000, episode_reward=1548.80 +/- 221.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=361000, episode_reward=1495.80 +/- 225.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=362000, episode_reward=1879.80 +/- 138.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=363000, episode_reward=1459.00 +/- 211.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=364000, episode_reward=1772.60 +/- 267.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=365000, episode_reward=1655.20 +/- 195.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=366000, episode_reward=1495.20 +/- 153.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=367000, episode_reward=1576.80 +/- 264.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=368000, episode_reward=1697.40 +/- 156.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=369000, episode_reward=1729.80 +/- 161.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=370000, episode_reward=1752.80 +/- 149.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=371000, episode_reward=1768.60 +/- 202.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=372000, episode_reward=1552.00 +/- 288.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=373000, episode_reward=1538.20 +/- 157.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=374000, episode_reward=1817.40 +/- 142.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=375000, episode_reward=1630.40 +/- 66.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=376000, episode_reward=1724.40 +/- 274.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=377000, episode_reward=1669.20 +/- 141.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=378000, episode_reward=1793.00 +/- 299.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=379000, episode_reward=1609.20 +/- 160.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=380000, episode_reward=1474.20 +/- 175.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=381000, episode_reward=1631.40 +/- 250.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=382000, episode_reward=1632.60 +/- 173.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=383000, episode_reward=1650.40 +/- 194.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=384000, episode_reward=1526.20 +/- 228.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=385000, episode_reward=1712.00 +/- 141.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=386000, episode_reward=1653.80 +/- 167.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=387000, episode_reward=1698.80 +/- 191.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=388000, episode_reward=1730.00 +/- 162.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=389000, episode_reward=1863.40 +/- 261.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=390000, episode_reward=1638.60 +/- 237.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=391000, episode_reward=1623.00 +/- 292.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=392000, episode_reward=1537.00 +/- 288.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=393000, episode_reward=1608.40 +/- 352.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=394000, episode_reward=1772.20 +/- 255.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=395000, episode_reward=1796.00 +/- 177.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=396000, episode_reward=1732.40 +/- 224.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=397000, episode_reward=1627.20 +/- 141.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=398000, episode_reward=1695.60 +/- 76.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=399000, episode_reward=1764.40 +/- 337.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=400000, episode_reward=1846.40 +/- 68.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=401000, episode_reward=1794.60 +/- 180.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=402000, episode_reward=1495.20 +/- 243.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=403000, episode_reward=1777.80 +/- 311.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=404000, episode_reward=1689.60 +/- 125.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=405000, episode_reward=1801.00 +/- 194.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=406000, episode_reward=1777.80 +/- 191.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=407000, episode_reward=1753.60 +/- 89.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=408000, episode_reward=1805.00 +/- 78.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=409000, episode_reward=1762.80 +/- 187.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=410000, episode_reward=1667.60 +/- 169.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=411000, episode_reward=1683.60 +/- 300.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=412000, episode_reward=1646.80 +/- 175.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=413000, episode_reward=1656.80 +/- 65.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=414000, episode_reward=1744.80 +/- 360.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=415000, episode_reward=1498.40 +/- 390.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=416000, episode_reward=1479.60 +/- 137.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=417000, episode_reward=1494.40 +/- 109.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=418000, episode_reward=1669.20 +/- 218.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=419000, episode_reward=1822.00 +/- 178.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=420000, episode_reward=1582.80 +/- 206.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=421000, episode_reward=1684.80 +/- 108.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=422000, episode_reward=1540.60 +/- 217.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=423000, episode_reward=1612.40 +/- 287.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=424000, episode_reward=1636.60 +/- 208.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=425000, episode_reward=1516.00 +/- 155.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=426000, episode_reward=1635.60 +/- 56.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=427000, episode_reward=1741.00 +/- 156.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=428000, episode_reward=1713.20 +/- 186.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=429000, episode_reward=1696.40 +/- 308.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=430000, episode_reward=1626.20 +/- 227.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=431000, episode_reward=1726.60 +/- 182.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=432000, episode_reward=1684.40 +/- 133.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=433000, episode_reward=1808.00 +/- 190.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=434000, episode_reward=1387.60 +/- 329.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=435000, episode_reward=1652.80 +/- 67.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=436000, episode_reward=1818.00 +/- 170.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=437000, episode_reward=1892.60 +/- 150.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=438000, episode_reward=1551.00 +/- 343.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=439000, episode_reward=1696.00 +/- 255.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=440000, episode_reward=1501.80 +/- 226.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=441000, episode_reward=1562.00 +/- 227.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=442000, episode_reward=1701.20 +/- 335.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=443000, episode_reward=1639.80 +/- 184.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=444000, episode_reward=1621.00 +/- 206.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=445000, episode_reward=1843.80 +/- 198.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=446000, episode_reward=1760.20 +/- 207.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=447000, episode_reward=1706.80 +/- 269.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=448000, episode_reward=1714.00 +/- 159.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=449000, episode_reward=1505.60 +/- 191.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=450000, episode_reward=1661.00 +/- 253.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=451000, episode_reward=1569.20 +/- 135.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=452000, episode_reward=1559.00 +/- 298.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=453000, episode_reward=1286.60 +/- 260.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=454000, episode_reward=1794.00 +/- 125.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=455000, episode_reward=1485.80 +/- 319.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=456000, episode_reward=1658.80 +/- 301.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=457000, episode_reward=1517.40 +/- 191.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=458000, episode_reward=1660.40 +/- 343.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=459000, episode_reward=1637.40 +/- 236.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=460000, episode_reward=1689.20 +/- 252.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=461000, episode_reward=1575.20 +/- 144.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=462000, episode_reward=1726.20 +/- 124.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=463000, episode_reward=1584.40 +/- 53.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=464000, episode_reward=1724.00 +/- 231.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=465000, episode_reward=1538.40 +/- 163.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=466000, episode_reward=1752.60 +/- 228.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=467000, episode_reward=1573.40 +/- 160.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=468000, episode_reward=1783.80 +/- 252.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=469000, episode_reward=1727.40 +/- 193.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=470000, episode_reward=1541.40 +/- 269.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=471000, episode_reward=1730.20 +/- 259.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=472000, episode_reward=1585.60 +/- 283.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=473000, episode_reward=1784.40 +/- 92.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=474000, episode_reward=1783.40 +/- 185.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=475000, episode_reward=1762.60 +/- 192.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=476000, episode_reward=1665.40 +/- 116.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=477000, episode_reward=1711.00 +/- 365.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=478000, episode_reward=1586.20 +/- 339.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=479000, episode_reward=1720.20 +/- 268.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=480000, episode_reward=1596.00 +/- 133.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=481000, episode_reward=1491.80 +/- 191.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=482000, episode_reward=1552.40 +/- 228.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=483000, episode_reward=1566.80 +/- 234.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=484000, episode_reward=1546.40 +/- 251.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=485000, episode_reward=1711.80 +/- 91.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=486000, episode_reward=1605.60 +/- 359.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=487000, episode_reward=1436.20 +/- 183.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=488000, episode_reward=1601.80 +/- 253.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=489000, episode_reward=1433.20 +/- 329.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=490000, episode_reward=1731.60 +/- 231.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=491000, episode_reward=1653.80 +/- 258.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=492000, episode_reward=1709.00 +/- 226.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=493000, episode_reward=1715.20 +/- 360.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=494000, episode_reward=1769.60 +/- 152.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=495000, episode_reward=1836.80 +/- 187.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=496000, episode_reward=1493.60 +/- 304.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=497000, episode_reward=1684.20 +/- 243.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=498000, episode_reward=1585.00 +/- 264.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=499000, episode_reward=1681.20 +/- 266.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=500000, episode_reward=1694.60 +/- 162.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=501000, episode_reward=1762.60 +/- 168.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Gamma = 0.9, Learning Rate = 0.0003, Activation Function = Tanh, Neurons = 64, Process noise = 0.1, Measurement noise = 0.0\n",
      "Eval num_timesteps=1000, episode_reward=-1760.20 +/- 122.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=-1731.20 +/- 185.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3000, episode_reward=316.00 +/- 169.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4000, episode_reward=325.60 +/- 99.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=5000, episode_reward=245.40 +/- 212.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=6000, episode_reward=562.20 +/- 215.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=7000, episode_reward=322.20 +/- 123.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=8000, episode_reward=477.60 +/- 214.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=9000, episode_reward=308.20 +/- 154.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=10000, episode_reward=205.20 +/- 193.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=11000, episode_reward=443.20 +/- 133.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=12000, episode_reward=327.80 +/- 161.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=13000, episode_reward=399.00 +/- 309.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=14000, episode_reward=445.00 +/- 242.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=15000, episode_reward=93.40 +/- 331.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=16000, episode_reward=233.60 +/- 159.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=17000, episode_reward=203.60 +/- 122.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=18000, episode_reward=381.40 +/- 138.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=19000, episode_reward=302.40 +/- 140.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=20000, episode_reward=380.80 +/- 126.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=21000, episode_reward=163.80 +/- 213.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=22000, episode_reward=324.60 +/- 102.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=23000, episode_reward=430.00 +/- 109.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=24000, episode_reward=366.00 +/- 285.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=25000, episode_reward=355.20 +/- 174.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=26000, episode_reward=321.00 +/- 140.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=27000, episode_reward=263.20 +/- 109.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=28000, episode_reward=430.40 +/- 301.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=29000, episode_reward=202.40 +/- 192.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=30000, episode_reward=354.60 +/- 163.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=31000, episode_reward=345.20 +/- 186.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=32000, episode_reward=563.00 +/- 116.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=33000, episode_reward=595.20 +/- 162.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=34000, episode_reward=539.60 +/- 121.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=35000, episode_reward=512.00 +/- 135.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=36000, episode_reward=633.20 +/- 157.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=37000, episode_reward=1017.40 +/- 293.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=38000, episode_reward=991.00 +/- 207.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=39000, episode_reward=889.40 +/- 230.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=40000, episode_reward=745.40 +/- 247.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=41000, episode_reward=1263.60 +/- 189.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=42000, episode_reward=691.60 +/- 224.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=43000, episode_reward=1030.60 +/- 104.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=44000, episode_reward=923.20 +/- 211.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=45000, episode_reward=832.40 +/- 141.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=46000, episode_reward=1082.80 +/- 123.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=47000, episode_reward=397.00 +/- 1401.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=48000, episode_reward=929.80 +/- 312.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=49000, episode_reward=1114.20 +/- 350.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=50000, episode_reward=1124.20 +/- 165.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=51000, episode_reward=426.80 +/- 1079.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=52000, episode_reward=310.60 +/- 1999.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=53000, episode_reward=-2045.00 +/- 6143.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=54000, episode_reward=1048.00 +/- 383.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=55000, episode_reward=1251.40 +/- 185.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=56000, episode_reward=1223.80 +/- 342.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=57000, episode_reward=1350.60 +/- 141.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=58000, episode_reward=1159.60 +/- 242.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=59000, episode_reward=1398.80 +/- 124.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=60000, episode_reward=1197.60 +/- 241.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=61000, episode_reward=1178.20 +/- 244.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=62000, episode_reward=1079.40 +/- 152.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=63000, episode_reward=1287.40 +/- 127.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=64000, episode_reward=1260.00 +/- 278.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=65000, episode_reward=1094.60 +/- 145.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=66000, episode_reward=-1787.00 +/- 5659.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=67000, episode_reward=1405.00 +/- 158.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=68000, episode_reward=1231.40 +/- 245.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=69000, episode_reward=1203.80 +/- 234.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=70000, episode_reward=1150.40 +/- 152.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=71000, episode_reward=1072.60 +/- 246.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=72000, episode_reward=1068.40 +/- 138.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=73000, episode_reward=1358.80 +/- 203.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=74000, episode_reward=1317.00 +/- 315.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=75000, episode_reward=1383.40 +/- 144.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=76000, episode_reward=1523.60 +/- 156.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=77000, episode_reward=1336.20 +/- 192.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=78000, episode_reward=1387.40 +/- 223.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=79000, episode_reward=1502.20 +/- 171.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=80000, episode_reward=1320.40 +/- 108.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=81000, episode_reward=1381.80 +/- 282.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=82000, episode_reward=1443.40 +/- 248.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=83000, episode_reward=575.20 +/- 1561.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=84000, episode_reward=-34.20 +/- 3611.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=85000, episode_reward=1350.20 +/- 189.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=86000, episode_reward=1456.80 +/- 141.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=87000, episode_reward=-1850.00 +/- 7065.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=88000, episode_reward=1416.20 +/- 171.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=89000, episode_reward=1464.20 +/- 219.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=90000, episode_reward=1634.20 +/- 178.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=91000, episode_reward=1651.40 +/- 150.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=92000, episode_reward=1390.00 +/- 262.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=93000, episode_reward=1440.00 +/- 332.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=94000, episode_reward=1526.40 +/- 130.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=95000, episode_reward=1436.60 +/- 139.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=96000, episode_reward=1486.80 +/- 293.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=97000, episode_reward=1647.20 +/- 165.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=98000, episode_reward=1694.40 +/- 270.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=99000, episode_reward=-439.00 +/- 3438.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=100000, episode_reward=1624.60 +/- 201.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=101000, episode_reward=1518.00 +/- 303.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=102000, episode_reward=1691.40 +/- 116.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=103000, episode_reward=1663.80 +/- 156.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=104000, episode_reward=599.00 +/- 2277.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=105000, episode_reward=1589.20 +/- 269.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=106000, episode_reward=1432.80 +/- 131.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=107000, episode_reward=1577.80 +/- 112.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=108000, episode_reward=1305.60 +/- 340.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=109000, episode_reward=1722.20 +/- 222.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=110000, episode_reward=1500.80 +/- 231.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=111000, episode_reward=1678.40 +/- 139.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=112000, episode_reward=1513.40 +/- 306.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=113000, episode_reward=-1944.40 +/- 7061.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=114000, episode_reward=1777.20 +/- 158.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=115000, episode_reward=1501.20 +/- 125.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=116000, episode_reward=1619.80 +/- 86.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=117000, episode_reward=1522.20 +/- 203.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=118000, episode_reward=1584.40 +/- 145.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=119000, episode_reward=1551.20 +/- 300.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=120000, episode_reward=1546.00 +/- 206.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=121000, episode_reward=1451.20 +/- 390.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=122000, episode_reward=1824.20 +/- 217.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=123000, episode_reward=1729.60 +/- 85.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=124000, episode_reward=1657.00 +/- 134.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=125000, episode_reward=1687.40 +/- 75.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=126000, episode_reward=1394.80 +/- 202.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=127000, episode_reward=1517.40 +/- 281.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=128000, episode_reward=1419.00 +/- 110.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=129000, episode_reward=1642.00 +/- 302.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=130000, episode_reward=1529.40 +/- 262.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=131000, episode_reward=1594.00 +/- 139.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=132000, episode_reward=1604.20 +/- 302.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=133000, episode_reward=1617.00 +/- 215.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=134000, episode_reward=1474.80 +/- 209.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=135000, episode_reward=1416.40 +/- 264.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=136000, episode_reward=1703.60 +/- 123.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=137000, episode_reward=1546.60 +/- 156.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=138000, episode_reward=1448.40 +/- 208.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=139000, episode_reward=1588.40 +/- 232.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=140000, episode_reward=1512.40 +/- 270.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=141000, episode_reward=1671.80 +/- 265.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=142000, episode_reward=1709.80 +/- 223.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=143000, episode_reward=1565.20 +/- 350.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=144000, episode_reward=1603.20 +/- 219.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=145000, episode_reward=1548.80 +/- 252.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=146000, episode_reward=1597.20 +/- 258.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=147000, episode_reward=1584.80 +/- 226.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=148000, episode_reward=1387.80 +/- 296.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=149000, episode_reward=1629.80 +/- 177.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=150000, episode_reward=1390.60 +/- 142.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=151000, episode_reward=1644.40 +/- 302.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=152000, episode_reward=1365.80 +/- 198.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=153000, episode_reward=1644.20 +/- 166.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=154000, episode_reward=1412.40 +/- 331.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=155000, episode_reward=1342.40 +/- 181.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=156000, episode_reward=1478.60 +/- 409.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=157000, episode_reward=1563.40 +/- 376.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=158000, episode_reward=1297.20 +/- 311.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=159000, episode_reward=1521.20 +/- 240.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=160000, episode_reward=1548.80 +/- 237.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=161000, episode_reward=1588.80 +/- 114.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=162000, episode_reward=1490.20 +/- 210.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=163000, episode_reward=1647.40 +/- 269.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=164000, episode_reward=1551.00 +/- 368.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=165000, episode_reward=1433.40 +/- 192.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=166000, episode_reward=1432.40 +/- 81.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=167000, episode_reward=1591.00 +/- 302.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=168000, episode_reward=1564.60 +/- 119.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=169000, episode_reward=1722.20 +/- 287.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=170000, episode_reward=1514.20 +/- 369.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=171000, episode_reward=1397.60 +/- 96.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=172000, episode_reward=1534.40 +/- 271.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=173000, episode_reward=1501.20 +/- 175.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=174000, episode_reward=1391.00 +/- 91.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=175000, episode_reward=1792.60 +/- 179.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=176000, episode_reward=1385.20 +/- 296.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=177000, episode_reward=1727.80 +/- 222.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=178000, episode_reward=1494.60 +/- 194.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=179000, episode_reward=1476.80 +/- 278.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=180000, episode_reward=1705.40 +/- 173.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=181000, episode_reward=1619.00 +/- 178.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=182000, episode_reward=1364.20 +/- 285.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=183000, episode_reward=1572.20 +/- 172.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=184000, episode_reward=1505.60 +/- 343.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=185000, episode_reward=1567.80 +/- 335.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=186000, episode_reward=1605.60 +/- 92.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=187000, episode_reward=1748.80 +/- 164.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=188000, episode_reward=1506.00 +/- 205.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=189000, episode_reward=1725.80 +/- 142.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=190000, episode_reward=1643.40 +/- 312.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=191000, episode_reward=1584.20 +/- 122.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=192000, episode_reward=1593.20 +/- 388.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=193000, episode_reward=1475.80 +/- 221.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=194000, episode_reward=1639.00 +/- 212.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=195000, episode_reward=1457.00 +/- 183.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=196000, episode_reward=1552.60 +/- 217.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=197000, episode_reward=1369.00 +/- 192.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=198000, episode_reward=1593.40 +/- 219.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=199000, episode_reward=1695.60 +/- 226.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=200000, episode_reward=1680.80 +/- 178.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=201000, episode_reward=1466.40 +/- 220.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=202000, episode_reward=1601.00 +/- 332.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=203000, episode_reward=1816.20 +/- 68.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=204000, episode_reward=1641.20 +/- 167.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=205000, episode_reward=1312.60 +/- 570.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=206000, episode_reward=1690.00 +/- 158.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=207000, episode_reward=1503.60 +/- 245.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=208000, episode_reward=1626.60 +/- 164.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=209000, episode_reward=1689.60 +/- 198.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=210000, episode_reward=1528.20 +/- 77.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=211000, episode_reward=1639.60 +/- 306.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=212000, episode_reward=1521.40 +/- 300.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=213000, episode_reward=1230.80 +/- 402.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=214000, episode_reward=1757.80 +/- 174.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=215000, episode_reward=1679.60 +/- 229.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=216000, episode_reward=1403.60 +/- 411.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=217000, episode_reward=1589.60 +/- 220.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=218000, episode_reward=1664.40 +/- 140.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=219000, episode_reward=1572.00 +/- 358.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=220000, episode_reward=1602.00 +/- 250.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=221000, episode_reward=1660.80 +/- 166.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=222000, episode_reward=1537.60 +/- 200.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=223000, episode_reward=1659.00 +/- 337.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=224000, episode_reward=1547.00 +/- 213.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=225000, episode_reward=1675.80 +/- 181.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=226000, episode_reward=1760.20 +/- 316.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=227000, episode_reward=1359.80 +/- 327.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=228000, episode_reward=1628.60 +/- 287.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=229000, episode_reward=1584.60 +/- 187.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=230000, episode_reward=1645.80 +/- 208.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=231000, episode_reward=1512.80 +/- 215.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=232000, episode_reward=1592.80 +/- 245.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=233000, episode_reward=1472.00 +/- 142.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=234000, episode_reward=1583.80 +/- 311.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=235000, episode_reward=1645.20 +/- 133.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=236000, episode_reward=1495.20 +/- 198.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=237000, episode_reward=1622.60 +/- 123.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=238000, episode_reward=1585.80 +/- 238.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=239000, episode_reward=1584.60 +/- 310.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=240000, episode_reward=1680.40 +/- 133.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=241000, episode_reward=1597.00 +/- 312.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=242000, episode_reward=1310.80 +/- 162.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=243000, episode_reward=1543.60 +/- 132.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=244000, episode_reward=1551.20 +/- 355.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=245000, episode_reward=1525.60 +/- 285.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=246000, episode_reward=1640.80 +/- 192.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=247000, episode_reward=1524.60 +/- 325.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=248000, episode_reward=1732.60 +/- 218.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=249000, episode_reward=1652.80 +/- 180.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=250000, episode_reward=1538.00 +/- 199.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=251000, episode_reward=1760.40 +/- 182.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=252000, episode_reward=1832.40 +/- 222.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=253000, episode_reward=1557.20 +/- 315.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=254000, episode_reward=1476.80 +/- 209.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=255000, episode_reward=1456.80 +/- 224.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=256000, episode_reward=1723.80 +/- 118.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=257000, episode_reward=1368.00 +/- 413.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=258000, episode_reward=1618.80 +/- 225.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=259000, episode_reward=1404.40 +/- 337.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=260000, episode_reward=1484.60 +/- 345.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=261000, episode_reward=1500.20 +/- 288.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=262000, episode_reward=1628.00 +/- 143.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=263000, episode_reward=1655.00 +/- 373.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=264000, episode_reward=1458.40 +/- 554.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=265000, episode_reward=1688.80 +/- 378.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=266000, episode_reward=1511.80 +/- 200.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=267000, episode_reward=1622.40 +/- 128.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=268000, episode_reward=1639.40 +/- 199.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=269000, episode_reward=1445.60 +/- 260.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=270000, episode_reward=1386.60 +/- 314.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=271000, episode_reward=1564.00 +/- 88.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=272000, episode_reward=1643.60 +/- 180.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=273000, episode_reward=1541.60 +/- 162.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=274000, episode_reward=1719.40 +/- 141.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=275000, episode_reward=1448.00 +/- 137.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=276000, episode_reward=1484.20 +/- 213.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=277000, episode_reward=1453.60 +/- 224.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=278000, episode_reward=1625.20 +/- 139.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=279000, episode_reward=1498.20 +/- 303.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=280000, episode_reward=1721.20 +/- 253.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=281000, episode_reward=1447.80 +/- 272.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=282000, episode_reward=1436.20 +/- 377.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=283000, episode_reward=1687.00 +/- 259.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=284000, episode_reward=1734.60 +/- 189.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=285000, episode_reward=1622.40 +/- 235.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=286000, episode_reward=1585.60 +/- 199.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=287000, episode_reward=1553.20 +/- 216.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=288000, episode_reward=1612.00 +/- 124.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=289000, episode_reward=1498.00 +/- 282.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=290000, episode_reward=1690.80 +/- 427.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=291000, episode_reward=1505.20 +/- 247.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=292000, episode_reward=1619.40 +/- 261.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=293000, episode_reward=1443.60 +/- 177.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=294000, episode_reward=1643.20 +/- 164.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=295000, episode_reward=1609.00 +/- 353.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=296000, episode_reward=1614.60 +/- 254.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=297000, episode_reward=1678.00 +/- 142.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=298000, episode_reward=1558.20 +/- 169.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=299000, episode_reward=1434.00 +/- 157.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=300000, episode_reward=1706.40 +/- 175.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=301000, episode_reward=1623.20 +/- 277.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=302000, episode_reward=1497.00 +/- 225.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=303000, episode_reward=1576.20 +/- 175.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=304000, episode_reward=1589.20 +/- 329.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=305000, episode_reward=1608.40 +/- 258.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=306000, episode_reward=1675.80 +/- 239.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=307000, episode_reward=1643.60 +/- 131.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=308000, episode_reward=1645.40 +/- 235.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=309000, episode_reward=1646.20 +/- 240.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=310000, episode_reward=1277.00 +/- 221.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=311000, episode_reward=1613.40 +/- 182.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=312000, episode_reward=1506.00 +/- 278.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=313000, episode_reward=1634.00 +/- 271.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=314000, episode_reward=1544.20 +/- 238.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=315000, episode_reward=1364.20 +/- 266.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=316000, episode_reward=1762.80 +/- 236.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=317000, episode_reward=1569.20 +/- 135.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=318000, episode_reward=1648.80 +/- 69.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=319000, episode_reward=1563.40 +/- 279.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=320000, episode_reward=1801.40 +/- 209.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=321000, episode_reward=1570.80 +/- 357.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=322000, episode_reward=1790.20 +/- 88.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=323000, episode_reward=1764.80 +/- 206.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=324000, episode_reward=1567.00 +/- 274.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=325000, episode_reward=1645.00 +/- 200.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=326000, episode_reward=1464.60 +/- 103.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=327000, episode_reward=1461.40 +/- 226.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=328000, episode_reward=1552.20 +/- 171.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=329000, episode_reward=1810.80 +/- 120.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=330000, episode_reward=1491.60 +/- 257.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=331000, episode_reward=1459.80 +/- 145.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=332000, episode_reward=1757.40 +/- 174.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=333000, episode_reward=1754.40 +/- 162.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=334000, episode_reward=1432.00 +/- 222.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=335000, episode_reward=1598.20 +/- 203.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=336000, episode_reward=1936.80 +/- 173.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=337000, episode_reward=1579.00 +/- 203.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=338000, episode_reward=1687.20 +/- 199.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=339000, episode_reward=1583.00 +/- 232.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=340000, episode_reward=1752.80 +/- 143.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=341000, episode_reward=1592.80 +/- 325.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=342000, episode_reward=1476.00 +/- 267.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=343000, episode_reward=1615.20 +/- 111.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=344000, episode_reward=1681.00 +/- 189.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=345000, episode_reward=1654.20 +/- 74.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=346000, episode_reward=1609.40 +/- 304.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=347000, episode_reward=1461.20 +/- 275.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=348000, episode_reward=1579.60 +/- 188.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=349000, episode_reward=1602.20 +/- 141.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=350000, episode_reward=1809.60 +/- 143.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=351000, episode_reward=1534.60 +/- 146.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=352000, episode_reward=1547.20 +/- 111.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=353000, episode_reward=1427.00 +/- 284.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=354000, episode_reward=1425.20 +/- 296.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=355000, episode_reward=1491.40 +/- 140.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=356000, episode_reward=1555.80 +/- 216.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=357000, episode_reward=1427.20 +/- 196.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=358000, episode_reward=1515.40 +/- 188.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=359000, episode_reward=1444.60 +/- 278.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=360000, episode_reward=1714.80 +/- 257.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=361000, episode_reward=1699.20 +/- 88.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=362000, episode_reward=1596.00 +/- 79.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=363000, episode_reward=1541.20 +/- 77.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=364000, episode_reward=1509.40 +/- 259.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=365000, episode_reward=1496.60 +/- 315.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=366000, episode_reward=1528.00 +/- 260.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=367000, episode_reward=1605.40 +/- 262.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=368000, episode_reward=1571.40 +/- 216.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=369000, episode_reward=1487.20 +/- 296.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=370000, episode_reward=1679.40 +/- 294.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=371000, episode_reward=1573.60 +/- 227.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=372000, episode_reward=1567.20 +/- 231.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=373000, episode_reward=1360.00 +/- 234.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=374000, episode_reward=1531.00 +/- 230.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=375000, episode_reward=1587.00 +/- 230.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=376000, episode_reward=1797.40 +/- 308.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=377000, episode_reward=1537.20 +/- 347.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=378000, episode_reward=1550.80 +/- 285.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=379000, episode_reward=1605.40 +/- 226.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=380000, episode_reward=1519.20 +/- 203.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=381000, episode_reward=1475.40 +/- 217.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=382000, episode_reward=1753.60 +/- 260.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=383000, episode_reward=1494.80 +/- 87.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=384000, episode_reward=1529.20 +/- 246.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=385000, episode_reward=1304.40 +/- 347.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=386000, episode_reward=1752.60 +/- 92.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=387000, episode_reward=1477.80 +/- 90.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=388000, episode_reward=1499.20 +/- 153.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=389000, episode_reward=1523.00 +/- 129.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=390000, episode_reward=1707.80 +/- 97.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=391000, episode_reward=1546.00 +/- 126.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=392000, episode_reward=1602.80 +/- 293.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=393000, episode_reward=1341.60 +/- 174.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=394000, episode_reward=1851.00 +/- 144.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=395000, episode_reward=1704.80 +/- 156.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=396000, episode_reward=1314.80 +/- 386.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=397000, episode_reward=1312.60 +/- 143.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=398000, episode_reward=1604.00 +/- 378.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=399000, episode_reward=1782.00 +/- 189.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=400000, episode_reward=1642.20 +/- 201.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=401000, episode_reward=1649.40 +/- 220.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=402000, episode_reward=1653.20 +/- 245.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=403000, episode_reward=1729.00 +/- 114.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=404000, episode_reward=1387.00 +/- 325.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=405000, episode_reward=1613.60 +/- 182.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=406000, episode_reward=1450.80 +/- 287.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=407000, episode_reward=1614.80 +/- 265.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=408000, episode_reward=1550.00 +/- 300.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=409000, episode_reward=1515.00 +/- 227.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=410000, episode_reward=1570.80 +/- 134.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=411000, episode_reward=1372.20 +/- 438.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=412000, episode_reward=1727.60 +/- 212.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=413000, episode_reward=1600.80 +/- 238.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=414000, episode_reward=1706.20 +/- 194.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=415000, episode_reward=1491.40 +/- 125.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=416000, episode_reward=1731.00 +/- 96.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=417000, episode_reward=1439.00 +/- 67.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=418000, episode_reward=1532.60 +/- 194.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=419000, episode_reward=1514.80 +/- 333.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=420000, episode_reward=1801.40 +/- 181.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=421000, episode_reward=1545.40 +/- 82.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=422000, episode_reward=1661.40 +/- 128.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=423000, episode_reward=1568.20 +/- 368.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=424000, episode_reward=1576.00 +/- 244.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=425000, episode_reward=1299.40 +/- 352.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=426000, episode_reward=1707.80 +/- 232.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=427000, episode_reward=1316.80 +/- 170.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=428000, episode_reward=1799.00 +/- 215.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=429000, episode_reward=1649.80 +/- 168.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=430000, episode_reward=1563.20 +/- 312.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=431000, episode_reward=1553.40 +/- 302.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=432000, episode_reward=1543.20 +/- 326.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=433000, episode_reward=1538.60 +/- 193.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=434000, episode_reward=1705.40 +/- 170.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=435000, episode_reward=1569.40 +/- 267.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=436000, episode_reward=1588.40 +/- 230.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=437000, episode_reward=1644.60 +/- 197.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=438000, episode_reward=1519.00 +/- 137.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=439000, episode_reward=1644.80 +/- 139.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=440000, episode_reward=1597.80 +/- 80.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=441000, episode_reward=1668.60 +/- 104.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=442000, episode_reward=1651.60 +/- 311.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=443000, episode_reward=1515.60 +/- 153.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=444000, episode_reward=1424.80 +/- 242.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=445000, episode_reward=1563.00 +/- 376.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=446000, episode_reward=1775.60 +/- 320.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=447000, episode_reward=1596.60 +/- 268.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=448000, episode_reward=1514.80 +/- 249.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=449000, episode_reward=1571.40 +/- 148.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=450000, episode_reward=1582.80 +/- 113.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=451000, episode_reward=1632.60 +/- 300.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=452000, episode_reward=1555.40 +/- 242.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=453000, episode_reward=1579.00 +/- 229.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=454000, episode_reward=1641.80 +/- 290.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=455000, episode_reward=1181.80 +/- 249.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=456000, episode_reward=1663.80 +/- 140.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=457000, episode_reward=1568.00 +/- 241.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=458000, episode_reward=1805.60 +/- 181.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=459000, episode_reward=1684.00 +/- 230.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=460000, episode_reward=1818.60 +/- 165.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=461000, episode_reward=1601.80 +/- 390.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=462000, episode_reward=1532.00 +/- 131.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=463000, episode_reward=1549.60 +/- 370.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=464000, episode_reward=1783.60 +/- 223.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=465000, episode_reward=1665.60 +/- 234.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=466000, episode_reward=1524.20 +/- 209.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=467000, episode_reward=1642.00 +/- 180.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=468000, episode_reward=1345.20 +/- 280.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=469000, episode_reward=1664.20 +/- 233.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=470000, episode_reward=1627.60 +/- 161.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=471000, episode_reward=1511.00 +/- 141.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=472000, episode_reward=1622.00 +/- 219.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=473000, episode_reward=1752.80 +/- 248.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=474000, episode_reward=1546.80 +/- 176.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=475000, episode_reward=1681.40 +/- 238.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=476000, episode_reward=1631.20 +/- 420.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=477000, episode_reward=1734.20 +/- 124.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=478000, episode_reward=1536.20 +/- 105.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=479000, episode_reward=1514.40 +/- 115.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=480000, episode_reward=1429.80 +/- 220.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=481000, episode_reward=1375.20 +/- 204.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=482000, episode_reward=1522.60 +/- 242.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=483000, episode_reward=1678.60 +/- 138.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=484000, episode_reward=1612.40 +/- 385.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=485000, episode_reward=1564.60 +/- 132.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=486000, episode_reward=1463.60 +/- 140.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=487000, episode_reward=1714.80 +/- 337.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=488000, episode_reward=1698.40 +/- 174.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=489000, episode_reward=1626.00 +/- 205.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=490000, episode_reward=1465.60 +/- 321.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=491000, episode_reward=1541.20 +/- 304.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=492000, episode_reward=1767.60 +/- 61.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=493000, episode_reward=1510.20 +/- 125.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=494000, episode_reward=1686.80 +/- 260.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=495000, episode_reward=1558.60 +/- 239.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=496000, episode_reward=1393.00 +/- 324.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=497000, episode_reward=1521.80 +/- 239.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=498000, episode_reward=1597.40 +/- 192.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=499000, episode_reward=1522.20 +/- 179.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=500000, episode_reward=1539.60 +/- 112.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=501000, episode_reward=1481.00 +/- 157.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Gamma = 0.9, Learning Rate = 0.0003, Activation Function = ReLU, Neurons = 8, Process noise = 0.1, Measurement noise = 0.0\n",
      "Eval num_timesteps=1000, episode_reward=-1615.80 +/- 129.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=-1665.80 +/- 83.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=3000, episode_reward=-6530.20 +/- 2433.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=4000, episode_reward=-5505.20 +/- 1750.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=5000, episode_reward=431.00 +/- 104.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=6000, episode_reward=406.60 +/- 118.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=7000, episode_reward=393.80 +/- 153.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=8000, episode_reward=315.40 +/- 129.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=9000, episode_reward=359.60 +/- 124.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=10000, episode_reward=460.60 +/- 164.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=11000, episode_reward=210.60 +/- 48.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=12000, episode_reward=315.80 +/- 252.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=13000, episode_reward=294.20 +/- 106.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=14000, episode_reward=335.20 +/- 124.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=15000, episode_reward=390.40 +/- 143.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=16000, episode_reward=209.40 +/- 105.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=17000, episode_reward=490.00 +/- 242.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=18000, episode_reward=447.40 +/- 73.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=19000, episode_reward=419.80 +/- 172.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=20000, episode_reward=267.00 +/- 175.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=21000, episode_reward=351.80 +/- 254.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=22000, episode_reward=254.20 +/- 105.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=23000, episode_reward=155.00 +/- 145.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=24000, episode_reward=429.20 +/- 141.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=25000, episode_reward=203.80 +/- 108.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=26000, episode_reward=517.00 +/- 99.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=27000, episode_reward=267.00 +/- 125.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=28000, episode_reward=230.40 +/- 206.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=29000, episode_reward=242.20 +/- 214.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=30000, episode_reward=321.40 +/- 122.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=31000, episode_reward=443.00 +/- 311.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=32000, episode_reward=474.40 +/- 257.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=33000, episode_reward=329.40 +/- 184.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=34000, episode_reward=266.40 +/- 209.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=35000, episode_reward=284.60 +/- 96.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=36000, episode_reward=484.20 +/- 142.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=37000, episode_reward=330.40 +/- 164.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=38000, episode_reward=424.20 +/- 149.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=39000, episode_reward=149.00 +/- 257.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=40000, episode_reward=355.00 +/- 119.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=41000, episode_reward=357.80 +/- 221.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=42000, episode_reward=510.20 +/- 194.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=43000, episode_reward=326.20 +/- 187.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=44000, episode_reward=348.60 +/- 302.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=45000, episode_reward=311.20 +/- 259.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=46000, episode_reward=377.40 +/- 307.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=47000, episode_reward=248.00 +/- 71.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=48000, episode_reward=393.00 +/- 201.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=49000, episode_reward=182.00 +/- 329.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=50000, episode_reward=299.60 +/- 256.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=51000, episode_reward=209.40 +/- 293.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=52000, episode_reward=421.80 +/- 169.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=53000, episode_reward=159.20 +/- 268.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=54000, episode_reward=232.00 +/- 172.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=55000, episode_reward=153.20 +/- 216.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=56000, episode_reward=268.80 +/- 227.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=57000, episode_reward=218.20 +/- 255.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=58000, episode_reward=240.00 +/- 198.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=59000, episode_reward=292.40 +/- 275.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=60000, episode_reward=305.60 +/- 252.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=61000, episode_reward=343.60 +/- 215.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=62000, episode_reward=338.40 +/- 122.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=63000, episode_reward=476.00 +/- 142.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=64000, episode_reward=351.20 +/- 206.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=65000, episode_reward=316.80 +/- 164.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=66000, episode_reward=330.00 +/- 208.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=67000, episode_reward=327.40 +/- 78.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=68000, episode_reward=358.00 +/- 141.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=69000, episode_reward=331.20 +/- 148.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=70000, episode_reward=327.00 +/- 138.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=71000, episode_reward=355.40 +/- 120.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=72000, episode_reward=327.40 +/- 317.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=73000, episode_reward=340.40 +/- 164.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=74000, episode_reward=330.40 +/- 196.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=75000, episode_reward=374.00 +/- 155.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=76000, episode_reward=371.60 +/- 218.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=77000, episode_reward=237.40 +/- 176.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=78000, episode_reward=259.60 +/- 100.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=79000, episode_reward=376.20 +/- 249.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=80000, episode_reward=382.20 +/- 116.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=81000, episode_reward=303.00 +/- 110.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=82000, episode_reward=536.60 +/- 149.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=83000, episode_reward=221.60 +/- 118.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=84000, episode_reward=207.80 +/- 190.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=85000, episode_reward=334.60 +/- 106.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=86000, episode_reward=295.00 +/- 65.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=87000, episode_reward=335.20 +/- 225.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=88000, episode_reward=374.80 +/- 91.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=89000, episode_reward=321.40 +/- 124.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=90000, episode_reward=347.80 +/- 160.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=91000, episode_reward=233.40 +/- 171.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=92000, episode_reward=202.60 +/- 54.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=93000, episode_reward=438.80 +/- 118.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=94000, episode_reward=125.80 +/- 161.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=95000, episode_reward=395.80 +/- 221.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=96000, episode_reward=364.80 +/- 141.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=97000, episode_reward=61.00 +/- 92.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=98000, episode_reward=282.00 +/- 177.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=99000, episode_reward=341.60 +/- 165.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=100000, episode_reward=273.80 +/- 91.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=101000, episode_reward=318.80 +/- 182.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=102000, episode_reward=225.20 +/- 176.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=103000, episode_reward=335.20 +/- 79.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=104000, episode_reward=267.60 +/- 236.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=105000, episode_reward=430.40 +/- 264.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=106000, episode_reward=312.60 +/- 180.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=107000, episode_reward=374.60 +/- 215.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=108000, episode_reward=359.20 +/- 190.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=109000, episode_reward=196.00 +/- 230.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=110000, episode_reward=360.80 +/- 149.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=111000, episode_reward=201.20 +/- 90.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=112000, episode_reward=461.60 +/- 186.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=113000, episode_reward=172.60 +/- 129.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=114000, episode_reward=155.00 +/- 172.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=115000, episode_reward=266.40 +/- 103.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=116000, episode_reward=293.40 +/- 69.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=117000, episode_reward=350.00 +/- 58.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=118000, episode_reward=356.60 +/- 232.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=119000, episode_reward=232.40 +/- 226.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=120000, episode_reward=348.20 +/- 255.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=121000, episode_reward=227.80 +/- 148.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=122000, episode_reward=253.40 +/- 146.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=123000, episode_reward=320.00 +/- 59.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=124000, episode_reward=410.60 +/- 164.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=125000, episode_reward=240.00 +/- 174.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=126000, episode_reward=435.60 +/- 255.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=127000, episode_reward=500.80 +/- 175.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=128000, episode_reward=150.60 +/- 141.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=129000, episode_reward=290.00 +/- 160.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=130000, episode_reward=364.40 +/- 166.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=131000, episode_reward=302.20 +/- 144.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=132000, episode_reward=319.40 +/- 236.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=133000, episode_reward=314.20 +/- 128.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=134000, episode_reward=309.00 +/- 129.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=135000, episode_reward=334.40 +/- 87.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=136000, episode_reward=438.20 +/- 180.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=137000, episode_reward=328.20 +/- 221.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=138000, episode_reward=289.00 +/- 253.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=139000, episode_reward=415.40 +/- 83.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=140000, episode_reward=485.80 +/- 110.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=141000, episode_reward=319.60 +/- 71.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=142000, episode_reward=441.80 +/- 191.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=143000, episode_reward=244.20 +/- 86.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=144000, episode_reward=172.40 +/- 172.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=145000, episode_reward=362.20 +/- 183.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=146000, episode_reward=329.60 +/- 232.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=147000, episode_reward=212.80 +/- 209.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=148000, episode_reward=337.60 +/- 308.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=149000, episode_reward=307.60 +/- 193.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=150000, episode_reward=345.80 +/- 251.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=151000, episode_reward=354.80 +/- 149.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=152000, episode_reward=259.60 +/- 67.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=153000, episode_reward=323.60 +/- 271.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=154000, episode_reward=782.60 +/- 205.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=155000, episode_reward=605.20 +/- 256.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=156000, episode_reward=480.60 +/- 221.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=157000, episode_reward=435.80 +/- 177.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=158000, episode_reward=592.40 +/- 126.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=159000, episode_reward=739.60 +/- 306.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=160000, episode_reward=612.40 +/- 331.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=161000, episode_reward=698.20 +/- 239.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=162000, episode_reward=830.20 +/- 211.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=163000, episode_reward=648.20 +/- 308.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=164000, episode_reward=641.40 +/- 186.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=165000, episode_reward=657.00 +/- 68.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=166000, episode_reward=536.80 +/- 221.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=167000, episode_reward=782.20 +/- 199.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=168000, episode_reward=753.80 +/- 195.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=169000, episode_reward=733.00 +/- 207.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=170000, episode_reward=634.20 +/- 196.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=171000, episode_reward=585.40 +/- 270.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=172000, episode_reward=822.20 +/- 265.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=173000, episode_reward=845.00 +/- 180.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=174000, episode_reward=1204.80 +/- 165.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=175000, episode_reward=912.20 +/- 203.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=176000, episode_reward=865.40 +/- 228.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=177000, episode_reward=609.00 +/- 170.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=178000, episode_reward=829.80 +/- 254.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=179000, episode_reward=877.60 +/- 369.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=180000, episode_reward=745.60 +/- 153.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=181000, episode_reward=989.00 +/- 292.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=182000, episode_reward=1047.20 +/- 126.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=183000, episode_reward=771.00 +/- 46.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=184000, episode_reward=859.40 +/- 182.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=185000, episode_reward=809.80 +/- 89.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=186000, episode_reward=962.60 +/- 157.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=187000, episode_reward=942.60 +/- 219.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=188000, episode_reward=965.60 +/- 192.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=189000, episode_reward=1017.00 +/- 178.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=190000, episode_reward=1126.20 +/- 220.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=191000, episode_reward=863.40 +/- 165.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=192000, episode_reward=1086.80 +/- 137.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=193000, episode_reward=1126.40 +/- 243.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=194000, episode_reward=1048.40 +/- 201.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=195000, episode_reward=887.00 +/- 185.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=196000, episode_reward=849.80 +/- 162.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=197000, episode_reward=855.40 +/- 418.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=198000, episode_reward=928.20 +/- 257.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=199000, episode_reward=1029.80 +/- 199.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=200000, episode_reward=904.40 +/- 349.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=201000, episode_reward=1013.80 +/- 169.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=202000, episode_reward=1054.20 +/- 130.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=203000, episode_reward=814.00 +/- 192.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=204000, episode_reward=1021.40 +/- 342.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=205000, episode_reward=910.00 +/- 172.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=206000, episode_reward=905.60 +/- 358.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=207000, episode_reward=1099.00 +/- 334.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=208000, episode_reward=1137.60 +/- 259.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=209000, episode_reward=890.60 +/- 92.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=210000, episode_reward=-225.40 +/- 2707.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=211000, episode_reward=1106.60 +/- 189.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=212000, episode_reward=1229.80 +/- 174.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=213000, episode_reward=1074.80 +/- 156.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=214000, episode_reward=1106.80 +/- 198.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=215000, episode_reward=1177.60 +/- 92.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=216000, episode_reward=1155.20 +/- 228.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=217000, episode_reward=1039.20 +/- 172.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=218000, episode_reward=1024.20 +/- 366.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=219000, episode_reward=1107.40 +/- 241.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=220000, episode_reward=1248.40 +/- 178.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=221000, episode_reward=945.00 +/- 201.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=222000, episode_reward=982.60 +/- 292.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=223000, episode_reward=1059.00 +/- 313.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=224000, episode_reward=1096.60 +/- 205.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=225000, episode_reward=1061.80 +/- 241.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=226000, episode_reward=511.00 +/- 1547.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=227000, episode_reward=1048.00 +/- 142.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=228000, episode_reward=1217.80 +/- 169.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=229000, episode_reward=1124.60 +/- 232.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=230000, episode_reward=927.40 +/- 364.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=231000, episode_reward=973.00 +/- 286.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=232000, episode_reward=931.40 +/- 170.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=233000, episode_reward=665.60 +/- 152.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=234000, episode_reward=915.20 +/- 277.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=235000, episode_reward=1063.40 +/- 205.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=236000, episode_reward=1154.20 +/- 199.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=237000, episode_reward=985.60 +/- 152.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=238000, episode_reward=-1940.60 +/- 6074.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=239000, episode_reward=1042.20 +/- 237.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=240000, episode_reward=1035.40 +/- 165.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=241000, episode_reward=1025.60 +/- 263.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=242000, episode_reward=1188.80 +/- 88.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=243000, episode_reward=1179.80 +/- 35.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=244000, episode_reward=870.00 +/- 324.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=245000, episode_reward=1066.80 +/- 266.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=246000, episode_reward=1332.40 +/- 310.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=247000, episode_reward=1039.80 +/- 154.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=248000, episode_reward=1318.40 +/- 278.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=249000, episode_reward=1149.60 +/- 228.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=250000, episode_reward=850.40 +/- 121.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=251000, episode_reward=1062.00 +/- 146.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=252000, episode_reward=936.40 +/- 294.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=253000, episode_reward=1078.20 +/- 354.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=254000, episode_reward=840.00 +/- 299.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=255000, episode_reward=1306.80 +/- 240.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=256000, episode_reward=1140.60 +/- 182.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=257000, episode_reward=1051.40 +/- 261.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=258000, episode_reward=1245.60 +/- 179.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=259000, episode_reward=1432.80 +/- 192.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=260000, episode_reward=1114.80 +/- 349.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=261000, episode_reward=1258.00 +/- 166.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=262000, episode_reward=1201.60 +/- 79.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=263000, episode_reward=1278.20 +/- 211.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=264000, episode_reward=1030.20 +/- 373.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=265000, episode_reward=1128.60 +/- 239.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=266000, episode_reward=-803.20 +/- 4304.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=267000, episode_reward=1192.40 +/- 347.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=268000, episode_reward=1119.80 +/- 406.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=269000, episode_reward=1184.60 +/- 190.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=270000, episode_reward=1275.40 +/- 218.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=271000, episode_reward=1304.20 +/- 322.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=272000, episode_reward=1173.60 +/- 49.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=273000, episode_reward=1423.00 +/- 130.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=274000, episode_reward=1024.20 +/- 294.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=275000, episode_reward=997.40 +/- 205.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=276000, episode_reward=1323.80 +/- 207.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=277000, episode_reward=-1149.20 +/- 4614.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=278000, episode_reward=1270.60 +/- 165.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=279000, episode_reward=1190.80 +/- 145.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=280000, episode_reward=1194.00 +/- 214.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=281000, episode_reward=1117.60 +/- 95.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=282000, episode_reward=1208.00 +/- 152.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=283000, episode_reward=1172.20 +/- 150.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=284000, episode_reward=1218.00 +/- 192.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=285000, episode_reward=1136.80 +/- 219.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=286000, episode_reward=-999.60 +/- 4388.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=287000, episode_reward=1034.80 +/- 277.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=288000, episode_reward=1177.60 +/- 193.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=289000, episode_reward=1164.80 +/- 264.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=290000, episode_reward=1429.20 +/- 206.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=291000, episode_reward=1294.80 +/- 217.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=292000, episode_reward=1293.20 +/- 181.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=293000, episode_reward=1086.40 +/- 177.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=294000, episode_reward=1060.60 +/- 263.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=295000, episode_reward=1447.40 +/- 294.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=296000, episode_reward=1292.80 +/- 236.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=297000, episode_reward=-827.80 +/- 4190.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=298000, episode_reward=1341.60 +/- 304.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=299000, episode_reward=1343.20 +/- 184.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=300000, episode_reward=998.40 +/- 213.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=301000, episode_reward=986.80 +/- 280.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=302000, episode_reward=1229.00 +/- 308.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=303000, episode_reward=1237.20 +/- 191.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=304000, episode_reward=1213.60 +/- 254.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=305000, episode_reward=1109.80 +/- 87.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=306000, episode_reward=946.40 +/- 370.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=307000, episode_reward=1052.60 +/- 181.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=308000, episode_reward=1150.60 +/- 347.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=309000, episode_reward=1333.20 +/- 150.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=310000, episode_reward=-1693.40 +/- 5612.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=311000, episode_reward=1151.00 +/- 204.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=312000, episode_reward=1152.00 +/- 267.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=313000, episode_reward=1315.20 +/- 177.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=314000, episode_reward=1296.20 +/- 286.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=315000, episode_reward=1156.00 +/- 159.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=316000, episode_reward=1094.20 +/- 136.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=317000, episode_reward=1146.40 +/- 120.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=318000, episode_reward=1284.60 +/- 203.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=319000, episode_reward=1055.80 +/- 119.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=320000, episode_reward=1160.00 +/- 146.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=321000, episode_reward=983.80 +/- 242.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=322000, episode_reward=1400.80 +/- 272.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=323000, episode_reward=1455.20 +/- 255.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=324000, episode_reward=1456.20 +/- 277.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=325000, episode_reward=1326.00 +/- 287.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=326000, episode_reward=1359.40 +/- 229.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=327000, episode_reward=1256.60 +/- 219.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=328000, episode_reward=1054.40 +/- 314.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=329000, episode_reward=1325.00 +/- 182.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=330000, episode_reward=1293.40 +/- 291.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=331000, episode_reward=1247.40 +/- 297.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=332000, episode_reward=1144.20 +/- 209.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=333000, episode_reward=1185.60 +/- 175.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=334000, episode_reward=1343.20 +/- 283.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=335000, episode_reward=1373.20 +/- 220.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=336000, episode_reward=1324.00 +/- 270.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=337000, episode_reward=1204.00 +/- 169.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=338000, episode_reward=1526.20 +/- 169.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=339000, episode_reward=1348.60 +/- 102.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=340000, episode_reward=1223.40 +/- 316.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=341000, episode_reward=1301.60 +/- 316.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=342000, episode_reward=1252.20 +/- 171.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=343000, episode_reward=994.80 +/- 338.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=344000, episode_reward=1182.80 +/- 215.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=345000, episode_reward=1041.00 +/- 392.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=346000, episode_reward=1451.20 +/- 240.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=347000, episode_reward=1130.40 +/- 318.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=348000, episode_reward=1051.80 +/- 296.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=349000, episode_reward=1152.60 +/- 215.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=350000, episode_reward=1329.00 +/- 227.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=351000, episode_reward=1437.80 +/- 159.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=352000, episode_reward=1308.40 +/- 154.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=353000, episode_reward=1132.80 +/- 182.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=354000, episode_reward=1188.60 +/- 165.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=355000, episode_reward=1071.60 +/- 320.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=356000, episode_reward=1291.20 +/- 290.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=357000, episode_reward=1395.00 +/- 323.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=358000, episode_reward=1413.20 +/- 179.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=359000, episode_reward=1210.80 +/- 278.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=360000, episode_reward=1112.20 +/- 191.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=361000, episode_reward=-1177.00 +/- 4836.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=362000, episode_reward=1263.60 +/- 111.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=363000, episode_reward=619.60 +/- 1670.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=364000, episode_reward=1343.80 +/- 264.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=365000, episode_reward=1446.40 +/- 258.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=366000, episode_reward=1169.80 +/- 367.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=367000, episode_reward=1128.60 +/- 325.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=368000, episode_reward=1330.60 +/- 169.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=369000, episode_reward=1078.40 +/- 259.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=370000, episode_reward=1572.80 +/- 117.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=371000, episode_reward=1124.20 +/- 228.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=372000, episode_reward=1321.40 +/- 325.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=373000, episode_reward=1298.80 +/- 158.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=374000, episode_reward=1261.20 +/- 144.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=375000, episode_reward=1243.20 +/- 190.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=376000, episode_reward=1286.00 +/- 288.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=377000, episode_reward=1393.00 +/- 133.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=378000, episode_reward=1274.40 +/- 152.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=379000, episode_reward=1359.20 +/- 163.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=380000, episode_reward=1184.80 +/- 215.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=381000, episode_reward=1209.60 +/- 403.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=382000, episode_reward=868.60 +/- 148.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=383000, episode_reward=1185.40 +/- 71.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=384000, episode_reward=1372.80 +/- 131.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=385000, episode_reward=1167.80 +/- 296.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=386000, episode_reward=1121.20 +/- 195.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=387000, episode_reward=1302.80 +/- 233.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=388000, episode_reward=1114.60 +/- 154.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=389000, episode_reward=1303.00 +/- 130.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=390000, episode_reward=1355.00 +/- 203.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=391000, episode_reward=1008.00 +/- 288.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=392000, episode_reward=1541.20 +/- 132.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=393000, episode_reward=1366.20 +/- 200.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=394000, episode_reward=1240.40 +/- 401.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=395000, episode_reward=1194.40 +/- 143.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=396000, episode_reward=1477.20 +/- 250.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=397000, episode_reward=1320.80 +/- 259.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=398000, episode_reward=1350.80 +/- 328.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=399000, episode_reward=1473.80 +/- 182.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=400000, episode_reward=1149.60 +/- 302.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=401000, episode_reward=1480.20 +/- 114.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=402000, episode_reward=1178.80 +/- 166.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=403000, episode_reward=1218.20 +/- 190.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=404000, episode_reward=1242.20 +/- 227.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=405000, episode_reward=1615.00 +/- 155.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=406000, episode_reward=1281.20 +/- 217.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=407000, episode_reward=1236.80 +/- 336.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=408000, episode_reward=1409.00 +/- 195.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=409000, episode_reward=1386.40 +/- 100.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=410000, episode_reward=1237.00 +/- 291.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=411000, episode_reward=1435.20 +/- 278.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=412000, episode_reward=1420.20 +/- 141.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=413000, episode_reward=1161.00 +/- 247.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=414000, episode_reward=1341.00 +/- 255.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=415000, episode_reward=1440.00 +/- 190.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=416000, episode_reward=1371.20 +/- 322.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=417000, episode_reward=1390.40 +/- 244.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=418000, episode_reward=1322.60 +/- 229.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=419000, episode_reward=1570.80 +/- 203.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=420000, episode_reward=1468.80 +/- 104.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=421000, episode_reward=1389.20 +/- 169.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=422000, episode_reward=1426.80 +/- 278.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=423000, episode_reward=1296.60 +/- 118.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=424000, episode_reward=1325.40 +/- 227.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=425000, episode_reward=1266.40 +/- 90.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=426000, episode_reward=1218.00 +/- 311.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=427000, episode_reward=1539.20 +/- 104.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=428000, episode_reward=1449.80 +/- 175.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=429000, episode_reward=1373.20 +/- 219.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=430000, episode_reward=1339.20 +/- 283.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=431000, episode_reward=1295.40 +/- 299.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=432000, episode_reward=1459.60 +/- 179.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=433000, episode_reward=1497.20 +/- 280.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=434000, episode_reward=1391.00 +/- 187.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=435000, episode_reward=1419.60 +/- 172.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=436000, episode_reward=1492.60 +/- 243.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=437000, episode_reward=1385.40 +/- 91.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=438000, episode_reward=1411.40 +/- 186.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=439000, episode_reward=1151.80 +/- 273.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=440000, episode_reward=1216.00 +/- 212.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=441000, episode_reward=1334.60 +/- 196.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=442000, episode_reward=1525.80 +/- 139.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=443000, episode_reward=1359.40 +/- 269.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=444000, episode_reward=1370.40 +/- 267.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=445000, episode_reward=1276.60 +/- 216.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=446000, episode_reward=1359.20 +/- 232.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=447000, episode_reward=1349.00 +/- 355.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=448000, episode_reward=1347.40 +/- 162.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=449000, episode_reward=1415.00 +/- 153.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=450000, episode_reward=1418.40 +/- 116.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=451000, episode_reward=1419.20 +/- 286.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=452000, episode_reward=1239.60 +/- 289.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=453000, episode_reward=1422.80 +/- 252.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=454000, episode_reward=1277.60 +/- 137.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=455000, episode_reward=1352.40 +/- 254.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=456000, episode_reward=1240.40 +/- 247.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=457000, episode_reward=1358.60 +/- 169.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=458000, episode_reward=1395.00 +/- 167.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=459000, episode_reward=1459.80 +/- 142.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=460000, episode_reward=1434.60 +/- 106.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=461000, episode_reward=1644.60 +/- 104.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=462000, episode_reward=1388.00 +/- 217.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=463000, episode_reward=1386.20 +/- 195.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=464000, episode_reward=1531.60 +/- 189.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=465000, episode_reward=1211.40 +/- 279.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=466000, episode_reward=1311.80 +/- 205.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=467000, episode_reward=1625.40 +/- 264.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=468000, episode_reward=1614.40 +/- 96.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=469000, episode_reward=1375.00 +/- 379.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=470000, episode_reward=1505.40 +/- 269.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=471000, episode_reward=1272.60 +/- 155.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=472000, episode_reward=1379.20 +/- 295.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=473000, episode_reward=1507.80 +/- 296.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=474000, episode_reward=1420.40 +/- 185.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=475000, episode_reward=1367.80 +/- 139.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=476000, episode_reward=1344.00 +/- 220.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=477000, episode_reward=1414.80 +/- 173.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=478000, episode_reward=1458.00 +/- 195.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=479000, episode_reward=1424.60 +/- 230.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=480000, episode_reward=1579.40 +/- 97.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=481000, episode_reward=1415.60 +/- 191.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=482000, episode_reward=1321.60 +/- 242.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=483000, episode_reward=1380.60 +/- 88.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=484000, episode_reward=1490.00 +/- 90.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=485000, episode_reward=1283.20 +/- 272.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=486000, episode_reward=1377.60 +/- 301.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=487000, episode_reward=1490.20 +/- 175.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=488000, episode_reward=1519.80 +/- 130.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=489000, episode_reward=1304.60 +/- 279.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=490000, episode_reward=1497.40 +/- 88.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=491000, episode_reward=1458.20 +/- 137.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=492000, episode_reward=1429.00 +/- 177.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=493000, episode_reward=1405.40 +/- 118.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=494000, episode_reward=1533.80 +/- 95.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=495000, episode_reward=1563.20 +/- 143.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=496000, episode_reward=1498.60 +/- 159.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=497000, episode_reward=1328.00 +/- 94.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=498000, episode_reward=1381.60 +/- 199.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=499000, episode_reward=1183.00 +/- 201.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=500000, episode_reward=1259.40 +/- 227.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=501000, episode_reward=1373.40 +/- 322.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Gamma = 0.9, Learning Rate = 0.0003, Activation Function = ReLU, Neurons = 32, Process noise = 0.1, Measurement noise = 0.0\n",
      "Eval num_timesteps=1000, episode_reward=-976.80 +/- 161.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=-821.60 +/- 273.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3000, episode_reward=-22.20 +/- 276.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4000, episode_reward=237.60 +/- 196.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=5000, episode_reward=245.00 +/- 219.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=6000, episode_reward=360.80 +/- 148.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=7000, episode_reward=309.60 +/- 155.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=8000, episode_reward=409.60 +/- 141.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=9000, episode_reward=301.40 +/- 97.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=10000, episode_reward=249.60 +/- 197.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=11000, episode_reward=609.40 +/- 179.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=12000, episode_reward=360.20 +/- 120.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=13000, episode_reward=463.60 +/- 208.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=14000, episode_reward=538.40 +/- 133.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=15000, episode_reward=430.20 +/- 132.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=16000, episode_reward=397.60 +/- 241.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=17000, episode_reward=423.20 +/- 167.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=18000, episode_reward=387.40 +/- 162.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=19000, episode_reward=363.60 +/- 185.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=20000, episode_reward=364.40 +/- 212.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=21000, episode_reward=432.40 +/- 94.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=22000, episode_reward=445.80 +/- 172.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=23000, episode_reward=79.60 +/- 89.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=24000, episode_reward=399.20 +/- 256.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=25000, episode_reward=392.40 +/- 148.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=26000, episode_reward=156.20 +/- 206.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=27000, episode_reward=458.40 +/- 182.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=28000, episode_reward=447.60 +/- 178.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=29000, episode_reward=410.00 +/- 136.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=30000, episode_reward=218.20 +/- 140.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=31000, episode_reward=433.80 +/- 209.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=32000, episode_reward=393.40 +/- 206.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=33000, episode_reward=378.80 +/- 156.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=34000, episode_reward=350.00 +/- 89.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=35000, episode_reward=290.80 +/- 198.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=36000, episode_reward=454.60 +/- 150.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=37000, episode_reward=262.60 +/- 198.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=38000, episode_reward=283.00 +/- 195.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=39000, episode_reward=282.60 +/- 265.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=40000, episode_reward=518.00 +/- 211.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=41000, episode_reward=281.40 +/- 173.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=42000, episode_reward=270.40 +/- 248.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=43000, episode_reward=402.60 +/- 210.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=44000, episode_reward=198.00 +/- 193.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=45000, episode_reward=207.00 +/- 156.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=46000, episode_reward=453.60 +/- 172.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=47000, episode_reward=388.60 +/- 138.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=48000, episode_reward=163.80 +/- 228.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=49000, episode_reward=338.80 +/- 84.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=50000, episode_reward=312.20 +/- 176.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=51000, episode_reward=237.60 +/- 378.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=52000, episode_reward=553.60 +/- 70.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=53000, episode_reward=239.20 +/- 208.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=54000, episode_reward=537.20 +/- 240.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=55000, episode_reward=246.40 +/- 84.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=56000, episode_reward=295.80 +/- 133.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=57000, episode_reward=284.20 +/- 75.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=58000, episode_reward=573.00 +/- 215.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=59000, episode_reward=105.40 +/- 223.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=60000, episode_reward=389.20 +/- 309.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=61000, episode_reward=534.60 +/- 423.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=62000, episode_reward=559.80 +/- 164.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=63000, episode_reward=672.80 +/- 218.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=64000, episode_reward=847.20 +/- 352.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=65000, episode_reward=495.80 +/- 393.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=66000, episode_reward=807.20 +/- 142.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=67000, episode_reward=820.20 +/- 128.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=68000, episode_reward=782.80 +/- 312.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=69000, episode_reward=617.80 +/- 328.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=70000, episode_reward=901.40 +/- 249.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=71000, episode_reward=784.20 +/- 297.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=72000, episode_reward=846.20 +/- 282.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=73000, episode_reward=739.60 +/- 203.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=74000, episode_reward=901.60 +/- 246.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=75000, episode_reward=1094.40 +/- 284.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=76000, episode_reward=836.40 +/- 108.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=77000, episode_reward=1055.80 +/- 190.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=78000, episode_reward=883.20 +/- 347.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=79000, episode_reward=1143.20 +/- 237.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=80000, episode_reward=906.80 +/- 218.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=81000, episode_reward=1049.00 +/- 163.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=82000, episode_reward=1009.60 +/- 410.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=83000, episode_reward=1026.20 +/- 192.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=84000, episode_reward=1200.40 +/- 242.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=85000, episode_reward=1232.60 +/- 171.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=86000, episode_reward=1048.60 +/- 188.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=87000, episode_reward=1241.60 +/- 172.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=88000, episode_reward=952.00 +/- 169.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=89000, episode_reward=1306.80 +/- 164.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=90000, episode_reward=1259.80 +/- 323.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=91000, episode_reward=1423.20 +/- 322.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=92000, episode_reward=1232.80 +/- 282.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=93000, episode_reward=1356.60 +/- 161.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=94000, episode_reward=1137.20 +/- 346.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=95000, episode_reward=1500.60 +/- 165.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=96000, episode_reward=1377.60 +/- 192.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=97000, episode_reward=1305.40 +/- 222.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=98000, episode_reward=1143.80 +/- 313.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=99000, episode_reward=1037.00 +/- 275.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=100000, episode_reward=1185.40 +/- 260.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=101000, episode_reward=1392.80 +/- 401.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=102000, episode_reward=1287.60 +/- 227.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=103000, episode_reward=1413.60 +/- 166.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=104000, episode_reward=1495.60 +/- 55.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=105000, episode_reward=1671.40 +/- 264.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=106000, episode_reward=1478.60 +/- 206.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=107000, episode_reward=-2079.00 +/- 7281.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=108000, episode_reward=1727.80 +/- 141.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=109000, episode_reward=1518.80 +/- 243.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=110000, episode_reward=1378.40 +/- 74.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=111000, episode_reward=1678.20 +/- 132.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=112000, episode_reward=1470.00 +/- 170.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=113000, episode_reward=1414.80 +/- 284.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=114000, episode_reward=1568.00 +/- 311.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=115000, episode_reward=1330.20 +/- 350.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=116000, episode_reward=1530.60 +/- 165.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=117000, episode_reward=1405.00 +/- 226.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=118000, episode_reward=1386.60 +/- 353.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=119000, episode_reward=1354.20 +/- 208.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=120000, episode_reward=1521.20 +/- 285.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=121000, episode_reward=1369.20 +/- 177.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=122000, episode_reward=1506.80 +/- 57.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=123000, episode_reward=1320.00 +/- 241.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=124000, episode_reward=1428.40 +/- 320.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=125000, episode_reward=1352.40 +/- 219.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=126000, episode_reward=1231.20 +/- 279.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=127000, episode_reward=1614.00 +/- 85.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=128000, episode_reward=1441.20 +/- 57.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=129000, episode_reward=1737.60 +/- 170.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=130000, episode_reward=1573.20 +/- 304.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=131000, episode_reward=1699.20 +/- 245.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=132000, episode_reward=1477.40 +/- 169.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=133000, episode_reward=1542.60 +/- 275.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=134000, episode_reward=603.20 +/- 2192.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=135000, episode_reward=1661.00 +/- 105.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=136000, episode_reward=1525.60 +/- 225.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=137000, episode_reward=1603.40 +/- 192.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=138000, episode_reward=1228.20 +/- 223.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=139000, episode_reward=1462.00 +/- 249.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=140000, episode_reward=1493.20 +/- 254.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=141000, episode_reward=1611.00 +/- 260.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=142000, episode_reward=1401.40 +/- 74.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=143000, episode_reward=1641.20 +/- 179.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=144000, episode_reward=1396.00 +/- 307.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=145000, episode_reward=1469.20 +/- 219.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=146000, episode_reward=1425.60 +/- 178.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=147000, episode_reward=1534.60 +/- 325.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=148000, episode_reward=1681.20 +/- 118.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=149000, episode_reward=1572.80 +/- 136.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=150000, episode_reward=1522.80 +/- 208.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=151000, episode_reward=1718.40 +/- 267.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=152000, episode_reward=1447.80 +/- 165.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=153000, episode_reward=1684.40 +/- 320.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=154000, episode_reward=1615.60 +/- 215.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=155000, episode_reward=1562.40 +/- 299.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=156000, episode_reward=1634.80 +/- 96.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=157000, episode_reward=1801.40 +/- 202.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=158000, episode_reward=1438.80 +/- 334.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=159000, episode_reward=1527.80 +/- 226.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=160000, episode_reward=1473.80 +/- 401.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=161000, episode_reward=1696.00 +/- 211.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=162000, episode_reward=1386.00 +/- 98.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=163000, episode_reward=1312.20 +/- 470.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=164000, episode_reward=1498.00 +/- 286.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=165000, episode_reward=1661.60 +/- 193.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=166000, episode_reward=1518.80 +/- 291.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=167000, episode_reward=1676.60 +/- 131.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=168000, episode_reward=1584.00 +/- 206.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=169000, episode_reward=129.00 +/- 2708.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=170000, episode_reward=1632.60 +/- 170.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=171000, episode_reward=1399.60 +/- 233.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=172000, episode_reward=1746.00 +/- 336.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=173000, episode_reward=1450.00 +/- 376.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=174000, episode_reward=1423.60 +/- 187.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=175000, episode_reward=1594.60 +/- 241.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=176000, episode_reward=1627.20 +/- 338.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=177000, episode_reward=1452.60 +/- 327.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=178000, episode_reward=1596.00 +/- 153.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=179000, episode_reward=1408.60 +/- 374.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=180000, episode_reward=1426.20 +/- 257.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=181000, episode_reward=1461.60 +/- 269.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=182000, episode_reward=1449.00 +/- 391.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=183000, episode_reward=1755.80 +/- 244.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=184000, episode_reward=1407.00 +/- 325.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=185000, episode_reward=1257.60 +/- 316.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=186000, episode_reward=1389.20 +/- 230.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=187000, episode_reward=1438.20 +/- 329.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=188000, episode_reward=1304.00 +/- 281.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=189000, episode_reward=1549.80 +/- 165.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=190000, episode_reward=1540.20 +/- 284.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=191000, episode_reward=1310.00 +/- 176.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=192000, episode_reward=1551.60 +/- 243.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=193000, episode_reward=1634.20 +/- 133.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=194000, episode_reward=1515.20 +/- 158.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=195000, episode_reward=1711.80 +/- 115.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=196000, episode_reward=1543.00 +/- 196.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=197000, episode_reward=1573.00 +/- 253.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=198000, episode_reward=1750.80 +/- 60.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=199000, episode_reward=1686.00 +/- 173.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=200000, episode_reward=1566.80 +/- 159.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=201000, episode_reward=1582.20 +/- 211.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=202000, episode_reward=1456.20 +/- 230.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=203000, episode_reward=1540.00 +/- 322.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=204000, episode_reward=1744.20 +/- 207.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=205000, episode_reward=1518.80 +/- 163.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=206000, episode_reward=1360.00 +/- 119.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=207000, episode_reward=1634.80 +/- 331.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=208000, episode_reward=1399.60 +/- 266.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=209000, episode_reward=1671.60 +/- 209.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=210000, episode_reward=1542.80 +/- 216.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=211000, episode_reward=1565.80 +/- 166.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=212000, episode_reward=1566.80 +/- 279.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=213000, episode_reward=1775.20 +/- 120.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=214000, episode_reward=1697.40 +/- 209.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=215000, episode_reward=1543.40 +/- 154.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=216000, episode_reward=1637.80 +/- 151.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=217000, episode_reward=1589.20 +/- 133.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=218000, episode_reward=1379.00 +/- 232.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=219000, episode_reward=1673.00 +/- 213.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=220000, episode_reward=1503.40 +/- 153.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=221000, episode_reward=1636.40 +/- 203.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=222000, episode_reward=1798.60 +/- 143.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=223000, episode_reward=1560.60 +/- 184.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=224000, episode_reward=1323.00 +/- 239.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=225000, episode_reward=1589.40 +/- 310.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=226000, episode_reward=1548.20 +/- 224.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=227000, episode_reward=1549.40 +/- 271.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=228000, episode_reward=1709.00 +/- 162.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=229000, episode_reward=1635.40 +/- 194.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=230000, episode_reward=1506.60 +/- 238.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=231000, episode_reward=1616.60 +/- 249.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=232000, episode_reward=1723.60 +/- 139.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=233000, episode_reward=1642.80 +/- 221.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=234000, episode_reward=1787.40 +/- 253.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=235000, episode_reward=1818.00 +/- 149.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=236000, episode_reward=1470.60 +/- 267.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=237000, episode_reward=1586.20 +/- 231.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=238000, episode_reward=1743.00 +/- 168.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=239000, episode_reward=1592.20 +/- 215.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=240000, episode_reward=1616.20 +/- 285.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=241000, episode_reward=1450.40 +/- 248.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=242000, episode_reward=1429.60 +/- 269.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=243000, episode_reward=1625.40 +/- 133.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=244000, episode_reward=1527.80 +/- 171.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=245000, episode_reward=1768.40 +/- 211.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=246000, episode_reward=1460.20 +/- 267.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=247000, episode_reward=1535.00 +/- 317.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=248000, episode_reward=1396.80 +/- 165.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=249000, episode_reward=1275.60 +/- 189.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=250000, episode_reward=1479.40 +/- 219.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=251000, episode_reward=1473.80 +/- 272.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=252000, episode_reward=1563.20 +/- 164.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=253000, episode_reward=1612.00 +/- 252.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=254000, episode_reward=1534.80 +/- 262.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=255000, episode_reward=1677.80 +/- 211.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=256000, episode_reward=1465.00 +/- 304.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=257000, episode_reward=1373.40 +/- 362.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=258000, episode_reward=1572.60 +/- 66.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=259000, episode_reward=1287.00 +/- 240.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=260000, episode_reward=1610.60 +/- 192.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=261000, episode_reward=1613.00 +/- 325.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=262000, episode_reward=1630.00 +/- 105.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=263000, episode_reward=1566.60 +/- 330.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=264000, episode_reward=1787.80 +/- 78.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=265000, episode_reward=1607.20 +/- 215.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=266000, episode_reward=1627.20 +/- 270.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=267000, episode_reward=1718.60 +/- 223.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=268000, episode_reward=1533.00 +/- 349.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=269000, episode_reward=1441.60 +/- 284.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=270000, episode_reward=1601.40 +/- 213.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=271000, episode_reward=1532.40 +/- 231.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=272000, episode_reward=1689.00 +/- 224.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=273000, episode_reward=1608.00 +/- 314.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=274000, episode_reward=1615.20 +/- 194.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=275000, episode_reward=1590.80 +/- 154.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=276000, episode_reward=1651.40 +/- 159.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=277000, episode_reward=1512.40 +/- 35.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=278000, episode_reward=1625.00 +/- 179.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=279000, episode_reward=1501.40 +/- 146.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=280000, episode_reward=1697.60 +/- 213.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=281000, episode_reward=1570.00 +/- 94.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=282000, episode_reward=1592.80 +/- 177.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=283000, episode_reward=1570.20 +/- 262.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=284000, episode_reward=1469.40 +/- 300.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=285000, episode_reward=1485.00 +/- 251.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=286000, episode_reward=1408.80 +/- 378.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=287000, episode_reward=1626.00 +/- 198.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=288000, episode_reward=1585.60 +/- 228.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=289000, episode_reward=1503.40 +/- 145.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=290000, episode_reward=1678.40 +/- 240.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=291000, episode_reward=1550.80 +/- 222.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=292000, episode_reward=1479.60 +/- 245.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=293000, episode_reward=1317.80 +/- 247.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=294000, episode_reward=1591.00 +/- 307.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=295000, episode_reward=1641.60 +/- 107.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=296000, episode_reward=1583.40 +/- 231.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=297000, episode_reward=1438.40 +/- 81.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=298000, episode_reward=1439.00 +/- 157.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=299000, episode_reward=1734.20 +/- 227.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=300000, episode_reward=1588.40 +/- 145.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=301000, episode_reward=1578.80 +/- 117.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=302000, episode_reward=1678.80 +/- 308.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=303000, episode_reward=1695.20 +/- 97.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=304000, episode_reward=1580.20 +/- 258.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=305000, episode_reward=1409.40 +/- 262.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=306000, episode_reward=1470.00 +/- 568.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=307000, episode_reward=1678.60 +/- 132.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=308000, episode_reward=1483.00 +/- 283.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=309000, episode_reward=1527.00 +/- 285.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=310000, episode_reward=1548.80 +/- 188.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=311000, episode_reward=1584.60 +/- 254.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=312000, episode_reward=1465.60 +/- 319.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=313000, episode_reward=1550.40 +/- 259.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=314000, episode_reward=1767.00 +/- 148.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=315000, episode_reward=1361.40 +/- 391.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=316000, episode_reward=1442.00 +/- 130.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=317000, episode_reward=1463.60 +/- 196.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=318000, episode_reward=1782.60 +/- 216.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=319000, episode_reward=1548.00 +/- 350.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=320000, episode_reward=1464.60 +/- 180.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=321000, episode_reward=1629.00 +/- 192.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=322000, episode_reward=1546.40 +/- 276.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=323000, episode_reward=1399.40 +/- 211.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=324000, episode_reward=1683.00 +/- 209.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=325000, episode_reward=1585.40 +/- 200.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=326000, episode_reward=1741.40 +/- 190.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=327000, episode_reward=1584.60 +/- 55.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=328000, episode_reward=1687.20 +/- 91.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=329000, episode_reward=1452.20 +/- 286.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=330000, episode_reward=1437.80 +/- 295.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=331000, episode_reward=1562.80 +/- 170.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=332000, episode_reward=1566.60 +/- 303.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=333000, episode_reward=1605.00 +/- 151.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=334000, episode_reward=1398.00 +/- 296.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=335000, episode_reward=1418.20 +/- 175.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=336000, episode_reward=1510.60 +/- 335.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=337000, episode_reward=1725.20 +/- 170.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=338000, episode_reward=1657.20 +/- 197.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=339000, episode_reward=1551.00 +/- 307.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=340000, episode_reward=1611.40 +/- 200.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=341000, episode_reward=1301.80 +/- 125.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=342000, episode_reward=1670.40 +/- 138.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=343000, episode_reward=1366.60 +/- 223.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=344000, episode_reward=1655.40 +/- 87.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=345000, episode_reward=1579.20 +/- 236.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=346000, episode_reward=1413.20 +/- 271.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=347000, episode_reward=1345.00 +/- 209.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=348000, episode_reward=1392.20 +/- 237.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=349000, episode_reward=1617.40 +/- 175.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=350000, episode_reward=1572.00 +/- 333.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=351000, episode_reward=1215.40 +/- 150.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=352000, episode_reward=1642.80 +/- 142.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=353000, episode_reward=1627.00 +/- 333.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=354000, episode_reward=1567.40 +/- 277.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=355000, episode_reward=1690.00 +/- 173.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=356000, episode_reward=1773.80 +/- 38.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=357000, episode_reward=1587.00 +/- 278.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=358000, episode_reward=1591.20 +/- 149.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=359000, episode_reward=1727.20 +/- 52.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=360000, episode_reward=1591.20 +/- 171.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=361000, episode_reward=1547.00 +/- 185.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=362000, episode_reward=1672.00 +/- 295.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=363000, episode_reward=1667.60 +/- 222.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=364000, episode_reward=1411.40 +/- 388.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=365000, episode_reward=1675.60 +/- 101.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=366000, episode_reward=1347.00 +/- 212.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=367000, episode_reward=1677.40 +/- 261.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=368000, episode_reward=1742.60 +/- 83.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=369000, episode_reward=1536.00 +/- 125.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=370000, episode_reward=1487.60 +/- 200.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=371000, episode_reward=1593.20 +/- 335.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=372000, episode_reward=1722.40 +/- 199.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=373000, episode_reward=1679.20 +/- 199.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=374000, episode_reward=1725.00 +/- 298.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=375000, episode_reward=1718.60 +/- 201.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=376000, episode_reward=1510.40 +/- 254.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=377000, episode_reward=1453.20 +/- 60.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=378000, episode_reward=1340.20 +/- 263.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=379000, episode_reward=1601.20 +/- 172.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=380000, episode_reward=1353.00 +/- 190.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=381000, episode_reward=1485.80 +/- 114.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=382000, episode_reward=1484.60 +/- 132.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=383000, episode_reward=1624.40 +/- 180.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=384000, episode_reward=1474.40 +/- 205.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=385000, episode_reward=1630.00 +/- 112.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=386000, episode_reward=1707.80 +/- 197.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=387000, episode_reward=1455.00 +/- 192.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=388000, episode_reward=1750.60 +/- 118.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=389000, episode_reward=1580.80 +/- 336.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=390000, episode_reward=1620.40 +/- 260.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=391000, episode_reward=1497.40 +/- 265.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=392000, episode_reward=1433.00 +/- 147.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=393000, episode_reward=1691.80 +/- 239.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=394000, episode_reward=1345.40 +/- 194.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=395000, episode_reward=1637.80 +/- 222.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=396000, episode_reward=1396.20 +/- 603.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=397000, episode_reward=1383.80 +/- 218.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=398000, episode_reward=1458.80 +/- 328.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=399000, episode_reward=1692.60 +/- 204.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=400000, episode_reward=1718.20 +/- 188.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=401000, episode_reward=1834.60 +/- 153.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=402000, episode_reward=1714.80 +/- 122.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=403000, episode_reward=1502.40 +/- 220.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=404000, episode_reward=1512.40 +/- 153.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=405000, episode_reward=1613.80 +/- 188.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=406000, episode_reward=1639.20 +/- 211.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=407000, episode_reward=1641.00 +/- 148.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=408000, episode_reward=1651.20 +/- 186.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=409000, episode_reward=1648.80 +/- 188.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=410000, episode_reward=1649.20 +/- 273.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=411000, episode_reward=1479.60 +/- 130.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=412000, episode_reward=1336.20 +/- 274.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=413000, episode_reward=1702.00 +/- 111.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=414000, episode_reward=1621.00 +/- 293.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=415000, episode_reward=1537.00 +/- 239.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=416000, episode_reward=1216.40 +/- 234.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=417000, episode_reward=1669.60 +/- 111.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=418000, episode_reward=1789.60 +/- 135.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=419000, episode_reward=1651.00 +/- 103.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=420000, episode_reward=1489.60 +/- 364.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=421000, episode_reward=1517.80 +/- 294.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=422000, episode_reward=1421.80 +/- 200.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=423000, episode_reward=1685.60 +/- 103.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=424000, episode_reward=1583.60 +/- 474.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=425000, episode_reward=1595.40 +/- 208.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=426000, episode_reward=1617.60 +/- 157.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=427000, episode_reward=1564.40 +/- 92.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=428000, episode_reward=1585.20 +/- 265.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=429000, episode_reward=1582.00 +/- 244.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=430000, episode_reward=1390.20 +/- 215.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=431000, episode_reward=1518.40 +/- 289.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=432000, episode_reward=1556.20 +/- 363.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=433000, episode_reward=1674.00 +/- 208.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=434000, episode_reward=1663.80 +/- 208.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=435000, episode_reward=1510.20 +/- 197.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=436000, episode_reward=1345.80 +/- 108.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=437000, episode_reward=1301.00 +/- 259.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=438000, episode_reward=1472.80 +/- 200.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=439000, episode_reward=1437.80 +/- 101.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=440000, episode_reward=1586.60 +/- 132.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=441000, episode_reward=1692.00 +/- 290.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=442000, episode_reward=1699.80 +/- 155.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=443000, episode_reward=1534.40 +/- 347.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=444000, episode_reward=1423.20 +/- 186.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=445000, episode_reward=1321.20 +/- 238.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=446000, episode_reward=1597.00 +/- 125.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=447000, episode_reward=1572.00 +/- 289.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=448000, episode_reward=1522.80 +/- 182.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=449000, episode_reward=1593.60 +/- 347.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=450000, episode_reward=1700.40 +/- 67.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=451000, episode_reward=1581.00 +/- 101.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=452000, episode_reward=1676.80 +/- 117.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=453000, episode_reward=1719.60 +/- 290.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=454000, episode_reward=1686.40 +/- 204.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=455000, episode_reward=1645.00 +/- 217.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=456000, episode_reward=1458.40 +/- 185.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=457000, episode_reward=1624.40 +/- 239.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=458000, episode_reward=1448.00 +/- 113.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=459000, episode_reward=1689.20 +/- 171.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=460000, episode_reward=1583.40 +/- 321.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=461000, episode_reward=1617.40 +/- 307.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=462000, episode_reward=1724.40 +/- 204.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=463000, episode_reward=1398.80 +/- 293.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=464000, episode_reward=1772.00 +/- 161.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=465000, episode_reward=1598.20 +/- 185.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=466000, episode_reward=1505.00 +/- 246.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=467000, episode_reward=1668.80 +/- 267.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=468000, episode_reward=1863.20 +/- 71.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=469000, episode_reward=1762.00 +/- 99.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=470000, episode_reward=1648.60 +/- 219.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=471000, episode_reward=1433.60 +/- 488.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=472000, episode_reward=1481.60 +/- 105.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=473000, episode_reward=1720.60 +/- 247.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=474000, episode_reward=1655.20 +/- 244.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=475000, episode_reward=1666.40 +/- 132.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=476000, episode_reward=1559.20 +/- 431.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=477000, episode_reward=1594.00 +/- 195.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=478000, episode_reward=1426.00 +/- 195.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=479000, episode_reward=1699.80 +/- 77.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=480000, episode_reward=1778.60 +/- 143.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=481000, episode_reward=1672.60 +/- 110.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=482000, episode_reward=1701.60 +/- 283.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=483000, episode_reward=1624.80 +/- 131.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=484000, episode_reward=1732.60 +/- 310.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=485000, episode_reward=1514.00 +/- 343.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=486000, episode_reward=1598.20 +/- 111.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=487000, episode_reward=1660.20 +/- 126.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=488000, episode_reward=1753.80 +/- 252.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=489000, episode_reward=1451.00 +/- 357.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=490000, episode_reward=1825.00 +/- 175.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=491000, episode_reward=1621.20 +/- 196.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=492000, episode_reward=1692.60 +/- 106.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=493000, episode_reward=1455.00 +/- 161.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=494000, episode_reward=1507.20 +/- 337.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=495000, episode_reward=1658.20 +/- 201.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=496000, episode_reward=1559.20 +/- 253.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=497000, episode_reward=1444.80 +/- 115.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=498000, episode_reward=1606.60 +/- 213.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=499000, episode_reward=1656.80 +/- 59.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=500000, episode_reward=1534.00 +/- 139.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=501000, episode_reward=1740.60 +/- 192.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Gamma = 0.9, Learning Rate = 0.0003, Activation Function = ReLU, Neurons = 64, Process noise = 0.1, Measurement noise = 0.0\n",
      "Eval num_timesteps=1000, episode_reward=-19010.00 +/- 76.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=-19036.00 +/- 61.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=3000, episode_reward=-3513.40 +/- 529.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4000, episode_reward=-3828.00 +/- 817.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=5000, episode_reward=-6147.00 +/- 838.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=6000, episode_reward=-7210.60 +/- 1296.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=7000, episode_reward=-6757.80 +/- 731.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=8000, episode_reward=-6092.60 +/- 1372.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=9000, episode_reward=-429.80 +/- 456.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=10000, episode_reward=-215.20 +/- 333.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=11000, episode_reward=251.80 +/- 193.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=12000, episode_reward=256.20 +/- 224.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=13000, episode_reward=230.40 +/- 282.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=14000, episode_reward=425.20 +/- 193.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=15000, episode_reward=282.00 +/- 149.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=16000, episode_reward=398.40 +/- 168.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=17000, episode_reward=119.00 +/- 79.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=18000, episode_reward=326.60 +/- 123.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=19000, episode_reward=292.80 +/- 197.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=20000, episode_reward=288.80 +/- 150.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=21000, episode_reward=170.40 +/- 185.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=22000, episode_reward=110.00 +/- 107.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=23000, episode_reward=99.00 +/- 265.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=24000, episode_reward=210.60 +/- 166.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=25000, episode_reward=329.80 +/- 150.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=26000, episode_reward=412.40 +/- 216.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=27000, episode_reward=303.00 +/- 164.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=28000, episode_reward=364.20 +/- 139.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=29000, episode_reward=355.00 +/- 109.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=30000, episode_reward=191.80 +/- 50.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=31000, episode_reward=521.60 +/- 212.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=32000, episode_reward=168.20 +/- 329.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=33000, episode_reward=310.00 +/- 266.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=34000, episode_reward=260.80 +/- 73.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=35000, episode_reward=363.60 +/- 299.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=36000, episode_reward=193.00 +/- 170.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=37000, episode_reward=121.00 +/- 217.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=38000, episode_reward=323.80 +/- 189.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=39000, episode_reward=178.00 +/- 247.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=40000, episode_reward=350.80 +/- 197.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=41000, episode_reward=167.80 +/- 240.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=42000, episode_reward=283.80 +/- 110.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=43000, episode_reward=195.80 +/- 235.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=44000, episode_reward=410.40 +/- 253.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=45000, episode_reward=189.80 +/- 296.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=46000, episode_reward=300.00 +/- 168.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=47000, episode_reward=142.20 +/- 109.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=48000, episode_reward=347.40 +/- 129.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=49000, episode_reward=471.40 +/- 225.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=50000, episode_reward=368.20 +/- 337.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=51000, episode_reward=503.20 +/- 147.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=52000, episode_reward=361.00 +/- 265.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=53000, episode_reward=398.60 +/- 235.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=54000, episode_reward=734.00 +/- 172.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=55000, episode_reward=742.80 +/- 256.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=56000, episode_reward=775.40 +/- 143.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=57000, episode_reward=942.00 +/- 358.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=58000, episode_reward=1017.40 +/- 326.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=59000, episode_reward=855.80 +/- 168.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=60000, episode_reward=904.00 +/- 349.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=61000, episode_reward=998.60 +/- 195.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=62000, episode_reward=1261.60 +/- 150.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=63000, episode_reward=1047.80 +/- 199.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=64000, episode_reward=1097.40 +/- 196.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=65000, episode_reward=1089.60 +/- 350.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=66000, episode_reward=1188.00 +/- 195.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=67000, episode_reward=1026.80 +/- 279.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=68000, episode_reward=1392.60 +/- 277.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=69000, episode_reward=948.20 +/- 212.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=70000, episode_reward=1129.60 +/- 243.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=71000, episode_reward=1111.60 +/- 310.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=72000, episode_reward=1239.60 +/- 341.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=73000, episode_reward=1115.40 +/- 384.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=74000, episode_reward=1139.80 +/- 155.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=75000, episode_reward=1000.40 +/- 227.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=76000, episode_reward=-179.00 +/- 3114.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=77000, episode_reward=1003.80 +/- 251.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=78000, episode_reward=166.40 +/- 1730.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=79000, episode_reward=1086.00 +/- 201.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=80000, episode_reward=1160.40 +/- 169.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=81000, episode_reward=1205.80 +/- 267.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=82000, episode_reward=1263.00 +/- 102.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=83000, episode_reward=-2042.20 +/- 6557.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=84000, episode_reward=984.20 +/- 122.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=85000, episode_reward=1208.20 +/- 379.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=86000, episode_reward=1023.80 +/- 286.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=87000, episode_reward=951.80 +/- 340.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=88000, episode_reward=1271.60 +/- 243.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=89000, episode_reward=1114.40 +/- 187.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=90000, episode_reward=1222.20 +/- 83.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=91000, episode_reward=72.80 +/- 2414.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=92000, episode_reward=1290.60 +/- 269.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=93000, episode_reward=1200.00 +/- 261.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=94000, episode_reward=1226.60 +/- 235.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=95000, episode_reward=1039.80 +/- 175.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=96000, episode_reward=1215.80 +/- 147.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=97000, episode_reward=1191.80 +/- 335.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=98000, episode_reward=1173.60 +/- 434.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=99000, episode_reward=1173.40 +/- 129.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=100000, episode_reward=1192.60 +/- 262.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=101000, episode_reward=1273.60 +/- 253.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=102000, episode_reward=1103.60 +/- 247.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=103000, episode_reward=1236.00 +/- 110.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=104000, episode_reward=1243.40 +/- 284.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=105000, episode_reward=994.40 +/- 159.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=106000, episode_reward=852.00 +/- 315.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=107000, episode_reward=1416.80 +/- 186.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=108000, episode_reward=1126.00 +/- 136.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=109000, episode_reward=1403.80 +/- 210.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=110000, episode_reward=1387.20 +/- 136.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=111000, episode_reward=-4790.00 +/- 7664.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=112000, episode_reward=1337.20 +/- 279.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=113000, episode_reward=1338.00 +/- 265.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=114000, episode_reward=1342.40 +/- 219.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=115000, episode_reward=1443.20 +/- 187.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=116000, episode_reward=-2394.60 +/- 7516.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=117000, episode_reward=1366.60 +/- 164.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=118000, episode_reward=1217.00 +/- 63.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=119000, episode_reward=1341.80 +/- 157.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=120000, episode_reward=1369.40 +/- 365.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=121000, episode_reward=1322.40 +/- 433.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=122000, episode_reward=1483.80 +/- 142.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=123000, episode_reward=1668.20 +/- 131.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=124000, episode_reward=1395.20 +/- 152.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=125000, episode_reward=1287.60 +/- 349.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=126000, episode_reward=1475.40 +/- 294.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=127000, episode_reward=1400.00 +/- 296.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=128000, episode_reward=1422.80 +/- 373.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=129000, episode_reward=1343.20 +/- 196.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=130000, episode_reward=1506.20 +/- 173.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=131000, episode_reward=1344.80 +/- 344.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=132000, episode_reward=1341.60 +/- 373.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=133000, episode_reward=1333.60 +/- 226.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=134000, episode_reward=1429.20 +/- 380.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=135000, episode_reward=1492.60 +/- 94.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=136000, episode_reward=1497.80 +/- 387.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=137000, episode_reward=1423.40 +/- 363.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=138000, episode_reward=1508.20 +/- 267.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=139000, episode_reward=1576.80 +/- 375.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=140000, episode_reward=1424.80 +/- 225.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=141000, episode_reward=1527.60 +/- 112.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=142000, episode_reward=1576.00 +/- 133.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=143000, episode_reward=1475.60 +/- 216.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=144000, episode_reward=1442.20 +/- 238.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=145000, episode_reward=1519.00 +/- 187.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=146000, episode_reward=1500.60 +/- 134.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=147000, episode_reward=1597.60 +/- 196.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=148000, episode_reward=1325.80 +/- 221.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=149000, episode_reward=1408.00 +/- 145.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=150000, episode_reward=1460.00 +/- 273.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=151000, episode_reward=1659.60 +/- 236.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=152000, episode_reward=1509.80 +/- 270.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=153000, episode_reward=1487.60 +/- 336.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=154000, episode_reward=1555.60 +/- 173.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=155000, episode_reward=1489.20 +/- 170.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=156000, episode_reward=1734.60 +/- 181.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=157000, episode_reward=1587.60 +/- 176.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=158000, episode_reward=1765.20 +/- 279.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=159000, episode_reward=1557.20 +/- 225.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=160000, episode_reward=1369.00 +/- 118.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=161000, episode_reward=1803.60 +/- 219.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=162000, episode_reward=1571.20 +/- 143.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=163000, episode_reward=1614.40 +/- 123.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=164000, episode_reward=1559.60 +/- 321.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=165000, episode_reward=1627.80 +/- 153.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=166000, episode_reward=1645.80 +/- 156.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=167000, episode_reward=1845.60 +/- 239.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=168000, episode_reward=1538.60 +/- 378.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=169000, episode_reward=1564.40 +/- 274.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=170000, episode_reward=1482.00 +/- 209.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=171000, episode_reward=1543.40 +/- 439.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=172000, episode_reward=1719.20 +/- 261.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=173000, episode_reward=1659.60 +/- 339.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=174000, episode_reward=1371.80 +/- 141.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=175000, episode_reward=1546.00 +/- 342.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=176000, episode_reward=1565.20 +/- 322.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=177000, episode_reward=1530.00 +/- 176.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=178000, episode_reward=1435.80 +/- 263.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=179000, episode_reward=1452.60 +/- 214.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=180000, episode_reward=1587.40 +/- 166.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=181000, episode_reward=1571.00 +/- 171.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=182000, episode_reward=1532.00 +/- 171.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=183000, episode_reward=1487.20 +/- 154.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=184000, episode_reward=1598.20 +/- 238.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=185000, episode_reward=1500.20 +/- 136.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=186000, episode_reward=1416.40 +/- 172.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=187000, episode_reward=1448.80 +/- 321.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=188000, episode_reward=1526.60 +/- 178.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=189000, episode_reward=1354.60 +/- 389.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=190000, episode_reward=1533.00 +/- 193.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=191000, episode_reward=1523.60 +/- 220.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=192000, episode_reward=1499.40 +/- 254.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=193000, episode_reward=1740.40 +/- 276.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=194000, episode_reward=1412.20 +/- 198.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=195000, episode_reward=1353.80 +/- 320.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=196000, episode_reward=1606.40 +/- 89.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=197000, episode_reward=1492.20 +/- 261.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=198000, episode_reward=1527.60 +/- 329.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=199000, episode_reward=1299.40 +/- 305.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=200000, episode_reward=1705.80 +/- 176.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=201000, episode_reward=1443.20 +/- 81.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=202000, episode_reward=575.60 +/- 1884.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=203000, episode_reward=1559.80 +/- 195.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=204000, episode_reward=1602.20 +/- 81.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=205000, episode_reward=1488.20 +/- 263.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=206000, episode_reward=1752.80 +/- 330.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=207000, episode_reward=1825.00 +/- 161.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=208000, episode_reward=1699.20 +/- 167.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=209000, episode_reward=1496.00 +/- 122.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=210000, episode_reward=1781.00 +/- 254.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=211000, episode_reward=1649.40 +/- 184.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=212000, episode_reward=1586.00 +/- 189.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=213000, episode_reward=1566.40 +/- 235.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=214000, episode_reward=1482.40 +/- 180.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=215000, episode_reward=1649.60 +/- 155.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=216000, episode_reward=1698.40 +/- 172.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=217000, episode_reward=1518.20 +/- 300.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=218000, episode_reward=1618.20 +/- 210.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=219000, episode_reward=1549.20 +/- 194.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=220000, episode_reward=1591.40 +/- 213.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=221000, episode_reward=1453.20 +/- 114.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=222000, episode_reward=1666.40 +/- 230.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=223000, episode_reward=1850.20 +/- 282.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=224000, episode_reward=1535.60 +/- 142.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=225000, episode_reward=1595.80 +/- 279.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=226000, episode_reward=1623.00 +/- 125.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=227000, episode_reward=1402.20 +/- 104.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=228000, episode_reward=1593.00 +/- 196.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=229000, episode_reward=1480.80 +/- 212.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=230000, episode_reward=1590.60 +/- 222.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=231000, episode_reward=1654.00 +/- 182.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=232000, episode_reward=1605.80 +/- 252.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=233000, episode_reward=1737.40 +/- 192.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=234000, episode_reward=1650.00 +/- 194.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=235000, episode_reward=1554.20 +/- 124.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=236000, episode_reward=1622.80 +/- 107.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=237000, episode_reward=1562.60 +/- 147.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=238000, episode_reward=1603.00 +/- 126.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=239000, episode_reward=1636.80 +/- 132.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=240000, episode_reward=1633.40 +/- 143.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=241000, episode_reward=1403.80 +/- 124.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=242000, episode_reward=1780.60 +/- 101.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=243000, episode_reward=1680.60 +/- 145.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=244000, episode_reward=1602.80 +/- 191.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=245000, episode_reward=1607.00 +/- 297.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=246000, episode_reward=1596.40 +/- 239.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=247000, episode_reward=1558.00 +/- 163.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=248000, episode_reward=1447.00 +/- 123.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=249000, episode_reward=1613.60 +/- 279.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=250000, episode_reward=1724.00 +/- 129.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=251000, episode_reward=1649.00 +/- 165.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=252000, episode_reward=1772.40 +/- 235.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=253000, episode_reward=1473.00 +/- 206.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=254000, episode_reward=1744.40 +/- 269.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=255000, episode_reward=1572.80 +/- 137.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=256000, episode_reward=1566.40 +/- 142.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=257000, episode_reward=1559.60 +/- 237.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=258000, episode_reward=1527.40 +/- 215.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=259000, episode_reward=1390.80 +/- 204.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=260000, episode_reward=1708.40 +/- 233.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=261000, episode_reward=1573.60 +/- 230.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=262000, episode_reward=1413.00 +/- 198.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=263000, episode_reward=1614.80 +/- 264.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=264000, episode_reward=1764.80 +/- 211.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=265000, episode_reward=1760.60 +/- 265.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=266000, episode_reward=1622.00 +/- 126.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=267000, episode_reward=1285.40 +/- 312.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=268000, episode_reward=1527.60 +/- 217.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=269000, episode_reward=1562.40 +/- 125.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=270000, episode_reward=1792.40 +/- 125.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=271000, episode_reward=1488.40 +/- 188.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=272000, episode_reward=1724.80 +/- 121.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=273000, episode_reward=1605.20 +/- 266.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=274000, episode_reward=1575.60 +/- 262.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=275000, episode_reward=1657.80 +/- 197.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=276000, episode_reward=1473.80 +/- 113.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=277000, episode_reward=1636.20 +/- 180.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=278000, episode_reward=1476.60 +/- 195.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=279000, episode_reward=1590.80 +/- 240.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=280000, episode_reward=1543.00 +/- 173.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=281000, episode_reward=1506.20 +/- 336.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=282000, episode_reward=1766.00 +/- 193.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=283000, episode_reward=1711.00 +/- 255.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=284000, episode_reward=1549.60 +/- 49.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=285000, episode_reward=1543.00 +/- 321.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=286000, episode_reward=1621.60 +/- 252.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=287000, episode_reward=1629.60 +/- 205.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=288000, episode_reward=1688.00 +/- 287.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=289000, episode_reward=1634.40 +/- 293.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=290000, episode_reward=1657.20 +/- 184.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=291000, episode_reward=1589.20 +/- 166.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=292000, episode_reward=1671.80 +/- 237.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=293000, episode_reward=1546.20 +/- 94.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=294000, episode_reward=1572.80 +/- 193.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=295000, episode_reward=1607.00 +/- 271.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=296000, episode_reward=1658.40 +/- 179.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=297000, episode_reward=1517.60 +/- 114.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=298000, episode_reward=1619.80 +/- 146.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=299000, episode_reward=1532.20 +/- 194.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=300000, episode_reward=1501.20 +/- 276.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=301000, episode_reward=1596.80 +/- 88.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=302000, episode_reward=1768.60 +/- 165.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=303000, episode_reward=1492.00 +/- 299.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=304000, episode_reward=1517.00 +/- 178.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=305000, episode_reward=1720.40 +/- 42.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=306000, episode_reward=1619.60 +/- 369.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=307000, episode_reward=1678.40 +/- 270.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=308000, episode_reward=1547.00 +/- 225.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=309000, episode_reward=1525.80 +/- 173.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=310000, episode_reward=1779.20 +/- 36.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=311000, episode_reward=1638.00 +/- 126.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=312000, episode_reward=1487.60 +/- 341.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=313000, episode_reward=1558.40 +/- 161.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=314000, episode_reward=1678.80 +/- 279.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=315000, episode_reward=1465.20 +/- 285.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=316000, episode_reward=1550.20 +/- 154.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=317000, episode_reward=1731.20 +/- 86.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=318000, episode_reward=1507.40 +/- 190.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=319000, episode_reward=1504.60 +/- 63.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=320000, episode_reward=1676.60 +/- 164.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=321000, episode_reward=1669.80 +/- 199.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=322000, episode_reward=1719.00 +/- 237.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=323000, episode_reward=1515.20 +/- 68.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=324000, episode_reward=1700.00 +/- 156.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=325000, episode_reward=1513.80 +/- 199.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=326000, episode_reward=1797.00 +/- 108.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=327000, episode_reward=1840.00 +/- 194.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=328000, episode_reward=1496.40 +/- 152.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=329000, episode_reward=1561.60 +/- 182.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=330000, episode_reward=1472.40 +/- 195.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=331000, episode_reward=1786.80 +/- 196.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=332000, episode_reward=1500.40 +/- 203.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=333000, episode_reward=1630.20 +/- 342.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=334000, episode_reward=1509.80 +/- 112.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=335000, episode_reward=1553.40 +/- 309.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=336000, episode_reward=1588.80 +/- 228.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=337000, episode_reward=1502.80 +/- 242.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=338000, episode_reward=1665.80 +/- 230.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=339000, episode_reward=1495.20 +/- 192.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=340000, episode_reward=1398.00 +/- 174.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=341000, episode_reward=1736.60 +/- 167.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=342000, episode_reward=1656.80 +/- 253.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=343000, episode_reward=1356.40 +/- 254.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=344000, episode_reward=1644.00 +/- 268.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=345000, episode_reward=1605.80 +/- 176.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=346000, episode_reward=1315.80 +/- 425.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=347000, episode_reward=1647.20 +/- 145.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=348000, episode_reward=1623.40 +/- 246.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=349000, episode_reward=1616.20 +/- 171.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=350000, episode_reward=1446.80 +/- 163.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=351000, episode_reward=1696.20 +/- 197.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=352000, episode_reward=1458.80 +/- 284.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=353000, episode_reward=1668.60 +/- 153.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=354000, episode_reward=1689.80 +/- 53.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=355000, episode_reward=1537.60 +/- 197.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=356000, episode_reward=1730.40 +/- 97.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=357000, episode_reward=1715.60 +/- 265.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=358000, episode_reward=1685.80 +/- 223.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=359000, episode_reward=1715.00 +/- 230.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=360000, episode_reward=1616.20 +/- 175.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=361000, episode_reward=1485.00 +/- 170.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=362000, episode_reward=1572.80 +/- 266.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=363000, episode_reward=1502.60 +/- 216.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=364000, episode_reward=1651.00 +/- 249.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=365000, episode_reward=1615.40 +/- 145.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=366000, episode_reward=1572.20 +/- 305.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=367000, episode_reward=1505.00 +/- 199.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=368000, episode_reward=1500.60 +/- 246.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=369000, episode_reward=1634.80 +/- 181.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=370000, episode_reward=1583.00 +/- 278.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=371000, episode_reward=1628.40 +/- 141.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=372000, episode_reward=1366.00 +/- 225.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=373000, episode_reward=1476.00 +/- 226.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=374000, episode_reward=1631.80 +/- 195.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=375000, episode_reward=1636.80 +/- 268.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=376000, episode_reward=1719.00 +/- 119.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=377000, episode_reward=1591.60 +/- 162.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=378000, episode_reward=1716.00 +/- 184.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=379000, episode_reward=1551.40 +/- 329.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=380000, episode_reward=1676.00 +/- 304.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=381000, episode_reward=1564.20 +/- 59.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=382000, episode_reward=1642.20 +/- 140.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=383000, episode_reward=1802.20 +/- 148.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=384000, episode_reward=1511.40 +/- 169.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=385000, episode_reward=1668.80 +/- 98.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=386000, episode_reward=1594.00 +/- 335.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=387000, episode_reward=792.20 +/- 1780.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=388000, episode_reward=1555.80 +/- 184.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=389000, episode_reward=1771.80 +/- 181.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=390000, episode_reward=1639.00 +/- 114.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=391000, episode_reward=1713.80 +/- 137.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=392000, episode_reward=1668.80 +/- 154.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=393000, episode_reward=1436.60 +/- 185.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=394000, episode_reward=1386.60 +/- 327.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=395000, episode_reward=1689.60 +/- 210.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=396000, episode_reward=1604.80 +/- 138.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=397000, episode_reward=1338.60 +/- 269.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=398000, episode_reward=1616.60 +/- 221.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=399000, episode_reward=1607.20 +/- 77.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=400000, episode_reward=1624.40 +/- 320.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=401000, episode_reward=1548.00 +/- 283.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=402000, episode_reward=1541.00 +/- 116.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=403000, episode_reward=1546.40 +/- 215.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=404000, episode_reward=1536.40 +/- 234.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=405000, episode_reward=1534.20 +/- 413.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=406000, episode_reward=1751.60 +/- 132.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=407000, episode_reward=1669.40 +/- 210.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=408000, episode_reward=1530.00 +/- 234.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=409000, episode_reward=1530.00 +/- 330.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=410000, episode_reward=1677.00 +/- 47.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=411000, episode_reward=1508.20 +/- 278.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=412000, episode_reward=1746.00 +/- 248.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=413000, episode_reward=1504.40 +/- 244.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=414000, episode_reward=1875.40 +/- 194.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=415000, episode_reward=1859.20 +/- 73.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=416000, episode_reward=1505.00 +/- 268.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=417000, episode_reward=1600.80 +/- 325.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=418000, episode_reward=1721.20 +/- 112.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=419000, episode_reward=1448.40 +/- 283.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=420000, episode_reward=1433.20 +/- 324.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=421000, episode_reward=1894.40 +/- 39.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=422000, episode_reward=1694.40 +/- 138.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=423000, episode_reward=1532.80 +/- 286.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=424000, episode_reward=1595.20 +/- 197.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=425000, episode_reward=1651.60 +/- 200.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=426000, episode_reward=1693.80 +/- 260.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=427000, episode_reward=1424.20 +/- 249.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=428000, episode_reward=1678.00 +/- 137.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=429000, episode_reward=1467.80 +/- 159.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=430000, episode_reward=1503.80 +/- 239.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=431000, episode_reward=1829.20 +/- 174.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=432000, episode_reward=1461.20 +/- 232.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=433000, episode_reward=1718.20 +/- 168.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=434000, episode_reward=1523.40 +/- 228.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=435000, episode_reward=1691.40 +/- 258.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=436000, episode_reward=1246.20 +/- 361.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=437000, episode_reward=1678.40 +/- 221.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=438000, episode_reward=1592.00 +/- 209.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=439000, episode_reward=1582.80 +/- 166.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=440000, episode_reward=1477.80 +/- 97.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=441000, episode_reward=1833.60 +/- 60.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=442000, episode_reward=1774.20 +/- 84.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=443000, episode_reward=1642.20 +/- 275.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=444000, episode_reward=1754.40 +/- 245.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=445000, episode_reward=1749.60 +/- 130.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=446000, episode_reward=1572.60 +/- 240.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=447000, episode_reward=1546.40 +/- 260.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=448000, episode_reward=1634.60 +/- 236.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=449000, episode_reward=1669.80 +/- 119.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=450000, episode_reward=1670.60 +/- 228.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=451000, episode_reward=1784.20 +/- 219.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=452000, episode_reward=1711.80 +/- 178.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=453000, episode_reward=1789.20 +/- 146.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=454000, episode_reward=1687.20 +/- 142.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=455000, episode_reward=1665.20 +/- 170.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=456000, episode_reward=1556.40 +/- 145.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=457000, episode_reward=1407.20 +/- 239.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=458000, episode_reward=1674.80 +/- 34.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=459000, episode_reward=1610.80 +/- 178.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=460000, episode_reward=1569.40 +/- 308.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=461000, episode_reward=1735.60 +/- 255.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=462000, episode_reward=1617.60 +/- 187.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=463000, episode_reward=1680.20 +/- 208.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=464000, episode_reward=1612.80 +/- 95.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=465000, episode_reward=1693.40 +/- 250.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=466000, episode_reward=1685.80 +/- 276.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=467000, episode_reward=1745.00 +/- 74.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=468000, episode_reward=1763.00 +/- 128.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=469000, episode_reward=1591.60 +/- 95.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=470000, episode_reward=1723.60 +/- 124.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=471000, episode_reward=1611.20 +/- 100.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=472000, episode_reward=1277.40 +/- 252.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=473000, episode_reward=1796.40 +/- 219.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=474000, episode_reward=1688.60 +/- 227.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=475000, episode_reward=1589.80 +/- 357.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=476000, episode_reward=1666.00 +/- 126.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=477000, episode_reward=1548.60 +/- 313.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=478000, episode_reward=1799.20 +/- 146.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=479000, episode_reward=1818.80 +/- 229.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=480000, episode_reward=1770.00 +/- 237.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=481000, episode_reward=1627.20 +/- 99.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=482000, episode_reward=1801.20 +/- 120.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=483000, episode_reward=1741.00 +/- 143.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=484000, episode_reward=1535.80 +/- 208.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=485000, episode_reward=1574.40 +/- 195.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=486000, episode_reward=1627.20 +/- 171.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=487000, episode_reward=1472.80 +/- 224.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=488000, episode_reward=1586.40 +/- 197.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=489000, episode_reward=1471.00 +/- 205.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=490000, episode_reward=1596.20 +/- 117.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=491000, episode_reward=1463.00 +/- 233.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=492000, episode_reward=1744.40 +/- 35.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=493000, episode_reward=1497.40 +/- 138.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=494000, episode_reward=1509.00 +/- 70.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=495000, episode_reward=1546.40 +/- 317.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=496000, episode_reward=1446.20 +/- 302.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=497000, episode_reward=1686.60 +/- 152.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=498000, episode_reward=1784.20 +/- 202.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=499000, episode_reward=1594.00 +/- 238.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=500000, episode_reward=1456.00 +/- 212.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=501000, episode_reward=1551.40 +/- 308.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Gamma = 0.9, Learning Rate = 3e-05, Activation Function = Tanh, Neurons = 8, Process noise = 0.1, Measurement noise = 0.0\n",
      "Eval num_timesteps=1000, episode_reward=-3633.00 +/- 425.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=-3953.00 +/- 610.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=3000, episode_reward=234.60 +/- 164.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4000, episode_reward=339.20 +/- 81.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=5000, episode_reward=-98.20 +/- 303.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=6000, episode_reward=-132.20 +/- 396.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=7000, episode_reward=-247.20 +/- 297.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=8000, episode_reward=-121.80 +/- 111.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=9000, episode_reward=-126.40 +/- 344.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=10000, episode_reward=108.40 +/- 257.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=11000, episode_reward=370.80 +/- 150.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=12000, episode_reward=332.20 +/- 172.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=13000, episode_reward=412.80 +/- 163.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=14000, episode_reward=316.60 +/- 96.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=15000, episode_reward=306.20 +/- 184.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=16000, episode_reward=315.20 +/- 162.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=17000, episode_reward=88.80 +/- 172.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=18000, episode_reward=232.20 +/- 207.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=19000, episode_reward=427.80 +/- 209.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=20000, episode_reward=325.20 +/- 115.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=21000, episode_reward=285.60 +/- 161.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=22000, episode_reward=77.80 +/- 189.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=23000, episode_reward=-23.40 +/- 344.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=24000, episode_reward=238.40 +/- 364.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=25000, episode_reward=181.80 +/- 152.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=26000, episode_reward=-611.40 +/- 697.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=27000, episode_reward=-38.60 +/- 260.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=28000, episode_reward=184.20 +/- 249.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=29000, episode_reward=34.20 +/- 231.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=30000, episode_reward=109.00 +/- 318.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=31000, episode_reward=-641.20 +/- 1445.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=32000, episode_reward=17.40 +/- 276.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=33000, episode_reward=-12.00 +/- 334.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=34000, episode_reward=-447.00 +/- 756.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=35000, episode_reward=293.00 +/- 305.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=36000, episode_reward=-203.60 +/- 296.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=37000, episode_reward=331.80 +/- 222.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=38000, episode_reward=205.20 +/- 214.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=39000, episode_reward=-321.40 +/- 311.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=40000, episode_reward=-210.60 +/- 504.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=41000, episode_reward=55.40 +/- 261.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=42000, episode_reward=165.60 +/- 302.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=43000, episode_reward=-316.20 +/- 428.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=44000, episode_reward=-183.40 +/- 899.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=45000, episode_reward=100.00 +/- 140.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=46000, episode_reward=-816.60 +/- 1029.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=47000, episode_reward=141.20 +/- 206.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=48000, episode_reward=205.20 +/- 380.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=49000, episode_reward=258.60 +/- 210.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=50000, episode_reward=113.00 +/- 396.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=51000, episode_reward=-12.20 +/- 414.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=52000, episode_reward=-125.40 +/- 420.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=53000, episode_reward=-351.20 +/- 560.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=54000, episode_reward=146.00 +/- 136.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=55000, episode_reward=-5.40 +/- 422.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=56000, episode_reward=-588.40 +/- 1482.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=57000, episode_reward=199.40 +/- 206.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=58000, episode_reward=87.80 +/- 462.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=59000, episode_reward=18.20 +/- 321.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=60000, episode_reward=-112.60 +/- 354.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=61000, episode_reward=-163.40 +/- 231.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=62000, episode_reward=-41.20 +/- 380.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=63000, episode_reward=182.80 +/- 335.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=64000, episode_reward=173.20 +/- 235.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=65000, episode_reward=249.60 +/- 274.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=66000, episode_reward=180.80 +/- 125.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=67000, episode_reward=-372.40 +/- 187.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=68000, episode_reward=-500.80 +/- 680.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=69000, episode_reward=-167.20 +/- 262.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=70000, episode_reward=-250.00 +/- 676.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=71000, episode_reward=-383.80 +/- 343.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=72000, episode_reward=-456.20 +/- 455.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=73000, episode_reward=-171.20 +/- 678.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=74000, episode_reward=-1147.40 +/- 1062.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=75000, episode_reward=-53.20 +/- 360.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=76000, episode_reward=-22.40 +/- 54.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=77000, episode_reward=-263.40 +/- 265.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=78000, episode_reward=-263.00 +/- 789.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=79000, episode_reward=-202.00 +/- 441.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=80000, episode_reward=-164.00 +/- 252.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=81000, episode_reward=-209.20 +/- 665.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=82000, episode_reward=179.20 +/- 105.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=83000, episode_reward=-298.80 +/- 332.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=84000, episode_reward=-264.00 +/- 385.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=85000, episode_reward=112.60 +/- 153.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=86000, episode_reward=191.00 +/- 344.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=87000, episode_reward=-684.60 +/- 976.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=88000, episode_reward=-217.00 +/- 597.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=89000, episode_reward=-228.20 +/- 682.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=90000, episode_reward=-513.60 +/- 1045.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=91000, episode_reward=-5.80 +/- 310.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=92000, episode_reward=-326.60 +/- 356.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=93000, episode_reward=-307.80 +/- 428.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=94000, episode_reward=-384.40 +/- 1125.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=95000, episode_reward=-465.00 +/- 753.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=96000, episode_reward=-391.80 +/- 704.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=97000, episode_reward=-127.80 +/- 232.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=98000, episode_reward=-166.80 +/- 873.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=99000, episode_reward=-753.00 +/- 1025.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=100000, episode_reward=149.20 +/- 220.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=101000, episode_reward=-275.60 +/- 410.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=102000, episode_reward=-574.20 +/- 830.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=103000, episode_reward=78.00 +/- 369.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=104000, episode_reward=16.60 +/- 249.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=105000, episode_reward=-57.40 +/- 346.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=106000, episode_reward=-114.40 +/- 352.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=107000, episode_reward=-238.40 +/- 555.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=108000, episode_reward=-469.00 +/- 437.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=109000, episode_reward=-116.20 +/- 240.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=110000, episode_reward=-577.00 +/- 740.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=111000, episode_reward=-246.20 +/- 199.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=112000, episode_reward=-392.80 +/- 303.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=113000, episode_reward=-178.80 +/- 704.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=114000, episode_reward=190.40 +/- 262.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=115000, episode_reward=-324.60 +/- 620.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=116000, episode_reward=-512.40 +/- 1314.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=117000, episode_reward=43.40 +/- 461.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=118000, episode_reward=-215.80 +/- 583.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=119000, episode_reward=-143.60 +/- 209.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=120000, episode_reward=205.20 +/- 70.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=121000, episode_reward=21.00 +/- 236.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=122000, episode_reward=-330.80 +/- 425.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=123000, episode_reward=79.40 +/- 161.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=124000, episode_reward=-47.20 +/- 104.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=125000, episode_reward=-124.80 +/- 317.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=126000, episode_reward=96.20 +/- 351.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=127000, episode_reward=-29.60 +/- 357.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=128000, episode_reward=-729.00 +/- 1196.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=129000, episode_reward=115.60 +/- 272.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=130000, episode_reward=34.00 +/- 261.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=131000, episode_reward=185.00 +/- 524.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=132000, episode_reward=-53.20 +/- 337.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=133000, episode_reward=-685.40 +/- 901.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=134000, episode_reward=-104.20 +/- 493.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=135000, episode_reward=-339.00 +/- 307.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=136000, episode_reward=-334.80 +/- 283.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=137000, episode_reward=115.40 +/- 344.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=138000, episode_reward=139.20 +/- 208.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=139000, episode_reward=-72.20 +/- 316.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=140000, episode_reward=-50.40 +/- 214.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=141000, episode_reward=-65.20 +/- 598.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=142000, episode_reward=-102.20 +/- 257.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=143000, episode_reward=-201.80 +/- 693.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=144000, episode_reward=-1.00 +/- 326.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=145000, episode_reward=-81.80 +/- 450.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=146000, episode_reward=-29.00 +/- 257.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=147000, episode_reward=-56.80 +/- 521.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=148000, episode_reward=-212.60 +/- 454.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=149000, episode_reward=163.00 +/- 250.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=150000, episode_reward=-326.80 +/- 751.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=151000, episode_reward=-196.80 +/- 238.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=152000, episode_reward=-72.20 +/- 270.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=153000, episode_reward=-150.60 +/- 619.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=154000, episode_reward=-208.00 +/- 500.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=155000, episode_reward=-576.40 +/- 736.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=156000, episode_reward=-68.20 +/- 220.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=157000, episode_reward=-329.80 +/- 699.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=158000, episode_reward=-51.00 +/- 430.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=159000, episode_reward=-572.80 +/- 865.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=160000, episode_reward=-337.00 +/- 530.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=161000, episode_reward=-140.80 +/- 311.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=162000, episode_reward=55.00 +/- 168.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=163000, episode_reward=-315.00 +/- 775.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=164000, episode_reward=1.00 +/- 423.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=165000, episode_reward=-226.40 +/- 206.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=166000, episode_reward=-117.60 +/- 415.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=167000, episode_reward=98.20 +/- 147.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=168000, episode_reward=-392.60 +/- 901.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=169000, episode_reward=-512.00 +/- 551.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=170000, episode_reward=-174.80 +/- 259.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=171000, episode_reward=-760.40 +/- 1195.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=172000, episode_reward=-107.40 +/- 747.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=173000, episode_reward=-25.00 +/- 377.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=174000, episode_reward=-356.20 +/- 310.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=175000, episode_reward=-1032.80 +/- 1363.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=176000, episode_reward=-239.00 +/- 562.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=177000, episode_reward=-314.80 +/- 760.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=178000, episode_reward=-647.60 +/- 1444.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=179000, episode_reward=-202.00 +/- 504.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=180000, episode_reward=-215.40 +/- 677.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=181000, episode_reward=-280.80 +/- 501.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=182000, episode_reward=-25.60 +/- 246.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=183000, episode_reward=-679.00 +/- 980.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=184000, episode_reward=286.00 +/- 241.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=185000, episode_reward=34.60 +/- 359.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=186000, episode_reward=-417.40 +/- 735.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=187000, episode_reward=-121.20 +/- 313.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=188000, episode_reward=-78.40 +/- 318.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=189000, episode_reward=-20.40 +/- 313.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=190000, episode_reward=-23.20 +/- 134.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=191000, episode_reward=-778.00 +/- 935.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=192000, episode_reward=-77.20 +/- 199.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=193000, episode_reward=165.80 +/- 293.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=194000, episode_reward=18.60 +/- 184.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=195000, episode_reward=-411.00 +/- 564.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=196000, episode_reward=-151.20 +/- 408.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=197000, episode_reward=119.00 +/- 196.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=198000, episode_reward=-333.60 +/- 506.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=199000, episode_reward=-239.80 +/- 456.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=200000, episode_reward=101.20 +/- 154.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=201000, episode_reward=306.40 +/- 219.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=202000, episode_reward=-44.80 +/- 631.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=203000, episode_reward=-190.00 +/- 420.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=204000, episode_reward=-154.80 +/- 203.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=205000, episode_reward=-192.20 +/- 676.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=206000, episode_reward=-71.60 +/- 233.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=207000, episode_reward=-351.20 +/- 856.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=208000, episode_reward=-472.20 +/- 409.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=209000, episode_reward=-121.00 +/- 356.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=210000, episode_reward=-124.80 +/- 295.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=211000, episode_reward=102.20 +/- 232.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=212000, episode_reward=-390.60 +/- 231.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=213000, episode_reward=78.60 +/- 281.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=214000, episode_reward=-1000.40 +/- 1340.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=215000, episode_reward=-360.40 +/- 533.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=216000, episode_reward=-102.60 +/- 391.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=217000, episode_reward=-254.00 +/- 467.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=218000, episode_reward=-715.60 +/- 1147.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=219000, episode_reward=-31.80 +/- 534.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=220000, episode_reward=-429.20 +/- 994.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=221000, episode_reward=127.00 +/- 265.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=222000, episode_reward=-436.60 +/- 459.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=223000, episode_reward=-311.20 +/- 588.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=224000, episode_reward=-119.00 +/- 297.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=225000, episode_reward=-142.40 +/- 407.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=226000, episode_reward=-31.00 +/- 85.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=227000, episode_reward=-284.00 +/- 198.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=228000, episode_reward=-497.00 +/- 573.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=229000, episode_reward=142.80 +/- 354.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=230000, episode_reward=-80.80 +/- 377.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=231000, episode_reward=-440.40 +/- 220.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=232000, episode_reward=-111.20 +/- 449.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=233000, episode_reward=-312.00 +/- 469.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=234000, episode_reward=-370.20 +/- 802.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=235000, episode_reward=-358.20 +/- 512.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=236000, episode_reward=262.20 +/- 159.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=237000, episode_reward=-391.40 +/- 492.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=238000, episode_reward=-37.20 +/- 198.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=239000, episode_reward=-232.00 +/- 736.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=240000, episode_reward=-68.60 +/- 174.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=241000, episode_reward=-535.60 +/- 889.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=242000, episode_reward=-156.40 +/- 593.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=243000, episode_reward=-157.40 +/- 282.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=244000, episode_reward=-340.60 +/- 812.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=245000, episode_reward=107.40 +/- 304.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=246000, episode_reward=-42.00 +/- 232.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=247000, episode_reward=-340.00 +/- 523.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=248000, episode_reward=30.20 +/- 286.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=249000, episode_reward=-129.00 +/- 330.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=250000, episode_reward=-32.60 +/- 359.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=251000, episode_reward=-217.80 +/- 674.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=252000, episode_reward=98.40 +/- 322.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=253000, episode_reward=-374.60 +/- 364.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=254000, episode_reward=63.60 +/- 306.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=255000, episode_reward=148.60 +/- 299.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=256000, episode_reward=-261.20 +/- 455.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=257000, episode_reward=-535.00 +/- 817.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=258000, episode_reward=-12.60 +/- 281.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=259000, episode_reward=-328.40 +/- 425.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=260000, episode_reward=-313.80 +/- 841.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=261000, episode_reward=-147.00 +/- 393.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=262000, episode_reward=107.80 +/- 139.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=263000, episode_reward=20.60 +/- 298.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=264000, episode_reward=182.20 +/- 115.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=265000, episode_reward=-228.60 +/- 637.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=266000, episode_reward=-340.20 +/- 721.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=267000, episode_reward=5.40 +/- 342.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=268000, episode_reward=-76.20 +/- 397.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=269000, episode_reward=-27.00 +/- 468.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=270000, episode_reward=-86.20 +/- 490.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=271000, episode_reward=-65.40 +/- 409.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=272000, episode_reward=-9.80 +/- 310.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=273000, episode_reward=-48.60 +/- 302.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=274000, episode_reward=-45.20 +/- 254.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=275000, episode_reward=203.80 +/- 221.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=276000, episode_reward=-106.20 +/- 278.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=277000, episode_reward=281.80 +/- 213.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=278000, episode_reward=-811.00 +/- 538.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=279000, episode_reward=30.80 +/- 219.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=280000, episode_reward=-66.20 +/- 364.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=281000, episode_reward=72.20 +/- 209.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=282000, episode_reward=-108.80 +/- 309.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=283000, episode_reward=158.80 +/- 298.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=284000, episode_reward=-37.60 +/- 321.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=285000, episode_reward=-111.00 +/- 466.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=286000, episode_reward=-37.80 +/- 245.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=287000, episode_reward=214.60 +/- 213.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=288000, episode_reward=135.60 +/- 303.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=289000, episode_reward=263.20 +/- 147.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=290000, episode_reward=216.40 +/- 216.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=291000, episode_reward=301.00 +/- 208.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=292000, episode_reward=54.40 +/- 214.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=293000, episode_reward=-141.60 +/- 332.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=294000, episode_reward=103.60 +/- 259.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=295000, episode_reward=66.80 +/- 213.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=296000, episode_reward=24.60 +/- 197.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=297000, episode_reward=-12.60 +/- 602.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=298000, episode_reward=-71.60 +/- 314.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=299000, episode_reward=-76.60 +/- 330.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=300000, episode_reward=-132.80 +/- 179.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=301000, episode_reward=210.20 +/- 98.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=302000, episode_reward=-135.80 +/- 325.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=303000, episode_reward=-58.20 +/- 261.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=304000, episode_reward=-58.60 +/- 199.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=305000, episode_reward=38.20 +/- 183.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=306000, episode_reward=115.00 +/- 289.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=307000, episode_reward=-36.00 +/- 227.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=308000, episode_reward=107.80 +/- 250.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=309000, episode_reward=256.60 +/- 254.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=310000, episode_reward=13.60 +/- 121.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=311000, episode_reward=195.40 +/- 251.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=312000, episode_reward=73.20 +/- 285.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=313000, episode_reward=124.80 +/- 152.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=314000, episode_reward=85.40 +/- 319.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=315000, episode_reward=102.40 +/- 282.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=316000, episode_reward=165.40 +/- 169.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=317000, episode_reward=215.80 +/- 97.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=318000, episode_reward=39.40 +/- 173.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=319000, episode_reward=170.40 +/- 196.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=320000, episode_reward=303.20 +/- 217.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=321000, episode_reward=213.80 +/- 306.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=322000, episode_reward=212.40 +/- 108.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=323000, episode_reward=266.60 +/- 220.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=324000, episode_reward=293.20 +/- 226.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=325000, episode_reward=142.20 +/- 287.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=326000, episode_reward=260.40 +/- 275.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=327000, episode_reward=112.00 +/- 99.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=328000, episode_reward=101.00 +/- 138.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=329000, episode_reward=103.00 +/- 220.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=330000, episode_reward=80.60 +/- 437.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=331000, episode_reward=196.20 +/- 163.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=332000, episode_reward=190.20 +/- 194.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=333000, episode_reward=208.00 +/- 198.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=334000, episode_reward=131.00 +/- 203.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=335000, episode_reward=61.20 +/- 81.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=336000, episode_reward=20.60 +/- 134.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=337000, episode_reward=120.40 +/- 303.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=338000, episode_reward=396.60 +/- 180.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=339000, episode_reward=-6.80 +/- 192.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=340000, episode_reward=354.60 +/- 171.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=341000, episode_reward=470.20 +/- 208.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=342000, episode_reward=43.40 +/- 249.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=343000, episode_reward=117.40 +/- 225.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=344000, episode_reward=239.60 +/- 228.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=345000, episode_reward=204.00 +/- 129.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=346000, episode_reward=224.20 +/- 246.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=347000, episode_reward=45.00 +/- 157.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=348000, episode_reward=355.40 +/- 165.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=349000, episode_reward=260.40 +/- 250.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=350000, episode_reward=156.60 +/- 142.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=351000, episode_reward=297.00 +/- 94.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=352000, episode_reward=118.80 +/- 144.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=353000, episode_reward=319.80 +/- 120.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=354000, episode_reward=138.60 +/- 395.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=355000, episode_reward=245.00 +/- 68.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=356000, episode_reward=290.00 +/- 256.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=357000, episode_reward=469.40 +/- 100.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=358000, episode_reward=130.00 +/- 225.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=359000, episode_reward=129.60 +/- 166.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=360000, episode_reward=368.60 +/- 218.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=361000, episode_reward=238.80 +/- 120.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=362000, episode_reward=40.00 +/- 223.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=363000, episode_reward=73.20 +/- 123.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=364000, episode_reward=317.80 +/- 84.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=365000, episode_reward=274.60 +/- 239.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=366000, episode_reward=102.40 +/- 238.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=367000, episode_reward=255.00 +/- 205.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=368000, episode_reward=260.20 +/- 199.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=369000, episode_reward=220.20 +/- 205.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=370000, episode_reward=273.60 +/- 157.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=371000, episode_reward=-31.80 +/- 244.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=372000, episode_reward=223.60 +/- 229.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=373000, episode_reward=263.80 +/- 162.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=374000, episode_reward=301.40 +/- 207.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=375000, episode_reward=66.60 +/- 244.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=376000, episode_reward=20.40 +/- 139.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=377000, episode_reward=384.20 +/- 63.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=378000, episode_reward=168.80 +/- 149.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=379000, episode_reward=117.20 +/- 74.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=380000, episode_reward=330.20 +/- 204.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=381000, episode_reward=168.00 +/- 202.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=382000, episode_reward=186.00 +/- 334.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=383000, episode_reward=245.80 +/- 123.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=384000, episode_reward=247.00 +/- 116.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=385000, episode_reward=349.80 +/- 184.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=386000, episode_reward=246.20 +/- 169.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=387000, episode_reward=370.00 +/- 89.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=388000, episode_reward=196.60 +/- 118.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=389000, episode_reward=193.00 +/- 103.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=390000, episode_reward=350.00 +/- 163.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=391000, episode_reward=139.80 +/- 260.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=392000, episode_reward=337.80 +/- 98.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=393000, episode_reward=64.20 +/- 174.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=394000, episode_reward=218.20 +/- 206.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=395000, episode_reward=421.00 +/- 143.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=396000, episode_reward=387.40 +/- 126.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=397000, episode_reward=268.60 +/- 206.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=398000, episode_reward=252.60 +/- 179.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=399000, episode_reward=292.00 +/- 230.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=400000, episode_reward=271.80 +/- 282.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=401000, episode_reward=190.20 +/- 190.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=402000, episode_reward=249.00 +/- 207.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=403000, episode_reward=336.60 +/- 197.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=404000, episode_reward=194.80 +/- 73.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=405000, episode_reward=343.40 +/- 236.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=406000, episode_reward=262.40 +/- 245.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=407000, episode_reward=204.80 +/- 188.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=408000, episode_reward=419.00 +/- 172.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=409000, episode_reward=352.60 +/- 109.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=410000, episode_reward=81.80 +/- 208.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=411000, episode_reward=311.20 +/- 127.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=412000, episode_reward=74.20 +/- 182.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=413000, episode_reward=301.20 +/- 249.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=414000, episode_reward=183.80 +/- 287.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=415000, episode_reward=244.40 +/- 159.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=416000, episode_reward=371.80 +/- 138.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=417000, episode_reward=262.00 +/- 74.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=418000, episode_reward=181.60 +/- 180.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=419000, episode_reward=285.60 +/- 69.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=420000, episode_reward=251.20 +/- 65.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=421000, episode_reward=334.00 +/- 159.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=422000, episode_reward=152.40 +/- 185.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=423000, episode_reward=113.60 +/- 259.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=424000, episode_reward=391.00 +/- 253.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=425000, episode_reward=428.80 +/- 130.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=426000, episode_reward=340.20 +/- 175.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=427000, episode_reward=244.20 +/- 160.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=428000, episode_reward=265.40 +/- 135.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=429000, episode_reward=373.60 +/- 206.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=430000, episode_reward=356.40 +/- 76.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=431000, episode_reward=209.40 +/- 208.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=432000, episode_reward=371.40 +/- 206.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=433000, episode_reward=350.60 +/- 199.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=434000, episode_reward=326.20 +/- 129.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=435000, episode_reward=202.40 +/- 133.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=436000, episode_reward=372.20 +/- 100.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=437000, episode_reward=170.60 +/- 271.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=438000, episode_reward=161.20 +/- 128.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=439000, episode_reward=293.20 +/- 209.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=440000, episode_reward=139.80 +/- 252.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=441000, episode_reward=444.20 +/- 340.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=442000, episode_reward=390.40 +/- 80.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=443000, episode_reward=297.60 +/- 119.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=444000, episode_reward=196.80 +/- 165.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=445000, episode_reward=426.00 +/- 120.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=446000, episode_reward=302.60 +/- 110.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=447000, episode_reward=280.00 +/- 185.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=448000, episode_reward=228.20 +/- 44.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=449000, episode_reward=230.40 +/- 102.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=450000, episode_reward=323.80 +/- 105.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=451000, episode_reward=269.80 +/- 103.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=452000, episode_reward=329.20 +/- 153.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=453000, episode_reward=463.00 +/- 121.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=454000, episode_reward=336.20 +/- 204.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=455000, episode_reward=165.20 +/- 161.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=456000, episode_reward=240.20 +/- 123.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=457000, episode_reward=120.80 +/- 194.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=458000, episode_reward=323.80 +/- 146.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=459000, episode_reward=278.60 +/- 123.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=460000, episode_reward=351.60 +/- 218.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=461000, episode_reward=155.80 +/- 222.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=462000, episode_reward=397.80 +/- 192.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=463000, episode_reward=278.40 +/- 149.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=464000, episode_reward=266.20 +/- 181.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=465000, episode_reward=425.20 +/- 126.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=466000, episode_reward=395.80 +/- 116.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=467000, episode_reward=233.40 +/- 231.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=468000, episode_reward=276.00 +/- 119.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=469000, episode_reward=301.20 +/- 167.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=470000, episode_reward=467.00 +/- 118.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=471000, episode_reward=208.80 +/- 155.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=472000, episode_reward=165.60 +/- 286.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=473000, episode_reward=252.40 +/- 109.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=474000, episode_reward=316.80 +/- 153.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=475000, episode_reward=227.00 +/- 115.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=476000, episode_reward=292.20 +/- 246.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=477000, episode_reward=277.40 +/- 232.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=478000, episode_reward=329.60 +/- 224.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=479000, episode_reward=347.60 +/- 180.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=480000, episode_reward=128.40 +/- 161.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=481000, episode_reward=344.80 +/- 159.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=482000, episode_reward=154.20 +/- 195.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=483000, episode_reward=295.40 +/- 107.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=484000, episode_reward=348.00 +/- 195.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=485000, episode_reward=-58.20 +/- 206.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=486000, episode_reward=309.60 +/- 167.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=487000, episode_reward=290.20 +/- 212.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=488000, episode_reward=235.40 +/- 59.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=489000, episode_reward=165.40 +/- 271.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=490000, episode_reward=255.40 +/- 190.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=491000, episode_reward=341.20 +/- 295.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=492000, episode_reward=285.80 +/- 162.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=493000, episode_reward=443.60 +/- 189.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=494000, episode_reward=448.40 +/- 97.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=495000, episode_reward=339.20 +/- 132.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=496000, episode_reward=274.60 +/- 141.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=497000, episode_reward=220.80 +/- 51.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=498000, episode_reward=316.80 +/- 118.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=499000, episode_reward=348.60 +/- 94.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=500000, episode_reward=257.80 +/- 132.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=501000, episode_reward=206.40 +/- 182.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Gamma = 0.9, Learning Rate = 3e-05, Activation Function = Tanh, Neurons = 32, Process noise = 0.1, Measurement noise = 0.0\n",
      "Eval num_timesteps=1000, episode_reward=-18981.00 +/- 58.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=-18981.00 +/- 118.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=3000, episode_reward=-3500.20 +/- 356.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4000, episode_reward=-3698.80 +/- 724.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=5000, episode_reward=-261.00 +/- 450.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=6000, episode_reward=-41.40 +/- 143.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=7000, episode_reward=-1048.40 +/- 763.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=8000, episode_reward=-609.60 +/- 158.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=9000, episode_reward=-601.20 +/- 562.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=10000, episode_reward=-158.80 +/- 365.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=11000, episode_reward=-937.20 +/- 847.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=12000, episode_reward=-568.40 +/- 1097.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=13000, episode_reward=-87.60 +/- 437.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=14000, episode_reward=-447.20 +/- 450.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=15000, episode_reward=-243.60 +/- 561.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=16000, episode_reward=-747.20 +/- 754.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=17000, episode_reward=-100.20 +/- 236.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=18000, episode_reward=-433.20 +/- 379.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=19000, episode_reward=-85.00 +/- 487.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=20000, episode_reward=-303.20 +/- 652.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=21000, episode_reward=-96.80 +/- 532.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=22000, episode_reward=94.60 +/- 149.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=23000, episode_reward=-42.00 +/- 443.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=24000, episode_reward=-118.60 +/- 515.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=25000, episode_reward=-765.40 +/- 1616.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=26000, episode_reward=-196.40 +/- 401.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=27000, episode_reward=235.60 +/- 320.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=28000, episode_reward=-107.60 +/- 282.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=29000, episode_reward=188.20 +/- 312.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=30000, episode_reward=-12.80 +/- 131.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=31000, episode_reward=-279.00 +/- 583.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=32000, episode_reward=-2.20 +/- 212.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=33000, episode_reward=-200.80 +/- 683.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=34000, episode_reward=-631.20 +/- 838.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=35000, episode_reward=-479.60 +/- 804.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=36000, episode_reward=49.20 +/- 204.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=37000, episode_reward=-329.20 +/- 502.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=38000, episode_reward=-220.40 +/- 284.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=39000, episode_reward=-338.80 +/- 653.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=40000, episode_reward=-322.00 +/- 555.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=41000, episode_reward=280.60 +/- 167.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=42000, episode_reward=375.00 +/- 217.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=43000, episode_reward=310.60 +/- 79.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=44000, episode_reward=432.40 +/- 196.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=45000, episode_reward=222.80 +/- 133.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=46000, episode_reward=308.60 +/- 127.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=47000, episode_reward=190.20 +/- 272.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=48000, episode_reward=113.40 +/- 267.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=49000, episode_reward=202.60 +/- 256.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=50000, episode_reward=214.60 +/- 97.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=51000, episode_reward=372.20 +/- 277.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=52000, episode_reward=97.20 +/- 255.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=53000, episode_reward=-53.00 +/- 332.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=54000, episode_reward=-24.20 +/- 325.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=55000, episode_reward=59.60 +/- 263.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=56000, episode_reward=142.00 +/- 165.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=57000, episode_reward=91.80 +/- 203.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=58000, episode_reward=79.40 +/- 210.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=59000, episode_reward=-87.00 +/- 460.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=60000, episode_reward=90.00 +/- 140.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=61000, episode_reward=-148.00 +/- 242.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=62000, episode_reward=-10.20 +/- 453.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=63000, episode_reward=57.00 +/- 246.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=64000, episode_reward=144.00 +/- 247.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=65000, episode_reward=144.00 +/- 127.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=66000, episode_reward=154.60 +/- 230.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=67000, episode_reward=230.00 +/- 310.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=68000, episode_reward=268.60 +/- 123.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=69000, episode_reward=190.60 +/- 260.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=70000, episode_reward=247.20 +/- 207.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=71000, episode_reward=452.80 +/- 249.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=72000, episode_reward=295.20 +/- 140.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=73000, episode_reward=241.20 +/- 154.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=74000, episode_reward=81.20 +/- 257.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=75000, episode_reward=237.20 +/- 181.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=76000, episode_reward=46.20 +/- 281.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=77000, episode_reward=239.60 +/- 268.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=78000, episode_reward=125.80 +/- 181.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=79000, episode_reward=41.20 +/- 261.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=80000, episode_reward=237.80 +/- 106.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=81000, episode_reward=283.40 +/- 242.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=82000, episode_reward=194.00 +/- 155.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=83000, episode_reward=255.60 +/- 40.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=84000, episode_reward=281.40 +/- 218.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=85000, episode_reward=376.20 +/- 205.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=86000, episode_reward=198.20 +/- 183.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=87000, episode_reward=286.20 +/- 221.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=88000, episode_reward=306.20 +/- 237.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=89000, episode_reward=375.00 +/- 202.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=90000, episode_reward=391.20 +/- 225.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=91000, episode_reward=120.00 +/- 115.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=92000, episode_reward=251.60 +/- 121.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=93000, episode_reward=242.60 +/- 258.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=94000, episode_reward=160.80 +/- 90.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=95000, episode_reward=273.20 +/- 170.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=96000, episode_reward=200.20 +/- 113.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=97000, episode_reward=275.80 +/- 54.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=98000, episode_reward=202.00 +/- 283.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=99000, episode_reward=102.80 +/- 263.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=100000, episode_reward=327.80 +/- 65.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=101000, episode_reward=257.60 +/- 223.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=102000, episode_reward=242.40 +/- 332.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=103000, episode_reward=410.40 +/- 150.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=104000, episode_reward=266.40 +/- 128.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=105000, episode_reward=273.60 +/- 150.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=106000, episode_reward=300.20 +/- 136.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=107000, episode_reward=380.20 +/- 215.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=108000, episode_reward=226.40 +/- 177.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=109000, episode_reward=395.40 +/- 247.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=110000, episode_reward=218.00 +/- 178.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=111000, episode_reward=321.00 +/- 112.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=112000, episode_reward=246.80 +/- 149.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=113000, episode_reward=340.60 +/- 99.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=114000, episode_reward=481.80 +/- 189.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=115000, episode_reward=339.20 +/- 214.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=116000, episode_reward=488.60 +/- 192.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=117000, episode_reward=224.40 +/- 95.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=118000, episode_reward=272.00 +/- 145.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=119000, episode_reward=270.40 +/- 164.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=120000, episode_reward=357.60 +/- 134.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=121000, episode_reward=350.80 +/- 200.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=122000, episode_reward=445.80 +/- 143.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=123000, episode_reward=456.60 +/- 102.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=124000, episode_reward=410.60 +/- 104.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=125000, episode_reward=253.20 +/- 122.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=126000, episode_reward=166.20 +/- 48.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=127000, episode_reward=318.00 +/- 112.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=128000, episode_reward=414.40 +/- 194.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=129000, episode_reward=251.00 +/- 182.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=130000, episode_reward=260.20 +/- 153.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=131000, episode_reward=301.80 +/- 99.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=132000, episode_reward=376.80 +/- 243.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=133000, episode_reward=126.60 +/- 99.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=134000, episode_reward=367.20 +/- 322.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=135000, episode_reward=414.40 +/- 198.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=136000, episode_reward=-24.80 +/- 141.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=137000, episode_reward=188.20 +/- 123.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=138000, episode_reward=314.60 +/- 85.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=139000, episode_reward=443.80 +/- 87.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=140000, episode_reward=430.20 +/- 111.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=141000, episode_reward=472.00 +/- 70.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=142000, episode_reward=262.40 +/- 256.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=143000, episode_reward=416.80 +/- 270.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=144000, episode_reward=396.40 +/- 229.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=145000, episode_reward=436.20 +/- 199.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=146000, episode_reward=364.40 +/- 245.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=147000, episode_reward=233.60 +/- 146.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=148000, episode_reward=405.20 +/- 86.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=149000, episode_reward=320.20 +/- 165.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=150000, episode_reward=326.00 +/- 173.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=151000, episode_reward=184.40 +/- 193.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=152000, episode_reward=547.40 +/- 201.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=153000, episode_reward=434.40 +/- 241.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=154000, episode_reward=192.20 +/- 136.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=155000, episode_reward=374.20 +/- 193.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=156000, episode_reward=445.40 +/- 244.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=157000, episode_reward=235.60 +/- 159.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=158000, episode_reward=317.00 +/- 236.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=159000, episode_reward=94.80 +/- 121.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=160000, episode_reward=276.60 +/- 176.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=161000, episode_reward=314.80 +/- 85.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=162000, episode_reward=477.20 +/- 132.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=163000, episode_reward=326.60 +/- 102.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=164000, episode_reward=205.80 +/- 216.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=165000, episode_reward=330.20 +/- 241.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=166000, episode_reward=188.40 +/- 240.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=167000, episode_reward=264.80 +/- 297.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=168000, episode_reward=421.40 +/- 157.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=169000, episode_reward=295.20 +/- 127.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=170000, episode_reward=373.20 +/- 310.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=171000, episode_reward=447.60 +/- 95.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=172000, episode_reward=371.60 +/- 196.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=173000, episode_reward=148.40 +/- 276.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=174000, episode_reward=359.00 +/- 97.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=175000, episode_reward=276.20 +/- 204.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=176000, episode_reward=302.20 +/- 204.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=177000, episode_reward=235.40 +/- 114.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=178000, episode_reward=334.80 +/- 173.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=179000, episode_reward=316.00 +/- 144.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=180000, episode_reward=434.20 +/- 23.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=181000, episode_reward=412.20 +/- 133.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=182000, episode_reward=252.00 +/- 231.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=183000, episode_reward=121.00 +/- 362.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=184000, episode_reward=202.40 +/- 129.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=185000, episode_reward=322.80 +/- 89.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=186000, episode_reward=431.00 +/- 143.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=187000, episode_reward=402.20 +/- 229.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=188000, episode_reward=183.60 +/- 170.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=189000, episode_reward=288.00 +/- 94.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=190000, episode_reward=431.00 +/- 280.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=191000, episode_reward=336.00 +/- 225.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=192000, episode_reward=574.80 +/- 217.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=193000, episode_reward=312.40 +/- 141.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=194000, episode_reward=362.60 +/- 177.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=195000, episode_reward=301.60 +/- 262.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=196000, episode_reward=312.80 +/- 177.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=197000, episode_reward=234.00 +/- 226.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=198000, episode_reward=412.40 +/- 139.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=199000, episode_reward=267.00 +/- 156.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=200000, episode_reward=302.60 +/- 135.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=201000, episode_reward=331.20 +/- 150.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=202000, episode_reward=372.20 +/- 148.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=203000, episode_reward=355.20 +/- 83.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=204000, episode_reward=184.80 +/- 101.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=205000, episode_reward=493.60 +/- 195.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=206000, episode_reward=490.80 +/- 62.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=207000, episode_reward=349.40 +/- 123.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=208000, episode_reward=341.40 +/- 226.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=209000, episode_reward=401.60 +/- 214.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=210000, episode_reward=185.40 +/- 190.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=211000, episode_reward=310.00 +/- 127.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=212000, episode_reward=283.60 +/- 65.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=213000, episode_reward=319.40 +/- 149.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=214000, episode_reward=244.00 +/- 97.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=215000, episode_reward=285.80 +/- 172.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=216000, episode_reward=326.40 +/- 252.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=217000, episode_reward=359.80 +/- 219.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=218000, episode_reward=246.40 +/- 197.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=219000, episode_reward=175.00 +/- 148.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=220000, episode_reward=295.20 +/- 146.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=221000, episode_reward=438.00 +/- 192.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=222000, episode_reward=297.20 +/- 205.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=223000, episode_reward=268.60 +/- 172.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=224000, episode_reward=221.80 +/- 95.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=225000, episode_reward=335.80 +/- 46.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=226000, episode_reward=334.60 +/- 170.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=227000, episode_reward=406.60 +/- 284.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=228000, episode_reward=288.40 +/- 179.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=229000, episode_reward=359.80 +/- 131.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=230000, episode_reward=279.40 +/- 193.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=231000, episode_reward=390.00 +/- 116.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=232000, episode_reward=273.60 +/- 176.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=233000, episode_reward=303.60 +/- 49.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=234000, episode_reward=395.60 +/- 63.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=235000, episode_reward=415.40 +/- 229.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=236000, episode_reward=338.20 +/- 126.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=237000, episode_reward=485.80 +/- 210.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=238000, episode_reward=331.80 +/- 113.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=239000, episode_reward=344.60 +/- 141.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=240000, episode_reward=250.80 +/- 137.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=241000, episode_reward=263.00 +/- 48.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=242000, episode_reward=363.60 +/- 88.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=243000, episode_reward=434.00 +/- 162.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=244000, episode_reward=367.40 +/- 102.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=245000, episode_reward=354.60 +/- 55.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=246000, episode_reward=377.80 +/- 108.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=247000, episode_reward=268.00 +/- 176.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=248000, episode_reward=306.20 +/- 152.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=249000, episode_reward=272.20 +/- 162.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=250000, episode_reward=267.80 +/- 156.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=251000, episode_reward=295.60 +/- 133.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=252000, episode_reward=257.40 +/- 113.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=253000, episode_reward=321.00 +/- 165.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=254000, episode_reward=280.20 +/- 129.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=255000, episode_reward=431.40 +/- 202.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=256000, episode_reward=159.00 +/- 152.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=257000, episode_reward=345.40 +/- 208.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=258000, episode_reward=343.60 +/- 124.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=259000, episode_reward=410.60 +/- 267.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=260000, episode_reward=327.00 +/- 127.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=261000, episode_reward=214.20 +/- 105.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=262000, episode_reward=277.40 +/- 167.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=263000, episode_reward=372.60 +/- 174.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=264000, episode_reward=420.80 +/- 189.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=265000, episode_reward=167.20 +/- 132.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=266000, episode_reward=281.80 +/- 213.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=267000, episode_reward=314.80 +/- 66.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=268000, episode_reward=319.00 +/- 176.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=269000, episode_reward=397.60 +/- 112.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=270000, episode_reward=350.40 +/- 241.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=271000, episode_reward=288.40 +/- 241.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=272000, episode_reward=481.40 +/- 241.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=273000, episode_reward=277.20 +/- 244.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=274000, episode_reward=338.40 +/- 131.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=275000, episode_reward=450.60 +/- 94.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=276000, episode_reward=86.80 +/- 123.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=277000, episode_reward=303.40 +/- 100.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=278000, episode_reward=286.20 +/- 186.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=279000, episode_reward=288.00 +/- 154.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=280000, episode_reward=364.00 +/- 112.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=281000, episode_reward=368.80 +/- 159.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=282000, episode_reward=355.60 +/- 234.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=283000, episode_reward=285.60 +/- 174.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=284000, episode_reward=365.80 +/- 165.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=285000, episode_reward=193.80 +/- 155.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=286000, episode_reward=382.40 +/- 257.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=287000, episode_reward=323.00 +/- 236.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=288000, episode_reward=301.40 +/- 292.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=289000, episode_reward=135.00 +/- 216.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=290000, episode_reward=436.00 +/- 165.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=291000, episode_reward=260.60 +/- 120.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=292000, episode_reward=358.40 +/- 129.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=293000, episode_reward=447.40 +/- 172.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=294000, episode_reward=416.80 +/- 121.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=295000, episode_reward=309.00 +/- 200.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=296000, episode_reward=282.60 +/- 86.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=297000, episode_reward=223.00 +/- 194.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=298000, episode_reward=325.40 +/- 151.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=299000, episode_reward=321.60 +/- 111.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=300000, episode_reward=329.20 +/- 157.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=301000, episode_reward=384.00 +/- 169.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=302000, episode_reward=299.20 +/- 183.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=303000, episode_reward=379.00 +/- 41.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=304000, episode_reward=291.00 +/- 209.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=305000, episode_reward=329.80 +/- 117.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=306000, episode_reward=272.60 +/- 75.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=307000, episode_reward=266.00 +/- 221.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=308000, episode_reward=228.80 +/- 191.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=309000, episode_reward=281.20 +/- 84.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=310000, episode_reward=360.40 +/- 220.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=311000, episode_reward=359.80 +/- 69.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=312000, episode_reward=420.80 +/- 115.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=313000, episode_reward=324.80 +/- 74.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=314000, episode_reward=444.60 +/- 209.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=315000, episode_reward=509.20 +/- 121.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=316000, episode_reward=392.40 +/- 130.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=317000, episode_reward=359.40 +/- 173.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=318000, episode_reward=386.60 +/- 119.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=319000, episode_reward=346.40 +/- 212.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=320000, episode_reward=361.60 +/- 136.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=321000, episode_reward=382.40 +/- 148.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=322000, episode_reward=371.20 +/- 162.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=323000, episode_reward=443.80 +/- 260.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=324000, episode_reward=329.80 +/- 187.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=325000, episode_reward=281.80 +/- 206.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=326000, episode_reward=331.80 +/- 198.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=327000, episode_reward=480.40 +/- 226.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=328000, episode_reward=238.40 +/- 61.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=329000, episode_reward=431.60 +/- 95.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=330000, episode_reward=424.20 +/- 187.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=331000, episode_reward=318.00 +/- 122.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=332000, episode_reward=356.00 +/- 94.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=333000, episode_reward=282.20 +/- 166.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=334000, episode_reward=256.80 +/- 198.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=335000, episode_reward=308.20 +/- 144.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=336000, episode_reward=305.20 +/- 181.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=337000, episode_reward=328.20 +/- 157.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=338000, episode_reward=309.00 +/- 219.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=339000, episode_reward=285.20 +/- 95.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=340000, episode_reward=448.40 +/- 177.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=341000, episode_reward=371.20 +/- 165.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=342000, episode_reward=401.20 +/- 156.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=343000, episode_reward=508.60 +/- 109.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=344000, episode_reward=351.60 +/- 178.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=345000, episode_reward=366.80 +/- 87.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=346000, episode_reward=303.00 +/- 79.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=347000, episode_reward=366.80 +/- 217.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=348000, episode_reward=405.20 +/- 197.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=349000, episode_reward=392.60 +/- 210.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=350000, episode_reward=352.60 +/- 165.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=351000, episode_reward=499.20 +/- 258.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=352000, episode_reward=419.00 +/- 179.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=353000, episode_reward=302.80 +/- 178.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=354000, episode_reward=283.00 +/- 47.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=355000, episode_reward=433.00 +/- 176.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=356000, episode_reward=127.60 +/- 181.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=357000, episode_reward=178.20 +/- 295.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=358000, episode_reward=342.40 +/- 96.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=359000, episode_reward=304.60 +/- 97.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=360000, episode_reward=505.00 +/- 132.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=361000, episode_reward=344.20 +/- 135.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=362000, episode_reward=397.60 +/- 171.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=363000, episode_reward=429.20 +/- 150.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=364000, episode_reward=313.00 +/- 141.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=365000, episode_reward=492.40 +/- 187.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=366000, episode_reward=141.00 +/- 132.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=367000, episode_reward=242.40 +/- 113.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=368000, episode_reward=381.20 +/- 265.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=369000, episode_reward=370.00 +/- 73.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=370000, episode_reward=394.80 +/- 271.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=371000, episode_reward=362.40 +/- 232.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=372000, episode_reward=406.40 +/- 255.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=373000, episode_reward=261.60 +/- 261.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=374000, episode_reward=401.20 +/- 149.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=375000, episode_reward=408.60 +/- 168.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=376000, episode_reward=270.80 +/- 125.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=377000, episode_reward=345.60 +/- 138.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=378000, episode_reward=435.00 +/- 360.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=379000, episode_reward=415.80 +/- 128.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=380000, episode_reward=413.80 +/- 186.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=381000, episode_reward=305.00 +/- 156.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=382000, episode_reward=179.20 +/- 117.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=383000, episode_reward=484.40 +/- 151.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=384000, episode_reward=362.80 +/- 181.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=385000, episode_reward=306.00 +/- 135.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=386000, episode_reward=440.80 +/- 140.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=387000, episode_reward=332.60 +/- 80.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=388000, episode_reward=417.40 +/- 159.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=389000, episode_reward=259.80 +/- 235.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=390000, episode_reward=348.20 +/- 133.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=391000, episode_reward=396.60 +/- 82.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=392000, episode_reward=367.80 +/- 75.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=393000, episode_reward=325.20 +/- 125.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=394000, episode_reward=346.40 +/- 86.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=395000, episode_reward=311.80 +/- 56.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=396000, episode_reward=325.60 +/- 263.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=397000, episode_reward=409.00 +/- 120.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=398000, episode_reward=291.60 +/- 118.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=399000, episode_reward=428.00 +/- 171.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=400000, episode_reward=336.00 +/- 249.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=401000, episode_reward=379.60 +/- 263.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=402000, episode_reward=350.20 +/- 215.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=403000, episode_reward=309.00 +/- 77.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=404000, episode_reward=317.20 +/- 103.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=405000, episode_reward=308.80 +/- 66.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=406000, episode_reward=416.40 +/- 140.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=407000, episode_reward=307.80 +/- 170.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=408000, episode_reward=169.00 +/- 119.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=409000, episode_reward=368.40 +/- 151.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=410000, episode_reward=309.20 +/- 122.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=411000, episode_reward=275.00 +/- 93.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=412000, episode_reward=374.80 +/- 46.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=413000, episode_reward=335.80 +/- 143.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=414000, episode_reward=196.20 +/- 105.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=415000, episode_reward=285.80 +/- 239.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=416000, episode_reward=389.00 +/- 119.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=417000, episode_reward=272.60 +/- 90.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=418000, episode_reward=233.60 +/- 245.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=419000, episode_reward=188.60 +/- 251.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=420000, episode_reward=376.20 +/- 149.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=421000, episode_reward=358.20 +/- 258.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=422000, episode_reward=286.00 +/- 266.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=423000, episode_reward=368.40 +/- 141.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=424000, episode_reward=393.20 +/- 153.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=425000, episode_reward=193.20 +/- 105.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=426000, episode_reward=295.20 +/- 167.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=427000, episode_reward=291.60 +/- 267.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=428000, episode_reward=480.20 +/- 122.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=429000, episode_reward=234.00 +/- 245.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=430000, episode_reward=505.40 +/- 106.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=431000, episode_reward=335.60 +/- 115.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=432000, episode_reward=286.80 +/- 163.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=433000, episode_reward=331.00 +/- 308.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=434000, episode_reward=315.40 +/- 86.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=435000, episode_reward=418.80 +/- 236.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=436000, episode_reward=457.80 +/- 208.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=437000, episode_reward=268.40 +/- 263.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=438000, episode_reward=368.20 +/- 195.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=439000, episode_reward=309.00 +/- 187.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=440000, episode_reward=275.00 +/- 136.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=441000, episode_reward=343.40 +/- 76.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=442000, episode_reward=266.80 +/- 158.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=443000, episode_reward=259.00 +/- 233.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=444000, episode_reward=476.40 +/- 241.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=445000, episode_reward=337.60 +/- 77.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=446000, episode_reward=251.20 +/- 100.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=447000, episode_reward=347.80 +/- 123.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=448000, episode_reward=398.40 +/- 189.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=449000, episode_reward=312.00 +/- 173.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=450000, episode_reward=285.20 +/- 379.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=451000, episode_reward=360.40 +/- 133.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=452000, episode_reward=383.40 +/- 81.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=453000, episode_reward=541.60 +/- 198.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=454000, episode_reward=256.00 +/- 107.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=455000, episode_reward=194.60 +/- 178.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=456000, episode_reward=400.40 +/- 211.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=457000, episode_reward=314.80 +/- 127.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=458000, episode_reward=402.60 +/- 217.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=459000, episode_reward=550.80 +/- 185.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=460000, episode_reward=438.00 +/- 348.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=461000, episode_reward=531.00 +/- 246.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=462000, episode_reward=415.60 +/- 134.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=463000, episode_reward=393.00 +/- 313.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=464000, episode_reward=568.60 +/- 205.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=465000, episode_reward=261.60 +/- 206.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=466000, episode_reward=334.80 +/- 252.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=467000, episode_reward=477.20 +/- 318.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=468000, episode_reward=613.40 +/- 211.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=469000, episode_reward=461.00 +/- 169.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=470000, episode_reward=512.00 +/- 340.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=471000, episode_reward=538.80 +/- 167.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=472000, episode_reward=721.00 +/- 365.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=473000, episode_reward=572.40 +/- 215.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=474000, episode_reward=499.20 +/- 220.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=475000, episode_reward=619.80 +/- 196.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=476000, episode_reward=583.00 +/- 386.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=477000, episode_reward=569.40 +/- 216.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=478000, episode_reward=537.80 +/- 217.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=479000, episode_reward=484.60 +/- 224.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=480000, episode_reward=583.20 +/- 242.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=481000, episode_reward=505.00 +/- 289.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=482000, episode_reward=410.00 +/- 282.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=483000, episode_reward=348.80 +/- 227.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=484000, episode_reward=503.00 +/- 153.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=485000, episode_reward=571.60 +/- 146.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=486000, episode_reward=571.60 +/- 268.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=487000, episode_reward=427.20 +/- 258.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=488000, episode_reward=796.00 +/- 115.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=489000, episode_reward=649.40 +/- 364.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=490000, episode_reward=709.00 +/- 200.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=491000, episode_reward=481.80 +/- 157.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=492000, episode_reward=675.40 +/- 319.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=493000, episode_reward=687.60 +/- 143.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=494000, episode_reward=788.60 +/- 401.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=495000, episode_reward=495.20 +/- 206.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=496000, episode_reward=789.80 +/- 249.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=497000, episode_reward=684.80 +/- 303.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=498000, episode_reward=974.00 +/- 165.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=499000, episode_reward=828.00 +/- 198.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=500000, episode_reward=568.20 +/- 225.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=501000, episode_reward=809.40 +/- 290.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Gamma = 0.9, Learning Rate = 3e-05, Activation Function = Tanh, Neurons = 64, Process noise = 0.1, Measurement noise = 0.0\n",
      "Eval num_timesteps=1000, episode_reward=42.60 +/- 176.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=-25.80 +/- 143.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=3000, episode_reward=166.40 +/- 171.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4000, episode_reward=-269.40 +/- 428.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=5000, episode_reward=103.20 +/- 273.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=6000, episode_reward=-177.60 +/- 512.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=7000, episode_reward=-641.20 +/- 565.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=8000, episode_reward=-404.00 +/- 689.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=9000, episode_reward=-173.80 +/- 403.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=10000, episode_reward=-200.40 +/- 404.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=11000, episode_reward=406.80 +/- 176.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=12000, episode_reward=502.80 +/- 217.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=13000, episode_reward=304.20 +/- 307.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=14000, episode_reward=249.00 +/- 250.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=15000, episode_reward=-159.00 +/- 519.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=16000, episode_reward=-684.80 +/- 1583.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=17000, episode_reward=-710.40 +/- 973.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=18000, episode_reward=85.80 +/- 426.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=19000, episode_reward=166.00 +/- 179.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=20000, episode_reward=183.40 +/- 329.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=21000, episode_reward=399.00 +/- 164.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=22000, episode_reward=344.80 +/- 163.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=23000, episode_reward=226.80 +/- 240.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=24000, episode_reward=401.80 +/- 245.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=25000, episode_reward=327.20 +/- 125.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=26000, episode_reward=225.20 +/- 270.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=27000, episode_reward=225.40 +/- 184.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=28000, episode_reward=283.60 +/- 207.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=29000, episode_reward=238.60 +/- 181.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=30000, episode_reward=254.20 +/- 292.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=31000, episode_reward=254.00 +/- 256.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=32000, episode_reward=192.60 +/- 220.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=33000, episode_reward=-17.20 +/- 523.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=34000, episode_reward=-253.20 +/- 364.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=35000, episode_reward=-503.40 +/- 243.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=36000, episode_reward=72.00 +/- 247.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=37000, episode_reward=-603.00 +/- 992.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=38000, episode_reward=-44.00 +/- 233.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=39000, episode_reward=-1164.40 +/- 815.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=40000, episode_reward=-936.40 +/- 1241.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=41000, episode_reward=304.40 +/- 149.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=42000, episode_reward=118.60 +/- 191.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=43000, episode_reward=51.00 +/- 204.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=44000, episode_reward=339.00 +/- 224.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=45000, episode_reward=179.80 +/- 282.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=46000, episode_reward=-30.20 +/- 335.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=47000, episode_reward=147.40 +/- 124.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=48000, episode_reward=-150.00 +/- 344.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=49000, episode_reward=26.00 +/- 207.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=50000, episode_reward=-55.60 +/- 205.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=51000, episode_reward=83.00 +/- 310.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=52000, episode_reward=83.40 +/- 128.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=53000, episode_reward=-12.80 +/- 321.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=54000, episode_reward=-41.20 +/- 225.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=55000, episode_reward=280.60 +/- 330.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=56000, episode_reward=151.40 +/- 204.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=57000, episode_reward=194.20 +/- 119.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=58000, episode_reward=168.40 +/- 147.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=59000, episode_reward=223.20 +/- 154.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=60000, episode_reward=372.40 +/- 139.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=61000, episode_reward=456.20 +/- 143.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=62000, episode_reward=190.20 +/- 245.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=63000, episode_reward=208.60 +/- 180.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=64000, episode_reward=-175.00 +/- 249.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=65000, episode_reward=201.80 +/- 279.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=66000, episode_reward=376.60 +/- 169.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=67000, episode_reward=148.00 +/- 165.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=68000, episode_reward=305.20 +/- 279.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=69000, episode_reward=301.20 +/- 216.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=70000, episode_reward=194.00 +/- 240.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=71000, episode_reward=26.00 +/- 345.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=72000, episode_reward=223.20 +/- 108.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=73000, episode_reward=-156.20 +/- 136.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=74000, episode_reward=29.60 +/- 198.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=75000, episode_reward=378.60 +/- 197.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=76000, episode_reward=299.60 +/- 153.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=77000, episode_reward=228.80 +/- 109.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=78000, episode_reward=160.00 +/- 263.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=79000, episode_reward=72.40 +/- 229.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=80000, episode_reward=239.60 +/- 140.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=81000, episode_reward=511.20 +/- 118.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=82000, episode_reward=271.00 +/- 256.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=83000, episode_reward=191.80 +/- 173.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=84000, episode_reward=270.60 +/- 136.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=85000, episode_reward=278.80 +/- 223.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=86000, episode_reward=124.60 +/- 224.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=87000, episode_reward=229.60 +/- 137.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=88000, episode_reward=53.80 +/- 178.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=89000, episode_reward=350.40 +/- 268.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=90000, episode_reward=105.40 +/- 186.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=91000, episode_reward=367.20 +/- 157.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=92000, episode_reward=354.80 +/- 258.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=93000, episode_reward=323.60 +/- 142.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=94000, episode_reward=271.60 +/- 125.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=95000, episode_reward=189.80 +/- 221.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=96000, episode_reward=382.00 +/- 268.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=97000, episode_reward=247.40 +/- 178.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=98000, episode_reward=363.60 +/- 135.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=99000, episode_reward=209.40 +/- 125.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=100000, episode_reward=323.00 +/- 248.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=101000, episode_reward=417.60 +/- 281.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=102000, episode_reward=474.20 +/- 112.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=103000, episode_reward=403.80 +/- 101.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=104000, episode_reward=326.40 +/- 146.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=105000, episode_reward=192.60 +/- 131.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=106000, episode_reward=261.40 +/- 156.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=107000, episode_reward=274.00 +/- 179.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=108000, episode_reward=514.80 +/- 215.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=109000, episode_reward=347.60 +/- 290.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=110000, episode_reward=414.60 +/- 79.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=111000, episode_reward=321.20 +/- 315.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=112000, episode_reward=428.00 +/- 205.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=113000, episode_reward=365.20 +/- 164.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=114000, episode_reward=441.80 +/- 157.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=115000, episode_reward=397.40 +/- 163.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=116000, episode_reward=337.80 +/- 113.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=117000, episode_reward=319.80 +/- 120.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=118000, episode_reward=255.80 +/- 189.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=119000, episode_reward=357.00 +/- 166.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=120000, episode_reward=390.60 +/- 179.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=121000, episode_reward=435.60 +/- 115.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=122000, episode_reward=346.60 +/- 75.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=123000, episode_reward=461.40 +/- 99.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=124000, episode_reward=249.80 +/- 216.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=125000, episode_reward=363.40 +/- 205.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=126000, episode_reward=479.40 +/- 213.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=127000, episode_reward=277.80 +/- 162.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=128000, episode_reward=452.40 +/- 111.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=129000, episode_reward=584.40 +/- 190.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=130000, episode_reward=376.00 +/- 215.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=131000, episode_reward=225.80 +/- 283.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=132000, episode_reward=292.60 +/- 83.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=133000, episode_reward=446.00 +/- 125.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=134000, episode_reward=410.00 +/- 212.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=135000, episode_reward=253.60 +/- 146.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=136000, episode_reward=525.80 +/- 79.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=137000, episode_reward=439.60 +/- 343.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=138000, episode_reward=309.20 +/- 302.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=139000, episode_reward=417.60 +/- 228.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=140000, episode_reward=482.20 +/- 119.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=141000, episode_reward=207.60 +/- 37.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=142000, episode_reward=357.60 +/- 279.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=143000, episode_reward=362.60 +/- 218.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=144000, episode_reward=319.40 +/- 182.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=145000, episode_reward=264.00 +/- 144.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=146000, episode_reward=359.60 +/- 155.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=147000, episode_reward=475.20 +/- 148.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=148000, episode_reward=351.20 +/- 304.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=149000, episode_reward=385.60 +/- 170.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=150000, episode_reward=379.00 +/- 55.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=151000, episode_reward=515.60 +/- 262.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=152000, episode_reward=394.00 +/- 125.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=153000, episode_reward=391.80 +/- 144.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=154000, episode_reward=392.80 +/- 186.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=155000, episode_reward=525.00 +/- 264.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=156000, episode_reward=318.80 +/- 211.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=157000, episode_reward=489.20 +/- 134.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=158000, episode_reward=467.60 +/- 245.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=159000, episode_reward=221.20 +/- 221.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=160000, episode_reward=325.20 +/- 216.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=161000, episode_reward=265.20 +/- 105.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=162000, episode_reward=275.20 +/- 164.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=163000, episode_reward=457.00 +/- 205.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=164000, episode_reward=457.00 +/- 109.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=165000, episode_reward=439.80 +/- 202.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=166000, episode_reward=452.20 +/- 239.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=167000, episode_reward=467.40 +/- 148.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=168000, episode_reward=321.20 +/- 162.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=169000, episode_reward=394.00 +/- 209.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=170000, episode_reward=271.40 +/- 274.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=171000, episode_reward=274.80 +/- 285.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=172000, episode_reward=259.60 +/- 221.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=173000, episode_reward=243.40 +/- 229.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=174000, episode_reward=328.00 +/- 209.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=175000, episode_reward=224.00 +/- 111.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=176000, episode_reward=250.60 +/- 177.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=177000, episode_reward=323.00 +/- 91.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=178000, episode_reward=442.20 +/- 168.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=179000, episode_reward=346.20 +/- 142.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=180000, episode_reward=484.20 +/- 127.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=181000, episode_reward=372.40 +/- 108.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=182000, episode_reward=338.80 +/- 202.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=183000, episode_reward=364.60 +/- 172.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=184000, episode_reward=372.40 +/- 154.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=185000, episode_reward=228.60 +/- 130.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=186000, episode_reward=383.20 +/- 206.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=187000, episode_reward=453.60 +/- 182.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=188000, episode_reward=288.80 +/- 222.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=189000, episode_reward=249.80 +/- 141.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=190000, episode_reward=312.60 +/- 171.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=191000, episode_reward=304.20 +/- 223.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=192000, episode_reward=220.00 +/- 281.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=193000, episode_reward=344.20 +/- 110.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=194000, episode_reward=429.60 +/- 110.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=195000, episode_reward=336.60 +/- 204.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=196000, episode_reward=286.40 +/- 179.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=197000, episode_reward=421.40 +/- 96.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=198000, episode_reward=353.40 +/- 121.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=199000, episode_reward=441.60 +/- 178.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=200000, episode_reward=246.40 +/- 162.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=201000, episode_reward=201.20 +/- 254.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=202000, episode_reward=310.80 +/- 61.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=203000, episode_reward=275.40 +/- 93.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=204000, episode_reward=395.20 +/- 128.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=205000, episode_reward=228.40 +/- 215.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=206000, episode_reward=518.40 +/- 87.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=207000, episode_reward=261.60 +/- 129.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=208000, episode_reward=499.80 +/- 133.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=209000, episode_reward=260.20 +/- 227.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=210000, episode_reward=293.00 +/- 104.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=211000, episode_reward=431.60 +/- 211.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=212000, episode_reward=252.40 +/- 241.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=213000, episode_reward=405.80 +/- 56.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=214000, episode_reward=276.80 +/- 188.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=215000, episode_reward=285.80 +/- 117.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=216000, episode_reward=364.20 +/- 102.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=217000, episode_reward=331.80 +/- 228.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=218000, episode_reward=335.40 +/- 321.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=219000, episode_reward=429.80 +/- 83.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=220000, episode_reward=182.00 +/- 159.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=221000, episode_reward=244.20 +/- 372.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=222000, episode_reward=366.80 +/- 147.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=223000, episode_reward=385.80 +/- 205.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=224000, episode_reward=249.40 +/- 177.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=225000, episode_reward=415.80 +/- 75.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=226000, episode_reward=473.80 +/- 82.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=227000, episode_reward=410.00 +/- 112.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=228000, episode_reward=339.00 +/- 152.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=229000, episode_reward=217.00 +/- 141.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=230000, episode_reward=479.00 +/- 159.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=231000, episode_reward=306.00 +/- 204.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=232000, episode_reward=351.00 +/- 48.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=233000, episode_reward=480.60 +/- 280.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=234000, episode_reward=455.20 +/- 184.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=235000, episode_reward=282.40 +/- 179.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=236000, episode_reward=295.20 +/- 176.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=237000, episode_reward=462.60 +/- 232.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=238000, episode_reward=248.60 +/- 68.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=239000, episode_reward=236.20 +/- 203.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=240000, episode_reward=383.80 +/- 44.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=241000, episode_reward=204.40 +/- 186.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=242000, episode_reward=239.00 +/- 154.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=243000, episode_reward=445.40 +/- 109.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=244000, episode_reward=277.60 +/- 159.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=245000, episode_reward=378.60 +/- 160.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=246000, episode_reward=383.20 +/- 165.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=247000, episode_reward=415.20 +/- 143.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=248000, episode_reward=328.40 +/- 268.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=249000, episode_reward=290.40 +/- 37.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=250000, episode_reward=171.20 +/- 66.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=251000, episode_reward=457.40 +/- 155.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=252000, episode_reward=277.20 +/- 340.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=253000, episode_reward=448.80 +/- 240.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=254000, episode_reward=298.60 +/- 153.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=255000, episode_reward=142.20 +/- 38.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=256000, episode_reward=323.80 +/- 177.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=257000, episode_reward=405.80 +/- 138.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=258000, episode_reward=237.00 +/- 140.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=259000, episode_reward=358.20 +/- 142.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=260000, episode_reward=330.00 +/- 152.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=261000, episode_reward=356.60 +/- 204.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=262000, episode_reward=414.20 +/- 88.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=263000, episode_reward=242.00 +/- 191.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=264000, episode_reward=207.60 +/- 198.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=265000, episode_reward=323.60 +/- 138.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=266000, episode_reward=345.80 +/- 163.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=267000, episode_reward=230.40 +/- 174.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=268000, episode_reward=381.00 +/- 115.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=269000, episode_reward=390.20 +/- 145.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=270000, episode_reward=229.80 +/- 83.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=271000, episode_reward=382.20 +/- 193.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=272000, episode_reward=421.00 +/- 159.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=273000, episode_reward=400.80 +/- 180.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=274000, episode_reward=327.60 +/- 208.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=275000, episode_reward=262.20 +/- 169.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=276000, episode_reward=330.40 +/- 147.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=277000, episode_reward=421.80 +/- 177.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=278000, episode_reward=322.60 +/- 212.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=279000, episode_reward=267.00 +/- 156.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=280000, episode_reward=407.20 +/- 240.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=281000, episode_reward=403.20 +/- 186.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=282000, episode_reward=468.00 +/- 162.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=283000, episode_reward=450.80 +/- 223.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=284000, episode_reward=363.00 +/- 196.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=285000, episode_reward=456.40 +/- 197.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=286000, episode_reward=254.60 +/- 220.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=287000, episode_reward=277.20 +/- 167.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=288000, episode_reward=213.60 +/- 100.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=289000, episode_reward=95.00 +/- 243.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=290000, episode_reward=507.20 +/- 151.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=291000, episode_reward=392.40 +/- 85.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=292000, episode_reward=301.20 +/- 188.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=293000, episode_reward=333.40 +/- 33.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=294000, episode_reward=290.00 +/- 200.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=295000, episode_reward=424.60 +/- 101.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=296000, episode_reward=324.60 +/- 214.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=297000, episode_reward=346.80 +/- 128.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=298000, episode_reward=425.60 +/- 270.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=299000, episode_reward=375.00 +/- 95.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=300000, episode_reward=214.80 +/- 141.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=301000, episode_reward=448.40 +/- 268.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=302000, episode_reward=465.20 +/- 71.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=303000, episode_reward=298.00 +/- 151.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=304000, episode_reward=481.40 +/- 130.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=305000, episode_reward=498.20 +/- 107.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=306000, episode_reward=287.80 +/- 306.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=307000, episode_reward=187.00 +/- 192.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=308000, episode_reward=382.20 +/- 291.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=309000, episode_reward=253.40 +/- 133.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=310000, episode_reward=217.00 +/- 113.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=311000, episode_reward=312.60 +/- 271.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=312000, episode_reward=247.00 +/- 161.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=313000, episode_reward=471.40 +/- 150.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=314000, episode_reward=390.00 +/- 167.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=315000, episode_reward=387.60 +/- 159.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=316000, episode_reward=376.40 +/- 83.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=317000, episode_reward=471.20 +/- 139.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=318000, episode_reward=183.20 +/- 212.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=319000, episode_reward=207.20 +/- 185.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=320000, episode_reward=496.80 +/- 110.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=321000, episode_reward=283.40 +/- 192.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=322000, episode_reward=371.40 +/- 180.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=323000, episode_reward=267.60 +/- 153.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=324000, episode_reward=459.60 +/- 199.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=325000, episode_reward=364.40 +/- 149.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=326000, episode_reward=419.40 +/- 156.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=327000, episode_reward=256.20 +/- 413.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=328000, episode_reward=264.60 +/- 239.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=329000, episode_reward=347.80 +/- 214.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=330000, episode_reward=291.80 +/- 214.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=331000, episode_reward=295.60 +/- 199.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=332000, episode_reward=431.00 +/- 217.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=333000, episode_reward=481.60 +/- 130.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=334000, episode_reward=453.80 +/- 84.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=335000, episode_reward=281.80 +/- 366.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=336000, episode_reward=205.40 +/- 115.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=337000, episode_reward=358.40 +/- 176.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=338000, episode_reward=80.60 +/- 224.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=339000, episode_reward=312.20 +/- 129.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=340000, episode_reward=279.60 +/- 135.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=341000, episode_reward=352.80 +/- 125.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=342000, episode_reward=235.20 +/- 199.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=343000, episode_reward=328.00 +/- 21.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=344000, episode_reward=105.40 +/- 173.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=345000, episode_reward=278.20 +/- 155.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=346000, episode_reward=251.60 +/- 148.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=347000, episode_reward=443.80 +/- 168.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=348000, episode_reward=332.60 +/- 137.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=349000, episode_reward=249.00 +/- 158.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=350000, episode_reward=227.60 +/- 121.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=351000, episode_reward=117.20 +/- 88.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=352000, episode_reward=38.80 +/- 282.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=353000, episode_reward=312.60 +/- 397.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=354000, episode_reward=321.20 +/- 88.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=355000, episode_reward=306.40 +/- 173.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=356000, episode_reward=202.40 +/- 117.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=357000, episode_reward=325.40 +/- 192.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=358000, episode_reward=242.60 +/- 319.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=359000, episode_reward=440.40 +/- 40.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=360000, episode_reward=325.00 +/- 111.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=361000, episode_reward=269.40 +/- 176.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=362000, episode_reward=415.20 +/- 124.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=363000, episode_reward=210.20 +/- 104.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=364000, episode_reward=263.20 +/- 178.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=365000, episode_reward=278.40 +/- 113.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=366000, episode_reward=354.80 +/- 184.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=367000, episode_reward=345.40 +/- 149.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=368000, episode_reward=273.80 +/- 200.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=369000, episode_reward=271.20 +/- 219.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=370000, episode_reward=335.20 +/- 207.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=371000, episode_reward=279.00 +/- 115.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=372000, episode_reward=403.00 +/- 90.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=373000, episode_reward=275.80 +/- 314.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=374000, episode_reward=306.00 +/- 120.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=375000, episode_reward=212.20 +/- 224.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=376000, episode_reward=334.80 +/- 207.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=377000, episode_reward=343.40 +/- 134.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=378000, episode_reward=286.40 +/- 242.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=379000, episode_reward=281.40 +/- 184.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=380000, episode_reward=243.20 +/- 240.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=381000, episode_reward=405.00 +/- 100.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=382000, episode_reward=393.40 +/- 193.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=383000, episode_reward=364.00 +/- 216.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=384000, episode_reward=334.20 +/- 104.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=385000, episode_reward=352.00 +/- 165.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=386000, episode_reward=476.20 +/- 234.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=387000, episode_reward=344.80 +/- 179.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=388000, episode_reward=283.60 +/- 212.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=389000, episode_reward=284.20 +/- 204.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=390000, episode_reward=380.80 +/- 290.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=391000, episode_reward=346.40 +/- 295.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=392000, episode_reward=417.20 +/- 183.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=393000, episode_reward=289.80 +/- 136.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=394000, episode_reward=228.00 +/- 153.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=395000, episode_reward=409.80 +/- 155.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=396000, episode_reward=295.00 +/- 148.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=397000, episode_reward=323.20 +/- 159.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=398000, episode_reward=249.40 +/- 115.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=399000, episode_reward=332.00 +/- 187.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=400000, episode_reward=279.40 +/- 76.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=401000, episode_reward=302.80 +/- 113.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=402000, episode_reward=394.40 +/- 93.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=403000, episode_reward=361.80 +/- 158.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=404000, episode_reward=219.60 +/- 149.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=405000, episode_reward=417.60 +/- 201.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=406000, episode_reward=360.40 +/- 126.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=407000, episode_reward=183.60 +/- 337.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=408000, episode_reward=372.40 +/- 182.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=409000, episode_reward=292.40 +/- 151.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=410000, episode_reward=192.40 +/- 227.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=411000, episode_reward=336.00 +/- 154.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=412000, episode_reward=197.40 +/- 117.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=413000, episode_reward=342.40 +/- 126.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=414000, episode_reward=220.40 +/- 206.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=415000, episode_reward=271.80 +/- 146.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=416000, episode_reward=308.60 +/- 131.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=417000, episode_reward=148.60 +/- 130.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=418000, episode_reward=274.20 +/- 187.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=419000, episode_reward=172.20 +/- 162.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=420000, episode_reward=251.00 +/- 184.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=421000, episode_reward=182.40 +/- 347.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=422000, episode_reward=554.40 +/- 251.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=423000, episode_reward=95.80 +/- 181.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=424000, episode_reward=431.80 +/- 134.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=425000, episode_reward=290.80 +/- 166.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=426000, episode_reward=301.40 +/- 206.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=427000, episode_reward=195.60 +/- 206.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=428000, episode_reward=314.00 +/- 238.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=429000, episode_reward=588.60 +/- 100.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=430000, episode_reward=290.00 +/- 242.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=431000, episode_reward=502.00 +/- 173.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=432000, episode_reward=303.60 +/- 144.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=433000, episode_reward=583.80 +/- 177.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=434000, episode_reward=474.00 +/- 324.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=435000, episode_reward=669.00 +/- 211.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=436000, episode_reward=628.80 +/- 217.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=437000, episode_reward=460.80 +/- 153.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=438000, episode_reward=385.00 +/- 280.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=439000, episode_reward=439.40 +/- 202.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=440000, episode_reward=604.60 +/- 173.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=441000, episode_reward=662.00 +/- 195.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=442000, episode_reward=341.80 +/- 267.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=443000, episode_reward=751.60 +/- 113.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=444000, episode_reward=405.80 +/- 145.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=445000, episode_reward=453.20 +/- 158.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=446000, episode_reward=768.00 +/- 230.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=447000, episode_reward=670.60 +/- 285.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=448000, episode_reward=456.00 +/- 319.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=449000, episode_reward=463.80 +/- 290.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=450000, episode_reward=642.60 +/- 113.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=451000, episode_reward=566.00 +/- 304.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=452000, episode_reward=869.60 +/- 225.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=453000, episode_reward=890.60 +/- 171.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=454000, episode_reward=737.40 +/- 263.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=455000, episode_reward=812.80 +/- 359.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=456000, episode_reward=668.40 +/- 267.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=457000, episode_reward=492.00 +/- 222.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=458000, episode_reward=784.80 +/- 120.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=459000, episode_reward=794.80 +/- 329.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=460000, episode_reward=922.80 +/- 97.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=461000, episode_reward=787.60 +/- 382.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=462000, episode_reward=910.40 +/- 322.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=463000, episode_reward=801.40 +/- 77.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=464000, episode_reward=960.20 +/- 169.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=465000, episode_reward=959.80 +/- 248.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=466000, episode_reward=898.20 +/- 280.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=467000, episode_reward=1129.00 +/- 397.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=468000, episode_reward=-2096.20 +/- 6259.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=469000, episode_reward=879.40 +/- 474.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=470000, episode_reward=817.00 +/- 190.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=471000, episode_reward=938.20 +/- 369.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=472000, episode_reward=374.60 +/- 957.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=473000, episode_reward=993.00 +/- 151.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=474000, episode_reward=-1920.00 +/- 5834.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=475000, episode_reward=1087.80 +/- 358.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=476000, episode_reward=971.40 +/- 178.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=477000, episode_reward=1243.80 +/- 169.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=478000, episode_reward=391.80 +/- 1390.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=479000, episode_reward=832.80 +/- 207.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=480000, episode_reward=533.40 +/- 1250.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=481000, episode_reward=999.20 +/- 227.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=482000, episode_reward=-2396.40 +/- 6895.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=483000, episode_reward=1197.20 +/- 388.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=484000, episode_reward=916.40 +/- 268.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=485000, episode_reward=1164.80 +/- 237.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=486000, episode_reward=1051.60 +/- 130.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=487000, episode_reward=1183.40 +/- 283.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=488000, episode_reward=1080.20 +/- 282.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=489000, episode_reward=1045.80 +/- 189.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=490000, episode_reward=1105.80 +/- 180.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=491000, episode_reward=1081.60 +/- 348.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=492000, episode_reward=-843.40 +/- 3621.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=493000, episode_reward=1108.40 +/- 188.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=494000, episode_reward=1202.00 +/- 219.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=495000, episode_reward=1140.40 +/- 165.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=496000, episode_reward=1206.60 +/- 252.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=497000, episode_reward=-1501.20 +/- 4961.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=498000, episode_reward=-302.40 +/- 2759.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=499000, episode_reward=1015.40 +/- 192.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=500000, episode_reward=1268.40 +/- 134.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=501000, episode_reward=963.80 +/- 379.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Gamma = 0.9, Learning Rate = 3e-05, Activation Function = ReLU, Neurons = 8, Process noise = 0.1, Measurement noise = 0.0\n",
      "Eval num_timesteps=1000, episode_reward=-1079.00 +/- 201.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=-1194.00 +/- 182.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=3000, episode_reward=266.60 +/- 158.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4000, episode_reward=212.20 +/- 247.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=5000, episode_reward=343.40 +/- 134.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=6000, episode_reward=181.00 +/- 228.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=7000, episode_reward=-158.80 +/- 260.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=8000, episode_reward=-143.80 +/- 150.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=9000, episode_reward=-378.00 +/- 155.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=10000, episode_reward=-296.20 +/- 192.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=11000, episode_reward=-4412.40 +/- 1180.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=12000, episode_reward=-5786.00 +/- 1390.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=13000, episode_reward=255.00 +/- 282.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=14000, episode_reward=279.60 +/- 228.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=15000, episode_reward=114.60 +/- 209.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=16000, episode_reward=307.20 +/- 158.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=17000, episode_reward=274.40 +/- 93.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=18000, episode_reward=456.40 +/- 122.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=19000, episode_reward=84.60 +/- 131.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=20000, episode_reward=51.60 +/- 112.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=21000, episode_reward=81.20 +/- 218.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=22000, episode_reward=132.00 +/- 102.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=23000, episode_reward=148.80 +/- 269.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=24000, episode_reward=-63.60 +/- 175.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=25000, episode_reward=-7.60 +/- 268.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=26000, episode_reward=-83.60 +/- 79.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=27000, episode_reward=85.60 +/- 311.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=28000, episode_reward=-49.20 +/- 184.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=29000, episode_reward=-1225.60 +/- 221.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=30000, episode_reward=-1066.80 +/- 281.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=31000, episode_reward=-298.00 +/- 184.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=32000, episode_reward=-495.00 +/- 255.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=33000, episode_reward=-326.60 +/- 311.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=34000, episode_reward=-371.60 +/- 116.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=35000, episode_reward=-432.00 +/- 180.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=36000, episode_reward=-289.80 +/- 54.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=37000, episode_reward=-688.60 +/- 386.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=38000, episode_reward=-740.40 +/- 389.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=39000, episode_reward=-181.60 +/- 215.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=40000, episode_reward=-266.80 +/- 355.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=41000, episode_reward=-371.40 +/- 257.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=42000, episode_reward=-358.00 +/- 48.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=43000, episode_reward=-459.60 +/- 310.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=44000, episode_reward=-287.40 +/- 156.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=45000, episode_reward=-342.00 +/- 232.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=46000, episode_reward=-581.40 +/- 216.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=47000, episode_reward=-303.20 +/- 146.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=48000, episode_reward=-3269.20 +/- 162.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=49000, episode_reward=-3067.20 +/- 559.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=50000, episode_reward=-155.60 +/- 233.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=51000, episode_reward=-169.20 +/- 64.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=52000, episode_reward=-151.80 +/- 150.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=53000, episode_reward=-236.40 +/- 350.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=54000, episode_reward=136.60 +/- 190.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=55000, episode_reward=233.40 +/- 218.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=56000, episode_reward=193.60 +/- 190.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=57000, episode_reward=245.00 +/- 123.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=58000, episode_reward=70.40 +/- 129.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=59000, episode_reward=290.60 +/- 68.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=60000, episode_reward=422.00 +/- 200.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=61000, episode_reward=259.40 +/- 181.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=62000, episode_reward=332.40 +/- 158.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=63000, episode_reward=443.20 +/- 185.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=64000, episode_reward=309.20 +/- 194.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=65000, episode_reward=356.80 +/- 136.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=66000, episode_reward=292.40 +/- 236.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=67000, episode_reward=146.20 +/- 173.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=68000, episode_reward=298.60 +/- 348.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=69000, episode_reward=346.00 +/- 245.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=70000, episode_reward=320.20 +/- 255.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=71000, episode_reward=337.00 +/- 186.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=72000, episode_reward=312.80 +/- 205.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=73000, episode_reward=312.00 +/- 236.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=74000, episode_reward=339.20 +/- 220.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=75000, episode_reward=344.40 +/- 177.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=76000, episode_reward=309.00 +/- 256.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=77000, episode_reward=435.80 +/- 165.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=78000, episode_reward=296.80 +/- 294.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=79000, episode_reward=360.60 +/- 160.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=80000, episode_reward=308.60 +/- 144.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=81000, episode_reward=264.60 +/- 182.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=82000, episode_reward=367.80 +/- 86.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=83000, episode_reward=338.20 +/- 188.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=84000, episode_reward=348.40 +/- 218.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=85000, episode_reward=415.00 +/- 149.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=86000, episode_reward=395.20 +/- 191.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=87000, episode_reward=457.00 +/- 92.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=88000, episode_reward=470.60 +/- 163.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=89000, episode_reward=287.80 +/- 216.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=90000, episode_reward=350.80 +/- 259.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=91000, episode_reward=397.00 +/- 220.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=92000, episode_reward=437.60 +/- 155.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=93000, episode_reward=366.40 +/- 202.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=94000, episode_reward=377.00 +/- 196.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=95000, episode_reward=319.20 +/- 250.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=96000, episode_reward=390.20 +/- 136.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=97000, episode_reward=332.40 +/- 217.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=98000, episode_reward=464.60 +/- 167.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=99000, episode_reward=303.00 +/- 327.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=100000, episode_reward=233.00 +/- 249.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=101000, episode_reward=296.40 +/- 179.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=102000, episode_reward=385.80 +/- 166.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=103000, episode_reward=500.60 +/- 182.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=104000, episode_reward=222.20 +/- 185.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=105000, episode_reward=462.00 +/- 149.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=106000, episode_reward=438.00 +/- 167.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=107000, episode_reward=327.40 +/- 132.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=108000, episode_reward=274.80 +/- 176.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=109000, episode_reward=85.20 +/- 242.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=110000, episode_reward=461.80 +/- 276.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=111000, episode_reward=382.00 +/- 157.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=112000, episode_reward=352.60 +/- 86.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=113000, episode_reward=471.80 +/- 62.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=114000, episode_reward=263.60 +/- 105.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=115000, episode_reward=364.00 +/- 168.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=116000, episode_reward=249.60 +/- 207.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=117000, episode_reward=374.00 +/- 169.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=118000, episode_reward=394.40 +/- 282.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=119000, episode_reward=279.60 +/- 152.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=120000, episode_reward=335.20 +/- 250.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=121000, episode_reward=426.40 +/- 148.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=122000, episode_reward=117.60 +/- 50.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=123000, episode_reward=497.80 +/- 166.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=124000, episode_reward=304.20 +/- 162.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=125000, episode_reward=198.20 +/- 226.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=126000, episode_reward=282.60 +/- 67.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=127000, episode_reward=292.00 +/- 183.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=128000, episode_reward=400.60 +/- 217.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=129000, episode_reward=214.60 +/- 243.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=130000, episode_reward=402.80 +/- 101.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=131000, episode_reward=566.00 +/- 90.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=132000, episode_reward=271.20 +/- 206.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=133000, episode_reward=208.80 +/- 227.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=134000, episode_reward=254.60 +/- 243.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=135000, episode_reward=294.80 +/- 168.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=136000, episode_reward=290.00 +/- 100.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=137000, episode_reward=139.00 +/- 110.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=138000, episode_reward=403.60 +/- 157.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=139000, episode_reward=166.00 +/- 163.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=140000, episode_reward=305.40 +/- 85.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=141000, episode_reward=359.60 +/- 114.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=142000, episode_reward=284.60 +/- 318.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=143000, episode_reward=192.20 +/- 181.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=144000, episode_reward=341.40 +/- 113.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=145000, episode_reward=487.60 +/- 106.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=146000, episode_reward=240.00 +/- 276.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=147000, episode_reward=273.40 +/- 80.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=148000, episode_reward=409.40 +/- 285.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=149000, episode_reward=405.20 +/- 199.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=150000, episode_reward=302.00 +/- 145.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=151000, episode_reward=348.40 +/- 211.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=152000, episode_reward=404.40 +/- 143.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=153000, episode_reward=246.20 +/- 190.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=154000, episode_reward=503.80 +/- 118.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=155000, episode_reward=411.20 +/- 115.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=156000, episode_reward=289.80 +/- 167.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=157000, episode_reward=340.00 +/- 108.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=158000, episode_reward=382.00 +/- 210.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=159000, episode_reward=270.80 +/- 242.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=160000, episode_reward=218.40 +/- 113.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=161000, episode_reward=334.40 +/- 162.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=162000, episode_reward=341.80 +/- 240.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=163000, episode_reward=369.80 +/- 135.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=164000, episode_reward=427.80 +/- 161.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=165000, episode_reward=368.20 +/- 140.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=166000, episode_reward=306.60 +/- 141.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=167000, episode_reward=211.60 +/- 356.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=168000, episode_reward=377.60 +/- 195.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=169000, episode_reward=221.40 +/- 244.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=170000, episode_reward=298.20 +/- 146.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=171000, episode_reward=260.40 +/- 146.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=172000, episode_reward=346.20 +/- 76.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=173000, episode_reward=306.80 +/- 208.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=174000, episode_reward=391.60 +/- 157.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=175000, episode_reward=361.60 +/- 76.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=176000, episode_reward=300.20 +/- 222.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=177000, episode_reward=389.00 +/- 174.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=178000, episode_reward=421.20 +/- 363.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=179000, episode_reward=440.00 +/- 155.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=180000, episode_reward=443.20 +/- 116.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=181000, episode_reward=477.40 +/- 174.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=182000, episode_reward=368.20 +/- 257.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=183000, episode_reward=532.80 +/- 257.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=184000, episode_reward=435.40 +/- 229.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=185000, episode_reward=402.00 +/- 77.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=186000, episode_reward=167.60 +/- 116.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=187000, episode_reward=335.20 +/- 257.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=188000, episode_reward=358.40 +/- 39.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=189000, episode_reward=244.00 +/- 329.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=190000, episode_reward=290.80 +/- 164.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=191000, episode_reward=363.60 +/- 124.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=192000, episode_reward=265.40 +/- 214.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=193000, episode_reward=328.40 +/- 173.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=194000, episode_reward=338.20 +/- 184.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=195000, episode_reward=445.40 +/- 206.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=196000, episode_reward=377.20 +/- 158.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=197000, episode_reward=245.40 +/- 265.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=198000, episode_reward=340.20 +/- 157.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=199000, episode_reward=393.00 +/- 203.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=200000, episode_reward=398.00 +/- 153.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=201000, episode_reward=160.40 +/- 154.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=202000, episode_reward=252.00 +/- 235.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=203000, episode_reward=386.20 +/- 89.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=204000, episode_reward=509.60 +/- 202.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=205000, episode_reward=404.60 +/- 40.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=206000, episode_reward=284.20 +/- 269.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=207000, episode_reward=282.60 +/- 163.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=208000, episode_reward=490.20 +/- 178.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=209000, episode_reward=384.20 +/- 169.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=210000, episode_reward=415.40 +/- 101.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=211000, episode_reward=337.40 +/- 124.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=212000, episode_reward=231.80 +/- 160.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=213000, episode_reward=444.60 +/- 221.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=214000, episode_reward=356.60 +/- 152.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=215000, episode_reward=305.40 +/- 180.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=216000, episode_reward=269.20 +/- 154.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=217000, episode_reward=296.40 +/- 110.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=218000, episode_reward=157.40 +/- 168.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=219000, episode_reward=429.00 +/- 234.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=220000, episode_reward=446.80 +/- 108.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=221000, episode_reward=429.20 +/- 221.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=222000, episode_reward=409.00 +/- 200.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=223000, episode_reward=252.00 +/- 131.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=224000, episode_reward=221.20 +/- 126.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=225000, episode_reward=364.40 +/- 248.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=226000, episode_reward=357.80 +/- 140.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=227000, episode_reward=325.80 +/- 156.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=228000, episode_reward=321.40 +/- 248.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=229000, episode_reward=318.40 +/- 89.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=230000, episode_reward=416.40 +/- 46.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=231000, episode_reward=508.80 +/- 134.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=232000, episode_reward=368.80 +/- 245.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=233000, episode_reward=291.40 +/- 252.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=234000, episode_reward=439.40 +/- 193.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=235000, episode_reward=295.20 +/- 248.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=236000, episode_reward=304.80 +/- 125.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=237000, episode_reward=220.40 +/- 201.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=238000, episode_reward=331.20 +/- 193.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=239000, episode_reward=254.20 +/- 180.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=240000, episode_reward=270.60 +/- 86.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=241000, episode_reward=316.20 +/- 122.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=242000, episode_reward=446.00 +/- 87.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=243000, episode_reward=324.80 +/- 184.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=244000, episode_reward=254.80 +/- 226.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=245000, episode_reward=354.80 +/- 126.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=246000, episode_reward=285.40 +/- 246.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=247000, episode_reward=421.60 +/- 303.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=248000, episode_reward=300.20 +/- 221.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=249000, episode_reward=125.40 +/- 273.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=250000, episode_reward=220.20 +/- 304.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=251000, episode_reward=340.80 +/- 96.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=252000, episode_reward=62.00 +/- 107.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=253000, episode_reward=93.00 +/- 185.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=254000, episode_reward=-5.80 +/- 178.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=255000, episode_reward=181.20 +/- 42.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=256000, episode_reward=69.80 +/- 185.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=257000, episode_reward=51.40 +/- 150.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=258000, episode_reward=63.00 +/- 183.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=259000, episode_reward=-76.60 +/- 59.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=260000, episode_reward=222.00 +/- 55.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=261000, episode_reward=26.60 +/- 165.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=262000, episode_reward=60.80 +/- 109.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=263000, episode_reward=47.20 +/- 155.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=264000, episode_reward=-29.40 +/- 168.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=265000, episode_reward=71.40 +/- 81.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=266000, episode_reward=82.20 +/- 104.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=267000, episode_reward=64.00 +/- 121.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=268000, episode_reward=-17.00 +/- 82.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=269000, episode_reward=36.60 +/- 163.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=270000, episode_reward=130.20 +/- 167.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=271000, episode_reward=60.20 +/- 141.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=272000, episode_reward=-13.40 +/- 191.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=273000, episode_reward=-8.60 +/- 155.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=274000, episode_reward=4.20 +/- 144.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=275000, episode_reward=-62.60 +/- 168.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=276000, episode_reward=-8.60 +/- 191.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=277000, episode_reward=201.00 +/- 46.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=278000, episode_reward=-1.60 +/- 127.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=279000, episode_reward=94.60 +/- 114.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=280000, episode_reward=190.20 +/- 93.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=281000, episode_reward=-21.00 +/- 140.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=282000, episode_reward=44.60 +/- 160.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=283000, episode_reward=-17.40 +/- 73.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=284000, episode_reward=114.00 +/- 130.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=285000, episode_reward=-41.20 +/- 193.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=286000, episode_reward=89.20 +/- 188.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=287000, episode_reward=42.00 +/- 142.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=288000, episode_reward=119.20 +/- 91.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=289000, episode_reward=-17.20 +/- 176.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=290000, episode_reward=89.40 +/- 118.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=291000, episode_reward=17.20 +/- 142.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=292000, episode_reward=33.60 +/- 194.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=293000, episode_reward=8.60 +/- 164.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=294000, episode_reward=14.20 +/- 83.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=295000, episode_reward=95.40 +/- 133.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=296000, episode_reward=-47.00 +/- 55.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=297000, episode_reward=122.40 +/- 111.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=298000, episode_reward=91.40 +/- 189.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=299000, episode_reward=-18.20 +/- 113.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=300000, episode_reward=50.20 +/- 264.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=301000, episode_reward=38.80 +/- 184.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=302000, episode_reward=-56.00 +/- 120.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=303000, episode_reward=73.20 +/- 107.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=304000, episode_reward=127.00 +/- 125.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=305000, episode_reward=179.60 +/- 82.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=306000, episode_reward=130.40 +/- 135.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=307000, episode_reward=55.20 +/- 171.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=308000, episode_reward=-84.60 +/- 136.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=309000, episode_reward=-92.80 +/- 208.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=310000, episode_reward=67.60 +/- 154.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=311000, episode_reward=167.40 +/- 64.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=312000, episode_reward=141.00 +/- 126.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=313000, episode_reward=46.80 +/- 114.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=314000, episode_reward=38.60 +/- 137.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=315000, episode_reward=120.20 +/- 140.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=316000, episode_reward=105.00 +/- 135.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=317000, episode_reward=222.80 +/- 46.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=318000, episode_reward=-46.80 +/- 99.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=319000, episode_reward=112.20 +/- 185.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=320000, episode_reward=-77.80 +/- 166.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=321000, episode_reward=-44.60 +/- 138.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=322000, episode_reward=46.40 +/- 148.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=323000, episode_reward=-19.00 +/- 120.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=324000, episode_reward=93.60 +/- 92.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=325000, episode_reward=67.20 +/- 158.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=326000, episode_reward=-61.40 +/- 220.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=327000, episode_reward=-79.40 +/- 212.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=328000, episode_reward=114.60 +/- 153.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=329000, episode_reward=-43.60 +/- 185.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=330000, episode_reward=30.00 +/- 168.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=331000, episode_reward=148.60 +/- 151.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=332000, episode_reward=55.20 +/- 117.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=333000, episode_reward=98.20 +/- 152.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=334000, episode_reward=63.40 +/- 139.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=335000, episode_reward=160.20 +/- 79.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=336000, episode_reward=58.40 +/- 169.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=337000, episode_reward=186.20 +/- 84.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=338000, episode_reward=69.60 +/- 128.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=339000, episode_reward=62.00 +/- 138.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=340000, episode_reward=-37.40 +/- 180.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=341000, episode_reward=-42.00 +/- 125.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=342000, episode_reward=66.80 +/- 156.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=343000, episode_reward=49.20 +/- 153.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=344000, episode_reward=-19.60 +/- 138.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=345000, episode_reward=147.60 +/- 130.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=346000, episode_reward=142.60 +/- 118.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=347000, episode_reward=-83.20 +/- 63.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=348000, episode_reward=17.80 +/- 96.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=349000, episode_reward=159.80 +/- 174.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=350000, episode_reward=123.40 +/- 141.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=351000, episode_reward=84.60 +/- 151.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=352000, episode_reward=57.20 +/- 180.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=353000, episode_reward=5.60 +/- 154.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=354000, episode_reward=73.20 +/- 134.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=355000, episode_reward=92.20 +/- 151.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=356000, episode_reward=95.00 +/- 115.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=357000, episode_reward=-111.20 +/- 135.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=358000, episode_reward=75.80 +/- 101.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=359000, episode_reward=61.60 +/- 127.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=360000, episode_reward=7.80 +/- 227.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=361000, episode_reward=-26.00 +/- 128.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=362000, episode_reward=61.20 +/- 155.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=363000, episode_reward=138.20 +/- 115.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=364000, episode_reward=22.20 +/- 171.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=365000, episode_reward=-47.00 +/- 111.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=366000, episode_reward=115.60 +/- 256.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=367000, episode_reward=79.00 +/- 129.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=368000, episode_reward=47.20 +/- 160.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=369000, episode_reward=148.00 +/- 10.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=370000, episode_reward=217.00 +/- 34.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=371000, episode_reward=199.20 +/- 76.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=372000, episode_reward=45.00 +/- 111.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=373000, episode_reward=25.20 +/- 174.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=374000, episode_reward=185.60 +/- 29.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=375000, episode_reward=159.60 +/- 146.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=376000, episode_reward=-35.40 +/- 170.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=377000, episode_reward=-44.00 +/- 136.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=378000, episode_reward=-14.20 +/- 208.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=379000, episode_reward=68.00 +/- 147.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=380000, episode_reward=104.20 +/- 179.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=381000, episode_reward=0.40 +/- 183.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=382000, episode_reward=47.40 +/- 177.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=383000, episode_reward=-43.60 +/- 128.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=384000, episode_reward=32.60 +/- 174.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=385000, episode_reward=45.60 +/- 171.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=386000, episode_reward=165.80 +/- 122.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=387000, episode_reward=14.80 +/- 128.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=388000, episode_reward=-30.60 +/- 182.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=389000, episode_reward=117.40 +/- 120.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=390000, episode_reward=64.00 +/- 163.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=391000, episode_reward=-147.40 +/- 37.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=392000, episode_reward=1.60 +/- 204.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=393000, episode_reward=198.40 +/- 228.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=394000, episode_reward=56.60 +/- 121.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=395000, episode_reward=-106.00 +/- 141.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=396000, episode_reward=135.60 +/- 28.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=397000, episode_reward=106.00 +/- 120.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=398000, episode_reward=28.60 +/- 127.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=399000, episode_reward=107.60 +/- 140.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=400000, episode_reward=148.60 +/- 142.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=401000, episode_reward=16.40 +/- 140.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=402000, episode_reward=12.80 +/- 171.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=403000, episode_reward=76.20 +/- 120.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=404000, episode_reward=151.80 +/- 102.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=405000, episode_reward=72.00 +/- 255.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=406000, episode_reward=-5.80 +/- 148.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=407000, episode_reward=124.80 +/- 65.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=408000, episode_reward=27.60 +/- 191.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=409000, episode_reward=-90.20 +/- 132.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=410000, episode_reward=60.20 +/- 221.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=411000, episode_reward=22.80 +/- 153.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=412000, episode_reward=93.60 +/- 155.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=413000, episode_reward=114.00 +/- 135.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=414000, episode_reward=7.00 +/- 146.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=415000, episode_reward=31.80 +/- 142.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=416000, episode_reward=4.00 +/- 142.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=417000, episode_reward=47.60 +/- 130.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=418000, episode_reward=46.40 +/- 167.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=419000, episode_reward=-35.60 +/- 152.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=420000, episode_reward=69.20 +/- 126.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=421000, episode_reward=6.00 +/- 119.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=422000, episode_reward=86.80 +/- 74.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=423000, episode_reward=39.60 +/- 156.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=424000, episode_reward=51.00 +/- 181.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=425000, episode_reward=120.20 +/- 44.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=426000, episode_reward=13.60 +/- 88.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=427000, episode_reward=-37.80 +/- 250.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=428000, episode_reward=42.40 +/- 217.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=429000, episode_reward=24.20 +/- 132.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=430000, episode_reward=29.20 +/- 126.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=431000, episode_reward=-0.60 +/- 140.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=432000, episode_reward=72.20 +/- 115.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=433000, episode_reward=120.80 +/- 134.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=434000, episode_reward=-57.80 +/- 156.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=435000, episode_reward=52.00 +/- 167.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=436000, episode_reward=153.60 +/- 123.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=437000, episode_reward=-128.60 +/- 109.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=438000, episode_reward=61.20 +/- 114.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=439000, episode_reward=-2.40 +/- 173.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=440000, episode_reward=33.40 +/- 145.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=441000, episode_reward=-68.60 +/- 191.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=442000, episode_reward=24.60 +/- 106.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=443000, episode_reward=-13.60 +/- 113.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=444000, episode_reward=-42.00 +/- 146.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=445000, episode_reward=160.60 +/- 39.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=446000, episode_reward=-45.40 +/- 196.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=447000, episode_reward=-126.80 +/- 157.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=448000, episode_reward=46.20 +/- 131.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=449000, episode_reward=-19.20 +/- 136.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=450000, episode_reward=7.60 +/- 129.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=451000, episode_reward=147.40 +/- 139.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=452000, episode_reward=-0.40 +/- 100.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=453000, episode_reward=38.20 +/- 183.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=454000, episode_reward=29.40 +/- 150.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=455000, episode_reward=0.20 +/- 159.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=456000, episode_reward=105.00 +/- 176.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=457000, episode_reward=119.40 +/- 141.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=458000, episode_reward=55.60 +/- 152.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=459000, episode_reward=209.00 +/- 72.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=460000, episode_reward=-38.20 +/- 144.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=461000, episode_reward=65.80 +/- 111.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=462000, episode_reward=95.40 +/- 55.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=463000, episode_reward=45.00 +/- 116.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=464000, episode_reward=51.80 +/- 137.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=465000, episode_reward=-49.20 +/- 200.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=466000, episode_reward=52.40 +/- 101.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=467000, episode_reward=6.20 +/- 194.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=468000, episode_reward=51.60 +/- 147.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=469000, episode_reward=118.40 +/- 57.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=470000, episode_reward=145.80 +/- 139.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=471000, episode_reward=8.20 +/- 132.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=472000, episode_reward=69.40 +/- 146.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=473000, episode_reward=-31.20 +/- 181.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=474000, episode_reward=19.80 +/- 121.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=475000, episode_reward=63.40 +/- 147.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=476000, episode_reward=29.20 +/- 151.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=477000, episode_reward=180.80 +/- 31.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=478000, episode_reward=115.80 +/- 54.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=479000, episode_reward=73.20 +/- 119.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=480000, episode_reward=-16.40 +/- 191.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=481000, episode_reward=69.00 +/- 181.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=482000, episode_reward=32.00 +/- 168.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=483000, episode_reward=11.60 +/- 140.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=484000, episode_reward=101.00 +/- 105.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=485000, episode_reward=120.20 +/- 131.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=486000, episode_reward=-14.60 +/- 191.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=487000, episode_reward=-11.00 +/- 85.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=488000, episode_reward=21.20 +/- 177.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=489000, episode_reward=120.40 +/- 110.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=490000, episode_reward=50.20 +/- 199.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=491000, episode_reward=-100.00 +/- 140.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=492000, episode_reward=77.60 +/- 131.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=493000, episode_reward=187.40 +/- 61.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=494000, episode_reward=-30.80 +/- 149.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=495000, episode_reward=-3.60 +/- 113.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=496000, episode_reward=166.80 +/- 54.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=497000, episode_reward=45.20 +/- 178.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=498000, episode_reward=141.80 +/- 119.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=499000, episode_reward=37.80 +/- 130.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=500000, episode_reward=38.60 +/- 124.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=501000, episode_reward=-1.80 +/- 136.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Gamma = 0.9, Learning Rate = 3e-05, Activation Function = ReLU, Neurons = 32, Process noise = 0.1, Measurement noise = 0.0\n",
      "Eval num_timesteps=1000, episode_reward=-7920.20 +/- 1699.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=-6960.00 +/- 1117.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3000, episode_reward=351.60 +/- 172.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4000, episode_reward=297.00 +/- 102.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=5000, episode_reward=246.20 +/- 214.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=6000, episode_reward=308.60 +/- 92.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=7000, episode_reward=190.20 +/- 207.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=8000, episode_reward=233.20 +/- 235.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=9000, episode_reward=103.20 +/- 394.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=10000, episode_reward=169.60 +/- 232.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=11000, episode_reward=262.00 +/- 111.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=12000, episode_reward=329.80 +/- 128.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=13000, episode_reward=271.00 +/- 160.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=14000, episode_reward=178.40 +/- 227.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=15000, episode_reward=14.60 +/- 485.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=16000, episode_reward=-181.00 +/- 175.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=17000, episode_reward=-99.40 +/- 591.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=18000, episode_reward=-137.80 +/- 406.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=19000, episode_reward=-842.00 +/- 1122.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=20000, episode_reward=302.20 +/- 151.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=21000, episode_reward=-288.40 +/- 500.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=22000, episode_reward=-363.80 +/- 314.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=23000, episode_reward=-198.00 +/- 157.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=24000, episode_reward=-498.00 +/- 665.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=25000, episode_reward=76.40 +/- 326.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=26000, episode_reward=-156.60 +/- 147.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=27000, episode_reward=70.40 +/- 170.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=28000, episode_reward=163.00 +/- 202.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=29000, episode_reward=34.40 +/- 330.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=30000, episode_reward=-54.40 +/- 538.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=31000, episode_reward=242.00 +/- 206.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=32000, episode_reward=52.20 +/- 274.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=33000, episode_reward=351.20 +/- 302.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=34000, episode_reward=109.80 +/- 103.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=35000, episode_reward=128.40 +/- 64.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=36000, episode_reward=19.60 +/- 228.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=37000, episode_reward=161.60 +/- 180.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=38000, episode_reward=350.80 +/- 69.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=39000, episode_reward=150.60 +/- 168.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=40000, episode_reward=427.40 +/- 125.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=41000, episode_reward=338.60 +/- 109.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=42000, episode_reward=401.40 +/- 128.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=43000, episode_reward=377.40 +/- 110.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=44000, episode_reward=104.40 +/- 279.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=45000, episode_reward=213.80 +/- 215.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=46000, episode_reward=354.40 +/- 119.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=47000, episode_reward=379.40 +/- 180.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=48000, episode_reward=362.80 +/- 135.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=49000, episode_reward=297.40 +/- 156.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=50000, episode_reward=235.00 +/- 156.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=51000, episode_reward=342.00 +/- 177.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=52000, episode_reward=308.60 +/- 154.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=53000, episode_reward=415.40 +/- 150.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=54000, episode_reward=388.80 +/- 42.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=55000, episode_reward=421.80 +/- 195.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=56000, episode_reward=114.20 +/- 171.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=57000, episode_reward=470.20 +/- 166.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=58000, episode_reward=383.80 +/- 152.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=59000, episode_reward=503.80 +/- 139.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=60000, episode_reward=335.00 +/- 95.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=61000, episode_reward=287.80 +/- 295.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=62000, episode_reward=382.40 +/- 182.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=63000, episode_reward=274.20 +/- 88.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=64000, episode_reward=404.40 +/- 139.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=65000, episode_reward=215.80 +/- 194.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=66000, episode_reward=331.60 +/- 143.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=67000, episode_reward=352.80 +/- 239.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=68000, episode_reward=429.00 +/- 126.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=69000, episode_reward=306.20 +/- 123.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=70000, episode_reward=325.20 +/- 190.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=71000, episode_reward=282.60 +/- 213.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=72000, episode_reward=267.40 +/- 134.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=73000, episode_reward=221.20 +/- 165.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=74000, episode_reward=261.80 +/- 186.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=75000, episode_reward=307.80 +/- 105.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=76000, episode_reward=332.80 +/- 129.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=77000, episode_reward=266.40 +/- 190.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=78000, episode_reward=61.80 +/- 226.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=79000, episode_reward=0.80 +/- 150.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=80000, episode_reward=16.00 +/- 146.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=81000, episode_reward=92.80 +/- 90.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=82000, episode_reward=285.60 +/- 217.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=83000, episode_reward=230.60 +/- 182.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=84000, episode_reward=194.60 +/- 179.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=85000, episode_reward=300.80 +/- 199.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=86000, episode_reward=353.40 +/- 203.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=87000, episode_reward=207.60 +/- 114.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=88000, episode_reward=320.20 +/- 101.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=89000, episode_reward=189.00 +/- 149.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=90000, episode_reward=271.20 +/- 274.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=91000, episode_reward=215.00 +/- 35.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=92000, episode_reward=408.60 +/- 185.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=93000, episode_reward=227.40 +/- 245.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=94000, episode_reward=229.00 +/- 92.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=95000, episode_reward=309.20 +/- 206.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=96000, episode_reward=259.80 +/- 190.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=97000, episode_reward=287.40 +/- 246.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=98000, episode_reward=320.40 +/- 292.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=99000, episode_reward=317.80 +/- 169.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=100000, episode_reward=214.00 +/- 166.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=101000, episode_reward=239.00 +/- 101.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=102000, episode_reward=332.60 +/- 97.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=103000, episode_reward=239.00 +/- 47.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=104000, episode_reward=461.00 +/- 167.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=105000, episode_reward=193.60 +/- 180.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=106000, episode_reward=329.60 +/- 132.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=107000, episode_reward=327.60 +/- 120.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=108000, episode_reward=247.00 +/- 160.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=109000, episode_reward=414.60 +/- 158.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=110000, episode_reward=291.20 +/- 207.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=111000, episode_reward=287.40 +/- 140.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=112000, episode_reward=193.40 +/- 51.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=113000, episode_reward=250.40 +/- 206.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=114000, episode_reward=308.40 +/- 166.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=115000, episode_reward=366.20 +/- 130.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=116000, episode_reward=254.20 +/- 134.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=117000, episode_reward=213.80 +/- 187.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=118000, episode_reward=174.40 +/- 231.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=119000, episode_reward=367.60 +/- 118.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=120000, episode_reward=247.40 +/- 144.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=121000, episode_reward=161.20 +/- 48.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=122000, episode_reward=272.60 +/- 186.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=123000, episode_reward=148.80 +/- 71.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=124000, episode_reward=250.40 +/- 110.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=125000, episode_reward=338.20 +/- 228.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=126000, episode_reward=311.40 +/- 287.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=127000, episode_reward=197.20 +/- 125.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=128000, episode_reward=153.20 +/- 283.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=129000, episode_reward=343.20 +/- 134.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=130000, episode_reward=444.60 +/- 202.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=131000, episode_reward=228.60 +/- 75.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=132000, episode_reward=399.00 +/- 136.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=133000, episode_reward=212.60 +/- 216.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=134000, episode_reward=292.60 +/- 108.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=135000, episode_reward=278.60 +/- 213.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=136000, episode_reward=326.00 +/- 102.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=137000, episode_reward=337.40 +/- 189.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=138000, episode_reward=278.60 +/- 263.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=139000, episode_reward=187.00 +/- 297.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=140000, episode_reward=331.60 +/- 109.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=141000, episode_reward=304.60 +/- 129.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=142000, episode_reward=417.40 +/- 155.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=143000, episode_reward=275.00 +/- 42.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=144000, episode_reward=400.80 +/- 185.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=145000, episode_reward=384.20 +/- 173.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=146000, episode_reward=435.40 +/- 134.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=147000, episode_reward=178.40 +/- 256.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=148000, episode_reward=229.40 +/- 186.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=149000, episode_reward=252.20 +/- 120.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=150000, episode_reward=266.80 +/- 263.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=151000, episode_reward=141.40 +/- 123.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=152000, episode_reward=306.60 +/- 256.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=153000, episode_reward=72.40 +/- 281.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=154000, episode_reward=290.60 +/- 172.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=155000, episode_reward=165.40 +/- 185.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=156000, episode_reward=272.20 +/- 190.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=157000, episode_reward=397.20 +/- 65.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=158000, episode_reward=468.80 +/- 126.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=159000, episode_reward=405.20 +/- 86.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=160000, episode_reward=453.60 +/- 134.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=161000, episode_reward=319.20 +/- 256.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=162000, episode_reward=455.40 +/- 201.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=163000, episode_reward=256.40 +/- 17.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=164000, episode_reward=277.80 +/- 194.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=165000, episode_reward=460.80 +/- 140.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=166000, episode_reward=388.20 +/- 231.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=167000, episode_reward=353.00 +/- 264.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=168000, episode_reward=412.00 +/- 137.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=169000, episode_reward=376.40 +/- 195.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=170000, episode_reward=496.00 +/- 132.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=171000, episode_reward=240.80 +/- 289.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=172000, episode_reward=412.80 +/- 153.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=173000, episode_reward=423.60 +/- 216.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=174000, episode_reward=350.40 +/- 161.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=175000, episode_reward=320.40 +/- 126.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=176000, episode_reward=249.20 +/- 116.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=177000, episode_reward=256.60 +/- 130.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=178000, episode_reward=383.60 +/- 179.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=179000, episode_reward=273.80 +/- 145.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=180000, episode_reward=208.40 +/- 81.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=181000, episode_reward=131.60 +/- 255.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=182000, episode_reward=237.00 +/- 209.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=183000, episode_reward=135.80 +/- 109.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=184000, episode_reward=207.20 +/- 212.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=185000, episode_reward=219.80 +/- 191.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=186000, episode_reward=136.80 +/- 77.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=187000, episode_reward=178.40 +/- 172.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=188000, episode_reward=75.40 +/- 245.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=189000, episode_reward=373.20 +/- 140.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=190000, episode_reward=324.40 +/- 110.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=191000, episode_reward=343.00 +/- 99.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=192000, episode_reward=169.40 +/- 74.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=193000, episode_reward=323.40 +/- 125.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=194000, episode_reward=323.20 +/- 230.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=195000, episode_reward=236.60 +/- 150.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=196000, episode_reward=221.00 +/- 118.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=197000, episode_reward=306.00 +/- 145.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=198000, episode_reward=495.40 +/- 46.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=199000, episode_reward=386.00 +/- 257.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=200000, episode_reward=252.20 +/- 103.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=201000, episode_reward=274.00 +/- 174.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=202000, episode_reward=409.80 +/- 143.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=203000, episode_reward=471.00 +/- 159.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=204000, episode_reward=383.40 +/- 95.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=205000, episode_reward=299.00 +/- 205.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=206000, episode_reward=470.40 +/- 112.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=207000, episode_reward=291.80 +/- 131.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=208000, episode_reward=377.00 +/- 158.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=209000, episode_reward=250.20 +/- 157.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=210000, episode_reward=254.00 +/- 176.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=211000, episode_reward=434.40 +/- 118.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=212000, episode_reward=215.40 +/- 174.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=213000, episode_reward=392.00 +/- 158.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=214000, episode_reward=347.00 +/- 125.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=215000, episode_reward=273.00 +/- 88.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=216000, episode_reward=275.60 +/- 270.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=217000, episode_reward=326.60 +/- 152.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=218000, episode_reward=330.60 +/- 150.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=219000, episode_reward=348.80 +/- 120.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=220000, episode_reward=182.20 +/- 137.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=221000, episode_reward=286.80 +/- 170.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=222000, episode_reward=300.40 +/- 109.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=223000, episode_reward=348.40 +/- 91.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=224000, episode_reward=315.20 +/- 144.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=225000, episode_reward=286.60 +/- 165.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=226000, episode_reward=336.20 +/- 172.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=227000, episode_reward=280.20 +/- 151.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=228000, episode_reward=244.40 +/- 187.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=229000, episode_reward=322.20 +/- 203.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=230000, episode_reward=75.60 +/- 114.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=231000, episode_reward=242.80 +/- 197.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=232000, episode_reward=168.60 +/- 185.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=233000, episode_reward=231.40 +/- 176.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=234000, episode_reward=376.60 +/- 173.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=235000, episode_reward=450.60 +/- 126.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=236000, episode_reward=344.80 +/- 129.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=237000, episode_reward=234.80 +/- 125.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=238000, episode_reward=299.20 +/- 135.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=239000, episode_reward=258.40 +/- 292.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=240000, episode_reward=297.60 +/- 126.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=241000, episode_reward=275.80 +/- 268.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=242000, episode_reward=306.60 +/- 120.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=243000, episode_reward=378.00 +/- 142.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=244000, episode_reward=452.00 +/- 106.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=245000, episode_reward=266.60 +/- 140.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=246000, episode_reward=400.80 +/- 164.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=247000, episode_reward=376.00 +/- 218.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=248000, episode_reward=415.20 +/- 161.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=249000, episode_reward=352.60 +/- 107.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=250000, episode_reward=298.60 +/- 177.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=251000, episode_reward=291.60 +/- 267.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=252000, episode_reward=329.00 +/- 148.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=253000, episode_reward=277.00 +/- 208.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=254000, episode_reward=466.60 +/- 28.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=255000, episode_reward=257.80 +/- 167.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=256000, episode_reward=352.40 +/- 241.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=257000, episode_reward=229.80 +/- 117.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=258000, episode_reward=280.00 +/- 142.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=259000, episode_reward=424.60 +/- 140.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=260000, episode_reward=335.40 +/- 188.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=261000, episode_reward=283.60 +/- 127.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=262000, episode_reward=380.20 +/- 169.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=263000, episode_reward=270.20 +/- 112.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=264000, episode_reward=254.00 +/- 216.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=265000, episode_reward=177.80 +/- 82.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=266000, episode_reward=256.60 +/- 183.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=267000, episode_reward=279.60 +/- 167.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=268000, episode_reward=318.20 +/- 129.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=269000, episode_reward=268.60 +/- 151.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=270000, episode_reward=221.80 +/- 110.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=271000, episode_reward=297.00 +/- 258.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=272000, episode_reward=234.20 +/- 143.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=273000, episode_reward=287.80 +/- 211.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=274000, episode_reward=403.20 +/- 136.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=275000, episode_reward=189.20 +/- 95.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=276000, episode_reward=389.00 +/- 216.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=277000, episode_reward=243.40 +/- 118.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=278000, episode_reward=393.40 +/- 166.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=279000, episode_reward=340.20 +/- 130.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=280000, episode_reward=250.40 +/- 55.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=281000, episode_reward=345.40 +/- 132.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=282000, episode_reward=324.40 +/- 130.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=283000, episode_reward=251.60 +/- 61.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=284000, episode_reward=247.40 +/- 188.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=285000, episode_reward=235.60 +/- 94.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=286000, episode_reward=345.80 +/- 128.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=287000, episode_reward=446.40 +/- 157.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=288000, episode_reward=264.80 +/- 157.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=289000, episode_reward=362.00 +/- 210.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=290000, episode_reward=474.00 +/- 156.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=291000, episode_reward=428.00 +/- 168.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=292000, episode_reward=428.60 +/- 94.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=293000, episode_reward=330.60 +/- 138.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=294000, episode_reward=287.80 +/- 232.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=295000, episode_reward=244.80 +/- 183.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=296000, episode_reward=277.80 +/- 149.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=297000, episode_reward=321.20 +/- 23.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=298000, episode_reward=320.20 +/- 46.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=299000, episode_reward=377.00 +/- 133.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=300000, episode_reward=190.80 +/- 51.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=301000, episode_reward=250.00 +/- 39.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=302000, episode_reward=292.40 +/- 73.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=303000, episode_reward=326.20 +/- 126.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=304000, episode_reward=182.40 +/- 297.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=305000, episode_reward=297.60 +/- 227.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=306000, episode_reward=298.40 +/- 176.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=307000, episode_reward=264.60 +/- 153.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=308000, episode_reward=315.60 +/- 54.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=309000, episode_reward=424.60 +/- 99.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=310000, episode_reward=512.40 +/- 177.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=311000, episode_reward=346.00 +/- 212.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=312000, episode_reward=393.60 +/- 242.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=313000, episode_reward=336.80 +/- 122.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=314000, episode_reward=369.00 +/- 249.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=315000, episode_reward=333.40 +/- 90.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=316000, episode_reward=417.20 +/- 227.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=317000, episode_reward=336.80 +/- 280.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=318000, episode_reward=441.00 +/- 163.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=319000, episode_reward=282.00 +/- 105.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=320000, episode_reward=184.20 +/- 169.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=321000, episode_reward=304.80 +/- 129.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=322000, episode_reward=347.60 +/- 74.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=323000, episode_reward=243.60 +/- 314.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=324000, episode_reward=317.40 +/- 168.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=325000, episode_reward=401.00 +/- 44.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=326000, episode_reward=370.60 +/- 190.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=327000, episode_reward=310.20 +/- 237.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=328000, episode_reward=297.00 +/- 168.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=329000, episode_reward=444.60 +/- 88.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=330000, episode_reward=374.20 +/- 128.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=331000, episode_reward=377.20 +/- 124.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=332000, episode_reward=280.60 +/- 90.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=333000, episode_reward=299.60 +/- 153.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=334000, episode_reward=379.00 +/- 148.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=335000, episode_reward=79.20 +/- 173.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=336000, episode_reward=175.60 +/- 106.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=337000, episode_reward=263.00 +/- 55.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=338000, episode_reward=299.60 +/- 96.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=339000, episode_reward=254.00 +/- 128.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=340000, episode_reward=401.80 +/- 161.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=341000, episode_reward=392.60 +/- 88.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=342000, episode_reward=493.40 +/- 223.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=343000, episode_reward=382.40 +/- 150.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=344000, episode_reward=394.60 +/- 95.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=345000, episode_reward=307.40 +/- 196.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=346000, episode_reward=406.00 +/- 280.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=347000, episode_reward=381.00 +/- 60.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=348000, episode_reward=320.80 +/- 132.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=349000, episode_reward=307.40 +/- 151.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=350000, episode_reward=184.00 +/- 106.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=351000, episode_reward=266.80 +/- 227.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=352000, episode_reward=316.00 +/- 86.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=353000, episode_reward=401.40 +/- 164.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=354000, episode_reward=430.00 +/- 72.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=355000, episode_reward=327.20 +/- 78.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=356000, episode_reward=326.00 +/- 110.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=357000, episode_reward=426.40 +/- 136.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=358000, episode_reward=158.20 +/- 156.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=359000, episode_reward=385.20 +/- 174.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=360000, episode_reward=308.80 +/- 171.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=361000, episode_reward=295.00 +/- 84.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=362000, episode_reward=279.40 +/- 217.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=363000, episode_reward=366.40 +/- 198.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=364000, episode_reward=228.40 +/- 54.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=365000, episode_reward=264.00 +/- 102.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=366000, episode_reward=372.00 +/- 155.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=367000, episode_reward=299.60 +/- 106.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=368000, episode_reward=305.40 +/- 186.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=369000, episode_reward=365.60 +/- 180.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=370000, episode_reward=345.80 +/- 219.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=371000, episode_reward=241.60 +/- 142.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=372000, episode_reward=333.40 +/- 162.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=373000, episode_reward=280.20 +/- 249.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=374000, episode_reward=451.80 +/- 186.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=375000, episode_reward=268.20 +/- 70.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=376000, episode_reward=476.40 +/- 201.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=377000, episode_reward=383.20 +/- 159.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=378000, episode_reward=355.40 +/- 107.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=379000, episode_reward=303.80 +/- 69.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=380000, episode_reward=355.80 +/- 85.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=381000, episode_reward=419.00 +/- 126.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=382000, episode_reward=276.20 +/- 140.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=383000, episode_reward=378.80 +/- 220.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=384000, episode_reward=236.20 +/- 220.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=385000, episode_reward=416.60 +/- 99.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=386000, episode_reward=420.20 +/- 220.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=387000, episode_reward=402.40 +/- 146.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=388000, episode_reward=303.60 +/- 163.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=389000, episode_reward=382.00 +/- 142.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=390000, episode_reward=270.80 +/- 68.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=391000, episode_reward=447.40 +/- 66.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=392000, episode_reward=117.00 +/- 193.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=393000, episode_reward=344.40 +/- 86.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=394000, episode_reward=238.00 +/- 250.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=395000, episode_reward=376.20 +/- 98.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=396000, episode_reward=387.60 +/- 177.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=397000, episode_reward=427.60 +/- 259.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=398000, episode_reward=449.40 +/- 171.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=399000, episode_reward=416.80 +/- 180.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=400000, episode_reward=307.40 +/- 98.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=401000, episode_reward=236.80 +/- 156.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=402000, episode_reward=296.40 +/- 241.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=403000, episode_reward=224.40 +/- 179.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=404000, episode_reward=295.20 +/- 225.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=405000, episode_reward=342.60 +/- 187.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=406000, episode_reward=469.40 +/- 188.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=407000, episode_reward=402.80 +/- 97.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=408000, episode_reward=262.60 +/- 192.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=409000, episode_reward=441.60 +/- 128.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=410000, episode_reward=225.40 +/- 137.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=411000, episode_reward=239.20 +/- 174.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=412000, episode_reward=255.00 +/- 153.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=413000, episode_reward=428.60 +/- 193.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=414000, episode_reward=167.00 +/- 163.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=415000, episode_reward=341.60 +/- 119.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=416000, episode_reward=354.40 +/- 193.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=417000, episode_reward=464.80 +/- 136.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=418000, episode_reward=348.80 +/- 257.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=419000, episode_reward=444.80 +/- 140.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=420000, episode_reward=306.60 +/- 41.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=421000, episode_reward=373.00 +/- 109.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=422000, episode_reward=273.00 +/- 147.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=423000, episode_reward=149.00 +/- 163.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=424000, episode_reward=391.40 +/- 106.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=425000, episode_reward=391.40 +/- 117.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=426000, episode_reward=242.20 +/- 91.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=427000, episode_reward=301.00 +/- 187.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=428000, episode_reward=305.20 +/- 49.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=429000, episode_reward=411.60 +/- 127.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=430000, episode_reward=450.20 +/- 159.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=431000, episode_reward=289.60 +/- 41.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=432000, episode_reward=403.60 +/- 64.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=433000, episode_reward=466.60 +/- 234.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=434000, episode_reward=330.20 +/- 117.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=435000, episode_reward=222.80 +/- 158.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=436000, episode_reward=206.00 +/- 114.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=437000, episode_reward=271.40 +/- 241.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=438000, episode_reward=122.40 +/- 215.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=439000, episode_reward=334.20 +/- 143.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=440000, episode_reward=353.20 +/- 216.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=441000, episode_reward=362.20 +/- 163.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=442000, episode_reward=370.40 +/- 164.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=443000, episode_reward=343.00 +/- 156.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=444000, episode_reward=489.20 +/- 126.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=445000, episode_reward=317.20 +/- 165.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=446000, episode_reward=285.40 +/- 88.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=447000, episode_reward=388.80 +/- 244.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=448000, episode_reward=345.20 +/- 69.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=449000, episode_reward=443.00 +/- 142.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=450000, episode_reward=307.20 +/- 178.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=451000, episode_reward=371.00 +/- 282.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=452000, episode_reward=492.00 +/- 212.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=453000, episode_reward=242.20 +/- 177.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=454000, episode_reward=305.20 +/- 90.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=455000, episode_reward=325.80 +/- 95.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=456000, episode_reward=359.60 +/- 106.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=457000, episode_reward=303.00 +/- 281.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=458000, episode_reward=280.40 +/- 148.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=459000, episode_reward=326.40 +/- 197.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=460000, episode_reward=298.40 +/- 26.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=461000, episode_reward=277.20 +/- 138.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=462000, episode_reward=336.60 +/- 134.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=463000, episode_reward=440.20 +/- 125.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=464000, episode_reward=313.40 +/- 182.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=465000, episode_reward=520.40 +/- 136.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=466000, episode_reward=257.20 +/- 257.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=467000, episode_reward=350.60 +/- 252.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=468000, episode_reward=296.60 +/- 120.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=469000, episode_reward=202.80 +/- 57.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=470000, episode_reward=361.00 +/- 128.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=471000, episode_reward=344.20 +/- 237.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=472000, episode_reward=392.40 +/- 144.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=473000, episode_reward=270.40 +/- 95.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=474000, episode_reward=401.80 +/- 88.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=475000, episode_reward=474.20 +/- 148.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=476000, episode_reward=342.40 +/- 88.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=477000, episode_reward=343.40 +/- 109.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=478000, episode_reward=447.20 +/- 84.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=479000, episode_reward=289.40 +/- 162.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=480000, episode_reward=196.80 +/- 206.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=481000, episode_reward=204.00 +/- 254.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=482000, episode_reward=225.80 +/- 175.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=483000, episode_reward=318.60 +/- 189.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=484000, episode_reward=356.60 +/- 110.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=485000, episode_reward=367.00 +/- 113.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=486000, episode_reward=442.00 +/- 207.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=487000, episode_reward=225.80 +/- 96.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=488000, episode_reward=312.00 +/- 69.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=489000, episode_reward=284.20 +/- 103.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=490000, episode_reward=242.80 +/- 89.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=491000, episode_reward=193.00 +/- 356.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=492000, episode_reward=388.60 +/- 206.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=493000, episode_reward=407.60 +/- 59.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=494000, episode_reward=318.20 +/- 237.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=495000, episode_reward=331.40 +/- 177.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=496000, episode_reward=237.20 +/- 133.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=497000, episode_reward=328.00 +/- 95.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=498000, episode_reward=268.80 +/- 165.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=499000, episode_reward=377.00 +/- 279.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=500000, episode_reward=203.20 +/- 83.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=501000, episode_reward=214.40 +/- 123.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Gamma = 0.9, Learning Rate = 3e-05, Activation Function = ReLU, Neurons = 64, Process noise = 0.1, Measurement noise = 0.0\n",
      "Eval num_timesteps=1000, episode_reward=-1459.00 +/- 129.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=-1427.00 +/- 70.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3000, episode_reward=-6876.00 +/- 2063.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=4000, episode_reward=-8097.40 +/- 1282.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=5000, episode_reward=-172.20 +/- 206.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=6000, episode_reward=-66.60 +/- 478.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=7000, episode_reward=-238.60 +/- 299.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=8000, episode_reward=-22.60 +/- 363.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=9000, episode_reward=360.00 +/- 175.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=10000, episode_reward=213.60 +/- 69.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=11000, episode_reward=434.40 +/- 207.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=12000, episode_reward=253.60 +/- 206.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=13000, episode_reward=326.80 +/- 119.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=14000, episode_reward=366.80 +/- 121.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=15000, episode_reward=468.20 +/- 220.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=16000, episode_reward=380.60 +/- 104.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=17000, episode_reward=196.00 +/- 203.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=18000, episode_reward=299.80 +/- 134.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=19000, episode_reward=357.00 +/- 43.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=20000, episode_reward=310.80 +/- 205.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=21000, episode_reward=338.20 +/- 128.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=22000, episode_reward=534.80 +/- 112.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=23000, episode_reward=198.60 +/- 205.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=24000, episode_reward=400.00 +/- 238.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=25000, episode_reward=261.60 +/- 245.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=26000, episode_reward=209.80 +/- 122.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=27000, episode_reward=419.60 +/- 193.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=28000, episode_reward=186.60 +/- 242.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=29000, episode_reward=308.80 +/- 260.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=30000, episode_reward=446.40 +/- 122.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=31000, episode_reward=389.60 +/- 318.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=32000, episode_reward=433.60 +/- 137.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=33000, episode_reward=388.80 +/- 146.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=34000, episode_reward=352.80 +/- 109.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=35000, episode_reward=414.60 +/- 181.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=36000, episode_reward=280.20 +/- 102.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=37000, episode_reward=133.60 +/- 162.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=38000, episode_reward=328.60 +/- 176.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=39000, episode_reward=433.40 +/- 154.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=40000, episode_reward=148.20 +/- 221.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=41000, episode_reward=252.20 +/- 141.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=42000, episode_reward=399.20 +/- 171.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=43000, episode_reward=246.40 +/- 87.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=44000, episode_reward=290.40 +/- 232.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=45000, episode_reward=274.60 +/- 274.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=46000, episode_reward=497.40 +/- 117.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=47000, episode_reward=361.20 +/- 167.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=48000, episode_reward=467.60 +/- 177.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=49000, episode_reward=327.40 +/- 205.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=50000, episode_reward=399.20 +/- 152.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=51000, episode_reward=230.00 +/- 81.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=52000, episode_reward=257.80 +/- 248.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=53000, episode_reward=294.40 +/- 227.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=54000, episode_reward=357.60 +/- 115.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=55000, episode_reward=340.80 +/- 107.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=56000, episode_reward=324.20 +/- 146.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=57000, episode_reward=439.60 +/- 69.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=58000, episode_reward=284.40 +/- 172.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=59000, episode_reward=448.00 +/- 130.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=60000, episode_reward=428.20 +/- 152.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=61000, episode_reward=407.60 +/- 144.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=62000, episode_reward=227.00 +/- 211.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=63000, episode_reward=341.60 +/- 252.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=64000, episode_reward=405.60 +/- 66.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=65000, episode_reward=480.20 +/- 116.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=66000, episode_reward=254.60 +/- 169.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=67000, episode_reward=386.40 +/- 100.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=68000, episode_reward=480.00 +/- 80.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=69000, episode_reward=423.60 +/- 207.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=70000, episode_reward=296.20 +/- 327.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=71000, episode_reward=333.40 +/- 184.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=72000, episode_reward=183.40 +/- 140.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=73000, episode_reward=277.80 +/- 86.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=74000, episode_reward=262.00 +/- 186.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=75000, episode_reward=268.00 +/- 165.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=76000, episode_reward=307.00 +/- 78.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=77000, episode_reward=261.80 +/- 135.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=78000, episode_reward=384.00 +/- 306.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=79000, episode_reward=394.20 +/- 165.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=80000, episode_reward=177.60 +/- 191.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=81000, episode_reward=294.40 +/- 138.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=82000, episode_reward=311.40 +/- 261.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=83000, episode_reward=343.60 +/- 125.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=84000, episode_reward=447.00 +/- 90.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=85000, episode_reward=297.80 +/- 155.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=86000, episode_reward=326.00 +/- 158.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=87000, episode_reward=456.60 +/- 264.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=88000, episode_reward=423.00 +/- 66.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=89000, episode_reward=286.60 +/- 249.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=90000, episode_reward=275.80 +/- 78.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=91000, episode_reward=364.00 +/- 141.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=92000, episode_reward=355.80 +/- 254.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=93000, episode_reward=329.80 +/- 60.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=94000, episode_reward=231.40 +/- 193.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=95000, episode_reward=348.20 +/- 299.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=96000, episode_reward=274.20 +/- 259.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=97000, episode_reward=290.80 +/- 191.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=98000, episode_reward=312.60 +/- 142.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=99000, episode_reward=328.60 +/- 197.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=100000, episode_reward=398.20 +/- 91.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=101000, episode_reward=487.20 +/- 156.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=102000, episode_reward=343.00 +/- 191.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=103000, episode_reward=389.80 +/- 43.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=104000, episode_reward=396.80 +/- 296.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=105000, episode_reward=206.60 +/- 166.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=106000, episode_reward=413.00 +/- 152.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=107000, episode_reward=471.20 +/- 176.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=108000, episode_reward=289.60 +/- 268.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=109000, episode_reward=319.00 +/- 153.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=110000, episode_reward=448.60 +/- 126.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=111000, episode_reward=476.40 +/- 178.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=112000, episode_reward=199.00 +/- 228.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=113000, episode_reward=173.00 +/- 117.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=114000, episode_reward=217.20 +/- 170.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=115000, episode_reward=370.80 +/- 119.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=116000, episode_reward=342.20 +/- 255.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=117000, episode_reward=205.20 +/- 368.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=118000, episode_reward=273.20 +/- 201.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=119000, episode_reward=197.80 +/- 124.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=120000, episode_reward=318.40 +/- 71.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=121000, episode_reward=287.40 +/- 114.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=122000, episode_reward=399.80 +/- 167.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=123000, episode_reward=316.60 +/- 73.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=124000, episode_reward=383.20 +/- 177.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=125000, episode_reward=398.80 +/- 113.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=126000, episode_reward=482.60 +/- 150.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=127000, episode_reward=414.00 +/- 103.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=128000, episode_reward=365.40 +/- 101.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=129000, episode_reward=352.60 +/- 135.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=130000, episode_reward=407.40 +/- 186.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=131000, episode_reward=290.80 +/- 190.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=132000, episode_reward=182.40 +/- 220.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=133000, episode_reward=272.80 +/- 142.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=134000, episode_reward=387.40 +/- 143.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=135000, episode_reward=473.20 +/- 172.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=136000, episode_reward=157.40 +/- 124.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=137000, episode_reward=305.80 +/- 137.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=138000, episode_reward=489.40 +/- 190.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=139000, episode_reward=393.00 +/- 123.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=140000, episode_reward=346.40 +/- 283.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=141000, episode_reward=273.00 +/- 166.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=142000, episode_reward=399.20 +/- 171.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=143000, episode_reward=443.00 +/- 60.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=144000, episode_reward=222.80 +/- 110.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=145000, episode_reward=359.20 +/- 102.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=146000, episode_reward=208.20 +/- 229.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=147000, episode_reward=253.20 +/- 183.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=148000, episode_reward=380.20 +/- 168.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=149000, episode_reward=318.40 +/- 100.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=150000, episode_reward=376.80 +/- 97.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=151000, episode_reward=271.40 +/- 271.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=152000, episode_reward=365.40 +/- 122.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=153000, episode_reward=306.20 +/- 178.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=154000, episode_reward=277.20 +/- 106.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=155000, episode_reward=353.80 +/- 193.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=156000, episode_reward=231.00 +/- 174.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=157000, episode_reward=349.40 +/- 185.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=158000, episode_reward=218.00 +/- 116.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=159000, episode_reward=322.60 +/- 81.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=160000, episode_reward=377.20 +/- 150.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=161000, episode_reward=187.80 +/- 197.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=162000, episode_reward=190.00 +/- 285.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=163000, episode_reward=216.80 +/- 301.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=164000, episode_reward=394.20 +/- 211.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=165000, episode_reward=315.40 +/- 156.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=166000, episode_reward=401.00 +/- 183.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=167000, episode_reward=262.40 +/- 205.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=168000, episode_reward=364.00 +/- 163.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=169000, episode_reward=396.00 +/- 184.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=170000, episode_reward=303.20 +/- 123.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=171000, episode_reward=405.00 +/- 59.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=172000, episode_reward=268.40 +/- 149.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=173000, episode_reward=274.40 +/- 175.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=174000, episode_reward=327.00 +/- 256.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=175000, episode_reward=409.00 +/- 141.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=176000, episode_reward=291.20 +/- 240.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=177000, episode_reward=327.80 +/- 173.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=178000, episode_reward=410.60 +/- 194.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=179000, episode_reward=281.20 +/- 228.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=180000, episode_reward=224.60 +/- 289.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=181000, episode_reward=408.20 +/- 148.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=182000, episode_reward=215.60 +/- 275.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=183000, episode_reward=184.80 +/- 210.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=184000, episode_reward=204.80 +/- 206.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=185000, episode_reward=502.00 +/- 84.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=186000, episode_reward=333.60 +/- 163.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=187000, episode_reward=357.00 +/- 90.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=188000, episode_reward=404.60 +/- 158.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=189000, episode_reward=313.00 +/- 265.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=190000, episode_reward=353.80 +/- 175.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=191000, episode_reward=344.40 +/- 197.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=192000, episode_reward=247.80 +/- 233.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=193000, episode_reward=424.40 +/- 139.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=194000, episode_reward=274.80 +/- 109.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=195000, episode_reward=310.40 +/- 298.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=196000, episode_reward=377.40 +/- 235.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=197000, episode_reward=378.60 +/- 151.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=198000, episode_reward=299.20 +/- 111.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=199000, episode_reward=285.20 +/- 134.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=200000, episode_reward=325.80 +/- 117.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=201000, episode_reward=289.00 +/- 209.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=202000, episode_reward=411.80 +/- 118.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=203000, episode_reward=314.00 +/- 191.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=204000, episode_reward=412.00 +/- 161.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=205000, episode_reward=433.00 +/- 76.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=206000, episode_reward=264.40 +/- 180.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=207000, episode_reward=434.20 +/- 225.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=208000, episode_reward=406.20 +/- 172.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=209000, episode_reward=436.40 +/- 135.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=210000, episode_reward=226.00 +/- 194.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=211000, episode_reward=228.60 +/- 113.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=212000, episode_reward=255.40 +/- 130.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=213000, episode_reward=380.00 +/- 164.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=214000, episode_reward=362.00 +/- 202.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=215000, episode_reward=576.80 +/- 68.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=216000, episode_reward=437.80 +/- 137.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=217000, episode_reward=231.60 +/- 207.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=218000, episode_reward=107.20 +/- 330.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=219000, episode_reward=141.80 +/- 189.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=220000, episode_reward=466.00 +/- 224.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=221000, episode_reward=300.60 +/- 222.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=222000, episode_reward=393.60 +/- 181.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=223000, episode_reward=408.80 +/- 145.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=224000, episode_reward=550.40 +/- 114.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=225000, episode_reward=417.20 +/- 207.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=226000, episode_reward=331.20 +/- 201.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=227000, episode_reward=391.80 +/- 80.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=228000, episode_reward=393.00 +/- 233.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=229000, episode_reward=509.60 +/- 132.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=230000, episode_reward=317.60 +/- 173.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=231000, episode_reward=326.20 +/- 315.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=232000, episode_reward=337.20 +/- 41.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=233000, episode_reward=353.40 +/- 138.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=234000, episode_reward=408.80 +/- 150.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=235000, episode_reward=484.00 +/- 178.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=236000, episode_reward=429.00 +/- 116.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=237000, episode_reward=324.80 +/- 168.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=238000, episode_reward=380.00 +/- 267.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=239000, episode_reward=493.40 +/- 197.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=240000, episode_reward=184.40 +/- 156.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=241000, episode_reward=388.80 +/- 88.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=242000, episode_reward=206.20 +/- 135.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=243000, episode_reward=375.00 +/- 192.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=244000, episode_reward=514.20 +/- 190.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=245000, episode_reward=444.00 +/- 186.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=246000, episode_reward=301.60 +/- 115.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=247000, episode_reward=391.60 +/- 201.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=248000, episode_reward=162.00 +/- 228.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=249000, episode_reward=206.80 +/- 232.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=250000, episode_reward=394.80 +/- 195.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=251000, episode_reward=424.00 +/- 173.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=252000, episode_reward=314.60 +/- 189.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=253000, episode_reward=430.00 +/- 267.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=254000, episode_reward=339.40 +/- 160.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=255000, episode_reward=379.20 +/- 112.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=256000, episode_reward=474.80 +/- 121.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=257000, episode_reward=343.20 +/- 166.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=258000, episode_reward=409.80 +/- 123.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=259000, episode_reward=350.80 +/- 187.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=260000, episode_reward=435.40 +/- 102.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=261000, episode_reward=467.40 +/- 117.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=262000, episode_reward=231.80 +/- 183.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=263000, episode_reward=260.80 +/- 157.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=264000, episode_reward=465.20 +/- 95.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=265000, episode_reward=354.20 +/- 151.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=266000, episode_reward=231.00 +/- 69.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=267000, episode_reward=259.20 +/- 163.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=268000, episode_reward=346.80 +/- 193.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=269000, episode_reward=245.60 +/- 277.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=270000, episode_reward=281.20 +/- 124.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=271000, episode_reward=266.40 +/- 278.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=272000, episode_reward=295.00 +/- 191.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=273000, episode_reward=413.20 +/- 194.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=274000, episode_reward=386.00 +/- 239.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=275000, episode_reward=287.80 +/- 282.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=276000, episode_reward=255.80 +/- 122.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=277000, episode_reward=248.60 +/- 260.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=278000, episode_reward=341.40 +/- 149.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=279000, episode_reward=368.00 +/- 219.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=280000, episode_reward=351.60 +/- 168.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=281000, episode_reward=328.80 +/- 134.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=282000, episode_reward=118.40 +/- 156.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=283000, episode_reward=402.80 +/- 197.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=284000, episode_reward=363.80 +/- 144.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=285000, episode_reward=370.00 +/- 158.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=286000, episode_reward=157.80 +/- 246.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=287000, episode_reward=369.40 +/- 108.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=288000, episode_reward=319.20 +/- 63.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=289000, episode_reward=311.40 +/- 235.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=290000, episode_reward=188.00 +/- 136.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=291000, episode_reward=268.20 +/- 282.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=292000, episode_reward=145.40 +/- 212.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=293000, episode_reward=317.00 +/- 123.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=294000, episode_reward=386.20 +/- 132.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=295000, episode_reward=404.00 +/- 53.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=296000, episode_reward=311.00 +/- 273.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=297000, episode_reward=199.00 +/- 187.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=298000, episode_reward=297.80 +/- 270.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=299000, episode_reward=411.20 +/- 235.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=300000, episode_reward=265.60 +/- 269.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=301000, episode_reward=563.40 +/- 86.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=302000, episode_reward=488.00 +/- 236.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=303000, episode_reward=466.00 +/- 154.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=304000, episode_reward=351.40 +/- 140.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=305000, episode_reward=420.60 +/- 132.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=306000, episode_reward=530.20 +/- 233.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=307000, episode_reward=367.80 +/- 226.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=308000, episode_reward=239.60 +/- 184.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=309000, episode_reward=342.40 +/- 129.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=310000, episode_reward=340.40 +/- 150.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=311000, episode_reward=228.20 +/- 185.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=312000, episode_reward=428.20 +/- 131.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=313000, episode_reward=365.20 +/- 330.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=314000, episode_reward=481.40 +/- 144.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=315000, episode_reward=329.40 +/- 204.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=316000, episode_reward=334.00 +/- 189.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=317000, episode_reward=276.60 +/- 278.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=318000, episode_reward=437.80 +/- 205.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=319000, episode_reward=342.80 +/- 204.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=320000, episode_reward=452.20 +/- 78.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=321000, episode_reward=367.20 +/- 89.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=322000, episode_reward=589.20 +/- 158.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=323000, episode_reward=353.60 +/- 183.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=324000, episode_reward=334.40 +/- 74.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=325000, episode_reward=432.20 +/- 173.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=326000, episode_reward=241.20 +/- 160.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=327000, episode_reward=327.80 +/- 47.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=328000, episode_reward=493.60 +/- 238.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=329000, episode_reward=367.20 +/- 122.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=330000, episode_reward=545.80 +/- 132.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=331000, episode_reward=396.20 +/- 325.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=332000, episode_reward=359.80 +/- 121.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=333000, episode_reward=410.80 +/- 123.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=334000, episode_reward=209.40 +/- 146.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=335000, episode_reward=398.60 +/- 182.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=336000, episode_reward=220.40 +/- 229.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=337000, episode_reward=321.60 +/- 201.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=338000, episode_reward=223.40 +/- 246.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=339000, episode_reward=357.00 +/- 128.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=340000, episode_reward=341.20 +/- 249.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=341000, episode_reward=318.20 +/- 158.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=342000, episode_reward=412.80 +/- 54.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=343000, episode_reward=470.40 +/- 239.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=344000, episode_reward=457.60 +/- 265.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=345000, episode_reward=327.80 +/- 129.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=346000, episode_reward=504.00 +/- 173.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=347000, episode_reward=227.80 +/- 121.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=348000, episode_reward=310.00 +/- 274.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=349000, episode_reward=550.80 +/- 161.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=350000, episode_reward=355.40 +/- 220.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=351000, episode_reward=475.20 +/- 97.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=352000, episode_reward=349.80 +/- 82.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=353000, episode_reward=480.60 +/- 75.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=354000, episode_reward=499.40 +/- 152.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=355000, episode_reward=368.00 +/- 84.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=356000, episode_reward=298.80 +/- 210.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=357000, episode_reward=362.20 +/- 179.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=358000, episode_reward=233.20 +/- 138.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=359000, episode_reward=370.20 +/- 150.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=360000, episode_reward=423.40 +/- 207.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=361000, episode_reward=333.20 +/- 139.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=362000, episode_reward=480.60 +/- 119.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=363000, episode_reward=424.40 +/- 203.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=364000, episode_reward=256.20 +/- 377.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=365000, episode_reward=281.40 +/- 316.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=366000, episode_reward=415.80 +/- 104.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=367000, episode_reward=319.20 +/- 196.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=368000, episode_reward=287.20 +/- 224.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=369000, episode_reward=357.80 +/- 81.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=370000, episode_reward=457.60 +/- 126.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=371000, episode_reward=248.00 +/- 262.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=372000, episode_reward=422.20 +/- 72.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=373000, episode_reward=338.60 +/- 340.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=374000, episode_reward=266.60 +/- 133.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=375000, episode_reward=408.00 +/- 159.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=376000, episode_reward=295.40 +/- 163.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=377000, episode_reward=456.00 +/- 118.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=378000, episode_reward=244.00 +/- 327.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=379000, episode_reward=154.20 +/- 260.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=380000, episode_reward=515.00 +/- 166.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=381000, episode_reward=195.60 +/- 66.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=382000, episode_reward=413.00 +/- 91.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=383000, episode_reward=443.80 +/- 203.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=384000, episode_reward=122.80 +/- 175.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=385000, episode_reward=377.60 +/- 82.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=386000, episode_reward=331.60 +/- 150.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=387000, episode_reward=467.60 +/- 211.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=388000, episode_reward=480.20 +/- 179.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=389000, episode_reward=492.60 +/- 158.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=390000, episode_reward=231.40 +/- 114.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=391000, episode_reward=338.40 +/- 209.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=392000, episode_reward=361.00 +/- 148.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=393000, episode_reward=253.00 +/- 203.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=394000, episode_reward=406.20 +/- 118.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=395000, episode_reward=198.40 +/- 118.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=396000, episode_reward=244.00 +/- 154.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=397000, episode_reward=371.20 +/- 262.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=398000, episode_reward=435.00 +/- 240.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=399000, episode_reward=493.00 +/- 217.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=400000, episode_reward=311.40 +/- 236.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=401000, episode_reward=376.80 +/- 201.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=402000, episode_reward=375.80 +/- 158.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=403000, episode_reward=418.00 +/- 113.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=404000, episode_reward=329.20 +/- 119.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=405000, episode_reward=584.00 +/- 227.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=406000, episode_reward=407.40 +/- 137.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=407000, episode_reward=179.80 +/- 300.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=408000, episode_reward=416.00 +/- 168.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=409000, episode_reward=379.00 +/- 127.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=410000, episode_reward=300.60 +/- 186.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=411000, episode_reward=343.00 +/- 206.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=412000, episode_reward=274.40 +/- 234.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=413000, episode_reward=431.00 +/- 235.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=414000, episode_reward=457.40 +/- 62.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=415000, episode_reward=339.40 +/- 218.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=416000, episode_reward=464.80 +/- 163.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=417000, episode_reward=313.60 +/- 134.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=418000, episode_reward=350.40 +/- 218.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=419000, episode_reward=366.60 +/- 169.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=420000, episode_reward=364.20 +/- 176.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=421000, episode_reward=343.00 +/- 173.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=422000, episode_reward=382.80 +/- 146.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=423000, episode_reward=400.60 +/- 135.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=424000, episode_reward=423.80 +/- 212.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=425000, episode_reward=454.20 +/- 196.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=426000, episode_reward=311.60 +/- 168.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=427000, episode_reward=277.20 +/- 147.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=428000, episode_reward=353.40 +/- 280.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=429000, episode_reward=413.00 +/- 137.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=430000, episode_reward=254.80 +/- 117.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=431000, episode_reward=429.40 +/- 298.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=432000, episode_reward=211.80 +/- 101.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=433000, episode_reward=425.60 +/- 93.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=434000, episode_reward=329.80 +/- 119.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=435000, episode_reward=277.60 +/- 223.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=436000, episode_reward=118.20 +/- 126.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=437000, episode_reward=426.40 +/- 73.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=438000, episode_reward=269.20 +/- 69.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=439000, episode_reward=338.80 +/- 199.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=440000, episode_reward=362.60 +/- 206.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=441000, episode_reward=342.80 +/- 135.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=442000, episode_reward=476.40 +/- 206.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=443000, episode_reward=359.80 +/- 161.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=444000, episode_reward=197.20 +/- 283.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=445000, episode_reward=426.40 +/- 329.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=446000, episode_reward=445.80 +/- 200.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=447000, episode_reward=447.60 +/- 166.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=448000, episode_reward=440.80 +/- 88.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=449000, episode_reward=331.20 +/- 115.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=450000, episode_reward=306.60 +/- 110.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=451000, episode_reward=325.40 +/- 275.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=452000, episode_reward=394.40 +/- 271.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=453000, episode_reward=489.60 +/- 144.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=454000, episode_reward=435.60 +/- 131.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=455000, episode_reward=471.20 +/- 157.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=456000, episode_reward=469.20 +/- 140.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=457000, episode_reward=351.00 +/- 130.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=458000, episode_reward=226.00 +/- 256.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=459000, episode_reward=338.80 +/- 156.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=460000, episode_reward=459.80 +/- 167.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=461000, episode_reward=225.00 +/- 256.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=462000, episode_reward=360.20 +/- 203.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=463000, episode_reward=363.20 +/- 125.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=464000, episode_reward=448.40 +/- 136.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=465000, episode_reward=644.00 +/- 131.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=466000, episode_reward=246.60 +/- 233.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=467000, episode_reward=446.80 +/- 117.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=468000, episode_reward=107.20 +/- 93.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=469000, episode_reward=476.20 +/- 300.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=470000, episode_reward=393.40 +/- 206.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=471000, episode_reward=370.00 +/- 242.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=472000, episode_reward=417.80 +/- 193.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=473000, episode_reward=539.40 +/- 123.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=474000, episode_reward=309.20 +/- 108.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=475000, episode_reward=499.00 +/- 144.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=476000, episode_reward=369.20 +/- 134.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=477000, episode_reward=251.00 +/- 130.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=478000, episode_reward=299.00 +/- 143.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=479000, episode_reward=419.00 +/- 179.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=480000, episode_reward=503.80 +/- 185.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=481000, episode_reward=258.40 +/- 92.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=482000, episode_reward=309.60 +/- 272.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=483000, episode_reward=407.20 +/- 221.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=484000, episode_reward=374.00 +/- 289.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=485000, episode_reward=114.60 +/- 135.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=486000, episode_reward=401.80 +/- 85.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=487000, episode_reward=373.80 +/- 138.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=488000, episode_reward=385.60 +/- 147.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=489000, episode_reward=423.00 +/- 195.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=490000, episode_reward=345.40 +/- 175.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=491000, episode_reward=387.20 +/- 189.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=492000, episode_reward=496.60 +/- 107.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=493000, episode_reward=341.80 +/- 192.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=494000, episode_reward=414.60 +/- 246.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=495000, episode_reward=257.80 +/- 246.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=496000, episode_reward=398.80 +/- 92.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=497000, episode_reward=348.00 +/- 136.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=498000, episode_reward=530.20 +/- 120.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=499000, episode_reward=187.60 +/- 266.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=500000, episode_reward=336.60 +/- 195.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=501000, episode_reward=514.00 +/- 253.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Gamma = 0.99, Learning Rate = 0.003, Activation Function = Tanh, Neurons = 8, Process noise = 0.1, Measurement noise = 0.0\n",
      "Eval num_timesteps=1000, episode_reward=-2096.80 +/- 445.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=-1719.00 +/- 217.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3000, episode_reward=-17.80 +/- 229.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4000, episode_reward=198.20 +/- 263.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=5000, episode_reward=290.20 +/- 161.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=6000, episode_reward=-216.80 +/- 483.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=7000, episode_reward=-140.60 +/- 889.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=8000, episode_reward=-481.40 +/- 958.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=9000, episode_reward=186.60 +/- 137.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=10000, episode_reward=302.20 +/- 187.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=11000, episode_reward=377.00 +/- 204.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=12000, episode_reward=275.80 +/- 147.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=13000, episode_reward=241.40 +/- 202.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=14000, episode_reward=362.80 +/- 95.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=15000, episode_reward=307.40 +/- 208.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=16000, episode_reward=188.00 +/- 195.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=17000, episode_reward=215.00 +/- 64.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=18000, episode_reward=270.80 +/- 247.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=19000, episode_reward=-14.40 +/- 251.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=20000, episode_reward=10.20 +/- 174.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=21000, episode_reward=9.80 +/- 169.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=22000, episode_reward=35.00 +/- 265.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=23000, episode_reward=60.40 +/- 145.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=24000, episode_reward=64.60 +/- 401.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=25000, episode_reward=448.20 +/- 252.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=26000, episode_reward=846.20 +/- 263.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=27000, episode_reward=863.20 +/- 196.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=28000, episode_reward=705.00 +/- 264.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=29000, episode_reward=1033.20 +/- 230.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=30000, episode_reward=1321.00 +/- 148.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=31000, episode_reward=1291.40 +/- 269.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=32000, episode_reward=1008.80 +/- 231.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=33000, episode_reward=1227.20 +/- 81.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=34000, episode_reward=917.00 +/- 248.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=35000, episode_reward=1394.00 +/- 168.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=36000, episode_reward=1075.80 +/- 183.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=37000, episode_reward=-1181.40 +/- 5092.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=38000, episode_reward=573.60 +/- 1158.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=39000, episode_reward=958.40 +/- 144.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=40000, episode_reward=-1206.00 +/- 4578.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=41000, episode_reward=1257.40 +/- 88.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=42000, episode_reward=-606.40 +/- 4060.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=43000, episode_reward=1333.00 +/- 258.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=44000, episode_reward=1550.00 +/- 128.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=45000, episode_reward=1516.00 +/- 260.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=46000, episode_reward=1380.20 +/- 167.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=47000, episode_reward=1559.20 +/- 227.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=48000, episode_reward=1583.80 +/- 300.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=49000, episode_reward=-1595.20 +/- 6087.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=50000, episode_reward=1619.20 +/- 283.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=51000, episode_reward=1518.80 +/- 308.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=52000, episode_reward=1502.60 +/- 207.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=53000, episode_reward=1529.60 +/- 215.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=54000, episode_reward=1483.60 +/- 312.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=55000, episode_reward=1625.80 +/- 256.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=56000, episode_reward=1540.40 +/- 128.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=57000, episode_reward=1356.40 +/- 316.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=58000, episode_reward=1667.80 +/- 113.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=59000, episode_reward=1502.60 +/- 316.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=60000, episode_reward=1552.20 +/- 341.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=61000, episode_reward=1654.20 +/- 281.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=62000, episode_reward=1532.00 +/- 68.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=63000, episode_reward=1447.00 +/- 138.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=64000, episode_reward=1558.20 +/- 211.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=65000, episode_reward=1684.00 +/- 90.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=66000, episode_reward=1397.60 +/- 153.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=67000, episode_reward=1463.40 +/- 207.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=68000, episode_reward=1350.40 +/- 222.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=69000, episode_reward=1565.60 +/- 266.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=70000, episode_reward=1488.60 +/- 299.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=71000, episode_reward=1629.60 +/- 66.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=72000, episode_reward=1552.40 +/- 125.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=73000, episode_reward=1613.20 +/- 235.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=74000, episode_reward=1389.40 +/- 313.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=75000, episode_reward=1457.20 +/- 332.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=76000, episode_reward=1434.60 +/- 402.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=77000, episode_reward=1475.20 +/- 292.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=78000, episode_reward=1434.80 +/- 207.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=79000, episode_reward=1556.20 +/- 334.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=80000, episode_reward=1559.60 +/- 199.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=81000, episode_reward=1504.80 +/- 338.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=82000, episode_reward=1627.00 +/- 233.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=83000, episode_reward=1572.40 +/- 455.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=84000, episode_reward=1441.00 +/- 257.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=85000, episode_reward=1475.40 +/- 165.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=86000, episode_reward=1666.00 +/- 214.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=87000, episode_reward=1388.40 +/- 329.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=88000, episode_reward=1584.40 +/- 166.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=89000, episode_reward=1398.60 +/- 162.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=90000, episode_reward=1063.20 +/- 303.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=91000, episode_reward=-2123.20 +/- 7370.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=92000, episode_reward=1679.80 +/- 121.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=93000, episode_reward=1693.40 +/- 294.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=94000, episode_reward=-460.60 +/- 3877.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=95000, episode_reward=1561.00 +/- 399.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=96000, episode_reward=1662.60 +/- 152.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=97000, episode_reward=1255.20 +/- 190.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=98000, episode_reward=1526.80 +/- 236.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=99000, episode_reward=841.40 +/- 414.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=100000, episode_reward=879.20 +/- 184.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=101000, episode_reward=1240.60 +/- 390.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=102000, episode_reward=1569.40 +/- 164.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=103000, episode_reward=1403.20 +/- 171.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=104000, episode_reward=1664.60 +/- 318.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=105000, episode_reward=1744.00 +/- 199.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=106000, episode_reward=1392.20 +/- 250.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=107000, episode_reward=1555.20 +/- 194.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=108000, episode_reward=1425.80 +/- 212.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=109000, episode_reward=1597.60 +/- 210.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=110000, episode_reward=1626.60 +/- 172.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=111000, episode_reward=1470.80 +/- 356.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=112000, episode_reward=1424.80 +/- 248.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=113000, episode_reward=1432.80 +/- 214.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=114000, episode_reward=1324.60 +/- 126.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=115000, episode_reward=1274.40 +/- 428.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=116000, episode_reward=1430.20 +/- 293.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=117000, episode_reward=1268.20 +/- 278.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=118000, episode_reward=1475.20 +/- 196.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=119000, episode_reward=1404.60 +/- 295.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=120000, episode_reward=1569.00 +/- 200.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=121000, episode_reward=1280.20 +/- 275.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=122000, episode_reward=1704.40 +/- 255.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=123000, episode_reward=-2558.80 +/- 5194.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=124000, episode_reward=1338.00 +/- 510.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=125000, episode_reward=1168.60 +/- 403.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=126000, episode_reward=1281.40 +/- 203.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=127000, episode_reward=-194.80 +/- 3421.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=128000, episode_reward=-3904.40 +/- 7136.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=129000, episode_reward=1245.80 +/- 664.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=130000, episode_reward=736.80 +/- 1490.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=131000, episode_reward=-2377.80 +/- 4581.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=132000, episode_reward=1242.80 +/- 799.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=133000, episode_reward=-4567.60 +/- 7275.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=134000, episode_reward=1703.00 +/- 143.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=135000, episode_reward=1637.20 +/- 94.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=136000, episode_reward=1566.00 +/- 279.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=137000, episode_reward=1534.00 +/- 135.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=138000, episode_reward=-4056.80 +/- 6972.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=139000, episode_reward=1204.60 +/- 201.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=140000, episode_reward=170.60 +/- 2252.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=141000, episode_reward=-992.20 +/- 4881.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=142000, episode_reward=323.60 +/- 2094.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=143000, episode_reward=-1380.00 +/- 4068.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=144000, episode_reward=1445.60 +/- 145.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=145000, episode_reward=1485.20 +/- 261.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=146000, episode_reward=1397.40 +/- 227.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=147000, episode_reward=1472.00 +/- 41.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=148000, episode_reward=1400.40 +/- 213.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=149000, episode_reward=1737.40 +/- 71.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=150000, episode_reward=720.40 +/- 1241.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=151000, episode_reward=1466.00 +/- 278.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=152000, episode_reward=1450.80 +/- 107.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=153000, episode_reward=1440.20 +/- 124.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=154000, episode_reward=1324.00 +/- 266.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=155000, episode_reward=1380.00 +/- 286.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=156000, episode_reward=1347.20 +/- 174.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=157000, episode_reward=1604.00 +/- 76.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=158000, episode_reward=1670.00 +/- 64.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=159000, episode_reward=1458.20 +/- 202.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=160000, episode_reward=1419.20 +/- 157.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=161000, episode_reward=1373.80 +/- 201.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=162000, episode_reward=1708.20 +/- 155.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=163000, episode_reward=1455.40 +/- 170.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=164000, episode_reward=1620.20 +/- 215.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=165000, episode_reward=1677.00 +/- 92.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=166000, episode_reward=1397.20 +/- 185.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=167000, episode_reward=1432.60 +/- 241.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=168000, episode_reward=1452.40 +/- 62.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=169000, episode_reward=1405.00 +/- 360.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=170000, episode_reward=1555.20 +/- 165.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=171000, episode_reward=1374.20 +/- 76.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=172000, episode_reward=1488.40 +/- 171.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=173000, episode_reward=1488.80 +/- 120.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=174000, episode_reward=1535.20 +/- 272.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=175000, episode_reward=1531.40 +/- 107.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=176000, episode_reward=1628.00 +/- 222.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=177000, episode_reward=343.20 +/- 2553.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=178000, episode_reward=1720.60 +/- 122.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=179000, episode_reward=1530.80 +/- 128.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=180000, episode_reward=1556.40 +/- 126.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=181000, episode_reward=1490.40 +/- 246.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=182000, episode_reward=-1055.40 +/- 5343.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=183000, episode_reward=11.80 +/- 3075.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=184000, episode_reward=1561.40 +/- 212.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=185000, episode_reward=1362.80 +/- 308.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=186000, episode_reward=1611.00 +/- 140.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=187000, episode_reward=1505.40 +/- 156.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=188000, episode_reward=1549.60 +/- 202.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=189000, episode_reward=1585.20 +/- 246.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=190000, episode_reward=1555.60 +/- 222.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=191000, episode_reward=1648.20 +/- 250.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=192000, episode_reward=1522.40 +/- 259.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=193000, episode_reward=1748.00 +/- 132.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=194000, episode_reward=1479.40 +/- 161.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=195000, episode_reward=1645.00 +/- 123.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=196000, episode_reward=1484.40 +/- 193.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=197000, episode_reward=1585.80 +/- 142.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=198000, episode_reward=1438.00 +/- 98.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=199000, episode_reward=1442.40 +/- 233.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=200000, episode_reward=1401.00 +/- 152.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=201000, episode_reward=1741.40 +/- 157.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=202000, episode_reward=1549.20 +/- 281.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=203000, episode_reward=1498.40 +/- 250.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=204000, episode_reward=1802.80 +/- 121.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=205000, episode_reward=1533.80 +/- 229.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=206000, episode_reward=-2024.60 +/- 7039.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=207000, episode_reward=1816.40 +/- 120.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=208000, episode_reward=1452.20 +/- 247.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=209000, episode_reward=1403.80 +/- 169.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=210000, episode_reward=1720.60 +/- 111.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=211000, episode_reward=1726.40 +/- 179.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=212000, episode_reward=1706.60 +/- 210.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=213000, episode_reward=1684.20 +/- 220.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=214000, episode_reward=1557.40 +/- 192.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=215000, episode_reward=1440.00 +/- 223.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=216000, episode_reward=1566.00 +/- 213.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=217000, episode_reward=1596.80 +/- 293.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=218000, episode_reward=-1320.20 +/- 5656.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=219000, episode_reward=1610.40 +/- 111.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=220000, episode_reward=1619.00 +/- 197.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=221000, episode_reward=1507.40 +/- 322.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=222000, episode_reward=1497.40 +/- 227.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=223000, episode_reward=1561.20 +/- 137.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=224000, episode_reward=1743.00 +/- 157.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=225000, episode_reward=1511.60 +/- 172.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=226000, episode_reward=1554.20 +/- 182.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=227000, episode_reward=1631.00 +/- 133.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=228000, episode_reward=1524.00 +/- 405.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=229000, episode_reward=1431.40 +/- 214.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=230000, episode_reward=1348.80 +/- 31.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=231000, episode_reward=1576.00 +/- 192.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=232000, episode_reward=1546.60 +/- 101.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=233000, episode_reward=1418.00 +/- 175.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=234000, episode_reward=1639.00 +/- 146.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=235000, episode_reward=1476.60 +/- 382.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=236000, episode_reward=1394.80 +/- 98.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=237000, episode_reward=1665.40 +/- 208.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=238000, episode_reward=1692.00 +/- 248.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=239000, episode_reward=1726.20 +/- 267.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=240000, episode_reward=1548.80 +/- 229.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=241000, episode_reward=1676.20 +/- 216.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=242000, episode_reward=1762.20 +/- 157.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=243000, episode_reward=1452.80 +/- 228.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=244000, episode_reward=1696.80 +/- 279.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=245000, episode_reward=1573.80 +/- 236.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=246000, episode_reward=1473.20 +/- 216.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=247000, episode_reward=1571.00 +/- 130.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=248000, episode_reward=1470.60 +/- 150.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=249000, episode_reward=1661.40 +/- 271.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=250000, episode_reward=1509.00 +/- 148.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=251000, episode_reward=1681.20 +/- 120.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=252000, episode_reward=1617.20 +/- 206.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=253000, episode_reward=1594.20 +/- 142.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=254000, episode_reward=1470.60 +/- 156.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=255000, episode_reward=1402.80 +/- 188.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=256000, episode_reward=1508.60 +/- 195.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=257000, episode_reward=1594.40 +/- 211.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=258000, episode_reward=1620.40 +/- 261.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=259000, episode_reward=1457.80 +/- 145.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=260000, episode_reward=1501.80 +/- 169.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=261000, episode_reward=1559.20 +/- 236.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=262000, episode_reward=1528.40 +/- 121.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=263000, episode_reward=1668.60 +/- 120.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=264000, episode_reward=1603.40 +/- 202.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=265000, episode_reward=1412.00 +/- 254.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=266000, episode_reward=1563.20 +/- 176.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=267000, episode_reward=1646.40 +/- 244.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=268000, episode_reward=1645.80 +/- 169.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=269000, episode_reward=1401.00 +/- 224.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=270000, episode_reward=1680.60 +/- 138.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=271000, episode_reward=1605.80 +/- 222.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=272000, episode_reward=1599.60 +/- 246.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=273000, episode_reward=1573.20 +/- 175.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=274000, episode_reward=1454.60 +/- 193.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=275000, episode_reward=1299.20 +/- 174.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=276000, episode_reward=1628.20 +/- 107.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=277000, episode_reward=1579.60 +/- 74.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=278000, episode_reward=1610.80 +/- 298.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=279000, episode_reward=1663.80 +/- 197.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=280000, episode_reward=1569.40 +/- 147.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=281000, episode_reward=1732.80 +/- 159.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=282000, episode_reward=1610.40 +/- 268.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=283000, episode_reward=1508.20 +/- 98.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=284000, episode_reward=1680.60 +/- 140.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=285000, episode_reward=1508.80 +/- 243.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=286000, episode_reward=1658.80 +/- 179.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=287000, episode_reward=1565.00 +/- 161.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=288000, episode_reward=1601.00 +/- 185.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=289000, episode_reward=1481.60 +/- 139.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=290000, episode_reward=1359.40 +/- 372.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=291000, episode_reward=1437.00 +/- 189.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=292000, episode_reward=1531.40 +/- 213.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=293000, episode_reward=1645.80 +/- 146.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=294000, episode_reward=1631.40 +/- 213.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=295000, episode_reward=1539.60 +/- 156.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=296000, episode_reward=1690.20 +/- 115.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=297000, episode_reward=1488.60 +/- 240.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=298000, episode_reward=1482.60 +/- 204.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=299000, episode_reward=1566.80 +/- 101.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=300000, episode_reward=1637.80 +/- 165.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=301000, episode_reward=1505.80 +/- 54.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=302000, episode_reward=1545.20 +/- 200.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=303000, episode_reward=1585.00 +/- 341.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=304000, episode_reward=1443.00 +/- 265.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=305000, episode_reward=1701.60 +/- 195.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=306000, episode_reward=1382.20 +/- 239.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=307000, episode_reward=1620.60 +/- 136.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=308000, episode_reward=1644.40 +/- 186.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=309000, episode_reward=1620.40 +/- 162.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=310000, episode_reward=1665.60 +/- 243.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=311000, episode_reward=1638.60 +/- 133.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=312000, episode_reward=1548.20 +/- 194.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=313000, episode_reward=1658.60 +/- 265.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=314000, episode_reward=1648.60 +/- 185.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=315000, episode_reward=1480.60 +/- 297.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=316000, episode_reward=1589.40 +/- 219.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=317000, episode_reward=1581.60 +/- 183.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=318000, episode_reward=1493.40 +/- 224.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=319000, episode_reward=1598.80 +/- 143.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=320000, episode_reward=1372.60 +/- 114.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=321000, episode_reward=1427.00 +/- 386.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=322000, episode_reward=1515.60 +/- 278.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=323000, episode_reward=1403.80 +/- 248.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=324000, episode_reward=1680.80 +/- 182.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=325000, episode_reward=1575.20 +/- 160.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=326000, episode_reward=1455.80 +/- 237.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=327000, episode_reward=1411.60 +/- 219.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=328000, episode_reward=1686.20 +/- 205.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=329000, episode_reward=1603.80 +/- 262.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=330000, episode_reward=1596.80 +/- 153.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=331000, episode_reward=1510.60 +/- 322.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=332000, episode_reward=1403.20 +/- 194.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=333000, episode_reward=1673.60 +/- 281.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=334000, episode_reward=1476.60 +/- 257.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=335000, episode_reward=1559.60 +/- 111.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=336000, episode_reward=1494.60 +/- 229.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=337000, episode_reward=1590.80 +/- 345.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=338000, episode_reward=1391.60 +/- 184.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=339000, episode_reward=1701.00 +/- 223.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=340000, episode_reward=1605.40 +/- 202.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=341000, episode_reward=1538.80 +/- 221.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=342000, episode_reward=1376.60 +/- 145.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=343000, episode_reward=1491.80 +/- 244.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=344000, episode_reward=1533.80 +/- 188.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=345000, episode_reward=1584.20 +/- 140.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=346000, episode_reward=1503.20 +/- 178.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=347000, episode_reward=1447.60 +/- 298.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=348000, episode_reward=1582.60 +/- 257.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=349000, episode_reward=1378.20 +/- 41.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=350000, episode_reward=1672.60 +/- 246.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=351000, episode_reward=1636.20 +/- 151.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=352000, episode_reward=1697.20 +/- 150.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=353000, episode_reward=1558.60 +/- 292.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=354000, episode_reward=1488.20 +/- 149.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=355000, episode_reward=1561.40 +/- 142.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=356000, episode_reward=1553.20 +/- 242.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=357000, episode_reward=1463.80 +/- 258.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=358000, episode_reward=1545.80 +/- 160.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=359000, episode_reward=1555.00 +/- 163.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=360000, episode_reward=1628.80 +/- 208.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=361000, episode_reward=1605.40 +/- 308.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=362000, episode_reward=1715.20 +/- 112.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=363000, episode_reward=1436.00 +/- 311.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=364000, episode_reward=1577.40 +/- 177.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=365000, episode_reward=1540.60 +/- 366.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=366000, episode_reward=1691.80 +/- 182.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=367000, episode_reward=1651.20 +/- 131.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=368000, episode_reward=1648.20 +/- 102.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=369000, episode_reward=1486.40 +/- 250.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=370000, episode_reward=1539.20 +/- 278.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=371000, episode_reward=1575.00 +/- 191.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=372000, episode_reward=1426.00 +/- 309.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=373000, episode_reward=1536.20 +/- 204.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=374000, episode_reward=1416.40 +/- 245.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=375000, episode_reward=1677.00 +/- 200.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=376000, episode_reward=1605.40 +/- 157.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=377000, episode_reward=1566.00 +/- 249.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=378000, episode_reward=1461.00 +/- 134.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=379000, episode_reward=1647.00 +/- 138.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=380000, episode_reward=1668.60 +/- 310.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=381000, episode_reward=1673.20 +/- 250.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=382000, episode_reward=1697.60 +/- 276.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=383000, episode_reward=1517.40 +/- 313.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=384000, episode_reward=1269.80 +/- 230.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=385000, episode_reward=1667.20 +/- 234.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=386000, episode_reward=1670.20 +/- 168.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=387000, episode_reward=1456.80 +/- 126.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=388000, episode_reward=1468.60 +/- 177.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=389000, episode_reward=1720.80 +/- 186.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=390000, episode_reward=1553.20 +/- 165.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=391000, episode_reward=1507.40 +/- 198.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=392000, episode_reward=1563.80 +/- 121.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=393000, episode_reward=1601.20 +/- 97.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=394000, episode_reward=1631.80 +/- 261.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=395000, episode_reward=1572.40 +/- 158.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=396000, episode_reward=1691.40 +/- 178.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=397000, episode_reward=1420.40 +/- 99.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=398000, episode_reward=1388.40 +/- 271.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=399000, episode_reward=1392.80 +/- 140.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=400000, episode_reward=1533.00 +/- 276.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=401000, episode_reward=1523.60 +/- 154.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=402000, episode_reward=1510.60 +/- 291.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=403000, episode_reward=1585.60 +/- 133.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=404000, episode_reward=1476.60 +/- 190.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=405000, episode_reward=1702.20 +/- 204.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=406000, episode_reward=1497.00 +/- 101.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=407000, episode_reward=1722.20 +/- 208.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=408000, episode_reward=1655.40 +/- 121.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=409000, episode_reward=1633.40 +/- 118.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=410000, episode_reward=1606.40 +/- 171.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=411000, episode_reward=1520.40 +/- 109.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=412000, episode_reward=1668.40 +/- 118.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=413000, episode_reward=1512.60 +/- 205.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=414000, episode_reward=1436.60 +/- 236.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=415000, episode_reward=1625.60 +/- 210.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=416000, episode_reward=1694.80 +/- 81.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=417000, episode_reward=1454.20 +/- 148.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=418000, episode_reward=1520.40 +/- 311.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=419000, episode_reward=1611.80 +/- 90.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=420000, episode_reward=1582.80 +/- 44.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=421000, episode_reward=1434.80 +/- 204.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=422000, episode_reward=1526.60 +/- 230.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=423000, episode_reward=1643.60 +/- 107.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=424000, episode_reward=1595.60 +/- 181.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=425000, episode_reward=1599.80 +/- 155.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=426000, episode_reward=1622.20 +/- 232.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=427000, episode_reward=1700.20 +/- 281.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=428000, episode_reward=1686.60 +/- 131.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=429000, episode_reward=1589.00 +/- 282.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=430000, episode_reward=1648.80 +/- 173.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=431000, episode_reward=1591.00 +/- 167.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=432000, episode_reward=1670.40 +/- 71.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=433000, episode_reward=1544.20 +/- 189.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=434000, episode_reward=1573.20 +/- 91.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=435000, episode_reward=1484.80 +/- 166.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=436000, episode_reward=1233.20 +/- 261.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=437000, episode_reward=1253.20 +/- 179.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=438000, episode_reward=1514.80 +/- 206.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=439000, episode_reward=1535.40 +/- 279.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=440000, episode_reward=1502.40 +/- 305.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=441000, episode_reward=1670.40 +/- 279.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=442000, episode_reward=1462.40 +/- 236.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=443000, episode_reward=1470.00 +/- 225.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=444000, episode_reward=1614.20 +/- 344.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=445000, episode_reward=1592.40 +/- 223.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=446000, episode_reward=1437.00 +/- 141.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=447000, episode_reward=1483.60 +/- 202.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=448000, episode_reward=1678.20 +/- 63.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=449000, episode_reward=1628.40 +/- 177.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=450000, episode_reward=1340.00 +/- 351.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=451000, episode_reward=1487.60 +/- 81.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=452000, episode_reward=1452.20 +/- 196.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=453000, episode_reward=1685.40 +/- 186.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=454000, episode_reward=1630.40 +/- 242.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=455000, episode_reward=1694.80 +/- 252.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=456000, episode_reward=1456.20 +/- 274.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=457000, episode_reward=1562.60 +/- 176.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=458000, episode_reward=1419.00 +/- 177.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=459000, episode_reward=1631.00 +/- 72.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=460000, episode_reward=1535.60 +/- 268.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=461000, episode_reward=1604.40 +/- 191.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=462000, episode_reward=1399.60 +/- 126.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=463000, episode_reward=1524.40 +/- 199.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=464000, episode_reward=1355.60 +/- 226.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=465000, episode_reward=1544.40 +/- 247.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=466000, episode_reward=1243.00 +/- 348.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=467000, episode_reward=1585.20 +/- 112.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=468000, episode_reward=1668.80 +/- 164.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=469000, episode_reward=1696.20 +/- 192.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=470000, episode_reward=1506.40 +/- 112.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=471000, episode_reward=1718.60 +/- 116.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=472000, episode_reward=1289.40 +/- 252.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=473000, episode_reward=1529.80 +/- 224.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=474000, episode_reward=1563.60 +/- 132.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=475000, episode_reward=1572.40 +/- 429.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=476000, episode_reward=1679.40 +/- 257.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=477000, episode_reward=1432.80 +/- 120.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=478000, episode_reward=1697.00 +/- 233.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=479000, episode_reward=1728.80 +/- 213.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=480000, episode_reward=1639.40 +/- 99.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=481000, episode_reward=1549.40 +/- 101.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=482000, episode_reward=1645.80 +/- 256.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=483000, episode_reward=1486.20 +/- 49.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=484000, episode_reward=1662.60 +/- 220.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=485000, episode_reward=1373.60 +/- 286.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=486000, episode_reward=1665.40 +/- 197.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=487000, episode_reward=1494.20 +/- 166.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=488000, episode_reward=1651.40 +/- 180.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=489000, episode_reward=1405.00 +/- 157.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=490000, episode_reward=1544.00 +/- 141.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=491000, episode_reward=1641.20 +/- 260.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=492000, episode_reward=1542.00 +/- 153.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=493000, episode_reward=1574.40 +/- 137.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=494000, episode_reward=1577.20 +/- 243.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=495000, episode_reward=1561.40 +/- 299.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=496000, episode_reward=1464.00 +/- 255.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=497000, episode_reward=1466.60 +/- 192.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=498000, episode_reward=1519.40 +/- 202.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=499000, episode_reward=1723.20 +/- 268.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=500000, episode_reward=1678.00 +/- 168.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=501000, episode_reward=1604.80 +/- 213.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Gamma = 0.99, Learning Rate = 0.003, Activation Function = Tanh, Neurons = 32, Process noise = 0.1, Measurement noise = 0.0\n",
      "Eval num_timesteps=1000, episode_reward=-526.80 +/- 177.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=-342.00 +/- 162.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3000, episode_reward=37.80 +/- 143.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4000, episode_reward=-37.80 +/- 321.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=5000, episode_reward=80.40 +/- 107.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=6000, episode_reward=270.80 +/- 125.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=7000, episode_reward=771.20 +/- 179.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=8000, episode_reward=905.60 +/- 150.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=9000, episode_reward=493.60 +/- 365.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=10000, episode_reward=598.60 +/- 316.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=11000, episode_reward=360.20 +/- 158.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=12000, episode_reward=307.60 +/- 101.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=13000, episode_reward=380.00 +/- 166.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=14000, episode_reward=347.00 +/- 161.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=15000, episode_reward=301.80 +/- 213.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=16000, episode_reward=260.40 +/- 234.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=17000, episode_reward=409.00 +/- 161.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=18000, episode_reward=375.00 +/- 209.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=19000, episode_reward=482.40 +/- 105.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=20000, episode_reward=238.60 +/- 176.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=21000, episode_reward=612.40 +/- 277.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=22000, episode_reward=617.80 +/- 212.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=23000, episode_reward=1351.60 +/- 415.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=24000, episode_reward=1092.80 +/- 192.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=25000, episode_reward=1210.60 +/- 214.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=26000, episode_reward=957.80 +/- 329.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=27000, episode_reward=1119.80 +/- 375.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=28000, episode_reward=1076.80 +/- 188.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=29000, episode_reward=1207.20 +/- 305.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=30000, episode_reward=-257.00 +/- 2585.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=31000, episode_reward=170.00 +/- 1885.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=32000, episode_reward=1240.40 +/- 240.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=33000, episode_reward=1366.00 +/- 189.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=34000, episode_reward=1366.20 +/- 166.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=35000, episode_reward=-814.40 +/- 4342.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=36000, episode_reward=1257.20 +/- 297.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=37000, episode_reward=1523.00 +/- 132.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=38000, episode_reward=1125.00 +/- 196.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=39000, episode_reward=883.40 +/- 258.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=40000, episode_reward=977.40 +/- 467.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=41000, episode_reward=916.60 +/- 276.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=42000, episode_reward=962.20 +/- 386.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=43000, episode_reward=918.40 +/- 242.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=44000, episode_reward=1020.00 +/- 232.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=45000, episode_reward=999.60 +/- 162.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=46000, episode_reward=1239.40 +/- 314.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=47000, episode_reward=930.80 +/- 252.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=48000, episode_reward=1306.40 +/- 260.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=49000, episode_reward=1213.20 +/- 407.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=50000, episode_reward=1529.80 +/- 176.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=51000, episode_reward=1615.00 +/- 207.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=52000, episode_reward=1245.60 +/- 198.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=53000, episode_reward=1388.40 +/- 125.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=54000, episode_reward=1388.20 +/- 228.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=55000, episode_reward=1562.80 +/- 569.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=56000, episode_reward=1450.60 +/- 410.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=57000, episode_reward=-750.40 +/- 4419.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=58000, episode_reward=-1140.40 +/- 4893.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=59000, episode_reward=1466.80 +/- 191.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=60000, episode_reward=1485.20 +/- 284.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=61000, episode_reward=1591.60 +/- 165.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=62000, episode_reward=1401.40 +/- 335.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=63000, episode_reward=1809.40 +/- 102.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=64000, episode_reward=1412.00 +/- 239.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=65000, episode_reward=1625.60 +/- 284.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=66000, episode_reward=1620.00 +/- 157.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=67000, episode_reward=1412.20 +/- 308.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=68000, episode_reward=1570.80 +/- 309.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=69000, episode_reward=1502.40 +/- 222.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=70000, episode_reward=1489.80 +/- 312.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=71000, episode_reward=1775.40 +/- 99.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=72000, episode_reward=1808.60 +/- 218.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=73000, episode_reward=1703.80 +/- 163.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=74000, episode_reward=1752.20 +/- 313.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=75000, episode_reward=1694.00 +/- 155.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=76000, episode_reward=1735.20 +/- 188.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=77000, episode_reward=1687.60 +/- 85.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=78000, episode_reward=1593.20 +/- 69.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=79000, episode_reward=1701.20 +/- 71.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=80000, episode_reward=1451.00 +/- 110.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=81000, episode_reward=1327.60 +/- 254.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=82000, episode_reward=1661.80 +/- 161.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=83000, episode_reward=1568.40 +/- 179.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=84000, episode_reward=1551.00 +/- 133.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=85000, episode_reward=1742.40 +/- 141.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=86000, episode_reward=1526.20 +/- 167.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=87000, episode_reward=1891.80 +/- 74.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=88000, episode_reward=1457.80 +/- 276.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=89000, episode_reward=1618.80 +/- 335.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=90000, episode_reward=1577.40 +/- 256.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=91000, episode_reward=1509.00 +/- 190.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=92000, episode_reward=1471.00 +/- 244.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=93000, episode_reward=1753.80 +/- 195.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=94000, episode_reward=1718.60 +/- 281.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=95000, episode_reward=1518.20 +/- 253.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=96000, episode_reward=1612.60 +/- 230.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=97000, episode_reward=1531.20 +/- 285.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=98000, episode_reward=1636.20 +/- 97.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=99000, episode_reward=1550.40 +/- 131.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=100000, episode_reward=1658.20 +/- 148.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=101000, episode_reward=1564.00 +/- 136.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=102000, episode_reward=1559.20 +/- 296.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=103000, episode_reward=1747.00 +/- 200.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=104000, episode_reward=1789.00 +/- 77.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=105000, episode_reward=1789.80 +/- 75.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=106000, episode_reward=1655.80 +/- 219.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=107000, episode_reward=1749.60 +/- 186.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=108000, episode_reward=1620.80 +/- 182.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=109000, episode_reward=1306.20 +/- 440.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=110000, episode_reward=1686.20 +/- 240.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=111000, episode_reward=1684.20 +/- 136.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=112000, episode_reward=1532.40 +/- 221.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=113000, episode_reward=1503.40 +/- 92.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=114000, episode_reward=1750.40 +/- 91.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=115000, episode_reward=1763.40 +/- 83.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=116000, episode_reward=1781.80 +/- 112.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=117000, episode_reward=1573.60 +/- 265.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=118000, episode_reward=1595.20 +/- 226.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=119000, episode_reward=1676.40 +/- 69.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=120000, episode_reward=1618.20 +/- 225.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=121000, episode_reward=1618.60 +/- 170.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=122000, episode_reward=1731.80 +/- 138.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=123000, episode_reward=1520.80 +/- 312.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=124000, episode_reward=1393.00 +/- 195.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=125000, episode_reward=1685.40 +/- 123.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=126000, episode_reward=1658.40 +/- 169.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=127000, episode_reward=1604.60 +/- 254.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=128000, episode_reward=1678.20 +/- 169.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=129000, episode_reward=1700.20 +/- 246.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=130000, episode_reward=1429.00 +/- 253.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=131000, episode_reward=1562.00 +/- 230.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=132000, episode_reward=1606.80 +/- 189.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=133000, episode_reward=1690.60 +/- 95.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=134000, episode_reward=1681.60 +/- 135.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=135000, episode_reward=1565.80 +/- 236.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=136000, episode_reward=1749.00 +/- 200.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=137000, episode_reward=1572.20 +/- 85.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=138000, episode_reward=1667.80 +/- 254.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=139000, episode_reward=1607.40 +/- 335.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=140000, episode_reward=1778.60 +/- 188.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=141000, episode_reward=1710.60 +/- 185.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=142000, episode_reward=1520.40 +/- 218.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=143000, episode_reward=1715.40 +/- 200.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=144000, episode_reward=1491.00 +/- 84.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=145000, episode_reward=1366.60 +/- 294.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=146000, episode_reward=1723.80 +/- 370.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=147000, episode_reward=1628.80 +/- 220.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=148000, episode_reward=1761.80 +/- 113.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=149000, episode_reward=1576.20 +/- 154.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=150000, episode_reward=1612.00 +/- 290.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=151000, episode_reward=1454.20 +/- 208.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=152000, episode_reward=1650.00 +/- 192.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=153000, episode_reward=1811.40 +/- 113.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=154000, episode_reward=1610.80 +/- 163.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=155000, episode_reward=1666.20 +/- 255.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=156000, episode_reward=1728.60 +/- 138.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=157000, episode_reward=1578.00 +/- 267.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=158000, episode_reward=1626.80 +/- 256.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=159000, episode_reward=1706.40 +/- 205.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=160000, episode_reward=1722.20 +/- 92.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=161000, episode_reward=1718.20 +/- 248.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=162000, episode_reward=1543.60 +/- 197.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=163000, episode_reward=1702.80 +/- 250.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=164000, episode_reward=1696.40 +/- 95.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=165000, episode_reward=1629.60 +/- 133.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=166000, episode_reward=1685.40 +/- 303.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=167000, episode_reward=1722.40 +/- 237.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=168000, episode_reward=1655.60 +/- 98.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=169000, episode_reward=1716.40 +/- 99.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=170000, episode_reward=1429.60 +/- 115.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=171000, episode_reward=1534.40 +/- 246.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=172000, episode_reward=1483.60 +/- 177.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=173000, episode_reward=1571.60 +/- 291.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=174000, episode_reward=1562.40 +/- 119.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=175000, episode_reward=1587.20 +/- 263.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=176000, episode_reward=1673.60 +/- 181.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=177000, episode_reward=1624.60 +/- 90.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=178000, episode_reward=1560.80 +/- 195.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=179000, episode_reward=1637.40 +/- 210.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=180000, episode_reward=1444.80 +/- 230.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=181000, episode_reward=1722.60 +/- 272.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=182000, episode_reward=1690.00 +/- 202.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=183000, episode_reward=1609.80 +/- 291.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=184000, episode_reward=1618.60 +/- 136.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=185000, episode_reward=1356.80 +/- 176.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=186000, episode_reward=1614.20 +/- 241.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=187000, episode_reward=1680.40 +/- 186.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=188000, episode_reward=1691.20 +/- 210.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=189000, episode_reward=1727.20 +/- 304.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=190000, episode_reward=1630.60 +/- 186.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=191000, episode_reward=1727.00 +/- 185.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=192000, episode_reward=1650.60 +/- 245.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=193000, episode_reward=1754.20 +/- 201.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=194000, episode_reward=1800.20 +/- 211.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=195000, episode_reward=1693.60 +/- 336.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=196000, episode_reward=1524.80 +/- 260.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=197000, episode_reward=1742.80 +/- 146.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=198000, episode_reward=1614.40 +/- 298.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=199000, episode_reward=1463.20 +/- 437.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=200000, episode_reward=1569.80 +/- 452.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=201000, episode_reward=1537.40 +/- 170.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=202000, episode_reward=1522.20 +/- 161.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=203000, episode_reward=1727.60 +/- 127.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=204000, episode_reward=1690.40 +/- 185.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=205000, episode_reward=1605.40 +/- 173.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=206000, episode_reward=1717.60 +/- 113.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=207000, episode_reward=1674.00 +/- 177.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=208000, episode_reward=1619.80 +/- 249.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=209000, episode_reward=1441.40 +/- 179.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=210000, episode_reward=1736.80 +/- 167.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=211000, episode_reward=1623.80 +/- 100.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=212000, episode_reward=1370.60 +/- 179.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=213000, episode_reward=1527.40 +/- 220.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=214000, episode_reward=1461.40 +/- 133.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=215000, episode_reward=1657.40 +/- 182.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=216000, episode_reward=1588.80 +/- 184.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=217000, episode_reward=1542.60 +/- 112.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=218000, episode_reward=1568.40 +/- 105.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=219000, episode_reward=1478.80 +/- 538.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=220000, episode_reward=1551.60 +/- 182.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=221000, episode_reward=1551.00 +/- 349.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=222000, episode_reward=1706.80 +/- 187.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=223000, episode_reward=1517.40 +/- 212.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=224000, episode_reward=1568.40 +/- 220.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=225000, episode_reward=1755.40 +/- 105.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=226000, episode_reward=1509.20 +/- 199.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=227000, episode_reward=1579.60 +/- 178.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=228000, episode_reward=1468.40 +/- 241.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=229000, episode_reward=1695.00 +/- 224.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=230000, episode_reward=1465.00 +/- 133.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=231000, episode_reward=1452.20 +/- 128.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=232000, episode_reward=1576.60 +/- 290.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=233000, episode_reward=1538.40 +/- 225.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=234000, episode_reward=1940.80 +/- 129.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=235000, episode_reward=1460.20 +/- 224.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=236000, episode_reward=1656.00 +/- 83.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=237000, episode_reward=1482.60 +/- 130.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=238000, episode_reward=1434.40 +/- 159.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=239000, episode_reward=1466.00 +/- 219.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=240000, episode_reward=1688.20 +/- 240.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=241000, episode_reward=1461.20 +/- 148.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=242000, episode_reward=1491.20 +/- 246.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=243000, episode_reward=1507.20 +/- 323.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=244000, episode_reward=1681.00 +/- 307.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=245000, episode_reward=1479.60 +/- 137.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=246000, episode_reward=1756.20 +/- 185.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=247000, episode_reward=1534.60 +/- 365.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=248000, episode_reward=1333.60 +/- 274.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=249000, episode_reward=1677.60 +/- 161.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=250000, episode_reward=1476.80 +/- 223.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=251000, episode_reward=1557.00 +/- 95.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=252000, episode_reward=1552.00 +/- 351.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=253000, episode_reward=1772.60 +/- 151.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=254000, episode_reward=1698.80 +/- 134.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=255000, episode_reward=1646.20 +/- 105.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=256000, episode_reward=1628.40 +/- 204.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=257000, episode_reward=1723.00 +/- 117.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=258000, episode_reward=1679.40 +/- 189.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=259000, episode_reward=1600.40 +/- 184.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=260000, episode_reward=1407.60 +/- 341.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=261000, episode_reward=1719.20 +/- 305.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=262000, episode_reward=1774.20 +/- 129.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=263000, episode_reward=1646.00 +/- 174.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=264000, episode_reward=1539.00 +/- 119.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=265000, episode_reward=1552.00 +/- 278.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=266000, episode_reward=1415.00 +/- 130.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=267000, episode_reward=1580.40 +/- 175.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=268000, episode_reward=1618.60 +/- 151.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=269000, episode_reward=1758.00 +/- 88.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=270000, episode_reward=1487.20 +/- 148.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=271000, episode_reward=1517.80 +/- 315.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=272000, episode_reward=1638.60 +/- 267.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=273000, episode_reward=1743.80 +/- 101.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=274000, episode_reward=1438.60 +/- 285.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=275000, episode_reward=1617.60 +/- 312.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=276000, episode_reward=1572.00 +/- 149.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=277000, episode_reward=1545.80 +/- 101.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=278000, episode_reward=1638.60 +/- 133.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=279000, episode_reward=1496.80 +/- 245.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=280000, episode_reward=1663.20 +/- 181.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=281000, episode_reward=1796.00 +/- 214.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=282000, episode_reward=1594.20 +/- 153.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=283000, episode_reward=1482.40 +/- 69.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=284000, episode_reward=1526.80 +/- 239.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=285000, episode_reward=1490.60 +/- 87.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=286000, episode_reward=1667.40 +/- 151.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=287000, episode_reward=1781.80 +/- 98.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=288000, episode_reward=1614.00 +/- 165.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=289000, episode_reward=1694.60 +/- 78.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=290000, episode_reward=1576.40 +/- 194.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=291000, episode_reward=1496.40 +/- 292.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=292000, episode_reward=1740.20 +/- 155.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=293000, episode_reward=1470.00 +/- 254.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=294000, episode_reward=1495.60 +/- 271.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=295000, episode_reward=1502.00 +/- 266.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=296000, episode_reward=1681.00 +/- 177.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=297000, episode_reward=1582.20 +/- 133.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=298000, episode_reward=1491.40 +/- 193.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=299000, episode_reward=1616.00 +/- 176.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=300000, episode_reward=1393.40 +/- 268.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=301000, episode_reward=1630.60 +/- 206.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=302000, episode_reward=1589.80 +/- 200.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=303000, episode_reward=1757.60 +/- 83.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=304000, episode_reward=1535.40 +/- 277.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=305000, episode_reward=1489.60 +/- 184.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=306000, episode_reward=1678.80 +/- 202.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=307000, episode_reward=1681.40 +/- 186.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=308000, episode_reward=1566.40 +/- 177.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=309000, episode_reward=1521.60 +/- 118.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=310000, episode_reward=1582.40 +/- 131.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=311000, episode_reward=1686.00 +/- 139.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=312000, episode_reward=1530.20 +/- 207.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=313000, episode_reward=1687.80 +/- 289.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=314000, episode_reward=1674.60 +/- 130.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=315000, episode_reward=1500.80 +/- 232.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=316000, episode_reward=1631.00 +/- 130.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=317000, episode_reward=1525.80 +/- 304.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=318000, episode_reward=1658.80 +/- 176.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=319000, episode_reward=1613.80 +/- 173.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=320000, episode_reward=1604.80 +/- 255.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=321000, episode_reward=1529.00 +/- 140.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=322000, episode_reward=1576.80 +/- 190.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=323000, episode_reward=1491.20 +/- 241.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=324000, episode_reward=1643.60 +/- 175.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=325000, episode_reward=1686.00 +/- 65.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=326000, episode_reward=1726.40 +/- 208.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=327000, episode_reward=1653.40 +/- 163.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=328000, episode_reward=1635.20 +/- 251.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=329000, episode_reward=1659.80 +/- 292.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=330000, episode_reward=1613.60 +/- 233.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=331000, episode_reward=1565.60 +/- 259.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=332000, episode_reward=1717.60 +/- 89.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=333000, episode_reward=1486.00 +/- 74.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=334000, episode_reward=1693.00 +/- 285.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=335000, episode_reward=1630.40 +/- 193.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=336000, episode_reward=1655.20 +/- 212.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=337000, episode_reward=1665.00 +/- 186.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=338000, episode_reward=1668.40 +/- 218.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=339000, episode_reward=1539.20 +/- 259.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=340000, episode_reward=1656.80 +/- 130.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=341000, episode_reward=1740.80 +/- 105.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=342000, episode_reward=1757.60 +/- 112.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=343000, episode_reward=1622.00 +/- 171.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=344000, episode_reward=1789.40 +/- 70.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=345000, episode_reward=1758.00 +/- 51.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=346000, episode_reward=1692.00 +/- 141.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=347000, episode_reward=1510.00 +/- 122.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=348000, episode_reward=1729.00 +/- 263.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=349000, episode_reward=1660.80 +/- 191.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=350000, episode_reward=1784.20 +/- 198.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=351000, episode_reward=1583.00 +/- 198.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=352000, episode_reward=1488.60 +/- 167.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=353000, episode_reward=1526.60 +/- 120.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=354000, episode_reward=1580.60 +/- 112.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=355000, episode_reward=1718.00 +/- 144.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=356000, episode_reward=1616.00 +/- 140.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=357000, episode_reward=1693.60 +/- 182.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=358000, episode_reward=-1526.40 +/- 6072.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=359000, episode_reward=1683.20 +/- 187.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=360000, episode_reward=1680.60 +/- 164.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=361000, episode_reward=1581.40 +/- 101.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=362000, episode_reward=1647.60 +/- 149.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=363000, episode_reward=1700.60 +/- 146.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=364000, episode_reward=1632.00 +/- 237.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=365000, episode_reward=1748.20 +/- 75.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=366000, episode_reward=1387.00 +/- 94.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=367000, episode_reward=1683.20 +/- 144.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=368000, episode_reward=1606.00 +/- 218.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=369000, episode_reward=1697.00 +/- 199.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=370000, episode_reward=1664.00 +/- 224.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=371000, episode_reward=1583.40 +/- 38.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=372000, episode_reward=1627.40 +/- 213.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=373000, episode_reward=1681.40 +/- 162.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=374000, episode_reward=1717.60 +/- 166.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=375000, episode_reward=1535.80 +/- 150.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=376000, episode_reward=1780.00 +/- 182.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=377000, episode_reward=1807.80 +/- 182.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=378000, episode_reward=1624.80 +/- 368.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=379000, episode_reward=1534.00 +/- 128.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=380000, episode_reward=1561.40 +/- 187.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=381000, episode_reward=1616.60 +/- 125.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=382000, episode_reward=1563.20 +/- 343.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=383000, episode_reward=1594.00 +/- 114.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=384000, episode_reward=1535.00 +/- 151.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=385000, episode_reward=1601.00 +/- 246.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=386000, episode_reward=1647.60 +/- 243.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=387000, episode_reward=1597.20 +/- 208.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=388000, episode_reward=1586.40 +/- 92.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=389000, episode_reward=1572.80 +/- 351.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=390000, episode_reward=1736.20 +/- 134.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=391000, episode_reward=1868.40 +/- 130.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=392000, episode_reward=1414.00 +/- 313.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=393000, episode_reward=1682.60 +/- 97.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=394000, episode_reward=1451.00 +/- 241.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=395000, episode_reward=1659.20 +/- 171.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=396000, episode_reward=1386.20 +/- 256.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=397000, episode_reward=1537.60 +/- 176.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=398000, episode_reward=1532.00 +/- 357.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=399000, episode_reward=1682.20 +/- 125.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=400000, episode_reward=1553.60 +/- 258.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=401000, episode_reward=1649.40 +/- 192.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=402000, episode_reward=1726.40 +/- 203.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=403000, episode_reward=1579.60 +/- 244.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=404000, episode_reward=1601.20 +/- 88.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=405000, episode_reward=1504.20 +/- 457.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=406000, episode_reward=1569.00 +/- 201.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=407000, episode_reward=1607.00 +/- 240.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=408000, episode_reward=1448.00 +/- 218.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=409000, episode_reward=1668.80 +/- 243.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=410000, episode_reward=1543.00 +/- 246.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=411000, episode_reward=1725.80 +/- 287.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=412000, episode_reward=1500.80 +/- 240.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=413000, episode_reward=1698.60 +/- 241.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=414000, episode_reward=1543.40 +/- 300.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=415000, episode_reward=1421.20 +/- 268.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=416000, episode_reward=1551.40 +/- 176.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=417000, episode_reward=1560.40 +/- 199.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=418000, episode_reward=1586.20 +/- 306.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=419000, episode_reward=1501.00 +/- 108.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=420000, episode_reward=1814.80 +/- 113.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=421000, episode_reward=1534.20 +/- 82.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=422000, episode_reward=1570.80 +/- 348.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=423000, episode_reward=1618.80 +/- 99.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=424000, episode_reward=1642.60 +/- 270.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=425000, episode_reward=1698.00 +/- 101.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=426000, episode_reward=1575.80 +/- 257.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=427000, episode_reward=1610.00 +/- 260.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=428000, episode_reward=1569.60 +/- 149.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=429000, episode_reward=1702.00 +/- 132.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=430000, episode_reward=1578.80 +/- 271.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=431000, episode_reward=1620.00 +/- 270.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=432000, episode_reward=1778.00 +/- 153.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=433000, episode_reward=1774.20 +/- 112.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=434000, episode_reward=1491.00 +/- 451.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=435000, episode_reward=1594.40 +/- 177.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=436000, episode_reward=1426.60 +/- 325.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=437000, episode_reward=1614.20 +/- 243.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=438000, episode_reward=1469.40 +/- 335.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=439000, episode_reward=1607.80 +/- 168.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=440000, episode_reward=1711.20 +/- 159.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=441000, episode_reward=1536.60 +/- 198.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=442000, episode_reward=1689.20 +/- 168.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=443000, episode_reward=1597.00 +/- 159.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=444000, episode_reward=1608.80 +/- 176.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=445000, episode_reward=1631.40 +/- 226.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=446000, episode_reward=1656.80 +/- 57.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=447000, episode_reward=1809.20 +/- 211.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=448000, episode_reward=1661.00 +/- 303.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=449000, episode_reward=1745.00 +/- 200.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=450000, episode_reward=1648.00 +/- 137.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=451000, episode_reward=1616.80 +/- 228.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=452000, episode_reward=1671.00 +/- 162.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=453000, episode_reward=1366.00 +/- 273.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=454000, episode_reward=1721.20 +/- 171.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=455000, episode_reward=1651.80 +/- 131.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=456000, episode_reward=1534.80 +/- 280.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=457000, episode_reward=1405.00 +/- 339.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=458000, episode_reward=1492.60 +/- 376.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=459000, episode_reward=1531.40 +/- 296.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=460000, episode_reward=1724.40 +/- 165.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=461000, episode_reward=1675.20 +/- 249.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=462000, episode_reward=1362.00 +/- 261.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=463000, episode_reward=1711.40 +/- 104.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=464000, episode_reward=1761.20 +/- 219.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=465000, episode_reward=1603.40 +/- 132.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=466000, episode_reward=1737.20 +/- 125.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=467000, episode_reward=1585.20 +/- 188.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=468000, episode_reward=1617.40 +/- 116.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=469000, episode_reward=1630.80 +/- 194.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=470000, episode_reward=1613.00 +/- 143.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=471000, episode_reward=1502.00 +/- 299.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=472000, episode_reward=1578.80 +/- 184.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=473000, episode_reward=1795.60 +/- 65.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=474000, episode_reward=1807.00 +/- 177.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=475000, episode_reward=1479.80 +/- 122.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=476000, episode_reward=1517.80 +/- 212.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=477000, episode_reward=1617.00 +/- 197.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=478000, episode_reward=1679.40 +/- 125.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=479000, episode_reward=1558.60 +/- 226.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=480000, episode_reward=1623.00 +/- 190.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=481000, episode_reward=1506.20 +/- 138.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=482000, episode_reward=1557.00 +/- 269.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=483000, episode_reward=1614.60 +/- 80.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=484000, episode_reward=1759.20 +/- 243.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=485000, episode_reward=1639.20 +/- 61.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=486000, episode_reward=1660.80 +/- 177.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=487000, episode_reward=1680.20 +/- 111.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=488000, episode_reward=1726.20 +/- 80.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=489000, episode_reward=1647.40 +/- 126.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=490000, episode_reward=1536.20 +/- 153.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=491000, episode_reward=1575.40 +/- 159.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=492000, episode_reward=1774.40 +/- 135.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=493000, episode_reward=1658.00 +/- 196.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=494000, episode_reward=1801.40 +/- 142.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=495000, episode_reward=1508.20 +/- 164.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=496000, episode_reward=1583.20 +/- 80.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=497000, episode_reward=1831.00 +/- 105.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=498000, episode_reward=1446.80 +/- 254.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=499000, episode_reward=1633.80 +/- 198.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=500000, episode_reward=1704.20 +/- 186.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=501000, episode_reward=1562.80 +/- 294.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Gamma = 0.99, Learning Rate = 0.003, Activation Function = Tanh, Neurons = 64, Process noise = 0.1, Measurement noise = 0.0\n",
      "Eval num_timesteps=1000, episode_reward=-4956.60 +/- 1249.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=-4885.00 +/- 1234.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3000, episode_reward=415.80 +/- 182.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4000, episode_reward=205.00 +/- 128.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=5000, episode_reward=221.80 +/- 230.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=6000, episode_reward=227.20 +/- 219.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=7000, episode_reward=281.00 +/- 270.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=8000, episode_reward=309.00 +/- 137.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=9000, episode_reward=289.60 +/- 337.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=10000, episode_reward=288.20 +/- 168.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=11000, episode_reward=226.40 +/- 168.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=12000, episode_reward=323.60 +/- 246.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=13000, episode_reward=263.60 +/- 176.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=14000, episode_reward=358.00 +/- 117.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=15000, episode_reward=83.40 +/- 253.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=16000, episode_reward=115.00 +/- 179.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=17000, episode_reward=313.40 +/- 192.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=18000, episode_reward=233.00 +/- 209.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=19000, episode_reward=167.40 +/- 167.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=20000, episode_reward=517.80 +/- 222.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=21000, episode_reward=544.80 +/- 58.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=22000, episode_reward=525.80 +/- 261.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=23000, episode_reward=708.80 +/- 131.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=24000, episode_reward=361.60 +/- 212.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=25000, episode_reward=877.20 +/- 162.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=26000, episode_reward=1001.60 +/- 365.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=27000, episode_reward=1002.40 +/- 254.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=28000, episode_reward=784.20 +/- 252.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=29000, episode_reward=-1510.60 +/- 4869.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=30000, episode_reward=1128.80 +/- 382.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=31000, episode_reward=1053.20 +/- 209.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=32000, episode_reward=1357.80 +/- 111.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=33000, episode_reward=902.80 +/- 273.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=34000, episode_reward=911.20 +/- 270.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=35000, episode_reward=424.40 +/- 1923.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=36000, episode_reward=1240.20 +/- 171.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=37000, episode_reward=1127.60 +/- 336.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=38000, episode_reward=1266.40 +/- 103.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=39000, episode_reward=1291.20 +/- 203.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=40000, episode_reward=-166.00 +/- 2342.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=41000, episode_reward=1257.00 +/- 132.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=42000, episode_reward=1300.00 +/- 254.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=43000, episode_reward=1229.40 +/- 230.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=44000, episode_reward=1239.00 +/- 134.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=45000, episode_reward=1428.60 +/- 255.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=46000, episode_reward=1242.40 +/- 322.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=47000, episode_reward=1304.60 +/- 261.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=48000, episode_reward=1543.40 +/- 288.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=49000, episode_reward=1349.60 +/- 162.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=50000, episode_reward=1488.00 +/- 237.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=51000, episode_reward=1336.00 +/- 177.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=52000, episode_reward=1471.40 +/- 204.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=53000, episode_reward=1602.20 +/- 143.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=54000, episode_reward=1372.00 +/- 228.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=55000, episode_reward=-2279.20 +/- 7421.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=56000, episode_reward=1345.40 +/- 121.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=57000, episode_reward=1296.60 +/- 115.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=58000, episode_reward=1652.40 +/- 190.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=59000, episode_reward=1372.20 +/- 299.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=60000, episode_reward=1590.60 +/- 154.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=61000, episode_reward=1412.60 +/- 184.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=62000, episode_reward=1641.40 +/- 208.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=63000, episode_reward=1546.40 +/- 183.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=64000, episode_reward=1656.60 +/- 169.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=65000, episode_reward=1434.20 +/- 262.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=66000, episode_reward=1807.20 +/- 77.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=67000, episode_reward=1572.60 +/- 158.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=68000, episode_reward=-2270.00 +/- 7460.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=69000, episode_reward=1631.20 +/- 150.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=70000, episode_reward=1331.40 +/- 252.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=71000, episode_reward=1540.20 +/- 201.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=72000, episode_reward=1405.40 +/- 200.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=73000, episode_reward=1503.20 +/- 136.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=74000, episode_reward=1353.80 +/- 240.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=75000, episode_reward=1431.40 +/- 216.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=76000, episode_reward=1707.40 +/- 256.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=77000, episode_reward=1536.60 +/- 103.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=78000, episode_reward=-1133.40 +/- 5305.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=79000, episode_reward=-2039.60 +/- 6936.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=80000, episode_reward=1469.00 +/- 225.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=81000, episode_reward=1368.60 +/- 250.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=82000, episode_reward=965.00 +/- 1357.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=83000, episode_reward=1498.00 +/- 144.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=84000, episode_reward=1590.40 +/- 145.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=85000, episode_reward=1559.60 +/- 198.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=86000, episode_reward=1569.20 +/- 212.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=87000, episode_reward=1697.80 +/- 177.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=88000, episode_reward=1510.80 +/- 204.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=89000, episode_reward=1543.60 +/- 299.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=90000, episode_reward=1639.40 +/- 157.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=91000, episode_reward=1339.20 +/- 235.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=92000, episode_reward=1673.40 +/- 211.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=93000, episode_reward=1501.80 +/- 206.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=94000, episode_reward=1600.40 +/- 223.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=95000, episode_reward=1547.20 +/- 204.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=96000, episode_reward=1706.60 +/- 157.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=97000, episode_reward=1581.40 +/- 318.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=98000, episode_reward=1678.80 +/- 137.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=99000, episode_reward=1636.80 +/- 221.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=100000, episode_reward=1475.20 +/- 189.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=101000, episode_reward=1635.60 +/- 236.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=102000, episode_reward=1584.80 +/- 248.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=103000, episode_reward=1592.00 +/- 124.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=104000, episode_reward=1688.60 +/- 158.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=105000, episode_reward=1557.60 +/- 239.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=106000, episode_reward=1405.00 +/- 132.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=107000, episode_reward=1409.40 +/- 295.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=108000, episode_reward=1538.80 +/- 296.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=109000, episode_reward=1787.60 +/- 232.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=110000, episode_reward=1650.00 +/- 204.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=111000, episode_reward=1604.80 +/- 121.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=112000, episode_reward=1388.80 +/- 257.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=113000, episode_reward=1513.40 +/- 231.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=114000, episode_reward=1688.00 +/- 88.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=115000, episode_reward=1448.40 +/- 173.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=116000, episode_reward=1507.60 +/- 171.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=117000, episode_reward=1634.00 +/- 161.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=118000, episode_reward=1751.60 +/- 136.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=119000, episode_reward=1648.60 +/- 121.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=120000, episode_reward=1707.40 +/- 287.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=121000, episode_reward=1714.40 +/- 149.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=122000, episode_reward=1745.40 +/- 178.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=123000, episode_reward=1378.20 +/- 157.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=124000, episode_reward=1582.40 +/- 302.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=125000, episode_reward=1730.00 +/- 95.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=126000, episode_reward=1621.20 +/- 84.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=127000, episode_reward=1576.40 +/- 156.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=128000, episode_reward=1604.60 +/- 137.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=129000, episode_reward=1463.00 +/- 209.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=130000, episode_reward=1471.80 +/- 146.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=131000, episode_reward=1541.20 +/- 351.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=132000, episode_reward=1553.00 +/- 183.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=133000, episode_reward=1659.00 +/- 143.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=134000, episode_reward=1501.60 +/- 213.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=135000, episode_reward=1421.40 +/- 240.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=136000, episode_reward=1660.40 +/- 153.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=137000, episode_reward=1543.80 +/- 131.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=138000, episode_reward=1588.40 +/- 101.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=139000, episode_reward=1618.40 +/- 126.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=140000, episode_reward=1554.40 +/- 30.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=141000, episode_reward=1716.60 +/- 148.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=142000, episode_reward=1594.20 +/- 92.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=143000, episode_reward=1379.40 +/- 117.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=144000, episode_reward=1625.80 +/- 201.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=145000, episode_reward=1601.40 +/- 147.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=146000, episode_reward=1741.40 +/- 134.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=147000, episode_reward=1638.20 +/- 170.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=148000, episode_reward=1576.20 +/- 232.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=149000, episode_reward=1462.80 +/- 255.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=150000, episode_reward=1602.00 +/- 261.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=151000, episode_reward=1625.80 +/- 178.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=152000, episode_reward=1500.00 +/- 105.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=153000, episode_reward=1517.00 +/- 168.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=154000, episode_reward=1513.40 +/- 197.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=155000, episode_reward=1712.40 +/- 158.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=156000, episode_reward=1757.20 +/- 189.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=157000, episode_reward=1633.20 +/- 308.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=158000, episode_reward=1494.00 +/- 159.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=159000, episode_reward=1483.40 +/- 223.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=160000, episode_reward=1538.80 +/- 320.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=161000, episode_reward=1559.80 +/- 123.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=162000, episode_reward=1419.60 +/- 305.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=163000, episode_reward=1420.00 +/- 119.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=164000, episode_reward=1587.60 +/- 234.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=165000, episode_reward=1523.80 +/- 54.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=166000, episode_reward=1456.60 +/- 217.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=167000, episode_reward=1711.80 +/- 201.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=168000, episode_reward=1498.00 +/- 119.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=169000, episode_reward=1321.40 +/- 127.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=170000, episode_reward=1569.80 +/- 147.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=171000, episode_reward=1451.00 +/- 83.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=172000, episode_reward=1564.00 +/- 78.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=173000, episode_reward=1490.00 +/- 116.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=174000, episode_reward=1585.80 +/- 201.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=175000, episode_reward=1496.40 +/- 57.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=176000, episode_reward=1604.80 +/- 66.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=177000, episode_reward=1578.40 +/- 142.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=178000, episode_reward=1427.20 +/- 141.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=179000, episode_reward=1568.40 +/- 224.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=180000, episode_reward=1567.40 +/- 235.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=181000, episode_reward=1479.40 +/- 250.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=182000, episode_reward=1496.60 +/- 159.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=183000, episode_reward=1569.20 +/- 90.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=184000, episode_reward=1680.60 +/- 166.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=185000, episode_reward=1607.60 +/- 102.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=186000, episode_reward=1495.60 +/- 248.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=187000, episode_reward=1393.00 +/- 232.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=188000, episode_reward=1506.80 +/- 221.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=189000, episode_reward=1646.60 +/- 178.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=190000, episode_reward=1677.20 +/- 112.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=191000, episode_reward=1502.40 +/- 222.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=192000, episode_reward=1440.60 +/- 217.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=193000, episode_reward=1620.00 +/- 100.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=194000, episode_reward=1667.80 +/- 121.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=195000, episode_reward=1653.00 +/- 268.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=196000, episode_reward=1543.40 +/- 212.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=197000, episode_reward=1729.20 +/- 210.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=198000, episode_reward=1691.60 +/- 100.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=199000, episode_reward=1443.20 +/- 221.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=200000, episode_reward=1560.60 +/- 188.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=201000, episode_reward=1584.20 +/- 221.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=202000, episode_reward=1427.00 +/- 159.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=203000, episode_reward=1546.60 +/- 321.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=204000, episode_reward=1566.80 +/- 242.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=205000, episode_reward=1728.00 +/- 195.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=206000, episode_reward=1673.20 +/- 197.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=207000, episode_reward=1638.00 +/- 164.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=208000, episode_reward=1584.60 +/- 171.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=209000, episode_reward=1557.60 +/- 227.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=210000, episode_reward=1455.80 +/- 231.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=211000, episode_reward=1547.20 +/- 183.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=212000, episode_reward=1630.60 +/- 126.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=213000, episode_reward=1712.40 +/- 87.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=214000, episode_reward=1613.40 +/- 277.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=215000, episode_reward=1743.20 +/- 246.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=216000, episode_reward=1445.80 +/- 276.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=217000, episode_reward=1749.60 +/- 145.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=218000, episode_reward=1509.60 +/- 178.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=219000, episode_reward=1702.20 +/- 165.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=220000, episode_reward=1651.20 +/- 220.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=221000, episode_reward=1623.20 +/- 174.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=222000, episode_reward=1504.80 +/- 188.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=223000, episode_reward=1602.40 +/- 85.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=224000, episode_reward=1529.00 +/- 315.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=225000, episode_reward=1618.00 +/- 235.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=226000, episode_reward=1591.60 +/- 143.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=227000, episode_reward=1487.20 +/- 286.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=228000, episode_reward=1450.60 +/- 185.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=229000, episode_reward=1700.00 +/- 73.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=230000, episode_reward=1451.00 +/- 170.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=231000, episode_reward=1717.20 +/- 125.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=232000, episode_reward=1541.40 +/- 268.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=233000, episode_reward=1443.20 +/- 207.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=234000, episode_reward=1563.40 +/- 236.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=235000, episode_reward=1672.40 +/- 181.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=236000, episode_reward=1499.60 +/- 323.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=237000, episode_reward=1563.00 +/- 154.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=238000, episode_reward=1619.80 +/- 288.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=239000, episode_reward=1549.00 +/- 52.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=240000, episode_reward=1514.80 +/- 240.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=241000, episode_reward=1496.00 +/- 98.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=242000, episode_reward=1482.00 +/- 149.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=243000, episode_reward=1500.80 +/- 104.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=244000, episode_reward=1545.00 +/- 244.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=245000, episode_reward=1613.40 +/- 379.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=246000, episode_reward=1590.60 +/- 137.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=247000, episode_reward=1655.00 +/- 187.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=248000, episode_reward=1689.20 +/- 95.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=249000, episode_reward=1649.00 +/- 214.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=250000, episode_reward=1637.40 +/- 213.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=251000, episode_reward=1618.80 +/- 208.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=252000, episode_reward=1692.00 +/- 195.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=253000, episode_reward=1625.20 +/- 74.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=254000, episode_reward=1545.00 +/- 95.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=255000, episode_reward=1460.00 +/- 198.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=256000, episode_reward=1617.00 +/- 163.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=257000, episode_reward=1469.40 +/- 291.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=258000, episode_reward=1637.20 +/- 137.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=259000, episode_reward=1596.40 +/- 237.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=260000, episode_reward=1379.80 +/- 256.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=261000, episode_reward=1548.00 +/- 122.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=262000, episode_reward=1632.60 +/- 165.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=263000, episode_reward=1600.20 +/- 231.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=264000, episode_reward=1570.60 +/- 209.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=265000, episode_reward=1701.00 +/- 58.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=266000, episode_reward=1663.40 +/- 181.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=267000, episode_reward=1629.40 +/- 118.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=268000, episode_reward=1450.60 +/- 169.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=269000, episode_reward=1573.00 +/- 187.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=270000, episode_reward=1610.80 +/- 105.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=271000, episode_reward=1675.80 +/- 195.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=272000, episode_reward=1563.80 +/- 102.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=273000, episode_reward=1589.40 +/- 151.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=274000, episode_reward=1404.60 +/- 232.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=275000, episode_reward=1538.20 +/- 144.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=276000, episode_reward=1548.20 +/- 221.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=277000, episode_reward=1624.80 +/- 170.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=278000, episode_reward=1490.80 +/- 126.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=279000, episode_reward=1633.60 +/- 173.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=280000, episode_reward=1521.00 +/- 205.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=281000, episode_reward=1528.20 +/- 188.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=282000, episode_reward=1601.20 +/- 134.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=283000, episode_reward=1494.20 +/- 169.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=284000, episode_reward=1572.60 +/- 172.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=285000, episode_reward=1619.60 +/- 93.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=286000, episode_reward=1446.60 +/- 180.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=287000, episode_reward=1758.00 +/- 252.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=288000, episode_reward=1514.20 +/- 146.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=289000, episode_reward=1390.20 +/- 170.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=290000, episode_reward=1675.40 +/- 125.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=291000, episode_reward=1619.00 +/- 322.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=292000, episode_reward=1530.00 +/- 106.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=293000, episode_reward=1706.00 +/- 79.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=294000, episode_reward=1675.60 +/- 187.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=295000, episode_reward=1610.40 +/- 352.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=296000, episode_reward=1759.60 +/- 115.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=297000, episode_reward=1693.20 +/- 236.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=298000, episode_reward=1631.20 +/- 135.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=299000, episode_reward=1660.20 +/- 317.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=300000, episode_reward=1985.80 +/- 175.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=301000, episode_reward=1343.40 +/- 130.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=302000, episode_reward=1633.00 +/- 230.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=303000, episode_reward=1752.60 +/- 188.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=304000, episode_reward=1643.80 +/- 198.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=305000, episode_reward=1733.40 +/- 78.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=306000, episode_reward=1617.80 +/- 208.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=307000, episode_reward=1551.80 +/- 103.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=308000, episode_reward=1730.60 +/- 132.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=309000, episode_reward=1697.40 +/- 151.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=310000, episode_reward=1765.40 +/- 193.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=311000, episode_reward=1726.20 +/- 177.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=312000, episode_reward=1605.60 +/- 126.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=313000, episode_reward=1653.80 +/- 119.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=314000, episode_reward=1719.80 +/- 282.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=315000, episode_reward=1581.20 +/- 235.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=316000, episode_reward=1574.20 +/- 190.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=317000, episode_reward=1548.80 +/- 116.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=318000, episode_reward=1849.80 +/- 92.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=319000, episode_reward=1749.80 +/- 133.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=320000, episode_reward=1730.20 +/- 144.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=321000, episode_reward=1507.00 +/- 389.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=322000, episode_reward=1471.00 +/- 188.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=323000, episode_reward=1723.20 +/- 268.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=324000, episode_reward=1317.00 +/- 157.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=325000, episode_reward=1679.00 +/- 177.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=326000, episode_reward=1694.80 +/- 112.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=327000, episode_reward=1651.80 +/- 178.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=328000, episode_reward=1739.60 +/- 178.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=329000, episode_reward=1670.00 +/- 112.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=330000, episode_reward=1867.60 +/- 82.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=331000, episode_reward=1692.00 +/- 103.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=332000, episode_reward=1721.80 +/- 217.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=333000, episode_reward=1800.40 +/- 76.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=334000, episode_reward=1618.40 +/- 232.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=335000, episode_reward=1661.00 +/- 158.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=336000, episode_reward=1655.00 +/- 209.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=337000, episode_reward=1505.00 +/- 291.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=338000, episode_reward=1537.40 +/- 253.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=339000, episode_reward=1430.80 +/- 312.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=340000, episode_reward=1688.40 +/- 265.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=341000, episode_reward=1543.60 +/- 187.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=342000, episode_reward=1609.60 +/- 462.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=343000, episode_reward=1580.40 +/- 239.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=344000, episode_reward=1541.40 +/- 252.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=345000, episode_reward=1616.60 +/- 182.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=346000, episode_reward=1596.40 +/- 201.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=347000, episode_reward=1661.80 +/- 121.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=348000, episode_reward=1685.80 +/- 95.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=349000, episode_reward=1419.80 +/- 133.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=350000, episode_reward=1657.80 +/- 144.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=351000, episode_reward=1560.00 +/- 372.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=352000, episode_reward=1462.00 +/- 95.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=353000, episode_reward=1583.20 +/- 251.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=354000, episode_reward=1797.80 +/- 171.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=355000, episode_reward=1641.80 +/- 286.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=356000, episode_reward=1579.00 +/- 254.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=357000, episode_reward=1769.20 +/- 194.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=358000, episode_reward=1729.00 +/- 171.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=359000, episode_reward=1434.40 +/- 199.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=360000, episode_reward=1642.60 +/- 194.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=361000, episode_reward=1648.00 +/- 123.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=362000, episode_reward=1606.80 +/- 204.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=363000, episode_reward=1578.60 +/- 272.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=364000, episode_reward=1411.40 +/- 229.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=365000, episode_reward=1717.20 +/- 127.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=366000, episode_reward=1486.40 +/- 255.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=367000, episode_reward=1754.60 +/- 270.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=368000, episode_reward=1830.80 +/- 174.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=369000, episode_reward=1670.00 +/- 255.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=370000, episode_reward=1747.80 +/- 161.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=371000, episode_reward=1514.20 +/- 119.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=372000, episode_reward=1519.40 +/- 386.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=373000, episode_reward=1353.00 +/- 166.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=374000, episode_reward=1624.40 +/- 133.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=375000, episode_reward=1455.60 +/- 214.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=376000, episode_reward=1555.60 +/- 134.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=377000, episode_reward=1666.80 +/- 224.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=378000, episode_reward=1603.60 +/- 216.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=379000, episode_reward=1579.60 +/- 252.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=380000, episode_reward=1649.60 +/- 83.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=381000, episode_reward=1481.40 +/- 343.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=382000, episode_reward=1747.40 +/- 158.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=383000, episode_reward=1618.20 +/- 219.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=384000, episode_reward=1592.20 +/- 134.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=385000, episode_reward=1521.40 +/- 255.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=386000, episode_reward=1701.60 +/- 193.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=387000, episode_reward=1583.00 +/- 352.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=388000, episode_reward=1534.20 +/- 206.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=389000, episode_reward=1427.80 +/- 239.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=390000, episode_reward=1716.00 +/- 230.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=391000, episode_reward=1428.00 +/- 272.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=392000, episode_reward=1706.40 +/- 225.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=393000, episode_reward=1541.00 +/- 229.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=394000, episode_reward=1522.00 +/- 173.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=395000, episode_reward=1704.00 +/- 262.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=396000, episode_reward=1537.20 +/- 108.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=397000, episode_reward=1638.40 +/- 217.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=398000, episode_reward=1612.60 +/- 105.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=399000, episode_reward=1792.80 +/- 208.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=400000, episode_reward=1692.40 +/- 321.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=401000, episode_reward=1752.00 +/- 267.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=402000, episode_reward=1718.00 +/- 144.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=403000, episode_reward=1660.40 +/- 154.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=404000, episode_reward=1644.20 +/- 210.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=405000, episode_reward=1304.60 +/- 145.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=406000, episode_reward=1554.40 +/- 210.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=407000, episode_reward=1531.40 +/- 273.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=408000, episode_reward=1676.60 +/- 128.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=409000, episode_reward=1677.60 +/- 219.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=410000, episode_reward=1502.80 +/- 213.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=411000, episode_reward=1674.80 +/- 131.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=412000, episode_reward=1333.40 +/- 391.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=413000, episode_reward=1763.00 +/- 122.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=414000, episode_reward=1551.80 +/- 190.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=415000, episode_reward=1543.20 +/- 135.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=416000, episode_reward=1614.00 +/- 152.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=417000, episode_reward=1652.60 +/- 145.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=418000, episode_reward=1519.20 +/- 450.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=419000, episode_reward=1582.80 +/- 168.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=420000, episode_reward=1394.60 +/- 253.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=421000, episode_reward=1623.60 +/- 141.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=422000, episode_reward=1578.80 +/- 130.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=423000, episode_reward=1595.60 +/- 126.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=424000, episode_reward=1494.40 +/- 243.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=425000, episode_reward=1554.40 +/- 182.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=426000, episode_reward=1558.20 +/- 173.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=427000, episode_reward=1625.60 +/- 191.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=428000, episode_reward=1505.20 +/- 230.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=429000, episode_reward=1613.40 +/- 151.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=430000, episode_reward=1526.40 +/- 312.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=431000, episode_reward=1383.20 +/- 317.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=432000, episode_reward=1760.40 +/- 278.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=433000, episode_reward=1719.00 +/- 192.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=434000, episode_reward=1581.20 +/- 210.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=435000, episode_reward=1750.40 +/- 267.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=436000, episode_reward=1713.60 +/- 240.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=437000, episode_reward=1736.60 +/- 178.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=438000, episode_reward=1720.20 +/- 71.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=439000, episode_reward=1820.40 +/- 231.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=440000, episode_reward=1565.40 +/- 268.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=441000, episode_reward=1628.40 +/- 220.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=442000, episode_reward=1612.60 +/- 183.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=443000, episode_reward=1586.00 +/- 225.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=444000, episode_reward=1777.60 +/- 306.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=445000, episode_reward=1647.00 +/- 242.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=446000, episode_reward=1683.60 +/- 262.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=447000, episode_reward=1542.60 +/- 129.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=448000, episode_reward=1650.20 +/- 102.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=449000, episode_reward=1542.20 +/- 251.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=450000, episode_reward=1639.00 +/- 136.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=451000, episode_reward=1434.60 +/- 349.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=452000, episode_reward=1638.00 +/- 213.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=453000, episode_reward=1398.80 +/- 213.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=454000, episode_reward=1673.60 +/- 192.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=455000, episode_reward=1462.40 +/- 318.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=456000, episode_reward=1650.20 +/- 209.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=457000, episode_reward=1683.20 +/- 171.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=458000, episode_reward=1740.20 +/- 107.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=459000, episode_reward=1698.00 +/- 190.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=460000, episode_reward=1611.60 +/- 115.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=461000, episode_reward=1693.20 +/- 320.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=462000, episode_reward=1543.60 +/- 190.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=463000, episode_reward=1710.00 +/- 118.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=464000, episode_reward=1719.40 +/- 270.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=465000, episode_reward=1660.20 +/- 110.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=466000, episode_reward=1518.40 +/- 199.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=467000, episode_reward=1714.40 +/- 126.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=468000, episode_reward=1689.40 +/- 188.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=469000, episode_reward=1603.20 +/- 274.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=470000, episode_reward=1664.00 +/- 263.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=471000, episode_reward=1568.60 +/- 118.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=472000, episode_reward=1563.60 +/- 276.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=473000, episode_reward=1705.40 +/- 127.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=474000, episode_reward=1636.80 +/- 70.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=475000, episode_reward=1638.00 +/- 220.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=476000, episode_reward=1502.00 +/- 311.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=477000, episode_reward=1603.80 +/- 266.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=478000, episode_reward=1730.80 +/- 254.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=479000, episode_reward=1917.60 +/- 76.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=480000, episode_reward=1844.40 +/- 168.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=481000, episode_reward=1598.20 +/- 150.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=482000, episode_reward=1434.20 +/- 111.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=483000, episode_reward=1624.80 +/- 328.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=484000, episode_reward=1528.60 +/- 170.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=485000, episode_reward=1697.20 +/- 135.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=486000, episode_reward=1526.60 +/- 331.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=487000, episode_reward=1766.60 +/- 122.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=488000, episode_reward=1891.20 +/- 105.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=489000, episode_reward=1671.40 +/- 312.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=490000, episode_reward=1539.60 +/- 187.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=491000, episode_reward=1551.60 +/- 128.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=492000, episode_reward=1568.20 +/- 168.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=493000, episode_reward=1584.00 +/- 338.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=494000, episode_reward=1640.20 +/- 192.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=495000, episode_reward=1525.20 +/- 253.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=496000, episode_reward=1593.20 +/- 163.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=497000, episode_reward=1708.00 +/- 153.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=498000, episode_reward=1679.00 +/- 164.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=499000, episode_reward=1717.80 +/- 88.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=500000, episode_reward=1730.60 +/- 195.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=501000, episode_reward=1474.00 +/- 341.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Gamma = 0.99, Learning Rate = 0.003, Activation Function = ReLU, Neurons = 8, Process noise = 0.1, Measurement noise = 0.0\n",
      "Eval num_timesteps=1000, episode_reward=-989.80 +/- 255.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=-1020.20 +/- 268.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=3000, episode_reward=-182.40 +/- 354.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4000, episode_reward=-129.80 +/- 425.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=5000, episode_reward=110.00 +/- 339.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=6000, episode_reward=-273.40 +/- 351.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=7000, episode_reward=-66.60 +/- 277.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=8000, episode_reward=163.80 +/- 184.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=9000, episode_reward=52.80 +/- 294.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=10000, episode_reward=56.80 +/- 232.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=11000, episode_reward=22.80 +/- 108.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=12000, episode_reward=-146.00 +/- 464.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=13000, episode_reward=-15.80 +/- 156.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=14000, episode_reward=218.20 +/- 243.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=15000, episode_reward=126.00 +/- 190.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=16000, episode_reward=66.00 +/- 208.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=17000, episode_reward=33.00 +/- 281.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=18000, episode_reward=384.20 +/- 213.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=19000, episode_reward=330.00 +/- 140.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=20000, episode_reward=94.40 +/- 318.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=21000, episode_reward=207.60 +/- 296.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=22000, episode_reward=139.20 +/- 160.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=23000, episode_reward=152.00 +/- 234.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=24000, episode_reward=339.20 +/- 264.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=25000, episode_reward=302.40 +/- 145.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=26000, episode_reward=260.40 +/- 202.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=27000, episode_reward=259.80 +/- 140.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=28000, episode_reward=465.80 +/- 221.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=29000, episode_reward=469.20 +/- 178.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=30000, episode_reward=697.20 +/- 245.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=31000, episode_reward=616.00 +/- 354.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=32000, episode_reward=521.20 +/- 321.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=33000, episode_reward=347.00 +/- 170.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=34000, episode_reward=764.40 +/- 459.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=35000, episode_reward=565.80 +/- 281.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=36000, episode_reward=703.80 +/- 112.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=37000, episode_reward=686.40 +/- 236.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=38000, episode_reward=613.80 +/- 365.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=39000, episode_reward=432.20 +/- 326.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=40000, episode_reward=404.60 +/- 162.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=41000, episode_reward=697.00 +/- 331.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=42000, episode_reward=800.40 +/- 441.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=43000, episode_reward=717.20 +/- 254.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=44000, episode_reward=1044.20 +/- 176.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=45000, episode_reward=776.80 +/- 212.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=46000, episode_reward=1086.80 +/- 239.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=47000, episode_reward=807.40 +/- 350.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=48000, episode_reward=1055.40 +/- 266.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=49000, episode_reward=1285.00 +/- 311.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=50000, episode_reward=1003.80 +/- 168.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=51000, episode_reward=964.80 +/- 240.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=52000, episode_reward=1194.00 +/- 239.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=53000, episode_reward=886.60 +/- 271.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=54000, episode_reward=1224.00 +/- 272.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=55000, episode_reward=1156.60 +/- 147.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=56000, episode_reward=903.00 +/- 402.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=57000, episode_reward=1015.80 +/- 208.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=58000, episode_reward=1200.40 +/- 135.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=59000, episode_reward=1275.80 +/- 433.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=60000, episode_reward=1358.80 +/- 252.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=61000, episode_reward=1252.40 +/- 374.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=62000, episode_reward=1291.40 +/- 251.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=63000, episode_reward=1122.00 +/- 297.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=64000, episode_reward=1302.20 +/- 199.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=65000, episode_reward=1376.20 +/- 272.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=66000, episode_reward=1342.20 +/- 135.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=67000, episode_reward=991.80 +/- 274.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=68000, episode_reward=1269.80 +/- 273.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=69000, episode_reward=1445.20 +/- 185.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=70000, episode_reward=1082.00 +/- 249.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=71000, episode_reward=1030.80 +/- 143.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=72000, episode_reward=1044.80 +/- 218.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=73000, episode_reward=1392.40 +/- 196.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=74000, episode_reward=1201.80 +/- 315.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=75000, episode_reward=1164.60 +/- 143.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=76000, episode_reward=1213.40 +/- 363.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=77000, episode_reward=1364.00 +/- 331.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=78000, episode_reward=1236.00 +/- 286.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=79000, episode_reward=1434.40 +/- 215.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=80000, episode_reward=1169.40 +/- 206.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=81000, episode_reward=1196.60 +/- 234.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=82000, episode_reward=-261.80 +/- 3098.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=83000, episode_reward=1283.80 +/- 240.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=84000, episode_reward=1239.20 +/- 264.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=85000, episode_reward=1217.40 +/- 193.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=86000, episode_reward=1157.40 +/- 133.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=87000, episode_reward=1149.60 +/- 225.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=88000, episode_reward=174.80 +/- 2324.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=89000, episode_reward=1426.00 +/- 281.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=90000, episode_reward=1181.60 +/- 275.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=91000, episode_reward=1305.00 +/- 95.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=92000, episode_reward=1210.60 +/- 312.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=93000, episode_reward=1426.20 +/- 116.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=94000, episode_reward=-559.60 +/- 3906.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=95000, episode_reward=1208.40 +/- 212.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=96000, episode_reward=1111.20 +/- 109.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=97000, episode_reward=1290.80 +/- 248.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=98000, episode_reward=1501.60 +/- 138.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=99000, episode_reward=1283.00 +/- 253.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=100000, episode_reward=1476.00 +/- 150.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=101000, episode_reward=1499.60 +/- 198.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=102000, episode_reward=1170.80 +/- 202.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=103000, episode_reward=1390.40 +/- 314.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=104000, episode_reward=1223.40 +/- 121.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=105000, episode_reward=1237.60 +/- 134.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=106000, episode_reward=1509.20 +/- 224.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=107000, episode_reward=1314.20 +/- 155.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=108000, episode_reward=1266.80 +/- 184.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=109000, episode_reward=1339.00 +/- 165.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=110000, episode_reward=1454.60 +/- 368.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=111000, episode_reward=1439.60 +/- 336.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=112000, episode_reward=1342.20 +/- 250.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=113000, episode_reward=1393.80 +/- 273.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=114000, episode_reward=1488.60 +/- 171.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=115000, episode_reward=1299.20 +/- 255.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=116000, episode_reward=1466.20 +/- 279.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=117000, episode_reward=1348.00 +/- 282.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=118000, episode_reward=1406.40 +/- 135.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=119000, episode_reward=1427.60 +/- 244.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=120000, episode_reward=807.80 +/- 1175.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=121000, episode_reward=1393.00 +/- 249.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=122000, episode_reward=1566.00 +/- 77.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=123000, episode_reward=1345.80 +/- 173.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=124000, episode_reward=1498.00 +/- 300.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=125000, episode_reward=1496.40 +/- 268.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=126000, episode_reward=1425.40 +/- 189.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=127000, episode_reward=1495.80 +/- 163.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=128000, episode_reward=1595.60 +/- 228.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=129000, episode_reward=1562.00 +/- 255.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=130000, episode_reward=1507.40 +/- 110.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=131000, episode_reward=1462.40 +/- 329.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=132000, episode_reward=1482.00 +/- 220.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=133000, episode_reward=1537.60 +/- 131.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=134000, episode_reward=1592.60 +/- 125.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=135000, episode_reward=1660.00 +/- 126.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=136000, episode_reward=1496.80 +/- 243.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=137000, episode_reward=1308.80 +/- 154.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=138000, episode_reward=1567.40 +/- 356.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=139000, episode_reward=1084.80 +/- 218.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=140000, episode_reward=1304.80 +/- 132.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=141000, episode_reward=1408.40 +/- 200.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=142000, episode_reward=1343.60 +/- 145.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=143000, episode_reward=1294.20 +/- 298.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=144000, episode_reward=1462.80 +/- 352.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=145000, episode_reward=1484.40 +/- 130.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=146000, episode_reward=1395.60 +/- 309.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=147000, episode_reward=1488.00 +/- 95.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=148000, episode_reward=1387.40 +/- 96.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=149000, episode_reward=1122.60 +/- 216.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=150000, episode_reward=1576.60 +/- 140.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=151000, episode_reward=1370.60 +/- 197.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=152000, episode_reward=1351.60 +/- 263.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=153000, episode_reward=1559.40 +/- 142.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=154000, episode_reward=1460.40 +/- 256.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=155000, episode_reward=1549.00 +/- 173.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=156000, episode_reward=1612.60 +/- 142.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=157000, episode_reward=1498.20 +/- 226.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=158000, episode_reward=1438.20 +/- 150.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=159000, episode_reward=1394.20 +/- 176.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=160000, episode_reward=1461.00 +/- 116.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=161000, episode_reward=1453.00 +/- 252.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=162000, episode_reward=1473.60 +/- 181.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=163000, episode_reward=1417.00 +/- 241.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=164000, episode_reward=1498.40 +/- 144.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=165000, episode_reward=1452.60 +/- 272.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=166000, episode_reward=1561.00 +/- 108.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=167000, episode_reward=1695.40 +/- 152.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=168000, episode_reward=1504.00 +/- 230.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=169000, episode_reward=1417.40 +/- 377.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=170000, episode_reward=1339.20 +/- 419.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=171000, episode_reward=1295.60 +/- 206.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=172000, episode_reward=1354.00 +/- 236.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=173000, episode_reward=1384.00 +/- 314.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=174000, episode_reward=1213.40 +/- 385.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=175000, episode_reward=1515.00 +/- 455.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=176000, episode_reward=1598.40 +/- 264.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=177000, episode_reward=1429.80 +/- 194.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=178000, episode_reward=1669.80 +/- 114.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=179000, episode_reward=1650.00 +/- 124.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=180000, episode_reward=1490.20 +/- 212.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=181000, episode_reward=1412.40 +/- 188.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=182000, episode_reward=1468.60 +/- 397.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=183000, episode_reward=1310.80 +/- 318.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=184000, episode_reward=1526.80 +/- 214.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=185000, episode_reward=1684.80 +/- 121.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=186000, episode_reward=1613.60 +/- 198.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=187000, episode_reward=1623.40 +/- 152.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=188000, episode_reward=1501.60 +/- 290.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=189000, episode_reward=1418.00 +/- 157.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=190000, episode_reward=1364.20 +/- 455.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=191000, episode_reward=1666.40 +/- 210.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=192000, episode_reward=1733.80 +/- 97.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=193000, episode_reward=1304.80 +/- 236.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=194000, episode_reward=1436.60 +/- 301.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=195000, episode_reward=1514.20 +/- 142.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=196000, episode_reward=1579.20 +/- 177.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=197000, episode_reward=1649.20 +/- 178.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=198000, episode_reward=1766.80 +/- 64.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=199000, episode_reward=1640.20 +/- 217.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=200000, episode_reward=1589.60 +/- 390.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=201000, episode_reward=1561.20 +/- 272.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=202000, episode_reward=1501.20 +/- 306.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=203000, episode_reward=1724.80 +/- 62.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=204000, episode_reward=1484.00 +/- 322.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=205000, episode_reward=1387.20 +/- 313.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=206000, episode_reward=1700.40 +/- 299.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=207000, episode_reward=1504.20 +/- 169.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=208000, episode_reward=1707.00 +/- 105.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=209000, episode_reward=1681.00 +/- 224.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=210000, episode_reward=1688.80 +/- 203.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=211000, episode_reward=1737.60 +/- 267.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=212000, episode_reward=1715.60 +/- 243.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=213000, episode_reward=1686.80 +/- 329.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=214000, episode_reward=1448.40 +/- 229.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=215000, episode_reward=1744.80 +/- 244.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=216000, episode_reward=1626.80 +/- 248.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=217000, episode_reward=1455.20 +/- 248.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=218000, episode_reward=1648.60 +/- 192.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=219000, episode_reward=1621.60 +/- 196.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=220000, episode_reward=1551.00 +/- 192.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=221000, episode_reward=1496.60 +/- 203.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=222000, episode_reward=1692.00 +/- 147.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=223000, episode_reward=1539.60 +/- 350.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=224000, episode_reward=1417.80 +/- 197.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=225000, episode_reward=1700.40 +/- 234.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=226000, episode_reward=1432.20 +/- 229.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=227000, episode_reward=1450.60 +/- 95.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=228000, episode_reward=1682.80 +/- 256.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=229000, episode_reward=1571.00 +/- 215.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=230000, episode_reward=1627.20 +/- 240.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=231000, episode_reward=1446.00 +/- 165.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=232000, episode_reward=1529.60 +/- 209.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=233000, episode_reward=1651.40 +/- 176.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=234000, episode_reward=1490.80 +/- 256.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=235000, episode_reward=1551.80 +/- 307.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=236000, episode_reward=1555.80 +/- 381.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=237000, episode_reward=1642.40 +/- 204.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=238000, episode_reward=1547.40 +/- 391.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=239000, episode_reward=1583.40 +/- 242.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=240000, episode_reward=1726.20 +/- 126.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=241000, episode_reward=1721.40 +/- 134.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=242000, episode_reward=1478.60 +/- 205.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=243000, episode_reward=1468.40 +/- 426.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=244000, episode_reward=1529.60 +/- 165.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=245000, episode_reward=1680.20 +/- 301.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=246000, episode_reward=1669.20 +/- 235.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=247000, episode_reward=1383.80 +/- 304.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=248000, episode_reward=1430.80 +/- 295.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=249000, episode_reward=1718.40 +/- 155.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=250000, episode_reward=1604.60 +/- 90.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=251000, episode_reward=1757.20 +/- 103.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=252000, episode_reward=1511.60 +/- 151.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=253000, episode_reward=1535.80 +/- 181.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=254000, episode_reward=1559.80 +/- 191.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=255000, episode_reward=1549.20 +/- 106.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=256000, episode_reward=1553.20 +/- 182.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=257000, episode_reward=1643.20 +/- 170.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=258000, episode_reward=1491.00 +/- 279.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=259000, episode_reward=1493.40 +/- 241.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=260000, episode_reward=1704.60 +/- 241.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=261000, episode_reward=1642.60 +/- 307.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=262000, episode_reward=1377.00 +/- 83.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=263000, episode_reward=1326.40 +/- 95.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=264000, episode_reward=1520.20 +/- 199.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=265000, episode_reward=1574.20 +/- 83.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=266000, episode_reward=1581.60 +/- 197.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=267000, episode_reward=1818.40 +/- 108.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=268000, episode_reward=1608.20 +/- 185.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=269000, episode_reward=1584.00 +/- 140.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=270000, episode_reward=1354.20 +/- 244.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=271000, episode_reward=1842.20 +/- 73.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=272000, episode_reward=1626.00 +/- 146.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=273000, episode_reward=1661.80 +/- 179.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=274000, episode_reward=1652.40 +/- 225.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=275000, episode_reward=1643.20 +/- 159.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=276000, episode_reward=1484.80 +/- 170.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=277000, episode_reward=1716.80 +/- 139.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=278000, episode_reward=1601.80 +/- 206.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=279000, episode_reward=1699.80 +/- 206.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=280000, episode_reward=1517.00 +/- 94.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=281000, episode_reward=1604.40 +/- 212.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=282000, episode_reward=1526.00 +/- 119.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=283000, episode_reward=1516.80 +/- 150.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=284000, episode_reward=1525.60 +/- 139.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=285000, episode_reward=1503.20 +/- 198.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=286000, episode_reward=1651.60 +/- 160.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=287000, episode_reward=1670.40 +/- 217.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=288000, episode_reward=1658.20 +/- 158.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=289000, episode_reward=1758.60 +/- 79.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=290000, episode_reward=1453.20 +/- 205.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=291000, episode_reward=1434.40 +/- 160.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=292000, episode_reward=1568.60 +/- 113.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=293000, episode_reward=1656.80 +/- 255.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=294000, episode_reward=1321.60 +/- 235.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=295000, episode_reward=1497.60 +/- 154.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=296000, episode_reward=1599.00 +/- 122.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=297000, episode_reward=1459.00 +/- 130.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=298000, episode_reward=1666.40 +/- 144.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=299000, episode_reward=1536.80 +/- 243.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=300000, episode_reward=1441.20 +/- 270.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=301000, episode_reward=1615.60 +/- 145.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=302000, episode_reward=1631.20 +/- 346.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=303000, episode_reward=1421.20 +/- 217.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=304000, episode_reward=1394.40 +/- 280.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=305000, episode_reward=1671.80 +/- 150.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=306000, episode_reward=1645.20 +/- 323.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=307000, episode_reward=1649.80 +/- 36.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=308000, episode_reward=1632.40 +/- 134.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=309000, episode_reward=1504.80 +/- 292.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=310000, episode_reward=1741.60 +/- 211.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=311000, episode_reward=1603.20 +/- 142.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=312000, episode_reward=1541.40 +/- 180.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=313000, episode_reward=1701.20 +/- 277.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=314000, episode_reward=1542.80 +/- 196.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=315000, episode_reward=1467.40 +/- 386.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=316000, episode_reward=1796.20 +/- 125.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=317000, episode_reward=1635.00 +/- 132.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=318000, episode_reward=1481.40 +/- 111.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=319000, episode_reward=1481.00 +/- 314.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=320000, episode_reward=1677.60 +/- 237.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=321000, episode_reward=1801.00 +/- 200.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=322000, episode_reward=1494.60 +/- 300.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=323000, episode_reward=1645.00 +/- 190.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=324000, episode_reward=1630.40 +/- 188.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=325000, episode_reward=1562.00 +/- 217.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=326000, episode_reward=1667.00 +/- 361.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=327000, episode_reward=1416.80 +/- 117.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=328000, episode_reward=1574.80 +/- 243.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=329000, episode_reward=1760.00 +/- 75.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=330000, episode_reward=1647.00 +/- 125.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=331000, episode_reward=1607.40 +/- 129.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=332000, episode_reward=1608.60 +/- 182.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=333000, episode_reward=1678.20 +/- 198.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=334000, episode_reward=1493.80 +/- 132.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=335000, episode_reward=1567.00 +/- 197.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=336000, episode_reward=1597.20 +/- 138.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=337000, episode_reward=1709.40 +/- 30.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=338000, episode_reward=1620.60 +/- 150.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=339000, episode_reward=1315.80 +/- 238.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=340000, episode_reward=1575.80 +/- 208.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=341000, episode_reward=1733.80 +/- 345.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=342000, episode_reward=1534.40 +/- 305.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=343000, episode_reward=1709.20 +/- 82.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=344000, episode_reward=1626.00 +/- 271.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=345000, episode_reward=1616.00 +/- 184.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=346000, episode_reward=1549.60 +/- 228.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=347000, episode_reward=1578.00 +/- 131.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=348000, episode_reward=1766.80 +/- 143.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=349000, episode_reward=1503.60 +/- 241.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=350000, episode_reward=1624.60 +/- 90.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=351000, episode_reward=1650.40 +/- 243.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=352000, episode_reward=1461.00 +/- 319.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=353000, episode_reward=1563.40 +/- 224.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=354000, episode_reward=1759.20 +/- 68.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=355000, episode_reward=1735.00 +/- 205.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=356000, episode_reward=1601.20 +/- 327.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=357000, episode_reward=1735.40 +/- 166.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=358000, episode_reward=1483.80 +/- 254.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=359000, episode_reward=1634.80 +/- 214.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=360000, episode_reward=1647.00 +/- 113.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=361000, episode_reward=1407.60 +/- 170.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=362000, episode_reward=1836.20 +/- 251.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=363000, episode_reward=1713.60 +/- 298.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=364000, episode_reward=1613.80 +/- 150.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=365000, episode_reward=1622.40 +/- 309.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=366000, episode_reward=1606.40 +/- 230.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=367000, episode_reward=1589.40 +/- 160.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=368000, episode_reward=1608.40 +/- 164.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=369000, episode_reward=1579.20 +/- 200.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=370000, episode_reward=1475.80 +/- 325.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=371000, episode_reward=1648.00 +/- 262.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=372000, episode_reward=1656.00 +/- 128.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=373000, episode_reward=1565.60 +/- 174.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=374000, episode_reward=1328.80 +/- 357.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=375000, episode_reward=1544.20 +/- 334.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=376000, episode_reward=1483.40 +/- 265.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=377000, episode_reward=1680.20 +/- 262.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=378000, episode_reward=1444.80 +/- 260.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=379000, episode_reward=1530.20 +/- 332.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=380000, episode_reward=1658.60 +/- 284.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=381000, episode_reward=1420.80 +/- 386.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=382000, episode_reward=1446.80 +/- 284.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=383000, episode_reward=1489.00 +/- 312.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=384000, episode_reward=1512.80 +/- 291.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=385000, episode_reward=1664.20 +/- 188.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=386000, episode_reward=1490.40 +/- 251.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=387000, episode_reward=1596.00 +/- 244.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=388000, episode_reward=1716.80 +/- 345.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=389000, episode_reward=1554.20 +/- 354.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=390000, episode_reward=1823.60 +/- 254.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=391000, episode_reward=1453.60 +/- 314.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=392000, episode_reward=1698.80 +/- 109.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=393000, episode_reward=1744.20 +/- 171.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=394000, episode_reward=1503.80 +/- 127.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=395000, episode_reward=1645.80 +/- 313.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=396000, episode_reward=1606.80 +/- 137.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=397000, episode_reward=1724.60 +/- 242.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=398000, episode_reward=1749.00 +/- 171.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=399000, episode_reward=1766.60 +/- 345.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=400000, episode_reward=1857.40 +/- 129.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=401000, episode_reward=1611.00 +/- 335.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=402000, episode_reward=1610.60 +/- 46.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=403000, episode_reward=1377.80 +/- 276.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=404000, episode_reward=1680.20 +/- 314.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=405000, episode_reward=1734.60 +/- 267.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=406000, episode_reward=1751.80 +/- 171.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=407000, episode_reward=1643.20 +/- 268.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=408000, episode_reward=1714.20 +/- 130.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=409000, episode_reward=1572.80 +/- 136.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=410000, episode_reward=1682.20 +/- 71.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=411000, episode_reward=1674.80 +/- 229.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=412000, episode_reward=1844.00 +/- 152.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=413000, episode_reward=1636.40 +/- 143.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=414000, episode_reward=1635.20 +/- 186.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=415000, episode_reward=1639.60 +/- 247.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=416000, episode_reward=1645.80 +/- 84.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=417000, episode_reward=1429.20 +/- 160.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=418000, episode_reward=1314.80 +/- 369.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=419000, episode_reward=1668.80 +/- 171.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=420000, episode_reward=1611.20 +/- 113.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=421000, episode_reward=1365.80 +/- 353.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=422000, episode_reward=1625.00 +/- 166.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=423000, episode_reward=1522.60 +/- 238.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=424000, episode_reward=1884.00 +/- 102.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=425000, episode_reward=1542.40 +/- 242.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=426000, episode_reward=1565.60 +/- 415.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=427000, episode_reward=1711.40 +/- 198.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=428000, episode_reward=1655.00 +/- 194.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=429000, episode_reward=1806.80 +/- 145.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=430000, episode_reward=1573.00 +/- 120.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=431000, episode_reward=1671.40 +/- 179.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=432000, episode_reward=1615.60 +/- 308.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=433000, episode_reward=1738.80 +/- 114.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=434000, episode_reward=1601.40 +/- 293.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=435000, episode_reward=1496.60 +/- 119.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=436000, episode_reward=1611.00 +/- 110.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=437000, episode_reward=1672.40 +/- 239.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=438000, episode_reward=1615.20 +/- 259.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=439000, episode_reward=1547.60 +/- 352.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=440000, episode_reward=1753.00 +/- 89.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=441000, episode_reward=1499.40 +/- 356.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=442000, episode_reward=1792.00 +/- 170.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=443000, episode_reward=1799.60 +/- 135.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=444000, episode_reward=1672.60 +/- 123.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=445000, episode_reward=1450.40 +/- 208.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=446000, episode_reward=1482.40 +/- 137.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=447000, episode_reward=1655.00 +/- 305.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=448000, episode_reward=1543.00 +/- 343.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=449000, episode_reward=1671.40 +/- 358.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=450000, episode_reward=1606.00 +/- 291.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=451000, episode_reward=1616.00 +/- 135.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=452000, episode_reward=1387.00 +/- 205.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=453000, episode_reward=1668.20 +/- 459.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=454000, episode_reward=1821.80 +/- 181.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=455000, episode_reward=1657.80 +/- 157.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=456000, episode_reward=1770.20 +/- 346.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=457000, episode_reward=1705.80 +/- 140.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=458000, episode_reward=1615.00 +/- 341.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=459000, episode_reward=1726.60 +/- 106.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=460000, episode_reward=1473.60 +/- 144.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=461000, episode_reward=1324.00 +/- 182.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=462000, episode_reward=1674.40 +/- 122.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=463000, episode_reward=1740.80 +/- 69.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=464000, episode_reward=1755.00 +/- 86.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=465000, episode_reward=1708.40 +/- 237.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=466000, episode_reward=1476.80 +/- 188.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=467000, episode_reward=1497.40 +/- 222.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=468000, episode_reward=1699.60 +/- 226.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=469000, episode_reward=1841.40 +/- 114.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=470000, episode_reward=1645.60 +/- 217.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=471000, episode_reward=1454.80 +/- 319.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=472000, episode_reward=1798.00 +/- 100.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=473000, episode_reward=1622.40 +/- 267.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=474000, episode_reward=1854.80 +/- 155.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=475000, episode_reward=1550.20 +/- 172.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=476000, episode_reward=1458.40 +/- 292.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=477000, episode_reward=1670.80 +/- 442.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=478000, episode_reward=1487.60 +/- 177.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=479000, episode_reward=1609.20 +/- 247.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=480000, episode_reward=1651.40 +/- 141.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=481000, episode_reward=1685.20 +/- 303.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=482000, episode_reward=1555.20 +/- 169.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=483000, episode_reward=1608.00 +/- 271.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=484000, episode_reward=1680.60 +/- 66.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=485000, episode_reward=1624.00 +/- 190.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=486000, episode_reward=1723.80 +/- 154.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=487000, episode_reward=1599.80 +/- 319.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=488000, episode_reward=1852.80 +/- 148.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=489000, episode_reward=1641.60 +/- 183.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=490000, episode_reward=1661.60 +/- 168.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=491000, episode_reward=1496.80 +/- 202.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=492000, episode_reward=1417.40 +/- 188.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=493000, episode_reward=1619.00 +/- 267.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=494000, episode_reward=1454.80 +/- 212.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=495000, episode_reward=1590.80 +/- 200.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=496000, episode_reward=1620.00 +/- 280.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=497000, episode_reward=1810.20 +/- 366.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=498000, episode_reward=1373.80 +/- 270.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=499000, episode_reward=1581.40 +/- 178.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=500000, episode_reward=1738.80 +/- 167.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=501000, episode_reward=1670.80 +/- 399.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Gamma = 0.99, Learning Rate = 0.003, Activation Function = ReLU, Neurons = 32, Process noise = 0.1, Measurement noise = 0.0\n",
      "Eval num_timesteps=1000, episode_reward=-19003.00 +/- 58.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=-18948.00 +/- 75.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3000, episode_reward=-115.60 +/- 97.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4000, episode_reward=32.60 +/- 306.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=5000, episode_reward=-12.80 +/- 173.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=6000, episode_reward=67.80 +/- 156.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=7000, episode_reward=83.00 +/- 167.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=8000, episode_reward=104.80 +/- 232.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=9000, episode_reward=255.60 +/- 252.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=10000, episode_reward=23.80 +/- 164.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=11000, episode_reward=332.20 +/- 154.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=12000, episode_reward=227.40 +/- 96.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=13000, episode_reward=263.40 +/- 204.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=14000, episode_reward=392.80 +/- 94.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=15000, episode_reward=335.20 +/- 167.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=16000, episode_reward=333.20 +/- 91.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=17000, episode_reward=351.40 +/- 160.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=18000, episode_reward=372.40 +/- 61.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=19000, episode_reward=357.80 +/- 207.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=20000, episode_reward=235.20 +/- 207.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=21000, episode_reward=370.20 +/- 38.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=22000, episode_reward=305.40 +/- 190.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=23000, episode_reward=526.00 +/- 164.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=24000, episode_reward=388.20 +/- 149.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=25000, episode_reward=254.00 +/- 118.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=26000, episode_reward=383.20 +/- 111.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=27000, episode_reward=81.20 +/- 187.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=28000, episode_reward=221.40 +/- 78.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=29000, episode_reward=377.00 +/- 63.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=30000, episode_reward=268.60 +/- 234.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=31000, episode_reward=1327.80 +/- 417.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=32000, episode_reward=1149.40 +/- 166.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=33000, episode_reward=1204.00 +/- 223.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=34000, episode_reward=1288.00 +/- 297.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=35000, episode_reward=926.80 +/- 339.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=36000, episode_reward=1221.60 +/- 225.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=37000, episode_reward=966.60 +/- 302.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=38000, episode_reward=1013.00 +/- 101.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=39000, episode_reward=1244.00 +/- 209.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=40000, episode_reward=1119.80 +/- 86.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=41000, episode_reward=1106.40 +/- 310.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=42000, episode_reward=1355.80 +/- 165.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=43000, episode_reward=1115.60 +/- 333.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=44000, episode_reward=1199.60 +/- 88.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=45000, episode_reward=1190.00 +/- 216.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=46000, episode_reward=1367.00 +/- 285.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=47000, episode_reward=1300.60 +/- 337.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=48000, episode_reward=1138.00 +/- 275.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=49000, episode_reward=1208.80 +/- 304.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=50000, episode_reward=1122.80 +/- 344.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=51000, episode_reward=1272.40 +/- 227.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=52000, episode_reward=1106.40 +/- 228.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=53000, episode_reward=1360.60 +/- 231.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=54000, episode_reward=1274.60 +/- 203.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=55000, episode_reward=1339.40 +/- 271.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=56000, episode_reward=1381.40 +/- 408.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=57000, episode_reward=-33.80 +/- 2730.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=58000, episode_reward=1512.20 +/- 147.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=59000, episode_reward=1185.60 +/- 100.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=60000, episode_reward=1621.00 +/- 184.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=61000, episode_reward=1388.80 +/- 195.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=62000, episode_reward=236.00 +/- 2679.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=63000, episode_reward=1407.40 +/- 317.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=64000, episode_reward=1443.20 +/- 146.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=65000, episode_reward=1366.40 +/- 349.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=66000, episode_reward=1358.80 +/- 335.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=67000, episode_reward=1434.80 +/- 211.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=68000, episode_reward=1408.20 +/- 64.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=69000, episode_reward=1440.20 +/- 194.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=70000, episode_reward=1402.40 +/- 309.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=71000, episode_reward=1170.00 +/- 285.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=72000, episode_reward=1510.20 +/- 223.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=73000, episode_reward=1415.40 +/- 217.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=74000, episode_reward=1091.20 +/- 214.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=75000, episode_reward=1359.60 +/- 235.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=76000, episode_reward=1463.60 +/- 227.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=77000, episode_reward=1256.00 +/- 220.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=78000, episode_reward=1226.40 +/- 337.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=79000, episode_reward=1717.20 +/- 213.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=80000, episode_reward=1436.00 +/- 89.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=81000, episode_reward=1525.20 +/- 200.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=82000, episode_reward=1571.40 +/- 142.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=83000, episode_reward=1059.00 +/- 143.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=84000, episode_reward=1605.00 +/- 180.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=85000, episode_reward=1476.20 +/- 174.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=86000, episode_reward=1606.40 +/- 196.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=87000, episode_reward=1354.00 +/- 291.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=88000, episode_reward=1395.00 +/- 263.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=89000, episode_reward=1374.60 +/- 224.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=90000, episode_reward=1287.60 +/- 177.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=91000, episode_reward=1144.20 +/- 494.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=92000, episode_reward=1258.40 +/- 128.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=93000, episode_reward=1447.80 +/- 50.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=94000, episode_reward=1212.80 +/- 260.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=95000, episode_reward=1095.60 +/- 211.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=96000, episode_reward=1036.40 +/- 274.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=97000, episode_reward=1315.80 +/- 151.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=98000, episode_reward=1178.40 +/- 197.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=99000, episode_reward=1441.00 +/- 217.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=100000, episode_reward=1438.40 +/- 248.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=101000, episode_reward=1242.40 +/- 252.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=102000, episode_reward=1164.40 +/- 75.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=103000, episode_reward=1545.80 +/- 97.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=104000, episode_reward=1323.40 +/- 187.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=105000, episode_reward=1212.20 +/- 265.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=106000, episode_reward=1436.00 +/- 119.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=107000, episode_reward=1441.60 +/- 188.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=108000, episode_reward=1406.20 +/- 161.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=109000, episode_reward=1439.40 +/- 120.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=110000, episode_reward=1398.80 +/- 132.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=111000, episode_reward=1355.00 +/- 219.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=112000, episode_reward=29.60 +/- 2546.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=113000, episode_reward=1469.80 +/- 137.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=114000, episode_reward=1379.40 +/- 193.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=115000, episode_reward=1318.00 +/- 157.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=116000, episode_reward=1228.20 +/- 327.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=117000, episode_reward=1251.40 +/- 321.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=118000, episode_reward=1272.60 +/- 202.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=119000, episode_reward=1375.00 +/- 170.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=120000, episode_reward=1529.40 +/- 181.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=121000, episode_reward=1418.80 +/- 118.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=122000, episode_reward=1217.20 +/- 215.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=123000, episode_reward=1573.80 +/- 130.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=124000, episode_reward=1218.40 +/- 351.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=125000, episode_reward=1417.00 +/- 128.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=126000, episode_reward=1260.20 +/- 302.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=127000, episode_reward=1112.60 +/- 192.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=128000, episode_reward=1292.80 +/- 177.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=129000, episode_reward=1395.00 +/- 365.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=130000, episode_reward=1387.20 +/- 223.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=131000, episode_reward=-95.20 +/- 3188.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=132000, episode_reward=1594.20 +/- 179.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=133000, episode_reward=1435.80 +/- 238.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=134000, episode_reward=1405.60 +/- 203.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=135000, episode_reward=1641.60 +/- 95.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=136000, episode_reward=1529.20 +/- 108.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=137000, episode_reward=1497.60 +/- 266.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=138000, episode_reward=1548.80 +/- 157.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=139000, episode_reward=1560.40 +/- 86.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=140000, episode_reward=1497.60 +/- 119.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=141000, episode_reward=1634.80 +/- 161.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=142000, episode_reward=1283.80 +/- 190.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=143000, episode_reward=1559.00 +/- 176.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=144000, episode_reward=1618.40 +/- 108.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=145000, episode_reward=1354.20 +/- 140.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=146000, episode_reward=1511.40 +/- 169.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=147000, episode_reward=1485.60 +/- 122.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=148000, episode_reward=1448.60 +/- 200.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=149000, episode_reward=1614.60 +/- 58.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=150000, episode_reward=1439.00 +/- 522.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=151000, episode_reward=1653.80 +/- 142.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=152000, episode_reward=1580.60 +/- 159.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=153000, episode_reward=1488.80 +/- 391.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=154000, episode_reward=1493.60 +/- 158.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=155000, episode_reward=1389.80 +/- 392.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=156000, episode_reward=1550.20 +/- 211.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=157000, episode_reward=1494.80 +/- 256.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=158000, episode_reward=1704.40 +/- 186.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=159000, episode_reward=1622.60 +/- 232.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=160000, episode_reward=1564.20 +/- 128.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=161000, episode_reward=1635.60 +/- 163.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=162000, episode_reward=1601.60 +/- 169.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=163000, episode_reward=1585.60 +/- 225.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=164000, episode_reward=1634.00 +/- 169.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=165000, episode_reward=1472.40 +/- 214.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=166000, episode_reward=1605.00 +/- 333.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=167000, episode_reward=1540.20 +/- 250.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=168000, episode_reward=1564.60 +/- 139.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=169000, episode_reward=1503.60 +/- 242.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=170000, episode_reward=1526.60 +/- 249.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=171000, episode_reward=1750.80 +/- 105.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=172000, episode_reward=1526.20 +/- 290.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=173000, episode_reward=1661.80 +/- 284.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=174000, episode_reward=1648.00 +/- 235.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=175000, episode_reward=1444.00 +/- 241.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=176000, episode_reward=1655.00 +/- 196.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=177000, episode_reward=1598.00 +/- 138.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=178000, episode_reward=1545.00 +/- 266.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=179000, episode_reward=1478.60 +/- 222.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=180000, episode_reward=1529.20 +/- 272.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=181000, episode_reward=1668.40 +/- 204.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=182000, episode_reward=1511.20 +/- 457.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=183000, episode_reward=1700.00 +/- 158.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=184000, episode_reward=1452.80 +/- 317.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=185000, episode_reward=1479.20 +/- 319.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=186000, episode_reward=1724.80 +/- 132.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=187000, episode_reward=1510.60 +/- 341.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=188000, episode_reward=1676.80 +/- 175.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=189000, episode_reward=1531.20 +/- 172.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=190000, episode_reward=1221.80 +/- 359.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=191000, episode_reward=1543.60 +/- 294.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=192000, episode_reward=1559.80 +/- 198.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=193000, episode_reward=1564.60 +/- 114.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=194000, episode_reward=1516.00 +/- 307.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=195000, episode_reward=1202.60 +/- 353.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=196000, episode_reward=1621.20 +/- 265.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=197000, episode_reward=1483.20 +/- 206.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=198000, episode_reward=1692.60 +/- 270.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=199000, episode_reward=1462.80 +/- 281.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=200000, episode_reward=1534.40 +/- 151.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=201000, episode_reward=1457.40 +/- 265.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=202000, episode_reward=1326.80 +/- 252.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=203000, episode_reward=1747.00 +/- 94.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=204000, episode_reward=1585.60 +/- 181.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=205000, episode_reward=1731.40 +/- 250.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=206000, episode_reward=1693.00 +/- 287.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=207000, episode_reward=1406.80 +/- 301.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=208000, episode_reward=1542.00 +/- 143.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=209000, episode_reward=1573.00 +/- 246.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=210000, episode_reward=1587.60 +/- 222.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=211000, episode_reward=1694.00 +/- 321.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=212000, episode_reward=1556.80 +/- 220.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=213000, episode_reward=1656.40 +/- 144.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=214000, episode_reward=1577.20 +/- 241.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=215000, episode_reward=1540.20 +/- 299.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=216000, episode_reward=1501.60 +/- 299.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=217000, episode_reward=1485.40 +/- 266.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=218000, episode_reward=1416.80 +/- 185.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=219000, episode_reward=1400.80 +/- 285.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=220000, episode_reward=1531.40 +/- 158.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=221000, episode_reward=1605.00 +/- 216.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=222000, episode_reward=1691.20 +/- 95.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=223000, episode_reward=1429.20 +/- 395.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=224000, episode_reward=1474.40 +/- 163.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=225000, episode_reward=1644.60 +/- 100.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=226000, episode_reward=1481.00 +/- 134.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=227000, episode_reward=1474.00 +/- 141.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=228000, episode_reward=1427.60 +/- 160.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=229000, episode_reward=1559.80 +/- 123.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=230000, episode_reward=1634.80 +/- 242.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=231000, episode_reward=1595.40 +/- 169.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=232000, episode_reward=1366.40 +/- 251.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=233000, episode_reward=1400.20 +/- 293.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=234000, episode_reward=1390.80 +/- 321.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=235000, episode_reward=1545.60 +/- 106.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=236000, episode_reward=1461.80 +/- 170.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=237000, episode_reward=1565.00 +/- 305.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=238000, episode_reward=1457.20 +/- 42.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=239000, episode_reward=1550.80 +/- 239.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=240000, episode_reward=1407.00 +/- 142.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=241000, episode_reward=1541.00 +/- 233.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=242000, episode_reward=1638.00 +/- 100.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=243000, episode_reward=1403.00 +/- 282.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=244000, episode_reward=1355.80 +/- 278.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=245000, episode_reward=1380.80 +/- 188.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=246000, episode_reward=1535.40 +/- 219.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=247000, episode_reward=1557.00 +/- 220.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=248000, episode_reward=1410.60 +/- 204.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=249000, episode_reward=1459.40 +/- 394.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=250000, episode_reward=1597.00 +/- 239.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=251000, episode_reward=1538.80 +/- 155.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=252000, episode_reward=1542.60 +/- 187.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=253000, episode_reward=1537.00 +/- 222.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=254000, episode_reward=1619.80 +/- 168.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=255000, episode_reward=1478.80 +/- 216.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=256000, episode_reward=1766.00 +/- 160.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=257000, episode_reward=1349.00 +/- 142.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=258000, episode_reward=1439.20 +/- 253.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=259000, episode_reward=1636.40 +/- 189.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=260000, episode_reward=1400.00 +/- 210.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=261000, episode_reward=1545.20 +/- 283.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=262000, episode_reward=1443.80 +/- 319.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=263000, episode_reward=1521.40 +/- 150.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=264000, episode_reward=1604.80 +/- 160.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=265000, episode_reward=1438.80 +/- 325.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=266000, episode_reward=1436.40 +/- 220.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=267000, episode_reward=1212.60 +/- 372.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=268000, episode_reward=1399.00 +/- 148.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=269000, episode_reward=1463.60 +/- 280.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=270000, episode_reward=1677.60 +/- 150.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=271000, episode_reward=1388.20 +/- 112.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=272000, episode_reward=1655.40 +/- 138.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=273000, episode_reward=1498.80 +/- 117.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=274000, episode_reward=1544.80 +/- 169.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=275000, episode_reward=1430.60 +/- 100.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=276000, episode_reward=1327.40 +/- 464.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=277000, episode_reward=1396.60 +/- 205.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=278000, episode_reward=1459.80 +/- 188.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=279000, episode_reward=1431.20 +/- 271.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=280000, episode_reward=1495.00 +/- 217.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=281000, episode_reward=1524.60 +/- 255.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=282000, episode_reward=1520.80 +/- 179.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=283000, episode_reward=1392.60 +/- 219.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=284000, episode_reward=1455.60 +/- 191.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=285000, episode_reward=1419.80 +/- 84.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=286000, episode_reward=1319.40 +/- 219.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=287000, episode_reward=1489.60 +/- 188.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=288000, episode_reward=1433.00 +/- 274.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=289000, episode_reward=1542.20 +/- 163.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=290000, episode_reward=1367.80 +/- 215.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=291000, episode_reward=1708.20 +/- 97.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=292000, episode_reward=1536.40 +/- 165.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=293000, episode_reward=1573.00 +/- 239.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=294000, episode_reward=1476.20 +/- 174.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=295000, episode_reward=1518.60 +/- 189.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=296000, episode_reward=1541.00 +/- 184.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=297000, episode_reward=1610.00 +/- 258.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=298000, episode_reward=1561.60 +/- 301.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=299000, episode_reward=1480.00 +/- 209.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=300000, episode_reward=1604.40 +/- 157.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=301000, episode_reward=1496.00 +/- 75.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=302000, episode_reward=1459.00 +/- 201.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=303000, episode_reward=1435.40 +/- 228.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=304000, episode_reward=1561.40 +/- 321.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=305000, episode_reward=1509.40 +/- 190.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=306000, episode_reward=1444.60 +/- 121.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=307000, episode_reward=1420.40 +/- 458.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=308000, episode_reward=1663.60 +/- 301.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=309000, episode_reward=1486.60 +/- 297.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=310000, episode_reward=1496.00 +/- 146.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=311000, episode_reward=1331.40 +/- 267.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=312000, episode_reward=1444.80 +/- 157.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=313000, episode_reward=1480.20 +/- 189.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=314000, episode_reward=1445.80 +/- 166.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=315000, episode_reward=1538.60 +/- 151.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=316000, episode_reward=1426.00 +/- 280.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=317000, episode_reward=1563.00 +/- 321.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=318000, episode_reward=1412.00 +/- 288.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=319000, episode_reward=1496.20 +/- 136.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=320000, episode_reward=1408.40 +/- 360.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=321000, episode_reward=1528.00 +/- 273.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=322000, episode_reward=1342.20 +/- 294.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=323000, episode_reward=1580.00 +/- 264.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=324000, episode_reward=1573.80 +/- 268.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=325000, episode_reward=1317.80 +/- 415.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=326000, episode_reward=1401.00 +/- 181.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=327000, episode_reward=1577.00 +/- 146.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=328000, episode_reward=1485.40 +/- 236.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=329000, episode_reward=1582.60 +/- 137.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=330000, episode_reward=1325.60 +/- 108.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=331000, episode_reward=1563.20 +/- 275.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=332000, episode_reward=1623.20 +/- 187.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=333000, episode_reward=1455.00 +/- 108.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=334000, episode_reward=1431.80 +/- 163.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=335000, episode_reward=1521.80 +/- 239.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=336000, episode_reward=1590.80 +/- 239.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=337000, episode_reward=1451.40 +/- 42.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=338000, episode_reward=1430.60 +/- 165.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=339000, episode_reward=1656.80 +/- 139.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=340000, episode_reward=1449.20 +/- 337.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=341000, episode_reward=1390.00 +/- 253.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=342000, episode_reward=1597.60 +/- 208.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=343000, episode_reward=1510.00 +/- 164.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=344000, episode_reward=1625.40 +/- 175.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=345000, episode_reward=1608.80 +/- 279.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=346000, episode_reward=1615.00 +/- 116.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=347000, episode_reward=1681.20 +/- 216.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=348000, episode_reward=1222.20 +/- 312.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=349000, episode_reward=1475.20 +/- 181.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=350000, episode_reward=1554.60 +/- 253.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=351000, episode_reward=1654.80 +/- 191.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=352000, episode_reward=1471.20 +/- 234.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=353000, episode_reward=1394.40 +/- 187.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=354000, episode_reward=1618.80 +/- 319.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=355000, episode_reward=1641.00 +/- 217.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=356000, episode_reward=1502.40 +/- 135.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=357000, episode_reward=1364.40 +/- 51.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=358000, episode_reward=1533.40 +/- 250.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=359000, episode_reward=1447.40 +/- 187.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=360000, episode_reward=1459.00 +/- 187.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=361000, episode_reward=1664.20 +/- 183.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=362000, episode_reward=1617.20 +/- 158.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=363000, episode_reward=1505.40 +/- 158.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=364000, episode_reward=1499.60 +/- 181.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=365000, episode_reward=1613.40 +/- 204.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=366000, episode_reward=1689.20 +/- 102.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=367000, episode_reward=1423.80 +/- 117.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=368000, episode_reward=1435.60 +/- 115.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=369000, episode_reward=1235.20 +/- 522.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=370000, episode_reward=1306.80 +/- 289.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=371000, episode_reward=1341.60 +/- 270.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=372000, episode_reward=1459.00 +/- 132.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=373000, episode_reward=1518.20 +/- 223.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=374000, episode_reward=1376.80 +/- 293.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=375000, episode_reward=1536.60 +/- 134.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=376000, episode_reward=1323.80 +/- 423.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=377000, episode_reward=1669.80 +/- 324.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=378000, episode_reward=1537.00 +/- 274.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=379000, episode_reward=1696.40 +/- 152.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=380000, episode_reward=1522.00 +/- 56.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=381000, episode_reward=1578.80 +/- 226.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=382000, episode_reward=1606.00 +/- 112.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=383000, episode_reward=1606.00 +/- 189.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=384000, episode_reward=1445.60 +/- 184.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=385000, episode_reward=1396.60 +/- 137.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=386000, episode_reward=1654.80 +/- 78.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=387000, episode_reward=1608.80 +/- 302.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=388000, episode_reward=1425.80 +/- 116.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=389000, episode_reward=1347.40 +/- 238.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=390000, episode_reward=1488.40 +/- 77.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=391000, episode_reward=1661.00 +/- 227.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=392000, episode_reward=1579.40 +/- 195.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=393000, episode_reward=1492.80 +/- 91.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=394000, episode_reward=1499.20 +/- 150.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=395000, episode_reward=1663.80 +/- 130.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=396000, episode_reward=1430.80 +/- 147.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=397000, episode_reward=1402.00 +/- 178.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=398000, episode_reward=1533.60 +/- 201.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=399000, episode_reward=1756.20 +/- 145.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=400000, episode_reward=1575.80 +/- 192.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=401000, episode_reward=1711.60 +/- 161.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=402000, episode_reward=1622.20 +/- 182.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=403000, episode_reward=1545.80 +/- 276.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=404000, episode_reward=1359.60 +/- 214.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=405000, episode_reward=1666.40 +/- 118.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=406000, episode_reward=1573.00 +/- 118.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=407000, episode_reward=1651.00 +/- 291.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=408000, episode_reward=1628.00 +/- 118.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=409000, episode_reward=1512.00 +/- 215.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=410000, episode_reward=1588.40 +/- 91.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=411000, episode_reward=1486.60 +/- 288.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=412000, episode_reward=1554.20 +/- 176.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=413000, episode_reward=1643.60 +/- 147.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=414000, episode_reward=1562.40 +/- 137.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=415000, episode_reward=1657.80 +/- 195.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=416000, episode_reward=1622.20 +/- 289.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=417000, episode_reward=1765.60 +/- 106.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=418000, episode_reward=1641.40 +/- 122.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=419000, episode_reward=1747.00 +/- 243.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=420000, episode_reward=1561.20 +/- 245.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=421000, episode_reward=1491.00 +/- 181.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=422000, episode_reward=1623.80 +/- 129.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=423000, episode_reward=1630.80 +/- 327.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=424000, episode_reward=1616.20 +/- 215.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=425000, episode_reward=1647.00 +/- 344.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=426000, episode_reward=1680.60 +/- 205.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=427000, episode_reward=1400.20 +/- 177.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=428000, episode_reward=1428.40 +/- 230.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=429000, episode_reward=1594.20 +/- 128.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=430000, episode_reward=1825.40 +/- 137.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=431000, episode_reward=1423.00 +/- 254.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=432000, episode_reward=1598.60 +/- 297.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=433000, episode_reward=1668.20 +/- 153.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=434000, episode_reward=1708.80 +/- 82.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=435000, episode_reward=1595.40 +/- 47.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=436000, episode_reward=1702.60 +/- 36.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=437000, episode_reward=1674.40 +/- 374.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=438000, episode_reward=1545.40 +/- 110.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=439000, episode_reward=1675.80 +/- 276.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=440000, episode_reward=1462.60 +/- 220.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=441000, episode_reward=1703.40 +/- 278.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=442000, episode_reward=1494.20 +/- 102.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=443000, episode_reward=1459.40 +/- 256.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=444000, episode_reward=1383.00 +/- 125.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=445000, episode_reward=1732.80 +/- 229.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=446000, episode_reward=1698.00 +/- 232.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=447000, episode_reward=1528.00 +/- 289.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=448000, episode_reward=1792.40 +/- 94.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=449000, episode_reward=1396.60 +/- 222.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=450000, episode_reward=1563.20 +/- 221.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=451000, episode_reward=1700.60 +/- 185.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=452000, episode_reward=1513.40 +/- 210.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=453000, episode_reward=1456.80 +/- 293.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=454000, episode_reward=1411.40 +/- 378.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=455000, episode_reward=1619.20 +/- 197.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=456000, episode_reward=1540.40 +/- 261.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=457000, episode_reward=1469.20 +/- 149.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=458000, episode_reward=1621.00 +/- 187.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=459000, episode_reward=1602.60 +/- 210.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=460000, episode_reward=1487.40 +/- 233.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=461000, episode_reward=1547.40 +/- 189.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=462000, episode_reward=1664.00 +/- 184.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=463000, episode_reward=1275.20 +/- 250.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=464000, episode_reward=1570.80 +/- 194.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=465000, episode_reward=1540.00 +/- 177.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=466000, episode_reward=1501.40 +/- 255.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=467000, episode_reward=1538.40 +/- 332.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=468000, episode_reward=1590.80 +/- 220.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=469000, episode_reward=1598.40 +/- 369.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=470000, episode_reward=1496.60 +/- 337.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=471000, episode_reward=1486.80 +/- 299.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=472000, episode_reward=1625.80 +/- 195.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=473000, episode_reward=1392.00 +/- 262.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=474000, episode_reward=1549.60 +/- 360.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=475000, episode_reward=1200.80 +/- 237.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=476000, episode_reward=1468.20 +/- 230.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=477000, episode_reward=1224.40 +/- 320.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=478000, episode_reward=1413.20 +/- 347.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=479000, episode_reward=1552.40 +/- 332.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=480000, episode_reward=1472.40 +/- 285.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=481000, episode_reward=1290.00 +/- 314.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=482000, episode_reward=1544.20 +/- 272.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=483000, episode_reward=1494.40 +/- 193.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=484000, episode_reward=1469.60 +/- 375.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=485000, episode_reward=1455.20 +/- 310.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=486000, episode_reward=1445.00 +/- 353.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=487000, episode_reward=1418.40 +/- 225.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=488000, episode_reward=1406.20 +/- 287.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=489000, episode_reward=1563.60 +/- 438.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=490000, episode_reward=1558.60 +/- 154.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=491000, episode_reward=1727.80 +/- 295.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=492000, episode_reward=1494.60 +/- 227.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=493000, episode_reward=1521.20 +/- 347.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=494000, episode_reward=1547.80 +/- 87.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=495000, episode_reward=1598.80 +/- 191.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=496000, episode_reward=1260.60 +/- 265.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=497000, episode_reward=1586.40 +/- 167.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=498000, episode_reward=1416.80 +/- 273.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=499000, episode_reward=1412.40 +/- 392.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=500000, episode_reward=1320.60 +/- 88.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=501000, episode_reward=1628.40 +/- 156.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Gamma = 0.99, Learning Rate = 0.003, Activation Function = ReLU, Neurons = 64, Process noise = 0.1, Measurement noise = 0.0\n",
      "Eval num_timesteps=1000, episode_reward=-2450.00 +/- 42.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=-2471.00 +/- 61.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=3000, episode_reward=-1099.00 +/- 135.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4000, episode_reward=-1175.80 +/- 228.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=5000, episode_reward=-374.40 +/- 111.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=6000, episode_reward=-563.40 +/- 65.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=7000, episode_reward=65.60 +/- 191.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=8000, episode_reward=-26.00 +/- 135.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=9000, episode_reward=-447.00 +/- 352.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=10000, episode_reward=-579.80 +/- 598.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=11000, episode_reward=112.20 +/- 88.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=12000, episode_reward=75.00 +/- 62.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=13000, episode_reward=231.40 +/- 167.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=14000, episode_reward=436.20 +/- 330.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=15000, episode_reward=-66.20 +/- 84.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=16000, episode_reward=-111.00 +/- 226.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=17000, episode_reward=-105.40 +/- 131.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=18000, episode_reward=-30.00 +/- 127.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=19000, episode_reward=-36.00 +/- 108.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=20000, episode_reward=-65.40 +/- 190.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=21000, episode_reward=162.80 +/- 397.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=22000, episode_reward=57.20 +/- 125.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=23000, episode_reward=392.80 +/- 266.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=24000, episode_reward=479.40 +/- 65.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=25000, episode_reward=109.40 +/- 212.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=26000, episode_reward=459.40 +/- 95.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=27000, episode_reward=172.80 +/- 166.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=28000, episode_reward=430.20 +/- 145.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=29000, episode_reward=470.20 +/- 215.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=30000, episode_reward=381.20 +/- 218.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=31000, episode_reward=299.20 +/- 190.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=32000, episode_reward=255.60 +/- 92.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=33000, episode_reward=518.20 +/- 146.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=34000, episode_reward=404.40 +/- 210.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=35000, episode_reward=363.40 +/- 203.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=36000, episode_reward=450.60 +/- 215.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=37000, episode_reward=283.40 +/- 126.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=38000, episode_reward=241.60 +/- 95.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=39000, episode_reward=249.40 +/- 271.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=40000, episode_reward=252.20 +/- 289.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=41000, episode_reward=289.20 +/- 231.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=42000, episode_reward=286.40 +/- 238.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=43000, episode_reward=538.40 +/- 202.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=44000, episode_reward=321.00 +/- 234.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=45000, episode_reward=590.60 +/- 201.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=46000, episode_reward=565.60 +/- 102.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=47000, episode_reward=279.60 +/- 180.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=48000, episode_reward=607.20 +/- 224.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=49000, episode_reward=617.60 +/- 98.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=50000, episode_reward=574.60 +/- 305.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=51000, episode_reward=470.20 +/- 212.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=52000, episode_reward=655.40 +/- 199.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=53000, episode_reward=615.00 +/- 300.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=54000, episode_reward=780.00 +/- 321.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=55000, episode_reward=582.60 +/- 376.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=56000, episode_reward=1153.20 +/- 166.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=57000, episode_reward=618.20 +/- 239.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=58000, episode_reward=845.80 +/- 183.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=59000, episode_reward=735.60 +/- 433.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=60000, episode_reward=904.60 +/- 357.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=61000, episode_reward=766.40 +/- 310.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=62000, episode_reward=868.00 +/- 232.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=63000, episode_reward=762.40 +/- 130.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=64000, episode_reward=985.40 +/- 319.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=65000, episode_reward=753.20 +/- 389.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=66000, episode_reward=984.60 +/- 350.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=67000, episode_reward=680.20 +/- 239.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=68000, episode_reward=995.00 +/- 172.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=69000, episode_reward=836.80 +/- 359.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=70000, episode_reward=938.00 +/- 238.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=71000, episode_reward=1073.40 +/- 241.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=72000, episode_reward=1016.80 +/- 188.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=73000, episode_reward=971.80 +/- 331.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=74000, episode_reward=1079.60 +/- 253.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=75000, episode_reward=812.40 +/- 74.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=76000, episode_reward=991.80 +/- 323.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=77000, episode_reward=1035.60 +/- 229.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=78000, episode_reward=1089.80 +/- 337.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=79000, episode_reward=951.80 +/- 226.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=80000, episode_reward=974.20 +/- 336.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=81000, episode_reward=728.00 +/- 229.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=82000, episode_reward=1042.60 +/- 310.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=83000, episode_reward=1455.00 +/- 185.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=84000, episode_reward=1266.60 +/- 231.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=85000, episode_reward=1083.80 +/- 74.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=86000, episode_reward=1079.80 +/- 88.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=87000, episode_reward=921.40 +/- 57.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=88000, episode_reward=999.60 +/- 209.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=89000, episode_reward=895.40 +/- 300.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=90000, episode_reward=1195.20 +/- 306.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=91000, episode_reward=1315.80 +/- 146.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=92000, episode_reward=1305.40 +/- 250.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=93000, episode_reward=1298.20 +/- 238.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=94000, episode_reward=1140.00 +/- 251.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=95000, episode_reward=1610.60 +/- 118.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=96000, episode_reward=1112.00 +/- 141.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=97000, episode_reward=1446.00 +/- 145.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=98000, episode_reward=1529.60 +/- 296.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=99000, episode_reward=1374.80 +/- 147.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=100000, episode_reward=1521.40 +/- 174.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=101000, episode_reward=1364.20 +/- 461.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=102000, episode_reward=1483.20 +/- 158.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=103000, episode_reward=1260.00 +/- 89.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=104000, episode_reward=1119.20 +/- 102.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=105000, episode_reward=-2440.00 +/- 7048.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=106000, episode_reward=1396.20 +/- 203.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=107000, episode_reward=1432.40 +/- 263.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=108000, episode_reward=1498.60 +/- 233.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=109000, episode_reward=1282.60 +/- 220.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=110000, episode_reward=1293.60 +/- 156.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=111000, episode_reward=1425.60 +/- 273.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=112000, episode_reward=1260.20 +/- 305.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=113000, episode_reward=1328.00 +/- 326.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=114000, episode_reward=1186.80 +/- 363.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=115000, episode_reward=1239.40 +/- 256.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=116000, episode_reward=1137.80 +/- 212.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=117000, episode_reward=1494.00 +/- 148.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=118000, episode_reward=1322.80 +/- 252.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=119000, episode_reward=1414.60 +/- 56.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=120000, episode_reward=1226.60 +/- 153.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=121000, episode_reward=1399.80 +/- 246.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=122000, episode_reward=1190.80 +/- 290.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=123000, episode_reward=1296.20 +/- 404.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=124000, episode_reward=1277.20 +/- 250.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=125000, episode_reward=949.20 +/- 217.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=126000, episode_reward=1049.60 +/- 360.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=127000, episode_reward=1207.20 +/- 383.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=128000, episode_reward=1451.20 +/- 259.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=129000, episode_reward=995.60 +/- 205.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=130000, episode_reward=1202.20 +/- 168.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=131000, episode_reward=1218.20 +/- 226.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=132000, episode_reward=1172.80 +/- 225.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=133000, episode_reward=1281.60 +/- 356.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=134000, episode_reward=995.20 +/- 344.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=135000, episode_reward=1186.60 +/- 180.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=136000, episode_reward=1260.00 +/- 295.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=137000, episode_reward=1155.60 +/- 137.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=138000, episode_reward=1469.40 +/- 236.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=139000, episode_reward=1349.60 +/- 160.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=140000, episode_reward=1324.60 +/- 216.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=141000, episode_reward=1634.00 +/- 204.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=142000, episode_reward=1357.20 +/- 381.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=143000, episode_reward=1493.40 +/- 338.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=144000, episode_reward=1327.60 +/- 311.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=145000, episode_reward=1299.60 +/- 250.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=146000, episode_reward=1504.80 +/- 54.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=147000, episode_reward=1144.40 +/- 331.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=148000, episode_reward=1277.80 +/- 221.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=149000, episode_reward=1485.80 +/- 226.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=150000, episode_reward=1420.40 +/- 183.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=151000, episode_reward=1417.40 +/- 167.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=152000, episode_reward=1703.60 +/- 139.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=153000, episode_reward=1125.00 +/- 426.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=154000, episode_reward=1457.40 +/- 173.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=155000, episode_reward=1594.40 +/- 176.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=156000, episode_reward=1529.60 +/- 273.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=157000, episode_reward=1611.40 +/- 228.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=158000, episode_reward=1502.40 +/- 261.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=159000, episode_reward=1633.60 +/- 134.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=160000, episode_reward=1478.40 +/- 180.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=161000, episode_reward=1723.80 +/- 241.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=162000, episode_reward=1478.80 +/- 382.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=163000, episode_reward=1568.00 +/- 122.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=164000, episode_reward=1575.80 +/- 210.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=165000, episode_reward=1585.20 +/- 221.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=166000, episode_reward=1552.40 +/- 93.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=167000, episode_reward=1512.40 +/- 309.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=168000, episode_reward=1588.00 +/- 213.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=169000, episode_reward=1518.20 +/- 283.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=170000, episode_reward=1527.00 +/- 238.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=171000, episode_reward=1558.00 +/- 306.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=172000, episode_reward=1597.60 +/- 191.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=173000, episode_reward=1356.60 +/- 232.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=174000, episode_reward=1600.80 +/- 360.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=175000, episode_reward=1615.60 +/- 227.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=176000, episode_reward=1655.60 +/- 131.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=177000, episode_reward=1700.80 +/- 175.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=178000, episode_reward=1652.00 +/- 351.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=179000, episode_reward=1578.60 +/- 103.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=180000, episode_reward=1527.60 +/- 156.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=181000, episode_reward=1725.80 +/- 187.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=182000, episode_reward=1740.00 +/- 130.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=183000, episode_reward=1511.20 +/- 289.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=184000, episode_reward=1709.20 +/- 157.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=185000, episode_reward=1740.60 +/- 277.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=186000, episode_reward=1541.80 +/- 339.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=187000, episode_reward=1639.60 +/- 57.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=188000, episode_reward=1611.20 +/- 121.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=189000, episode_reward=1726.20 +/- 205.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=190000, episode_reward=1530.20 +/- 134.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=191000, episode_reward=1525.60 +/- 153.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=192000, episode_reward=1447.80 +/- 126.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=193000, episode_reward=1666.20 +/- 250.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=194000, episode_reward=1790.00 +/- 141.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=195000, episode_reward=1375.40 +/- 313.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=196000, episode_reward=1725.60 +/- 150.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=197000, episode_reward=1701.20 +/- 203.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=198000, episode_reward=1547.60 +/- 305.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=199000, episode_reward=1794.60 +/- 102.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=200000, episode_reward=1516.80 +/- 211.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=201000, episode_reward=1656.60 +/- 242.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=202000, episode_reward=1580.80 +/- 212.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=203000, episode_reward=1696.00 +/- 76.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=204000, episode_reward=1604.00 +/- 195.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=205000, episode_reward=1740.20 +/- 193.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=206000, episode_reward=1615.00 +/- 337.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=207000, episode_reward=1672.80 +/- 99.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=208000, episode_reward=1570.60 +/- 195.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=209000, episode_reward=1653.60 +/- 274.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=210000, episode_reward=1521.60 +/- 164.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=211000, episode_reward=1572.60 +/- 182.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=212000, episode_reward=1518.60 +/- 187.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=213000, episode_reward=1432.20 +/- 146.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=214000, episode_reward=1558.40 +/- 171.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=215000, episode_reward=1402.60 +/- 155.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=216000, episode_reward=1594.60 +/- 202.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=217000, episode_reward=1650.00 +/- 148.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=218000, episode_reward=1566.20 +/- 124.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=219000, episode_reward=1589.20 +/- 309.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=220000, episode_reward=1630.80 +/- 197.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=221000, episode_reward=1547.60 +/- 196.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=222000, episode_reward=1597.80 +/- 236.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=223000, episode_reward=1497.60 +/- 199.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=224000, episode_reward=1513.00 +/- 236.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=225000, episode_reward=1487.00 +/- 252.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=226000, episode_reward=1474.60 +/- 275.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=227000, episode_reward=1588.00 +/- 105.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=228000, episode_reward=1850.40 +/- 203.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=229000, episode_reward=1602.40 +/- 214.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=230000, episode_reward=1575.60 +/- 262.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=231000, episode_reward=1557.80 +/- 233.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=232000, episode_reward=1678.00 +/- 138.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=233000, episode_reward=1643.20 +/- 222.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=234000, episode_reward=1544.60 +/- 212.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=235000, episode_reward=1683.20 +/- 179.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=236000, episode_reward=1690.40 +/- 190.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=237000, episode_reward=1679.60 +/- 160.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=238000, episode_reward=1790.00 +/- 157.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=239000, episode_reward=1743.20 +/- 258.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=240000, episode_reward=1579.80 +/- 239.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=241000, episode_reward=1617.00 +/- 196.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=242000, episode_reward=1510.80 +/- 116.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=243000, episode_reward=1661.00 +/- 136.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=244000, episode_reward=1622.80 +/- 137.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=245000, episode_reward=1706.40 +/- 167.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=246000, episode_reward=1694.60 +/- 230.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=247000, episode_reward=1480.40 +/- 241.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=248000, episode_reward=1615.20 +/- 181.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=249000, episode_reward=1547.80 +/- 342.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=250000, episode_reward=1697.80 +/- 66.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=251000, episode_reward=1670.80 +/- 298.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=252000, episode_reward=1774.40 +/- 148.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=253000, episode_reward=1692.20 +/- 211.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=254000, episode_reward=1524.40 +/- 178.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=255000, episode_reward=1458.00 +/- 221.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=256000, episode_reward=1360.00 +/- 207.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=257000, episode_reward=1480.20 +/- 90.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=258000, episode_reward=1478.60 +/- 311.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=259000, episode_reward=1570.00 +/- 182.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=260000, episode_reward=1606.60 +/- 189.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=261000, episode_reward=1670.20 +/- 220.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=262000, episode_reward=1617.60 +/- 175.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=263000, episode_reward=1786.20 +/- 286.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=264000, episode_reward=1387.40 +/- 248.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=265000, episode_reward=1384.60 +/- 149.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=266000, episode_reward=1446.60 +/- 115.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=267000, episode_reward=1548.40 +/- 309.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=268000, episode_reward=1824.60 +/- 122.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=269000, episode_reward=1437.20 +/- 368.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=270000, episode_reward=1651.00 +/- 152.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=271000, episode_reward=1588.00 +/- 277.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=272000, episode_reward=1278.20 +/- 136.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=273000, episode_reward=1541.40 +/- 156.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=274000, episode_reward=1439.20 +/- 304.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=275000, episode_reward=1598.40 +/- 92.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=276000, episode_reward=1602.40 +/- 205.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=277000, episode_reward=1810.60 +/- 130.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=278000, episode_reward=1462.80 +/- 237.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=279000, episode_reward=1689.40 +/- 169.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=280000, episode_reward=1647.40 +/- 163.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=281000, episode_reward=1538.00 +/- 189.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=282000, episode_reward=1693.60 +/- 189.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=283000, episode_reward=1688.40 +/- 135.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=284000, episode_reward=1478.20 +/- 178.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=285000, episode_reward=1569.40 +/- 146.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=286000, episode_reward=1458.20 +/- 230.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=287000, episode_reward=1711.60 +/- 131.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=288000, episode_reward=1513.20 +/- 313.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=289000, episode_reward=1444.40 +/- 249.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=290000, episode_reward=1705.80 +/- 153.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=291000, episode_reward=1713.00 +/- 131.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=292000, episode_reward=1646.00 +/- 162.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=293000, episode_reward=1662.60 +/- 155.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=294000, episode_reward=1693.20 +/- 108.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=295000, episode_reward=1652.00 +/- 162.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=296000, episode_reward=1544.00 +/- 219.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=297000, episode_reward=1642.40 +/- 81.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=298000, episode_reward=1599.80 +/- 137.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=299000, episode_reward=1644.20 +/- 122.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=300000, episode_reward=1623.20 +/- 134.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=301000, episode_reward=1657.60 +/- 185.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=302000, episode_reward=1650.80 +/- 188.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=303000, episode_reward=1601.60 +/- 126.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=304000, episode_reward=1699.20 +/- 121.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=305000, episode_reward=1578.00 +/- 189.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=306000, episode_reward=1467.60 +/- 390.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=307000, episode_reward=1699.60 +/- 149.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=308000, episode_reward=1526.40 +/- 163.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=309000, episode_reward=1630.80 +/- 206.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=310000, episode_reward=1549.80 +/- 188.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=311000, episode_reward=1600.40 +/- 105.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=312000, episode_reward=1712.40 +/- 149.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=313000, episode_reward=1432.80 +/- 162.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=314000, episode_reward=1719.80 +/- 159.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=315000, episode_reward=1643.40 +/- 118.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=316000, episode_reward=1644.80 +/- 297.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=317000, episode_reward=1344.20 +/- 304.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=318000, episode_reward=1445.40 +/- 268.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=319000, episode_reward=1523.80 +/- 130.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=320000, episode_reward=1558.80 +/- 191.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=321000, episode_reward=1674.00 +/- 109.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=322000, episode_reward=1240.80 +/- 305.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=323000, episode_reward=1576.80 +/- 144.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=324000, episode_reward=1682.20 +/- 120.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=325000, episode_reward=1440.00 +/- 188.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=326000, episode_reward=1620.40 +/- 208.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=327000, episode_reward=1600.80 +/- 257.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=328000, episode_reward=1700.80 +/- 254.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=329000, episode_reward=1445.20 +/- 158.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=330000, episode_reward=1541.80 +/- 99.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=331000, episode_reward=1685.40 +/- 78.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=332000, episode_reward=1643.00 +/- 283.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=333000, episode_reward=1584.80 +/- 243.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=334000, episode_reward=1517.40 +/- 278.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=335000, episode_reward=1613.00 +/- 256.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=336000, episode_reward=1692.80 +/- 218.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=337000, episode_reward=1429.80 +/- 219.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=338000, episode_reward=1592.00 +/- 266.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=339000, episode_reward=1557.20 +/- 350.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=340000, episode_reward=1601.80 +/- 96.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=341000, episode_reward=1629.80 +/- 270.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=342000, episode_reward=1684.80 +/- 193.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=343000, episode_reward=1638.80 +/- 193.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=344000, episode_reward=1792.00 +/- 189.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=345000, episode_reward=1820.20 +/- 115.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=346000, episode_reward=1345.40 +/- 360.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=347000, episode_reward=1389.80 +/- 248.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=348000, episode_reward=1340.00 +/- 236.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=349000, episode_reward=1555.00 +/- 333.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=350000, episode_reward=1600.80 +/- 109.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=351000, episode_reward=1540.40 +/- 253.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=352000, episode_reward=1685.00 +/- 257.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=353000, episode_reward=1528.00 +/- 226.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=354000, episode_reward=1460.60 +/- 246.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=355000, episode_reward=1733.00 +/- 184.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=356000, episode_reward=1571.60 +/- 122.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=357000, episode_reward=1453.40 +/- 260.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=358000, episode_reward=1723.60 +/- 146.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=359000, episode_reward=1656.40 +/- 145.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=360000, episode_reward=1697.00 +/- 169.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=361000, episode_reward=1553.40 +/- 164.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=362000, episode_reward=1552.20 +/- 178.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=363000, episode_reward=1381.00 +/- 206.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=364000, episode_reward=1715.20 +/- 171.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=365000, episode_reward=1700.80 +/- 89.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=366000, episode_reward=1665.20 +/- 474.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=367000, episode_reward=1641.40 +/- 238.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=368000, episode_reward=1690.60 +/- 208.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=369000, episode_reward=1783.60 +/- 124.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=370000, episode_reward=1760.00 +/- 47.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=371000, episode_reward=1567.40 +/- 179.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=372000, episode_reward=1659.20 +/- 278.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=373000, episode_reward=1608.40 +/- 86.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=374000, episode_reward=1630.00 +/- 134.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=375000, episode_reward=1605.60 +/- 243.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=376000, episode_reward=1427.60 +/- 238.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=377000, episode_reward=1540.20 +/- 97.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=378000, episode_reward=1635.40 +/- 143.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=379000, episode_reward=1518.20 +/- 335.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=380000, episode_reward=1568.80 +/- 129.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=381000, episode_reward=1753.60 +/- 300.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=382000, episode_reward=1655.20 +/- 141.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=383000, episode_reward=1634.60 +/- 150.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=384000, episode_reward=1627.40 +/- 176.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=385000, episode_reward=1607.60 +/- 222.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=386000, episode_reward=1484.00 +/- 193.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=387000, episode_reward=1476.80 +/- 170.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=388000, episode_reward=1683.00 +/- 300.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=389000, episode_reward=1678.00 +/- 188.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=390000, episode_reward=1399.80 +/- 154.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=391000, episode_reward=1550.40 +/- 152.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=392000, episode_reward=1572.20 +/- 164.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=393000, episode_reward=1475.60 +/- 222.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=394000, episode_reward=1800.40 +/- 152.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=395000, episode_reward=1581.20 +/- 171.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=396000, episode_reward=1538.40 +/- 79.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=397000, episode_reward=1605.60 +/- 92.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=398000, episode_reward=1420.60 +/- 206.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=399000, episode_reward=1565.20 +/- 140.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=400000, episode_reward=1700.40 +/- 144.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=401000, episode_reward=1637.00 +/- 182.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=402000, episode_reward=1712.20 +/- 157.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=403000, episode_reward=1848.60 +/- 161.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=404000, episode_reward=1510.60 +/- 150.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=405000, episode_reward=1505.40 +/- 158.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=406000, episode_reward=1494.00 +/- 182.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=407000, episode_reward=1752.60 +/- 169.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=408000, episode_reward=1591.00 +/- 94.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=409000, episode_reward=1482.00 +/- 254.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=410000, episode_reward=1581.20 +/- 356.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=411000, episode_reward=1711.60 +/- 152.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=412000, episode_reward=1771.20 +/- 115.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=413000, episode_reward=1655.80 +/- 220.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=414000, episode_reward=1892.60 +/- 191.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=415000, episode_reward=1701.80 +/- 179.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=416000, episode_reward=1629.00 +/- 259.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=417000, episode_reward=1651.40 +/- 181.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=418000, episode_reward=1631.60 +/- 379.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=419000, episode_reward=1850.40 +/- 293.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=420000, episode_reward=1578.40 +/- 290.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=421000, episode_reward=1543.20 +/- 299.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=422000, episode_reward=1479.80 +/- 384.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=423000, episode_reward=1689.80 +/- 282.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=424000, episode_reward=1578.00 +/- 278.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=425000, episode_reward=1477.40 +/- 352.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=426000, episode_reward=1818.80 +/- 195.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=427000, episode_reward=1625.20 +/- 186.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=428000, episode_reward=1511.60 +/- 140.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=429000, episode_reward=1645.00 +/- 324.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=430000, episode_reward=1802.20 +/- 147.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=431000, episode_reward=1700.40 +/- 233.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=432000, episode_reward=1647.60 +/- 163.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=433000, episode_reward=1529.20 +/- 121.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=434000, episode_reward=1538.20 +/- 322.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=435000, episode_reward=1624.80 +/- 253.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=436000, episode_reward=1524.60 +/- 239.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=437000, episode_reward=1500.60 +/- 169.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=438000, episode_reward=1511.80 +/- 191.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=439000, episode_reward=1650.20 +/- 230.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=440000, episode_reward=1538.60 +/- 302.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=441000, episode_reward=1608.80 +/- 256.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=442000, episode_reward=1337.80 +/- 331.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=443000, episode_reward=1569.00 +/- 172.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=444000, episode_reward=1609.60 +/- 289.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=445000, episode_reward=1723.00 +/- 165.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=446000, episode_reward=1515.20 +/- 161.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=447000, episode_reward=1397.00 +/- 217.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=448000, episode_reward=1544.80 +/- 159.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=449000, episode_reward=1633.60 +/- 186.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=450000, episode_reward=1603.40 +/- 385.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=451000, episode_reward=1650.20 +/- 168.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=452000, episode_reward=1583.60 +/- 235.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=453000, episode_reward=1532.80 +/- 296.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=454000, episode_reward=1371.80 +/- 321.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=455000, episode_reward=1525.20 +/- 216.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=456000, episode_reward=1508.40 +/- 221.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=457000, episode_reward=1484.80 +/- 256.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=458000, episode_reward=1450.60 +/- 442.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=459000, episode_reward=1487.20 +/- 105.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=460000, episode_reward=1555.00 +/- 169.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=461000, episode_reward=1553.80 +/- 132.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=462000, episode_reward=1814.60 +/- 103.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=463000, episode_reward=1753.60 +/- 80.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=464000, episode_reward=1639.80 +/- 119.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=465000, episode_reward=1595.20 +/- 272.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=466000, episode_reward=1752.00 +/- 138.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=467000, episode_reward=1565.80 +/- 187.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=468000, episode_reward=1614.60 +/- 238.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=469000, episode_reward=1519.20 +/- 221.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=470000, episode_reward=1535.80 +/- 225.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=471000, episode_reward=1587.60 +/- 318.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=472000, episode_reward=1640.80 +/- 179.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=473000, episode_reward=1466.80 +/- 161.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=474000, episode_reward=1745.40 +/- 112.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=475000, episode_reward=1655.20 +/- 196.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=476000, episode_reward=1496.00 +/- 127.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=477000, episode_reward=1597.80 +/- 252.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=478000, episode_reward=1649.80 +/- 244.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=479000, episode_reward=1609.40 +/- 86.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=480000, episode_reward=1533.20 +/- 227.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=481000, episode_reward=1677.00 +/- 177.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=482000, episode_reward=1766.20 +/- 133.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=483000, episode_reward=1594.20 +/- 160.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=484000, episode_reward=1634.40 +/- 168.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=485000, episode_reward=1797.20 +/- 188.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=486000, episode_reward=1644.00 +/- 152.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=487000, episode_reward=1497.80 +/- 247.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=488000, episode_reward=1573.20 +/- 101.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=489000, episode_reward=1543.20 +/- 132.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=490000, episode_reward=1713.60 +/- 166.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=491000, episode_reward=1605.00 +/- 269.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=492000, episode_reward=1645.40 +/- 158.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=493000, episode_reward=1703.00 +/- 202.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=494000, episode_reward=1761.20 +/- 125.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=495000, episode_reward=1688.40 +/- 227.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=496000, episode_reward=1679.00 +/- 125.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=497000, episode_reward=1552.60 +/- 170.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=498000, episode_reward=1537.00 +/- 179.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=499000, episode_reward=1612.60 +/- 241.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=500000, episode_reward=1469.20 +/- 87.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=501000, episode_reward=1577.60 +/- 279.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Gamma = 0.99, Learning Rate = 0.0003, Activation Function = Tanh, Neurons = 8, Process noise = 0.1, Measurement noise = 0.0\n",
      "Eval num_timesteps=1000, episode_reward=-7492.00 +/- 40.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=-7509.00 +/- 63.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=3000, episode_reward=-2269.20 +/- 1194.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4000, episode_reward=-6367.40 +/- 2597.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=5000, episode_reward=-7409.80 +/- 1391.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=6000, episode_reward=-5411.20 +/- 1321.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=7000, episode_reward=-6303.40 +/- 1352.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=8000, episode_reward=-7485.00 +/- 2204.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=9000, episode_reward=-6184.00 +/- 2131.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=10000, episode_reward=-5969.00 +/- 805.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=11000, episode_reward=-8122.00 +/- 896.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=12000, episode_reward=-6781.80 +/- 1767.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=13000, episode_reward=-7013.20 +/- 1909.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=14000, episode_reward=-5890.40 +/- 854.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=15000, episode_reward=-7077.40 +/- 1515.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=16000, episode_reward=-6071.40 +/- 2372.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=17000, episode_reward=203.00 +/- 172.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=18000, episode_reward=144.40 +/- 127.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=19000, episode_reward=172.60 +/- 284.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=20000, episode_reward=-15.80 +/- 227.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=21000, episode_reward=209.80 +/- 113.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=22000, episode_reward=214.00 +/- 132.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=23000, episode_reward=129.40 +/- 188.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=24000, episode_reward=-101.80 +/- 151.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=25000, episode_reward=422.40 +/- 224.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=26000, episode_reward=159.60 +/- 254.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=27000, episode_reward=238.40 +/- 208.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=28000, episode_reward=258.40 +/- 75.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=29000, episode_reward=216.60 +/- 208.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=30000, episode_reward=109.40 +/- 195.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=31000, episode_reward=431.00 +/- 49.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=32000, episode_reward=725.00 +/- 266.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=33000, episode_reward=-34.00 +/- 121.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=34000, episode_reward=88.20 +/- 146.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=35000, episode_reward=156.20 +/- 392.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=36000, episode_reward=62.00 +/- 233.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=37000, episode_reward=314.20 +/- 187.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=38000, episode_reward=211.60 +/- 231.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=39000, episode_reward=376.80 +/- 128.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=40000, episode_reward=268.60 +/- 172.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=41000, episode_reward=83.60 +/- 253.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=42000, episode_reward=192.40 +/- 142.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=43000, episode_reward=67.00 +/- 147.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=44000, episode_reward=92.20 +/- 240.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=45000, episode_reward=89.60 +/- 223.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=46000, episode_reward=143.80 +/- 214.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=47000, episode_reward=210.00 +/- 218.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=48000, episode_reward=279.40 +/- 180.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=49000, episode_reward=261.20 +/- 196.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=50000, episode_reward=343.40 +/- 140.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=51000, episode_reward=407.40 +/- 121.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=52000, episode_reward=268.60 +/- 275.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=53000, episode_reward=212.20 +/- 212.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=54000, episode_reward=355.00 +/- 219.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=55000, episode_reward=97.00 +/- 225.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=56000, episode_reward=633.00 +/- 325.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=57000, episode_reward=345.20 +/- 210.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=58000, episode_reward=423.00 +/- 156.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=59000, episode_reward=603.60 +/- 282.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=60000, episode_reward=766.60 +/- 219.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=61000, episode_reward=751.20 +/- 169.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=62000, episode_reward=638.00 +/- 332.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=63000, episode_reward=569.40 +/- 140.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=64000, episode_reward=775.80 +/- 248.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=65000, episode_reward=819.60 +/- 177.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=66000, episode_reward=691.60 +/- 115.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=67000, episode_reward=722.40 +/- 154.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=68000, episode_reward=833.20 +/- 180.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=69000, episode_reward=641.00 +/- 160.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=70000, episode_reward=902.20 +/- 92.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=71000, episode_reward=786.20 +/- 189.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=72000, episode_reward=809.60 +/- 163.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=73000, episode_reward=976.20 +/- 129.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=74000, episode_reward=757.00 +/- 381.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=75000, episode_reward=738.20 +/- 360.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=76000, episode_reward=564.80 +/- 484.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=77000, episode_reward=689.00 +/- 257.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=78000, episode_reward=793.20 +/- 307.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=79000, episode_reward=854.40 +/- 436.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=80000, episode_reward=771.20 +/- 274.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=81000, episode_reward=773.20 +/- 200.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=82000, episode_reward=891.60 +/- 89.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=83000, episode_reward=1072.40 +/- 178.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=84000, episode_reward=1113.40 +/- 182.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=85000, episode_reward=891.40 +/- 214.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=86000, episode_reward=916.40 +/- 212.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=87000, episode_reward=796.60 +/- 452.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=88000, episode_reward=989.60 +/- 150.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=89000, episode_reward=789.60 +/- 503.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=90000, episode_reward=996.00 +/- 278.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=91000, episode_reward=796.00 +/- 257.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=92000, episode_reward=1098.20 +/- 190.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=93000, episode_reward=813.00 +/- 400.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=94000, episode_reward=1172.60 +/- 176.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=95000, episode_reward=1212.60 +/- 344.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=96000, episode_reward=763.20 +/- 105.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=97000, episode_reward=1121.00 +/- 293.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=98000, episode_reward=1087.60 +/- 140.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=99000, episode_reward=865.00 +/- 128.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=100000, episode_reward=893.60 +/- 310.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=101000, episode_reward=1028.40 +/- 59.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=102000, episode_reward=950.80 +/- 193.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=103000, episode_reward=1054.60 +/- 184.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=104000, episode_reward=1004.40 +/- 91.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=105000, episode_reward=1110.80 +/- 271.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=106000, episode_reward=944.60 +/- 82.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=107000, episode_reward=1143.80 +/- 105.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=108000, episode_reward=1022.00 +/- 171.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=109000, episode_reward=1190.80 +/- 307.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=110000, episode_reward=1014.20 +/- 150.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=111000, episode_reward=1074.00 +/- 217.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=112000, episode_reward=1116.80 +/- 196.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=113000, episode_reward=1077.80 +/- 428.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=114000, episode_reward=1061.60 +/- 329.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=115000, episode_reward=1237.40 +/- 104.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=116000, episode_reward=1093.80 +/- 183.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=117000, episode_reward=1235.60 +/- 146.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=118000, episode_reward=1093.80 +/- 203.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=119000, episode_reward=1122.60 +/- 249.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=120000, episode_reward=1343.00 +/- 234.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=121000, episode_reward=1255.40 +/- 136.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=122000, episode_reward=1300.60 +/- 208.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=123000, episode_reward=1208.60 +/- 187.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=124000, episode_reward=1031.40 +/- 295.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=125000, episode_reward=1262.40 +/- 31.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=126000, episode_reward=1060.80 +/- 205.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=127000, episode_reward=1218.80 +/- 275.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=128000, episode_reward=1216.20 +/- 83.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=129000, episode_reward=1079.00 +/- 77.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=130000, episode_reward=1233.00 +/- 332.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=131000, episode_reward=1405.40 +/- 241.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=132000, episode_reward=1223.40 +/- 138.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=133000, episode_reward=1064.60 +/- 228.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=134000, episode_reward=1225.60 +/- 263.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=135000, episode_reward=1304.60 +/- 306.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=136000, episode_reward=1111.80 +/- 258.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=137000, episode_reward=985.20 +/- 326.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=138000, episode_reward=1366.40 +/- 120.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=139000, episode_reward=1030.80 +/- 157.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=140000, episode_reward=1156.20 +/- 203.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=141000, episode_reward=1176.20 +/- 145.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=142000, episode_reward=1419.20 +/- 202.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=143000, episode_reward=1277.40 +/- 275.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=144000, episode_reward=1094.20 +/- 189.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=145000, episode_reward=1162.40 +/- 170.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=146000, episode_reward=1273.20 +/- 145.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=147000, episode_reward=1273.80 +/- 194.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=148000, episode_reward=1060.80 +/- 271.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=149000, episode_reward=1222.40 +/- 174.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=150000, episode_reward=1175.60 +/- 100.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=151000, episode_reward=974.00 +/- 178.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=152000, episode_reward=1190.40 +/- 166.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=153000, episode_reward=1068.40 +/- 350.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=154000, episode_reward=1104.80 +/- 152.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=155000, episode_reward=1157.00 +/- 191.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=156000, episode_reward=1068.80 +/- 336.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=157000, episode_reward=773.40 +/- 228.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=158000, episode_reward=1141.00 +/- 302.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=159000, episode_reward=1059.40 +/- 231.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=160000, episode_reward=974.00 +/- 257.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=161000, episode_reward=1244.60 +/- 208.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=162000, episode_reward=1229.80 +/- 129.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=163000, episode_reward=756.60 +/- 280.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=164000, episode_reward=1061.80 +/- 178.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=165000, episode_reward=1226.40 +/- 176.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=166000, episode_reward=1125.00 +/- 219.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=167000, episode_reward=1047.20 +/- 244.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=168000, episode_reward=1214.20 +/- 156.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=169000, episode_reward=1018.00 +/- 247.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=170000, episode_reward=1072.80 +/- 200.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=171000, episode_reward=1122.00 +/- 250.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=172000, episode_reward=1045.80 +/- 91.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=173000, episode_reward=1113.00 +/- 234.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=174000, episode_reward=962.40 +/- 305.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=175000, episode_reward=1119.80 +/- 161.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=176000, episode_reward=1107.00 +/- 199.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=177000, episode_reward=872.20 +/- 349.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=178000, episode_reward=1112.60 +/- 379.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=179000, episode_reward=803.00 +/- 241.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=180000, episode_reward=1300.20 +/- 169.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=181000, episode_reward=828.00 +/- 270.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=182000, episode_reward=871.20 +/- 280.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=183000, episode_reward=846.80 +/- 141.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=184000, episode_reward=1084.40 +/- 149.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=185000, episode_reward=1116.20 +/- 269.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=186000, episode_reward=1008.40 +/- 230.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=187000, episode_reward=1190.60 +/- 164.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=188000, episode_reward=710.60 +/- 291.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=189000, episode_reward=1172.40 +/- 187.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=190000, episode_reward=991.40 +/- 233.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=191000, episode_reward=1098.20 +/- 259.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=192000, episode_reward=1226.40 +/- 359.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=193000, episode_reward=1274.20 +/- 182.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=194000, episode_reward=1063.60 +/- 145.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=195000, episode_reward=959.60 +/- 147.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=196000, episode_reward=1179.60 +/- 133.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=197000, episode_reward=1365.40 +/- 102.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=198000, episode_reward=1354.80 +/- 198.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=199000, episode_reward=1137.20 +/- 239.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=200000, episode_reward=1039.80 +/- 169.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=201000, episode_reward=1331.60 +/- 214.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=202000, episode_reward=1322.80 +/- 120.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=203000, episode_reward=1308.80 +/- 138.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=204000, episode_reward=1278.00 +/- 214.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=205000, episode_reward=1149.00 +/- 421.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=206000, episode_reward=1365.60 +/- 213.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=207000, episode_reward=1168.00 +/- 258.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=208000, episode_reward=1467.20 +/- 158.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=209000, episode_reward=1178.00 +/- 199.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=210000, episode_reward=-2048.60 +/- 6987.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=211000, episode_reward=1334.20 +/- 180.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=212000, episode_reward=1452.20 +/- 111.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=213000, episode_reward=1230.00 +/- 328.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=214000, episode_reward=1312.60 +/- 286.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=215000, episode_reward=1433.80 +/- 224.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=216000, episode_reward=1375.80 +/- 239.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=217000, episode_reward=1155.80 +/- 273.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=218000, episode_reward=1304.60 +/- 115.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=219000, episode_reward=1411.80 +/- 252.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=220000, episode_reward=1364.80 +/- 246.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=221000, episode_reward=1253.00 +/- 157.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=222000, episode_reward=1246.40 +/- 347.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=223000, episode_reward=1432.00 +/- 235.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=224000, episode_reward=1476.80 +/- 55.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=225000, episode_reward=1479.80 +/- 69.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=226000, episode_reward=1440.60 +/- 217.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=227000, episode_reward=1379.40 +/- 205.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=228000, episode_reward=1354.00 +/- 259.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=229000, episode_reward=1200.20 +/- 202.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=230000, episode_reward=1484.20 +/- 272.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=231000, episode_reward=1417.80 +/- 216.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=232000, episode_reward=1370.80 +/- 256.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=233000, episode_reward=1427.20 +/- 137.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=234000, episode_reward=511.20 +/- 1756.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=235000, episode_reward=1567.40 +/- 264.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=236000, episode_reward=1509.00 +/- 208.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=237000, episode_reward=1458.20 +/- 83.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=238000, episode_reward=1170.40 +/- 192.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=239000, episode_reward=-637.80 +/- 3750.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=240000, episode_reward=1208.00 +/- 46.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=241000, episode_reward=1532.20 +/- 198.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=242000, episode_reward=1422.20 +/- 222.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=243000, episode_reward=1239.40 +/- 272.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=244000, episode_reward=1330.40 +/- 95.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=245000, episode_reward=1123.00 +/- 228.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=246000, episode_reward=1527.60 +/- 126.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=247000, episode_reward=1320.40 +/- 205.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=248000, episode_reward=1427.00 +/- 221.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=249000, episode_reward=1516.80 +/- 190.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=250000, episode_reward=1601.00 +/- 113.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=251000, episode_reward=1383.80 +/- 219.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=252000, episode_reward=1507.40 +/- 252.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=253000, episode_reward=1276.20 +/- 250.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=254000, episode_reward=1585.40 +/- 193.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=255000, episode_reward=1412.80 +/- 238.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=256000, episode_reward=1274.40 +/- 345.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=257000, episode_reward=1435.80 +/- 184.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=258000, episode_reward=1367.00 +/- 277.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=259000, episode_reward=1579.20 +/- 128.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=260000, episode_reward=1601.80 +/- 227.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=261000, episode_reward=1318.00 +/- 300.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=262000, episode_reward=1535.20 +/- 251.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=263000, episode_reward=1483.40 +/- 82.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=264000, episode_reward=1339.20 +/- 206.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=265000, episode_reward=1652.40 +/- 95.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=266000, episode_reward=1217.20 +/- 215.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=267000, episode_reward=1555.40 +/- 137.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=268000, episode_reward=1584.40 +/- 37.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=269000, episode_reward=1448.60 +/- 270.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=270000, episode_reward=1165.00 +/- 110.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=271000, episode_reward=1437.80 +/- 175.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=272000, episode_reward=1420.00 +/- 229.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=273000, episode_reward=1403.00 +/- 263.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=274000, episode_reward=1238.80 +/- 177.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=275000, episode_reward=1395.60 +/- 124.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=276000, episode_reward=-1679.60 +/- 5999.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=277000, episode_reward=1568.40 +/- 324.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=278000, episode_reward=1403.80 +/- 186.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=279000, episode_reward=1407.80 +/- 290.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=280000, episode_reward=1567.00 +/- 179.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=281000, episode_reward=1430.00 +/- 291.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=282000, episode_reward=1420.40 +/- 164.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=283000, episode_reward=1515.00 +/- 338.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=284000, episode_reward=1457.20 +/- 155.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=285000, episode_reward=1409.00 +/- 258.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=286000, episode_reward=1293.20 +/- 283.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=287000, episode_reward=1565.80 +/- 234.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=288000, episode_reward=1528.20 +/- 309.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=289000, episode_reward=1575.60 +/- 210.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=290000, episode_reward=1411.60 +/- 256.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=291000, episode_reward=1449.80 +/- 220.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=292000, episode_reward=1385.20 +/- 219.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=293000, episode_reward=1552.20 +/- 158.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=294000, episode_reward=1755.40 +/- 207.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=295000, episode_reward=1424.80 +/- 130.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=296000, episode_reward=1437.20 +/- 132.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=297000, episode_reward=1441.60 +/- 303.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=298000, episode_reward=1520.40 +/- 187.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=299000, episode_reward=1648.60 +/- 166.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=300000, episode_reward=1454.20 +/- 176.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=301000, episode_reward=1459.60 +/- 128.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=302000, episode_reward=1540.40 +/- 270.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=303000, episode_reward=1387.60 +/- 464.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=304000, episode_reward=1539.80 +/- 183.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=305000, episode_reward=1481.80 +/- 110.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=306000, episode_reward=1509.40 +/- 206.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=307000, episode_reward=1531.80 +/- 135.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=308000, episode_reward=1676.20 +/- 178.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=309000, episode_reward=1539.60 +/- 114.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=310000, episode_reward=1518.00 +/- 128.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=311000, episode_reward=1341.60 +/- 145.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=312000, episode_reward=1465.60 +/- 237.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=313000, episode_reward=-1994.20 +/- 7339.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=314000, episode_reward=1532.60 +/- 92.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=315000, episode_reward=1513.20 +/- 298.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=316000, episode_reward=1558.40 +/- 280.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=317000, episode_reward=1316.80 +/- 188.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=318000, episode_reward=1623.40 +/- 221.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=319000, episode_reward=1601.60 +/- 200.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=320000, episode_reward=1214.00 +/- 134.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=321000, episode_reward=1563.00 +/- 220.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=322000, episode_reward=1447.80 +/- 91.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=323000, episode_reward=1420.40 +/- 111.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=324000, episode_reward=1293.20 +/- 275.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=325000, episode_reward=1377.00 +/- 336.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=326000, episode_reward=1484.40 +/- 174.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=327000, episode_reward=1381.20 +/- 431.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=328000, episode_reward=1542.00 +/- 202.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=329000, episode_reward=-2068.80 +/- 7105.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=330000, episode_reward=1603.60 +/- 305.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=331000, episode_reward=1573.40 +/- 148.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=332000, episode_reward=1490.60 +/- 142.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=333000, episode_reward=1705.80 +/- 275.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=334000, episode_reward=1262.20 +/- 333.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=335000, episode_reward=1390.80 +/- 418.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=336000, episode_reward=-1817.60 +/- 6254.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=337000, episode_reward=1497.40 +/- 200.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=338000, episode_reward=1539.20 +/- 221.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=339000, episode_reward=1564.60 +/- 192.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=340000, episode_reward=1520.00 +/- 274.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=341000, episode_reward=1533.60 +/- 175.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=342000, episode_reward=1418.80 +/- 164.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=343000, episode_reward=1566.00 +/- 185.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=344000, episode_reward=1629.80 +/- 233.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=345000, episode_reward=1301.20 +/- 241.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=346000, episode_reward=1571.80 +/- 217.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=347000, episode_reward=1324.40 +/- 123.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=348000, episode_reward=1628.20 +/- 182.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=349000, episode_reward=1327.80 +/- 232.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=350000, episode_reward=1419.00 +/- 211.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=351000, episode_reward=1414.20 +/- 231.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=352000, episode_reward=1314.20 +/- 354.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=353000, episode_reward=1473.00 +/- 271.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=354000, episode_reward=1664.40 +/- 293.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=355000, episode_reward=1510.60 +/- 124.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=356000, episode_reward=1514.60 +/- 110.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=357000, episode_reward=1478.20 +/- 351.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=358000, episode_reward=1503.80 +/- 159.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=359000, episode_reward=1509.20 +/- 215.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=360000, episode_reward=1535.80 +/- 245.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=361000, episode_reward=1589.80 +/- 252.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=362000, episode_reward=1201.20 +/- 224.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=363000, episode_reward=1625.20 +/- 122.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=364000, episode_reward=1644.00 +/- 184.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=365000, episode_reward=1371.40 +/- 402.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=366000, episode_reward=1574.20 +/- 87.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=367000, episode_reward=1638.80 +/- 347.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=368000, episode_reward=1722.00 +/- 306.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=369000, episode_reward=1592.00 +/- 255.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=370000, episode_reward=-1661.40 +/- 6331.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=371000, episode_reward=1351.00 +/- 152.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=372000, episode_reward=1568.20 +/- 222.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=373000, episode_reward=1828.80 +/- 152.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=374000, episode_reward=1649.80 +/- 229.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=375000, episode_reward=1537.60 +/- 235.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=376000, episode_reward=1525.80 +/- 291.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=377000, episode_reward=1566.60 +/- 184.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=378000, episode_reward=1590.40 +/- 130.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=379000, episode_reward=1625.80 +/- 174.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=380000, episode_reward=1563.00 +/- 187.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=381000, episode_reward=1715.60 +/- 238.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=382000, episode_reward=1429.80 +/- 271.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=383000, episode_reward=1379.60 +/- 252.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=384000, episode_reward=1546.60 +/- 201.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=385000, episode_reward=1322.40 +/- 252.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=386000, episode_reward=1425.00 +/- 275.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=387000, episode_reward=1496.00 +/- 197.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=388000, episode_reward=1386.80 +/- 240.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=389000, episode_reward=1716.60 +/- 163.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=390000, episode_reward=1498.80 +/- 365.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=391000, episode_reward=1614.80 +/- 113.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=392000, episode_reward=1563.80 +/- 229.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=393000, episode_reward=1385.40 +/- 294.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=394000, episode_reward=1626.60 +/- 160.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=395000, episode_reward=1504.80 +/- 139.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=396000, episode_reward=1174.00 +/- 345.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=397000, episode_reward=1480.20 +/- 284.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=398000, episode_reward=1594.80 +/- 204.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=399000, episode_reward=1529.40 +/- 184.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=400000, episode_reward=1584.60 +/- 300.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=401000, episode_reward=1545.60 +/- 76.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=402000, episode_reward=1464.20 +/- 281.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=403000, episode_reward=-118.00 +/- 3471.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=404000, episode_reward=1580.60 +/- 165.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=405000, episode_reward=1667.40 +/- 145.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=406000, episode_reward=1541.40 +/- 120.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=407000, episode_reward=1385.80 +/- 410.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=408000, episode_reward=1315.20 +/- 261.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=409000, episode_reward=1503.80 +/- 237.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=410000, episode_reward=1550.40 +/- 251.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=411000, episode_reward=1558.00 +/- 269.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=412000, episode_reward=1511.80 +/- 314.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=413000, episode_reward=1511.40 +/- 271.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=414000, episode_reward=1407.80 +/- 140.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=415000, episode_reward=1313.00 +/- 250.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=416000, episode_reward=1528.80 +/- 378.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=417000, episode_reward=1338.40 +/- 222.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=418000, episode_reward=1542.40 +/- 292.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=419000, episode_reward=1415.40 +/- 263.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=420000, episode_reward=1454.00 +/- 224.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=421000, episode_reward=1549.80 +/- 237.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=422000, episode_reward=1376.40 +/- 334.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=423000, episode_reward=1399.00 +/- 213.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=424000, episode_reward=1341.00 +/- 339.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=425000, episode_reward=1387.20 +/- 186.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=426000, episode_reward=1475.00 +/- 274.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=427000, episode_reward=1472.80 +/- 401.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=428000, episode_reward=1305.20 +/- 308.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=429000, episode_reward=1524.20 +/- 315.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=430000, episode_reward=1551.40 +/- 277.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=431000, episode_reward=1335.80 +/- 218.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=432000, episode_reward=1618.40 +/- 266.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=433000, episode_reward=1314.60 +/- 322.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=434000, episode_reward=1573.80 +/- 227.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=435000, episode_reward=1551.20 +/- 333.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=436000, episode_reward=1620.80 +/- 315.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=437000, episode_reward=1528.60 +/- 430.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=438000, episode_reward=1405.00 +/- 173.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=439000, episode_reward=1632.40 +/- 195.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=440000, episode_reward=1237.00 +/- 404.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=441000, episode_reward=1556.80 +/- 240.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=442000, episode_reward=1379.00 +/- 306.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=443000, episode_reward=1407.00 +/- 294.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=444000, episode_reward=1515.00 +/- 275.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=445000, episode_reward=1472.40 +/- 287.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=446000, episode_reward=1545.60 +/- 378.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=447000, episode_reward=1497.60 +/- 264.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=448000, episode_reward=1572.40 +/- 178.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=449000, episode_reward=1658.60 +/- 140.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=450000, episode_reward=1587.00 +/- 151.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=451000, episode_reward=1639.00 +/- 226.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=452000, episode_reward=1360.00 +/- 173.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=453000, episode_reward=1755.40 +/- 199.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=454000, episode_reward=1519.60 +/- 326.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=455000, episode_reward=1734.00 +/- 296.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=456000, episode_reward=1705.60 +/- 83.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=457000, episode_reward=1696.80 +/- 278.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=458000, episode_reward=1548.20 +/- 237.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=459000, episode_reward=1354.40 +/- 210.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=460000, episode_reward=1448.00 +/- 434.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=461000, episode_reward=1683.20 +/- 303.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=462000, episode_reward=1285.00 +/- 214.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=463000, episode_reward=1568.60 +/- 248.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=464000, episode_reward=1517.40 +/- 265.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=465000, episode_reward=1489.80 +/- 384.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=466000, episode_reward=1414.80 +/- 304.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=467000, episode_reward=1732.80 +/- 189.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=468000, episode_reward=1645.00 +/- 127.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=469000, episode_reward=1615.20 +/- 234.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=470000, episode_reward=1460.20 +/- 200.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=471000, episode_reward=1475.60 +/- 192.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=472000, episode_reward=1639.20 +/- 252.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=473000, episode_reward=1509.80 +/- 301.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=474000, episode_reward=1422.80 +/- 223.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=475000, episode_reward=1533.40 +/- 271.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=476000, episode_reward=-334.40 +/- 3811.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=477000, episode_reward=1483.20 +/- 347.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=478000, episode_reward=1692.40 +/- 199.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=479000, episode_reward=1650.80 +/- 214.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=480000, episode_reward=1785.60 +/- 113.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=481000, episode_reward=-423.20 +/- 3824.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=482000, episode_reward=-2015.40 +/- 7343.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=483000, episode_reward=1629.60 +/- 181.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=484000, episode_reward=1515.60 +/- 154.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=485000, episode_reward=1560.40 +/- 197.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=486000, episode_reward=1523.40 +/- 253.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=487000, episode_reward=1689.20 +/- 210.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=488000, episode_reward=1330.20 +/- 391.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=489000, episode_reward=1651.00 +/- 62.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=490000, episode_reward=1608.40 +/- 187.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=491000, episode_reward=1559.80 +/- 214.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=492000, episode_reward=-1796.00 +/- 6909.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=493000, episode_reward=1556.80 +/- 295.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=494000, episode_reward=1496.80 +/- 239.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=495000, episode_reward=1592.20 +/- 177.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=496000, episode_reward=1677.40 +/- 315.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=497000, episode_reward=1451.20 +/- 209.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=498000, episode_reward=1556.40 +/- 302.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=499000, episode_reward=-1792.80 +/- 6309.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=500000, episode_reward=1556.40 +/- 234.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=501000, episode_reward=1584.20 +/- 186.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Gamma = 0.99, Learning Rate = 0.0003, Activation Function = Tanh, Neurons = 32, Process noise = 0.1, Measurement noise = 0.0\n",
      "Eval num_timesteps=1000, episode_reward=-18998.00 +/- 72.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=-18959.00 +/- 95.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3000, episode_reward=-7657.60 +/- 2991.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4000, episode_reward=-6037.00 +/- 999.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=5000, episode_reward=26.20 +/- 246.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=6000, episode_reward=163.80 +/- 246.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=7000, episode_reward=-5485.40 +/- 1487.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=8000, episode_reward=-7599.80 +/- 2478.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=9000, episode_reward=387.20 +/- 158.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=10000, episode_reward=249.20 +/- 329.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=11000, episode_reward=184.20 +/- 146.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=12000, episode_reward=242.80 +/- 264.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=13000, episode_reward=201.20 +/- 220.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=14000, episode_reward=383.00 +/- 216.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=15000, episode_reward=412.80 +/- 154.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=16000, episode_reward=248.20 +/- 162.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=17000, episode_reward=661.20 +/- 167.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=18000, episode_reward=433.00 +/- 168.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=19000, episode_reward=396.80 +/- 171.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=20000, episode_reward=500.40 +/- 250.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=21000, episode_reward=560.20 +/- 300.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=22000, episode_reward=581.20 +/- 145.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=23000, episode_reward=257.80 +/- 113.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=24000, episode_reward=350.60 +/- 312.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=25000, episode_reward=358.80 +/- 190.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=26000, episode_reward=498.40 +/- 236.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=27000, episode_reward=374.60 +/- 127.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=28000, episode_reward=215.20 +/- 68.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=29000, episode_reward=396.00 +/- 209.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=30000, episode_reward=375.80 +/- 107.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=31000, episode_reward=244.20 +/- 194.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=32000, episode_reward=359.80 +/- 171.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=33000, episode_reward=12.80 +/- 209.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=34000, episode_reward=481.60 +/- 158.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=35000, episode_reward=370.40 +/- 127.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=36000, episode_reward=279.80 +/- 312.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=37000, episode_reward=194.20 +/- 107.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=38000, episode_reward=363.40 +/- 183.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=39000, episode_reward=238.20 +/- 170.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=40000, episode_reward=261.40 +/- 236.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=41000, episode_reward=442.80 +/- 99.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=42000, episode_reward=170.40 +/- 273.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=43000, episode_reward=377.00 +/- 166.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=44000, episode_reward=279.40 +/- 246.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=45000, episode_reward=283.40 +/- 113.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=46000, episode_reward=498.00 +/- 314.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=47000, episode_reward=471.20 +/- 392.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=48000, episode_reward=472.80 +/- 104.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=49000, episode_reward=524.80 +/- 162.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=50000, episode_reward=310.40 +/- 264.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=51000, episode_reward=431.80 +/- 205.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=52000, episode_reward=911.00 +/- 248.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=53000, episode_reward=818.40 +/- 255.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=54000, episode_reward=634.80 +/- 334.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=55000, episode_reward=775.80 +/- 340.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=56000, episode_reward=717.20 +/- 72.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=57000, episode_reward=688.80 +/- 379.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=58000, episode_reward=671.00 +/- 364.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=59000, episode_reward=1143.00 +/- 385.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=60000, episode_reward=625.60 +/- 266.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=61000, episode_reward=744.20 +/- 282.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=62000, episode_reward=1145.00 +/- 310.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=63000, episode_reward=996.40 +/- 238.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=64000, episode_reward=920.40 +/- 181.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=65000, episode_reward=923.60 +/- 268.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=66000, episode_reward=1026.60 +/- 236.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=67000, episode_reward=1166.00 +/- 304.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=68000, episode_reward=1125.80 +/- 351.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=69000, episode_reward=1089.40 +/- 142.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=70000, episode_reward=1013.80 +/- 274.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=71000, episode_reward=1210.00 +/- 306.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=72000, episode_reward=1123.40 +/- 246.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=73000, episode_reward=1215.60 +/- 210.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=74000, episode_reward=1135.80 +/- 425.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=75000, episode_reward=1198.20 +/- 59.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=76000, episode_reward=1009.60 +/- 229.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=77000, episode_reward=974.60 +/- 247.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=78000, episode_reward=1152.20 +/- 100.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=79000, episode_reward=1175.60 +/- 300.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=80000, episode_reward=1084.00 +/- 208.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=81000, episode_reward=1216.60 +/- 156.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=82000, episode_reward=1011.40 +/- 234.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=83000, episode_reward=1218.60 +/- 175.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=84000, episode_reward=1313.20 +/- 295.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=85000, episode_reward=1311.00 +/- 331.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=86000, episode_reward=1273.60 +/- 296.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=87000, episode_reward=911.60 +/- 358.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=88000, episode_reward=1167.00 +/- 94.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=89000, episode_reward=1338.80 +/- 166.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=90000, episode_reward=1163.20 +/- 213.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=91000, episode_reward=1242.00 +/- 206.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=92000, episode_reward=1230.60 +/- 190.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=93000, episode_reward=1339.40 +/- 200.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=94000, episode_reward=1213.20 +/- 120.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=95000, episode_reward=1213.60 +/- 263.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=96000, episode_reward=1241.40 +/- 193.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=97000, episode_reward=1279.20 +/- 115.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=98000, episode_reward=-1945.00 +/- 6481.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=99000, episode_reward=1205.40 +/- 125.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=100000, episode_reward=1145.60 +/- 138.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=101000, episode_reward=1405.00 +/- 259.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=102000, episode_reward=1169.00 +/- 185.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=103000, episode_reward=1202.00 +/- 288.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=104000, episode_reward=1193.40 +/- 220.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=105000, episode_reward=1305.00 +/- 260.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=106000, episode_reward=1124.40 +/- 268.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=107000, episode_reward=1057.40 +/- 345.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=108000, episode_reward=1415.20 +/- 186.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=109000, episode_reward=1325.80 +/- 193.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=110000, episode_reward=1274.40 +/- 139.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=111000, episode_reward=1292.20 +/- 270.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=112000, episode_reward=1411.00 +/- 119.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=113000, episode_reward=1281.60 +/- 260.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=114000, episode_reward=1234.00 +/- 262.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=115000, episode_reward=1373.00 +/- 152.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=116000, episode_reward=1425.40 +/- 106.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=117000, episode_reward=1161.80 +/- 278.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=118000, episode_reward=1436.80 +/- 225.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=119000, episode_reward=1340.00 +/- 204.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=120000, episode_reward=1194.60 +/- 181.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=121000, episode_reward=1471.60 +/- 227.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=122000, episode_reward=1339.00 +/- 244.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=123000, episode_reward=1416.00 +/- 224.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=124000, episode_reward=1549.80 +/- 192.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=125000, episode_reward=1394.20 +/- 166.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=126000, episode_reward=-978.40 +/- 4853.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=127000, episode_reward=1357.80 +/- 278.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=128000, episode_reward=1502.00 +/- 118.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=129000, episode_reward=1602.40 +/- 177.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=130000, episode_reward=-305.40 +/- 3430.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=131000, episode_reward=1464.60 +/- 308.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=132000, episode_reward=1507.40 +/- 201.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=133000, episode_reward=1432.40 +/- 240.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=134000, episode_reward=-839.80 +/- 4749.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=135000, episode_reward=653.00 +/- 1293.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=136000, episode_reward=1146.80 +/- 391.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=137000, episode_reward=1462.60 +/- 180.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=138000, episode_reward=1322.40 +/- 232.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=139000, episode_reward=1308.40 +/- 142.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=140000, episode_reward=1575.00 +/- 254.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=141000, episode_reward=1276.00 +/- 163.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=142000, episode_reward=1394.60 +/- 326.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=143000, episode_reward=1377.80 +/- 242.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=144000, episode_reward=1310.40 +/- 240.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=145000, episode_reward=1470.20 +/- 210.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=146000, episode_reward=1423.00 +/- 209.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=147000, episode_reward=1459.40 +/- 235.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=148000, episode_reward=1507.20 +/- 362.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=149000, episode_reward=1440.80 +/- 305.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=150000, episode_reward=1642.80 +/- 124.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=151000, episode_reward=1577.80 +/- 226.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=152000, episode_reward=1586.20 +/- 254.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=153000, episode_reward=1478.60 +/- 159.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=154000, episode_reward=1419.80 +/- 286.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=155000, episode_reward=1311.00 +/- 468.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=156000, episode_reward=1347.00 +/- 296.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=157000, episode_reward=1392.00 +/- 132.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=158000, episode_reward=1518.60 +/- 105.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=159000, episode_reward=1395.00 +/- 553.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=160000, episode_reward=1405.60 +/- 208.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=161000, episode_reward=1578.60 +/- 108.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=162000, episode_reward=1278.00 +/- 525.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=163000, episode_reward=-1177.60 +/- 5436.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=164000, episode_reward=-1864.00 +/- 6986.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=165000, episode_reward=1625.00 +/- 104.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=166000, episode_reward=1345.20 +/- 259.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=167000, episode_reward=-790.80 +/- 4493.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=168000, episode_reward=-561.80 +/- 4128.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=169000, episode_reward=-1348.00 +/- 5786.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=170000, episode_reward=1041.60 +/- 1156.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=171000, episode_reward=1488.00 +/- 172.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=172000, episode_reward=1109.00 +/- 1175.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=173000, episode_reward=-2183.80 +/- 7152.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=174000, episode_reward=1364.40 +/- 597.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=175000, episode_reward=-1661.80 +/- 6700.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=176000, episode_reward=-2548.80 +/- 6175.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=177000, episode_reward=1700.60 +/- 104.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=178000, episode_reward=1512.60 +/- 188.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=179000, episode_reward=1625.40 +/- 196.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=180000, episode_reward=-1483.80 +/- 6029.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=181000, episode_reward=1630.80 +/- 236.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=182000, episode_reward=1488.40 +/- 172.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=183000, episode_reward=1695.80 +/- 108.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=184000, episode_reward=1540.80 +/- 102.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=185000, episode_reward=1627.60 +/- 256.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=186000, episode_reward=987.60 +/- 1224.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=187000, episode_reward=1608.40 +/- 152.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=188000, episode_reward=1514.40 +/- 198.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=189000, episode_reward=1625.40 +/- 295.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=190000, episode_reward=-795.60 +/- 5055.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=191000, episode_reward=1700.80 +/- 175.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=192000, episode_reward=1563.60 +/- 190.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=193000, episode_reward=1594.80 +/- 178.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=194000, episode_reward=-1364.00 +/- 6225.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=195000, episode_reward=1517.20 +/- 227.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=196000, episode_reward=1449.00 +/- 95.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=197000, episode_reward=1526.00 +/- 240.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=198000, episode_reward=1388.00 +/- 314.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=199000, episode_reward=1599.80 +/- 206.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=200000, episode_reward=-3271.00 +/- 5667.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=201000, episode_reward=-1451.60 +/- 5938.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=202000, episode_reward=-210.80 +/- 3317.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=203000, episode_reward=960.20 +/- 743.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=204000, episode_reward=1671.60 +/- 129.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=205000, episode_reward=1636.60 +/- 224.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=206000, episode_reward=1511.00 +/- 404.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=207000, episode_reward=-1018.20 +/- 5231.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=208000, episode_reward=1567.20 +/- 221.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=209000, episode_reward=1589.80 +/- 249.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=210000, episode_reward=1538.00 +/- 256.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=211000, episode_reward=-1291.40 +/- 5991.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=212000, episode_reward=1561.80 +/- 243.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=213000, episode_reward=1585.20 +/- 200.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=214000, episode_reward=1510.80 +/- 537.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=215000, episode_reward=78.80 +/- 2930.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=216000, episode_reward=1680.80 +/- 200.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=217000, episode_reward=1699.40 +/- 181.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=218000, episode_reward=1636.60 +/- 121.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=219000, episode_reward=-3580.80 +/- 6419.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=220000, episode_reward=1360.60 +/- 263.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=221000, episode_reward=1730.20 +/- 189.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=222000, episode_reward=1561.00 +/- 332.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=223000, episode_reward=-514.60 +/- 4224.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=224000, episode_reward=1750.20 +/- 141.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=225000, episode_reward=1508.00 +/- 77.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=226000, episode_reward=1677.00 +/- 376.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=227000, episode_reward=1582.20 +/- 271.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=228000, episode_reward=1554.40 +/- 134.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=229000, episode_reward=1698.20 +/- 132.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=230000, episode_reward=1375.20 +/- 184.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=231000, episode_reward=1693.20 +/- 282.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=232000, episode_reward=1552.60 +/- 290.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=233000, episode_reward=1654.60 +/- 70.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=234000, episode_reward=1589.20 +/- 275.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=235000, episode_reward=1436.00 +/- 151.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=236000, episode_reward=188.00 +/- 2889.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=237000, episode_reward=1457.40 +/- 202.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=238000, episode_reward=1713.40 +/- 113.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=239000, episode_reward=1489.80 +/- 186.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=240000, episode_reward=1404.00 +/- 359.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=241000, episode_reward=1559.00 +/- 314.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=242000, episode_reward=1619.80 +/- 121.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=243000, episode_reward=1448.20 +/- 325.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=244000, episode_reward=1591.20 +/- 208.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=245000, episode_reward=1500.80 +/- 215.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=246000, episode_reward=1745.60 +/- 146.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=247000, episode_reward=1581.00 +/- 213.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=248000, episode_reward=1522.80 +/- 309.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=249000, episode_reward=1779.20 +/- 182.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=250000, episode_reward=1599.40 +/- 243.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=251000, episode_reward=1409.60 +/- 210.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=252000, episode_reward=1538.80 +/- 247.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=253000, episode_reward=1646.80 +/- 194.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=254000, episode_reward=1554.00 +/- 93.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=255000, episode_reward=1338.20 +/- 301.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=256000, episode_reward=1704.60 +/- 142.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=257000, episode_reward=1821.80 +/- 297.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=258000, episode_reward=1647.20 +/- 391.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=259000, episode_reward=1439.00 +/- 285.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=260000, episode_reward=1705.40 +/- 135.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=261000, episode_reward=1567.00 +/- 168.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=262000, episode_reward=1540.20 +/- 252.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=263000, episode_reward=1679.00 +/- 147.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=264000, episode_reward=1625.00 +/- 284.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=265000, episode_reward=1396.60 +/- 298.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=266000, episode_reward=1687.40 +/- 154.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=267000, episode_reward=1673.20 +/- 184.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=268000, episode_reward=1321.80 +/- 694.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=269000, episode_reward=1491.20 +/- 208.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=270000, episode_reward=1718.60 +/- 224.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=271000, episode_reward=1507.40 +/- 234.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=272000, episode_reward=1663.20 +/- 228.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=273000, episode_reward=1526.20 +/- 201.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=274000, episode_reward=1499.80 +/- 126.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=275000, episode_reward=1166.60 +/- 115.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=276000, episode_reward=1437.80 +/- 175.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=277000, episode_reward=1420.80 +/- 244.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=278000, episode_reward=1711.20 +/- 166.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=279000, episode_reward=1541.80 +/- 186.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=280000, episode_reward=1410.40 +/- 269.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=281000, episode_reward=1606.60 +/- 323.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=282000, episode_reward=1663.40 +/- 132.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=283000, episode_reward=1377.60 +/- 259.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=284000, episode_reward=1370.20 +/- 160.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=285000, episode_reward=1579.20 +/- 126.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=286000, episode_reward=1465.20 +/- 314.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=287000, episode_reward=1649.60 +/- 115.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=288000, episode_reward=1542.40 +/- 264.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=289000, episode_reward=1598.20 +/- 248.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=290000, episode_reward=1422.20 +/- 197.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=291000, episode_reward=1495.60 +/- 275.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=292000, episode_reward=1735.40 +/- 112.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=293000, episode_reward=1564.00 +/- 284.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=294000, episode_reward=1479.80 +/- 249.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=295000, episode_reward=1466.60 +/- 138.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=296000, episode_reward=1634.20 +/- 223.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=297000, episode_reward=1540.60 +/- 238.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=298000, episode_reward=1463.00 +/- 225.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=299000, episode_reward=1623.80 +/- 248.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=300000, episode_reward=1552.80 +/- 358.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=301000, episode_reward=1549.40 +/- 230.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=302000, episode_reward=1507.60 +/- 430.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=303000, episode_reward=1568.80 +/- 288.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=304000, episode_reward=1650.80 +/- 150.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=305000, episode_reward=1652.20 +/- 162.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=306000, episode_reward=1566.20 +/- 331.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=307000, episode_reward=1693.20 +/- 170.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=308000, episode_reward=1358.40 +/- 68.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=309000, episode_reward=1620.40 +/- 313.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=310000, episode_reward=1448.60 +/- 259.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=311000, episode_reward=1441.20 +/- 96.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=312000, episode_reward=1379.60 +/- 492.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=313000, episode_reward=1639.00 +/- 274.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=314000, episode_reward=1309.00 +/- 219.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=315000, episode_reward=1585.60 +/- 231.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=316000, episode_reward=1672.00 +/- 142.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=317000, episode_reward=1482.00 +/- 327.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=318000, episode_reward=1505.20 +/- 249.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=319000, episode_reward=1509.20 +/- 189.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=320000, episode_reward=1393.00 +/- 163.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=321000, episode_reward=1504.40 +/- 419.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=322000, episode_reward=1452.20 +/- 220.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=323000, episode_reward=1610.60 +/- 74.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=324000, episode_reward=1497.00 +/- 82.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=325000, episode_reward=1503.20 +/- 234.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=326000, episode_reward=1596.60 +/- 191.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=327000, episode_reward=1520.00 +/- 240.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=328000, episode_reward=1579.80 +/- 229.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=329000, episode_reward=1569.40 +/- 67.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=330000, episode_reward=1418.80 +/- 271.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=331000, episode_reward=1519.20 +/- 153.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=332000, episode_reward=1546.60 +/- 175.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=333000, episode_reward=1652.40 +/- 199.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=334000, episode_reward=1680.00 +/- 127.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=335000, episode_reward=1701.00 +/- 119.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=336000, episode_reward=1757.40 +/- 197.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=337000, episode_reward=1515.80 +/- 295.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=338000, episode_reward=1664.40 +/- 342.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=339000, episode_reward=1487.80 +/- 263.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=340000, episode_reward=1659.20 +/- 167.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=341000, episode_reward=1595.80 +/- 167.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=342000, episode_reward=1616.40 +/- 122.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=343000, episode_reward=1508.00 +/- 254.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=344000, episode_reward=1456.20 +/- 255.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=345000, episode_reward=1499.60 +/- 316.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=346000, episode_reward=1554.40 +/- 350.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=347000, episode_reward=1545.00 +/- 136.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=348000, episode_reward=1572.00 +/- 180.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=349000, episode_reward=1697.80 +/- 77.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=350000, episode_reward=1681.80 +/- 188.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=351000, episode_reward=1543.00 +/- 183.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=352000, episode_reward=1539.80 +/- 153.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=353000, episode_reward=1533.40 +/- 199.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=354000, episode_reward=1672.60 +/- 294.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=355000, episode_reward=1517.80 +/- 269.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=356000, episode_reward=1607.80 +/- 201.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=357000, episode_reward=1612.00 +/- 377.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=358000, episode_reward=1587.40 +/- 187.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=359000, episode_reward=1571.80 +/- 107.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=360000, episode_reward=1522.60 +/- 286.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=361000, episode_reward=1607.80 +/- 258.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=362000, episode_reward=1610.40 +/- 328.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=363000, episode_reward=1713.00 +/- 273.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=364000, episode_reward=1563.00 +/- 155.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=365000, episode_reward=1758.20 +/- 123.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=366000, episode_reward=1510.80 +/- 68.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=367000, episode_reward=1550.00 +/- 349.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=368000, episode_reward=1644.40 +/- 295.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=369000, episode_reward=1618.40 +/- 117.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=370000, episode_reward=1639.20 +/- 314.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=371000, episode_reward=1552.80 +/- 130.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=372000, episode_reward=1696.00 +/- 112.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=373000, episode_reward=1548.80 +/- 70.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=374000, episode_reward=1413.60 +/- 181.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=375000, episode_reward=1563.60 +/- 342.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=376000, episode_reward=1559.60 +/- 133.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=377000, episode_reward=1628.00 +/- 188.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=378000, episode_reward=1507.00 +/- 126.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=379000, episode_reward=1412.20 +/- 225.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=380000, episode_reward=1650.60 +/- 288.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=381000, episode_reward=1485.20 +/- 245.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=382000, episode_reward=1650.60 +/- 274.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=383000, episode_reward=1723.20 +/- 185.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=384000, episode_reward=1602.40 +/- 306.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=385000, episode_reward=1561.20 +/- 152.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=386000, episode_reward=1649.20 +/- 266.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=387000, episode_reward=1476.80 +/- 95.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=388000, episode_reward=1473.60 +/- 181.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=389000, episode_reward=1807.40 +/- 152.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=390000, episode_reward=1551.40 +/- 156.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=391000, episode_reward=1619.00 +/- 116.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=392000, episode_reward=1424.20 +/- 145.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=393000, episode_reward=1586.20 +/- 247.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=394000, episode_reward=1461.40 +/- 184.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=395000, episode_reward=1605.60 +/- 291.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=396000, episode_reward=1757.80 +/- 176.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=397000, episode_reward=1640.40 +/- 128.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=398000, episode_reward=1652.20 +/- 187.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=399000, episode_reward=1429.60 +/- 158.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=400000, episode_reward=1560.40 +/- 196.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=401000, episode_reward=1503.80 +/- 123.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=402000, episode_reward=1541.60 +/- 339.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=403000, episode_reward=1584.20 +/- 82.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=404000, episode_reward=1682.00 +/- 56.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=405000, episode_reward=1525.20 +/- 130.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=406000, episode_reward=1534.40 +/- 389.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=407000, episode_reward=1450.40 +/- 288.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=408000, episode_reward=1655.20 +/- 131.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=409000, episode_reward=1513.80 +/- 217.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=410000, episode_reward=1522.20 +/- 210.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=411000, episode_reward=1489.00 +/- 226.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=412000, episode_reward=1493.40 +/- 170.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=413000, episode_reward=1502.20 +/- 187.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=414000, episode_reward=1472.20 +/- 164.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=415000, episode_reward=1602.20 +/- 217.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=416000, episode_reward=1409.60 +/- 237.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=417000, episode_reward=1826.80 +/- 76.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=418000, episode_reward=1489.60 +/- 216.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=419000, episode_reward=1652.20 +/- 221.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=420000, episode_reward=1680.00 +/- 135.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=421000, episode_reward=1546.60 +/- 186.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=422000, episode_reward=1675.20 +/- 211.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=423000, episode_reward=1611.20 +/- 305.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=424000, episode_reward=1583.40 +/- 161.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=425000, episode_reward=1681.40 +/- 193.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=426000, episode_reward=1484.40 +/- 334.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=427000, episode_reward=1554.40 +/- 82.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=428000, episode_reward=1752.20 +/- 254.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=429000, episode_reward=1585.20 +/- 191.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=430000, episode_reward=1507.20 +/- 199.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=431000, episode_reward=1712.60 +/- 114.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=432000, episode_reward=1582.40 +/- 236.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=433000, episode_reward=1655.20 +/- 101.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=434000, episode_reward=1461.80 +/- 305.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=435000, episode_reward=1711.60 +/- 141.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=436000, episode_reward=1665.00 +/- 149.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=437000, episode_reward=1518.40 +/- 386.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=438000, episode_reward=1468.80 +/- 238.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=439000, episode_reward=1470.40 +/- 384.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=440000, episode_reward=1643.40 +/- 255.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=441000, episode_reward=1661.40 +/- 154.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=442000, episode_reward=1344.40 +/- 534.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=443000, episode_reward=1565.20 +/- 224.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=444000, episode_reward=1366.00 +/- 208.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=445000, episode_reward=1431.00 +/- 263.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=446000, episode_reward=1374.40 +/- 364.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=447000, episode_reward=1699.00 +/- 119.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=448000, episode_reward=1638.20 +/- 132.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=449000, episode_reward=1634.60 +/- 243.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=450000, episode_reward=1551.20 +/- 219.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=451000, episode_reward=1593.80 +/- 56.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=452000, episode_reward=1509.00 +/- 258.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=453000, episode_reward=1638.80 +/- 213.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=454000, episode_reward=1623.20 +/- 143.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=455000, episode_reward=1673.80 +/- 304.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=456000, episode_reward=1805.40 +/- 186.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=457000, episode_reward=1730.20 +/- 149.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=458000, episode_reward=1594.40 +/- 296.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=459000, episode_reward=1610.40 +/- 178.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=460000, episode_reward=1619.60 +/- 390.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=461000, episode_reward=1715.80 +/- 80.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=462000, episode_reward=1659.60 +/- 253.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=463000, episode_reward=1685.00 +/- 156.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=464000, episode_reward=1622.60 +/- 448.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=465000, episode_reward=1616.40 +/- 361.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=466000, episode_reward=1564.60 +/- 212.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=467000, episode_reward=1462.40 +/- 406.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=468000, episode_reward=1645.80 +/- 74.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=469000, episode_reward=1555.00 +/- 229.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=470000, episode_reward=1603.40 +/- 134.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=471000, episode_reward=1658.00 +/- 206.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=472000, episode_reward=1578.40 +/- 239.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=473000, episode_reward=1554.80 +/- 181.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=474000, episode_reward=1731.40 +/- 353.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=475000, episode_reward=1580.40 +/- 165.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=476000, episode_reward=1544.60 +/- 173.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=477000, episode_reward=1485.00 +/- 241.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=478000, episode_reward=1658.00 +/- 257.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=479000, episode_reward=1585.00 +/- 223.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=480000, episode_reward=1483.20 +/- 92.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=481000, episode_reward=1555.60 +/- 202.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=482000, episode_reward=1576.00 +/- 179.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=483000, episode_reward=1549.40 +/- 261.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=484000, episode_reward=1387.60 +/- 295.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=485000, episode_reward=1712.20 +/- 307.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=486000, episode_reward=1621.00 +/- 399.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=487000, episode_reward=1504.60 +/- 273.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=488000, episode_reward=1528.20 +/- 206.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=489000, episode_reward=1612.60 +/- 239.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=490000, episode_reward=1646.00 +/- 273.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=491000, episode_reward=1662.60 +/- 271.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=492000, episode_reward=1737.00 +/- 156.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=493000, episode_reward=1582.00 +/- 135.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=494000, episode_reward=1819.20 +/- 122.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=495000, episode_reward=1659.60 +/- 151.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=496000, episode_reward=1521.80 +/- 231.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=497000, episode_reward=1514.80 +/- 167.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=498000, episode_reward=1561.20 +/- 249.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=499000, episode_reward=1699.20 +/- 195.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=500000, episode_reward=1585.80 +/- 420.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=501000, episode_reward=1562.80 +/- 170.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Gamma = 0.99, Learning Rate = 0.0003, Activation Function = Tanh, Neurons = 64, Process noise = 0.1, Measurement noise = 0.0\n",
      "Eval num_timesteps=1000, episode_reward=-1090.60 +/- 221.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=-944.20 +/- 163.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3000, episode_reward=-48.80 +/- 336.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4000, episode_reward=43.60 +/- 296.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=5000, episode_reward=-383.20 +/- 312.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=6000, episode_reward=-170.80 +/- 135.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=7000, episode_reward=-3601.40 +/- 1028.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=8000, episode_reward=-3595.20 +/- 1335.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=9000, episode_reward=9.80 +/- 161.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=10000, episode_reward=148.60 +/- 157.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=11000, episode_reward=245.00 +/- 158.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=12000, episode_reward=337.80 +/- 120.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=13000, episode_reward=157.40 +/- 243.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=14000, episode_reward=187.40 +/- 221.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=15000, episode_reward=317.20 +/- 123.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=16000, episode_reward=187.00 +/- 345.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=17000, episode_reward=375.40 +/- 280.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=18000, episode_reward=375.60 +/- 189.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=19000, episode_reward=70.60 +/- 186.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=20000, episode_reward=25.80 +/- 200.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=21000, episode_reward=250.60 +/- 338.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=22000, episode_reward=323.20 +/- 246.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=23000, episode_reward=250.80 +/- 261.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=24000, episode_reward=49.40 +/- 247.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=25000, episode_reward=-180.80 +/- 104.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=26000, episode_reward=-30.60 +/- 401.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=27000, episode_reward=373.00 +/- 329.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=28000, episode_reward=-50.60 +/- 481.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=29000, episode_reward=1.60 +/- 86.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=30000, episode_reward=53.40 +/- 335.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=31000, episode_reward=112.40 +/- 140.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=32000, episode_reward=-39.20 +/- 133.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=33000, episode_reward=246.20 +/- 114.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=34000, episode_reward=92.80 +/- 78.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=35000, episode_reward=171.00 +/- 146.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=36000, episode_reward=327.60 +/- 242.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=37000, episode_reward=104.20 +/- 106.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=38000, episode_reward=230.20 +/- 193.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=39000, episode_reward=310.20 +/- 197.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=40000, episode_reward=223.20 +/- 259.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=41000, episode_reward=268.00 +/- 185.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=42000, episode_reward=244.80 +/- 196.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=43000, episode_reward=300.80 +/- 151.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=44000, episode_reward=303.80 +/- 56.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=45000, episode_reward=172.40 +/- 267.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=46000, episode_reward=199.40 +/- 320.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=47000, episode_reward=471.80 +/- 167.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=48000, episode_reward=519.40 +/- 233.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=49000, episode_reward=409.60 +/- 364.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=50000, episode_reward=493.00 +/- 276.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=51000, episode_reward=500.20 +/- 533.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=52000, episode_reward=665.60 +/- 296.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=53000, episode_reward=389.40 +/- 184.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=54000, episode_reward=648.80 +/- 74.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=55000, episode_reward=671.00 +/- 366.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=56000, episode_reward=330.60 +/- 218.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=57000, episode_reward=471.20 +/- 286.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=58000, episode_reward=663.20 +/- 265.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=59000, episode_reward=727.60 +/- 286.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=60000, episode_reward=777.60 +/- 380.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=61000, episode_reward=854.80 +/- 238.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=62000, episode_reward=847.20 +/- 266.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=63000, episode_reward=895.80 +/- 371.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=64000, episode_reward=1038.00 +/- 276.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=65000, episode_reward=1043.20 +/- 168.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=66000, episode_reward=1195.80 +/- 268.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=67000, episode_reward=1178.20 +/- 191.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=68000, episode_reward=1081.00 +/- 230.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=69000, episode_reward=1234.40 +/- 221.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=70000, episode_reward=1426.60 +/- 225.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=71000, episode_reward=1166.40 +/- 262.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=72000, episode_reward=985.60 +/- 242.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=73000, episode_reward=1045.20 +/- 341.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=74000, episode_reward=1196.60 +/- 376.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=75000, episode_reward=1233.00 +/- 148.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=76000, episode_reward=885.60 +/- 259.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=77000, episode_reward=916.80 +/- 466.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=78000, episode_reward=993.20 +/- 171.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=79000, episode_reward=1194.40 +/- 381.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=80000, episode_reward=1277.20 +/- 284.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=81000, episode_reward=1213.40 +/- 248.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=82000, episode_reward=1264.60 +/- 218.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=83000, episode_reward=1101.80 +/- 230.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=84000, episode_reward=1032.00 +/- 569.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=85000, episode_reward=1396.20 +/- 241.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=86000, episode_reward=1318.00 +/- 168.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=87000, episode_reward=1344.00 +/- 148.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=88000, episode_reward=911.00 +/- 315.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=89000, episode_reward=1166.60 +/- 152.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=90000, episode_reward=1463.60 +/- 275.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=91000, episode_reward=1109.20 +/- 132.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=92000, episode_reward=1329.60 +/- 275.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=93000, episode_reward=1404.80 +/- 216.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=94000, episode_reward=1160.60 +/- 175.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=95000, episode_reward=1134.00 +/- 151.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=96000, episode_reward=1164.20 +/- 226.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=97000, episode_reward=1036.00 +/- 228.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=98000, episode_reward=1337.40 +/- 88.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=99000, episode_reward=1445.20 +/- 182.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=100000, episode_reward=1026.60 +/- 309.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=101000, episode_reward=-1135.80 +/- 4962.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=102000, episode_reward=1184.60 +/- 259.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=103000, episode_reward=1472.60 +/- 122.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=104000, episode_reward=1312.00 +/- 239.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=105000, episode_reward=1314.40 +/- 234.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=106000, episode_reward=1211.80 +/- 196.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=107000, episode_reward=-1213.00 +/- 4621.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=108000, episode_reward=1036.00 +/- 326.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=109000, episode_reward=1327.20 +/- 210.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=110000, episode_reward=1278.40 +/- 153.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=111000, episode_reward=1281.80 +/- 363.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=112000, episode_reward=1196.80 +/- 142.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=113000, episode_reward=1284.60 +/- 107.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=114000, episode_reward=1329.20 +/- 380.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=115000, episode_reward=1269.00 +/- 229.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=116000, episode_reward=-2125.20 +/- 6875.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=117000, episode_reward=-1112.60 +/- 4556.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=118000, episode_reward=1284.60 +/- 139.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=119000, episode_reward=1173.00 +/- 181.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=120000, episode_reward=1207.60 +/- 358.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=121000, episode_reward=1166.40 +/- 141.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=122000, episode_reward=-1459.00 +/- 5474.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=123000, episode_reward=1159.20 +/- 237.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=124000, episode_reward=1281.80 +/- 224.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=125000, episode_reward=1419.40 +/- 181.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=126000, episode_reward=1130.40 +/- 251.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=127000, episode_reward=1304.20 +/- 172.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=128000, episode_reward=1378.00 +/- 255.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=129000, episode_reward=1512.00 +/- 177.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=130000, episode_reward=1467.20 +/- 187.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=131000, episode_reward=1411.80 +/- 185.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=132000, episode_reward=1502.40 +/- 186.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=133000, episode_reward=1364.20 +/- 317.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=134000, episode_reward=1499.20 +/- 191.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=135000, episode_reward=1192.80 +/- 202.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=136000, episode_reward=997.40 +/- 236.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=137000, episode_reward=1141.80 +/- 173.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=138000, episode_reward=1142.60 +/- 392.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=139000, episode_reward=1391.60 +/- 68.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=140000, episode_reward=1407.00 +/- 256.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=141000, episode_reward=1449.00 +/- 154.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=142000, episode_reward=347.60 +/- 2140.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=143000, episode_reward=-518.60 +/- 3782.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=144000, episode_reward=1229.80 +/- 311.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=145000, episode_reward=1447.80 +/- 250.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=146000, episode_reward=1444.20 +/- 190.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=147000, episode_reward=1461.60 +/- 101.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=148000, episode_reward=1543.20 +/- 140.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=149000, episode_reward=1486.60 +/- 169.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=150000, episode_reward=1477.40 +/- 225.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=151000, episode_reward=1491.20 +/- 370.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=152000, episode_reward=1370.80 +/- 243.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=153000, episode_reward=1641.00 +/- 228.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=154000, episode_reward=1401.60 +/- 93.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=155000, episode_reward=1504.00 +/- 146.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=156000, episode_reward=1693.40 +/- 127.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=157000, episode_reward=1544.60 +/- 214.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=158000, episode_reward=1287.20 +/- 292.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=159000, episode_reward=1454.40 +/- 239.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=160000, episode_reward=1593.20 +/- 147.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=161000, episode_reward=1452.80 +/- 125.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=162000, episode_reward=1528.00 +/- 86.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=163000, episode_reward=1327.00 +/- 138.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=164000, episode_reward=1573.60 +/- 267.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=165000, episode_reward=1310.00 +/- 220.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=166000, episode_reward=1534.00 +/- 179.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=167000, episode_reward=809.00 +/- 1663.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=168000, episode_reward=1409.20 +/- 205.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=169000, episode_reward=1520.20 +/- 126.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=170000, episode_reward=1588.20 +/- 182.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=171000, episode_reward=1495.00 +/- 79.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=172000, episode_reward=-793.40 +/- 4718.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=173000, episode_reward=1626.00 +/- 172.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=174000, episode_reward=1509.40 +/- 314.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=175000, episode_reward=1536.40 +/- 203.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=176000, episode_reward=-1544.20 +/- 6026.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=177000, episode_reward=1489.40 +/- 210.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=178000, episode_reward=1716.40 +/- 184.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=179000, episode_reward=-191.00 +/- 3706.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=180000, episode_reward=1477.20 +/- 425.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=181000, episode_reward=1495.20 +/- 146.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=182000, episode_reward=1358.00 +/- 249.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=183000, episode_reward=1404.40 +/- 209.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=184000, episode_reward=1508.00 +/- 204.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=185000, episode_reward=1346.40 +/- 325.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=186000, episode_reward=-598.40 +/- 4067.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=187000, episode_reward=1604.00 +/- 97.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=188000, episode_reward=1470.20 +/- 189.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=189000, episode_reward=1581.60 +/- 202.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=190000, episode_reward=1525.20 +/- 204.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=191000, episode_reward=1410.60 +/- 209.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=192000, episode_reward=1521.80 +/- 169.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=193000, episode_reward=1551.20 +/- 132.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=194000, episode_reward=-1217.80 +/- 5671.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=195000, episode_reward=1568.20 +/- 250.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=196000, episode_reward=1549.00 +/- 144.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=197000, episode_reward=1519.20 +/- 227.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=198000, episode_reward=1478.80 +/- 209.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=199000, episode_reward=1660.40 +/- 141.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=200000, episode_reward=1553.40 +/- 213.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=201000, episode_reward=-645.20 +/- 4535.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=202000, episode_reward=1529.80 +/- 238.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=203000, episode_reward=1607.20 +/- 86.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=204000, episode_reward=1605.60 +/- 254.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=205000, episode_reward=1453.40 +/- 364.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=206000, episode_reward=1520.20 +/- 156.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=207000, episode_reward=1589.60 +/- 153.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=208000, episode_reward=1570.20 +/- 257.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=209000, episode_reward=1601.60 +/- 269.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=210000, episode_reward=1590.40 +/- 202.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=211000, episode_reward=1685.60 +/- 292.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=212000, episode_reward=1518.80 +/- 116.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=213000, episode_reward=1518.40 +/- 180.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=214000, episode_reward=1567.80 +/- 114.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=215000, episode_reward=1794.60 +/- 211.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=216000, episode_reward=-2232.60 +/- 7567.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=217000, episode_reward=1635.40 +/- 173.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=218000, episode_reward=1558.40 +/- 97.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=219000, episode_reward=1583.00 +/- 184.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=220000, episode_reward=1584.40 +/- 151.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=221000, episode_reward=1466.80 +/- 322.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=222000, episode_reward=1610.20 +/- 181.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=223000, episode_reward=1445.00 +/- 97.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=224000, episode_reward=1592.40 +/- 251.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=225000, episode_reward=1638.20 +/- 242.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=226000, episode_reward=-1034.40 +/- 4925.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=227000, episode_reward=1493.00 +/- 295.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=228000, episode_reward=1771.20 +/- 169.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=229000, episode_reward=1524.80 +/- 149.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=230000, episode_reward=1660.00 +/- 216.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=231000, episode_reward=1610.00 +/- 185.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=232000, episode_reward=1714.40 +/- 96.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=233000, episode_reward=1421.60 +/- 239.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=234000, episode_reward=1624.60 +/- 173.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=235000, episode_reward=1529.40 +/- 122.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=236000, episode_reward=1710.60 +/- 310.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=237000, episode_reward=1738.20 +/- 192.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=238000, episode_reward=1541.80 +/- 129.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=239000, episode_reward=1570.80 +/- 180.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=240000, episode_reward=1680.80 +/- 122.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=241000, episode_reward=1446.00 +/- 149.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=242000, episode_reward=1595.00 +/- 225.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=243000, episode_reward=1409.00 +/- 267.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=244000, episode_reward=1560.20 +/- 131.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=245000, episode_reward=1504.60 +/- 273.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=246000, episode_reward=1495.00 +/- 167.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=247000, episode_reward=1473.20 +/- 151.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=248000, episode_reward=1775.20 +/- 127.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=249000, episode_reward=1576.20 +/- 263.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=250000, episode_reward=1493.80 +/- 277.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=251000, episode_reward=1681.60 +/- 135.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=252000, episode_reward=1709.00 +/- 116.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=253000, episode_reward=1326.00 +/- 215.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=254000, episode_reward=1438.40 +/- 251.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=255000, episode_reward=1497.20 +/- 204.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=256000, episode_reward=1618.00 +/- 264.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=257000, episode_reward=1613.80 +/- 172.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=258000, episode_reward=1546.00 +/- 215.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=259000, episode_reward=1616.20 +/- 103.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=260000, episode_reward=1512.60 +/- 205.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=261000, episode_reward=1626.00 +/- 163.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=262000, episode_reward=753.80 +/- 1869.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=263000, episode_reward=1584.80 +/- 190.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=264000, episode_reward=1497.60 +/- 261.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=265000, episode_reward=1467.20 +/- 146.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=266000, episode_reward=1616.40 +/- 160.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=267000, episode_reward=1675.20 +/- 90.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=268000, episode_reward=1456.00 +/- 314.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=269000, episode_reward=1437.80 +/- 149.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=270000, episode_reward=1673.40 +/- 158.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=271000, episode_reward=1613.40 +/- 105.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=272000, episode_reward=1612.60 +/- 203.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=273000, episode_reward=1540.20 +/- 206.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=274000, episode_reward=1477.20 +/- 183.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=275000, episode_reward=1425.60 +/- 252.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=276000, episode_reward=-1197.60 +/- 3495.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=277000, episode_reward=1743.20 +/- 90.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=278000, episode_reward=1563.40 +/- 309.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=279000, episode_reward=1651.00 +/- 176.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=280000, episode_reward=1656.60 +/- 231.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=281000, episode_reward=884.80 +/- 1699.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=282000, episode_reward=1309.80 +/- 245.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=283000, episode_reward=1646.60 +/- 94.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=284000, episode_reward=1537.00 +/- 376.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=285000, episode_reward=1414.40 +/- 184.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=286000, episode_reward=-241.00 +/- 3480.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=287000, episode_reward=1644.20 +/- 200.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=288000, episode_reward=1673.20 +/- 153.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=289000, episode_reward=1537.20 +/- 244.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=290000, episode_reward=1636.80 +/- 155.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=291000, episode_reward=1668.60 +/- 185.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=292000, episode_reward=1563.00 +/- 251.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=293000, episode_reward=1371.60 +/- 492.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=294000, episode_reward=1582.60 +/- 413.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=295000, episode_reward=1657.60 +/- 238.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=296000, episode_reward=1634.60 +/- 239.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=297000, episode_reward=1452.20 +/- 286.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=298000, episode_reward=1461.60 +/- 270.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=299000, episode_reward=1509.80 +/- 194.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=300000, episode_reward=1601.20 +/- 151.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=301000, episode_reward=1590.00 +/- 158.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=302000, episode_reward=1565.40 +/- 208.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=303000, episode_reward=1516.20 +/- 204.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=304000, episode_reward=1632.20 +/- 220.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=305000, episode_reward=1349.20 +/- 112.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=306000, episode_reward=1609.20 +/- 156.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=307000, episode_reward=1586.20 +/- 137.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=308000, episode_reward=1588.80 +/- 73.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=309000, episode_reward=1607.20 +/- 272.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=310000, episode_reward=1635.80 +/- 276.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=311000, episode_reward=1601.60 +/- 172.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=312000, episode_reward=1550.20 +/- 252.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=313000, episode_reward=1498.80 +/- 236.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=314000, episode_reward=-1828.00 +/- 6969.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=315000, episode_reward=1487.00 +/- 280.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=316000, episode_reward=1595.00 +/- 200.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=317000, episode_reward=-1078.40 +/- 5293.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=318000, episode_reward=1491.20 +/- 247.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=319000, episode_reward=-1572.80 +/- 6092.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=320000, episode_reward=1547.20 +/- 168.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=321000, episode_reward=1526.00 +/- 224.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=322000, episode_reward=1633.00 +/- 255.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=323000, episode_reward=1486.60 +/- 158.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=324000, episode_reward=1689.00 +/- 220.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=325000, episode_reward=1445.80 +/- 109.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=326000, episode_reward=1476.20 +/- 264.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=327000, episode_reward=-1508.00 +/- 5775.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=328000, episode_reward=1440.20 +/- 182.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=329000, episode_reward=1728.40 +/- 183.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=330000, episode_reward=1382.60 +/- 133.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=331000, episode_reward=1496.80 +/- 278.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=332000, episode_reward=1527.80 +/- 301.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=333000, episode_reward=1561.40 +/- 225.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=334000, episode_reward=1518.20 +/- 144.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=335000, episode_reward=1715.60 +/- 156.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=336000, episode_reward=1589.00 +/- 233.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=337000, episode_reward=1447.40 +/- 91.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=338000, episode_reward=1699.00 +/- 186.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=339000, episode_reward=1474.00 +/- 269.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=340000, episode_reward=1694.00 +/- 125.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=341000, episode_reward=1430.60 +/- 232.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=342000, episode_reward=1395.40 +/- 357.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=343000, episode_reward=1631.40 +/- 168.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=344000, episode_reward=1544.20 +/- 175.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=345000, episode_reward=1632.20 +/- 146.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=346000, episode_reward=1489.20 +/- 65.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=347000, episode_reward=1474.60 +/- 74.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=348000, episode_reward=217.20 +/- 2276.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=349000, episode_reward=-9.80 +/- 3338.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=350000, episode_reward=1535.20 +/- 218.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=351000, episode_reward=1503.60 +/- 210.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=352000, episode_reward=1377.20 +/- 143.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=353000, episode_reward=1437.00 +/- 303.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=354000, episode_reward=1377.20 +/- 265.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=355000, episode_reward=1582.60 +/- 146.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=356000, episode_reward=1533.60 +/- 162.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=357000, episode_reward=1487.80 +/- 199.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=358000, episode_reward=1753.80 +/- 248.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=359000, episode_reward=1549.60 +/- 211.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=360000, episode_reward=1551.20 +/- 81.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=361000, episode_reward=1554.00 +/- 159.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=362000, episode_reward=1669.20 +/- 143.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=363000, episode_reward=1531.00 +/- 283.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=364000, episode_reward=1521.00 +/- 206.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=365000, episode_reward=1354.20 +/- 101.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=366000, episode_reward=1423.60 +/- 297.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=367000, episode_reward=1453.80 +/- 197.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=368000, episode_reward=1474.00 +/- 153.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=369000, episode_reward=1487.80 +/- 134.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=370000, episode_reward=1382.00 +/- 128.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=371000, episode_reward=1507.60 +/- 150.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=372000, episode_reward=1724.80 +/- 166.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=373000, episode_reward=1581.60 +/- 199.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=374000, episode_reward=1502.20 +/- 217.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=375000, episode_reward=1510.80 +/- 177.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=376000, episode_reward=1366.80 +/- 299.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=377000, episode_reward=1483.20 +/- 105.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=378000, episode_reward=1760.60 +/- 147.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=379000, episode_reward=1666.80 +/- 61.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=380000, episode_reward=1585.00 +/- 202.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=381000, episode_reward=1597.20 +/- 189.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=382000, episode_reward=1428.80 +/- 326.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=383000, episode_reward=1517.60 +/- 161.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=384000, episode_reward=1476.40 +/- 306.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=385000, episode_reward=1421.40 +/- 201.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=386000, episode_reward=1557.20 +/- 293.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=387000, episode_reward=1667.20 +/- 189.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=388000, episode_reward=1610.40 +/- 168.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=389000, episode_reward=1606.40 +/- 248.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=390000, episode_reward=175.20 +/- 2771.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=391000, episode_reward=1453.20 +/- 187.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=392000, episode_reward=1632.00 +/- 207.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=393000, episode_reward=1379.00 +/- 156.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=394000, episode_reward=1644.20 +/- 90.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=395000, episode_reward=155.00 +/- 2185.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=396000, episode_reward=1514.80 +/- 166.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=397000, episode_reward=1529.40 +/- 318.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=398000, episode_reward=-2182.00 +/- 7529.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=399000, episode_reward=1640.60 +/- 186.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=400000, episode_reward=1525.00 +/- 194.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=401000, episode_reward=1553.80 +/- 152.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=402000, episode_reward=1424.80 +/- 186.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=403000, episode_reward=1597.60 +/- 181.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=404000, episode_reward=1690.60 +/- 197.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=405000, episode_reward=1434.20 +/- 93.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=406000, episode_reward=1565.40 +/- 151.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=407000, episode_reward=1294.80 +/- 280.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=408000, episode_reward=1571.20 +/- 223.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=409000, episode_reward=1520.40 +/- 153.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=410000, episode_reward=1608.60 +/- 221.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=411000, episode_reward=1687.80 +/- 128.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=412000, episode_reward=1695.60 +/- 271.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=413000, episode_reward=1550.20 +/- 183.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=414000, episode_reward=-1875.00 +/- 6966.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=415000, episode_reward=1486.80 +/- 184.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=416000, episode_reward=1698.40 +/- 165.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=417000, episode_reward=-1624.20 +/- 6057.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=418000, episode_reward=518.40 +/- 2025.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=419000, episode_reward=1379.60 +/- 196.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=420000, episode_reward=1468.00 +/- 216.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=421000, episode_reward=1506.80 +/- 140.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=422000, episode_reward=1614.20 +/- 167.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=423000, episode_reward=1511.80 +/- 206.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=424000, episode_reward=1402.40 +/- 205.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=425000, episode_reward=1388.60 +/- 235.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=426000, episode_reward=1538.80 +/- 145.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=427000, episode_reward=1700.20 +/- 199.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=428000, episode_reward=1429.00 +/- 70.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=429000, episode_reward=1654.80 +/- 156.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=430000, episode_reward=1675.40 +/- 161.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=431000, episode_reward=1699.80 +/- 99.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=432000, episode_reward=1587.60 +/- 198.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=433000, episode_reward=-1336.80 +/- 5576.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=434000, episode_reward=1481.60 +/- 144.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=435000, episode_reward=1662.80 +/- 91.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=436000, episode_reward=1642.00 +/- 127.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=437000, episode_reward=1543.60 +/- 286.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=438000, episode_reward=1577.80 +/- 188.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=439000, episode_reward=1676.40 +/- 186.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=440000, episode_reward=1473.60 +/- 289.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=441000, episode_reward=1599.00 +/- 127.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=442000, episode_reward=1485.40 +/- 148.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=443000, episode_reward=1555.60 +/- 172.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=444000, episode_reward=1631.20 +/- 151.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=445000, episode_reward=-1566.60 +/- 5953.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=446000, episode_reward=1615.80 +/- 210.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=447000, episode_reward=1671.80 +/- 320.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=448000, episode_reward=1773.60 +/- 136.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=449000, episode_reward=-1676.20 +/- 6559.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=450000, episode_reward=1433.80 +/- 324.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=451000, episode_reward=1528.20 +/- 124.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=452000, episode_reward=1536.20 +/- 40.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=453000, episode_reward=1554.20 +/- 108.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=454000, episode_reward=1676.80 +/- 208.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=455000, episode_reward=-1615.80 +/- 6363.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=456000, episode_reward=1454.40 +/- 228.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=457000, episode_reward=1634.00 +/- 262.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=458000, episode_reward=1566.40 +/- 155.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=459000, episode_reward=1442.80 +/- 192.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=460000, episode_reward=1673.40 +/- 91.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=461000, episode_reward=1497.20 +/- 99.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=462000, episode_reward=1643.40 +/- 125.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=463000, episode_reward=1725.20 +/- 109.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=464000, episode_reward=1662.00 +/- 301.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=465000, episode_reward=1590.40 +/- 224.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=466000, episode_reward=1319.00 +/- 138.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=467000, episode_reward=1628.00 +/- 303.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=468000, episode_reward=1780.00 +/- 152.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=469000, episode_reward=1818.40 +/- 200.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=470000, episode_reward=1580.00 +/- 151.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=471000, episode_reward=1575.60 +/- 265.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=472000, episode_reward=1602.20 +/- 290.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=473000, episode_reward=1566.40 +/- 216.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=474000, episode_reward=1695.60 +/- 245.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=475000, episode_reward=1541.40 +/- 186.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=476000, episode_reward=1603.60 +/- 156.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=477000, episode_reward=1519.80 +/- 238.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=478000, episode_reward=1411.80 +/- 392.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=479000, episode_reward=1447.00 +/- 276.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=480000, episode_reward=1645.00 +/- 223.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=481000, episode_reward=1535.40 +/- 140.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=482000, episode_reward=1699.40 +/- 145.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=483000, episode_reward=1749.00 +/- 121.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=484000, episode_reward=1560.60 +/- 203.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=485000, episode_reward=1527.00 +/- 111.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=486000, episode_reward=1591.00 +/- 98.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=487000, episode_reward=1489.40 +/- 202.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=488000, episode_reward=1501.20 +/- 417.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=489000, episode_reward=1707.80 +/- 201.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=490000, episode_reward=1552.20 +/- 226.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=491000, episode_reward=1640.80 +/- 150.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=492000, episode_reward=1639.60 +/- 211.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=493000, episode_reward=1678.80 +/- 105.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=494000, episode_reward=1614.60 +/- 268.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=495000, episode_reward=1506.00 +/- 111.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=496000, episode_reward=1615.80 +/- 137.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=497000, episode_reward=1649.40 +/- 178.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=498000, episode_reward=1620.80 +/- 97.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=499000, episode_reward=1540.20 +/- 194.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=500000, episode_reward=1604.20 +/- 333.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=501000, episode_reward=1426.00 +/- 230.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Gamma = 0.99, Learning Rate = 0.0003, Activation Function = ReLU, Neurons = 8, Process noise = 0.1, Measurement noise = 0.0\n",
      "Eval num_timesteps=1000, episode_reward=-18968.00 +/- 22.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=-18952.00 +/- 25.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3000, episode_reward=-5803.60 +/- 2667.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4000, episode_reward=-5221.20 +/- 2079.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=5000, episode_reward=-7886.60 +/- 1212.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=6000, episode_reward=-6557.80 +/- 1469.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=7000, episode_reward=-5607.80 +/- 1553.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=8000, episode_reward=-5055.40 +/- 1598.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=9000, episode_reward=-5142.00 +/- 939.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=10000, episode_reward=-5585.60 +/- 2594.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=11000, episode_reward=-5678.20 +/- 967.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=12000, episode_reward=-7460.20 +/- 1771.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=13000, episode_reward=-4529.20 +/- 1300.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=14000, episode_reward=-5256.20 +/- 1037.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=15000, episode_reward=-6551.80 +/- 1028.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=16000, episode_reward=-7274.60 +/- 1033.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=17000, episode_reward=-6222.00 +/- 763.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=18000, episode_reward=-6682.40 +/- 1062.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=19000, episode_reward=132.20 +/- 111.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=20000, episode_reward=243.80 +/- 226.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=21000, episode_reward=233.40 +/- 194.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=22000, episode_reward=70.20 +/- 95.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=23000, episode_reward=-4.60 +/- 127.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=24000, episode_reward=48.60 +/- 141.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=25000, episode_reward=35.40 +/- 165.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=26000, episode_reward=34.40 +/- 167.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=27000, episode_reward=-47.00 +/- 159.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=28000, episode_reward=176.20 +/- 141.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=29000, episode_reward=-17.20 +/- 132.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=30000, episode_reward=78.40 +/- 150.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=31000, episode_reward=-25.80 +/- 134.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=32000, episode_reward=-2.20 +/- 155.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=33000, episode_reward=15.60 +/- 127.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=34000, episode_reward=143.40 +/- 154.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=35000, episode_reward=-38.40 +/- 131.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=36000, episode_reward=-44.80 +/- 57.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=37000, episode_reward=351.00 +/- 118.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=38000, episode_reward=385.60 +/- 115.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=39000, episode_reward=259.40 +/- 119.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=40000, episode_reward=295.00 +/- 258.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=41000, episode_reward=128.00 +/- 542.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=42000, episode_reward=-233.40 +/- 491.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=43000, episode_reward=-829.00 +/- 726.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=44000, episode_reward=-151.60 +/- 561.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=45000, episode_reward=-607.20 +/- 971.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=46000, episode_reward=-1274.40 +/- 1061.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=47000, episode_reward=-228.00 +/- 246.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=48000, episode_reward=-66.60 +/- 606.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=49000, episode_reward=148.80 +/- 171.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=50000, episode_reward=463.40 +/- 110.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=51000, episode_reward=251.60 +/- 173.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=52000, episode_reward=-604.60 +/- 719.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=53000, episode_reward=-234.60 +/- 638.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=54000, episode_reward=-18.20 +/- 222.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=55000, episode_reward=-489.00 +/- 730.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=56000, episode_reward=-576.80 +/- 526.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=57000, episode_reward=-520.00 +/- 426.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=58000, episode_reward=-200.20 +/- 332.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=59000, episode_reward=-10.80 +/- 235.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=60000, episode_reward=246.20 +/- 216.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=61000, episode_reward=-38.00 +/- 390.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=62000, episode_reward=6.00 +/- 434.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=63000, episode_reward=-176.80 +/- 299.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=64000, episode_reward=238.00 +/- 177.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=65000, episode_reward=171.80 +/- 301.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=66000, episode_reward=241.20 +/- 42.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=67000, episode_reward=176.60 +/- 354.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=68000, episode_reward=341.40 +/- 178.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=69000, episode_reward=269.20 +/- 164.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=70000, episode_reward=230.60 +/- 119.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=71000, episode_reward=262.40 +/- 137.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=72000, episode_reward=138.20 +/- 275.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=73000, episode_reward=325.60 +/- 345.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=74000, episode_reward=303.60 +/- 186.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=75000, episode_reward=236.20 +/- 264.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=76000, episode_reward=363.80 +/- 211.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=77000, episode_reward=409.80 +/- 150.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=78000, episode_reward=270.20 +/- 198.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=79000, episode_reward=449.40 +/- 53.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=80000, episode_reward=454.40 +/- 174.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=81000, episode_reward=274.80 +/- 221.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=82000, episode_reward=446.60 +/- 226.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=83000, episode_reward=157.80 +/- 122.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=84000, episode_reward=348.00 +/- 91.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=85000, episode_reward=296.60 +/- 160.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=86000, episode_reward=311.20 +/- 202.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=87000, episode_reward=269.00 +/- 223.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=88000, episode_reward=296.00 +/- 195.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=89000, episode_reward=308.00 +/- 200.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=90000, episode_reward=261.80 +/- 138.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=91000, episode_reward=327.20 +/- 83.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=92000, episode_reward=344.20 +/- 206.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=93000, episode_reward=414.20 +/- 145.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=94000, episode_reward=361.20 +/- 219.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=95000, episode_reward=359.80 +/- 218.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=96000, episode_reward=308.40 +/- 126.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=97000, episode_reward=219.20 +/- 129.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=98000, episode_reward=524.60 +/- 159.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=99000, episode_reward=349.20 +/- 186.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=100000, episode_reward=197.20 +/- 128.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=101000, episode_reward=88.20 +/- 271.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=102000, episode_reward=370.80 +/- 139.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=103000, episode_reward=152.20 +/- 279.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=104000, episode_reward=190.20 +/- 176.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=105000, episode_reward=75.20 +/- 227.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=106000, episode_reward=388.00 +/- 107.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=107000, episode_reward=374.40 +/- 104.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=108000, episode_reward=241.80 +/- 61.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=109000, episode_reward=378.20 +/- 149.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=110000, episode_reward=159.20 +/- 160.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=111000, episode_reward=229.60 +/- 207.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=112000, episode_reward=260.60 +/- 142.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=113000, episode_reward=214.40 +/- 249.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=114000, episode_reward=394.40 +/- 152.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=115000, episode_reward=422.80 +/- 19.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=116000, episode_reward=168.40 +/- 286.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=117000, episode_reward=264.40 +/- 116.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=118000, episode_reward=208.80 +/- 195.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=119000, episode_reward=354.60 +/- 248.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=120000, episode_reward=421.40 +/- 183.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=121000, episode_reward=272.20 +/- 173.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=122000, episode_reward=283.00 +/- 130.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=123000, episode_reward=291.40 +/- 116.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=124000, episode_reward=473.80 +/- 56.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=125000, episode_reward=450.20 +/- 215.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=126000, episode_reward=411.40 +/- 228.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=127000, episode_reward=646.40 +/- 203.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=128000, episode_reward=699.40 +/- 185.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=129000, episode_reward=483.40 +/- 133.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=130000, episode_reward=421.40 +/- 158.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=131000, episode_reward=527.40 +/- 228.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=132000, episode_reward=334.00 +/- 192.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=133000, episode_reward=178.60 +/- 186.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=134000, episode_reward=486.40 +/- 240.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=135000, episode_reward=205.40 +/- 210.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=136000, episode_reward=449.40 +/- 92.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=137000, episode_reward=314.40 +/- 131.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=138000, episode_reward=485.80 +/- 183.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=139000, episode_reward=285.00 +/- 84.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=140000, episode_reward=327.00 +/- 255.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=141000, episode_reward=329.00 +/- 126.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=142000, episode_reward=525.40 +/- 186.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=143000, episode_reward=316.60 +/- 253.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=144000, episode_reward=278.60 +/- 199.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=145000, episode_reward=431.20 +/- 197.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=146000, episode_reward=437.40 +/- 154.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=147000, episode_reward=611.20 +/- 118.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=148000, episode_reward=479.20 +/- 330.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=149000, episode_reward=615.60 +/- 218.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=150000, episode_reward=308.20 +/- 219.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=151000, episode_reward=524.40 +/- 176.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=152000, episode_reward=739.20 +/- 205.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=153000, episode_reward=682.40 +/- 259.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=154000, episode_reward=626.60 +/- 215.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=155000, episode_reward=429.00 +/- 260.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=156000, episode_reward=484.40 +/- 133.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=157000, episode_reward=495.00 +/- 369.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=158000, episode_reward=558.60 +/- 274.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=159000, episode_reward=601.40 +/- 370.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=160000, episode_reward=809.00 +/- 211.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=161000, episode_reward=723.20 +/- 216.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=162000, episode_reward=665.80 +/- 152.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=163000, episode_reward=811.40 +/- 206.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=164000, episode_reward=711.60 +/- 223.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=165000, episode_reward=514.80 +/- 138.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=166000, episode_reward=665.40 +/- 205.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=167000, episode_reward=697.40 +/- 230.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=168000, episode_reward=749.20 +/- 179.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=169000, episode_reward=546.60 +/- 128.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=170000, episode_reward=662.20 +/- 191.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=171000, episode_reward=724.80 +/- 176.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=172000, episode_reward=678.80 +/- 294.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=173000, episode_reward=599.60 +/- 219.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=174000, episode_reward=553.00 +/- 229.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=175000, episode_reward=531.20 +/- 139.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=176000, episode_reward=628.80 +/- 115.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=177000, episode_reward=772.60 +/- 254.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=178000, episode_reward=612.80 +/- 188.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=179000, episode_reward=876.80 +/- 224.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=180000, episode_reward=783.00 +/- 310.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=181000, episode_reward=887.20 +/- 267.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=182000, episode_reward=840.80 +/- 122.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=183000, episode_reward=477.00 +/- 301.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=184000, episode_reward=666.00 +/- 309.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=185000, episode_reward=848.60 +/- 292.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=186000, episode_reward=535.60 +/- 155.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=187000, episode_reward=956.60 +/- 338.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=188000, episode_reward=711.80 +/- 331.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=189000, episode_reward=965.60 +/- 117.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=190000, episode_reward=879.40 +/- 114.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=191000, episode_reward=824.20 +/- 205.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=192000, episode_reward=763.80 +/- 401.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=193000, episode_reward=1054.40 +/- 182.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=194000, episode_reward=761.00 +/- 274.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=195000, episode_reward=1007.00 +/- 237.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=196000, episode_reward=935.60 +/- 203.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=197000, episode_reward=745.40 +/- 311.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=198000, episode_reward=593.20 +/- 297.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=199000, episode_reward=903.60 +/- 188.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=200000, episode_reward=1001.40 +/- 183.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=201000, episode_reward=897.40 +/- 123.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=202000, episode_reward=939.40 +/- 353.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=203000, episode_reward=677.00 +/- 222.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=204000, episode_reward=670.20 +/- 110.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=205000, episode_reward=903.80 +/- 262.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=206000, episode_reward=981.80 +/- 214.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=207000, episode_reward=848.80 +/- 330.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=208000, episode_reward=1013.00 +/- 205.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=209000, episode_reward=699.60 +/- 267.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=210000, episode_reward=744.60 +/- 296.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=211000, episode_reward=971.20 +/- 356.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=212000, episode_reward=911.60 +/- 275.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=213000, episode_reward=931.20 +/- 248.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=214000, episode_reward=710.40 +/- 198.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=215000, episode_reward=837.20 +/- 131.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=216000, episode_reward=485.20 +/- 225.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=217000, episode_reward=969.80 +/- 193.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=218000, episode_reward=1005.60 +/- 157.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=219000, episode_reward=879.40 +/- 343.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=220000, episode_reward=696.40 +/- 256.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=221000, episode_reward=663.40 +/- 128.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=222000, episode_reward=1233.00 +/- 267.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=223000, episode_reward=633.40 +/- 415.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=224000, episode_reward=1113.60 +/- 220.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=225000, episode_reward=907.20 +/- 220.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=226000, episode_reward=1022.00 +/- 145.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=227000, episode_reward=1001.40 +/- 202.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=228000, episode_reward=976.40 +/- 276.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=229000, episode_reward=749.60 +/- 325.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=230000, episode_reward=820.40 +/- 241.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=231000, episode_reward=881.60 +/- 372.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=232000, episode_reward=915.00 +/- 58.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=233000, episode_reward=934.80 +/- 314.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=234000, episode_reward=983.80 +/- 238.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=235000, episode_reward=1004.40 +/- 244.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=236000, episode_reward=942.40 +/- 255.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=237000, episode_reward=803.80 +/- 337.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=238000, episode_reward=1035.40 +/- 238.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=239000, episode_reward=945.00 +/- 203.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=240000, episode_reward=1181.40 +/- 240.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=241000, episode_reward=783.60 +/- 264.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=242000, episode_reward=1006.00 +/- 191.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=243000, episode_reward=1296.80 +/- 249.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=244000, episode_reward=1013.00 +/- 197.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=245000, episode_reward=1008.40 +/- 357.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=246000, episode_reward=970.20 +/- 192.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=247000, episode_reward=827.60 +/- 292.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=248000, episode_reward=861.40 +/- 238.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=249000, episode_reward=1040.80 +/- 274.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=250000, episode_reward=886.60 +/- 152.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=251000, episode_reward=1155.80 +/- 213.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=252000, episode_reward=716.80 +/- 282.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=253000, episode_reward=864.80 +/- 189.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=254000, episode_reward=906.80 +/- 191.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=255000, episode_reward=944.40 +/- 236.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=256000, episode_reward=1210.20 +/- 124.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=257000, episode_reward=973.60 +/- 269.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=258000, episode_reward=1295.20 +/- 251.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=259000, episode_reward=956.80 +/- 285.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=260000, episode_reward=1013.00 +/- 55.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=261000, episode_reward=1031.60 +/- 263.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=262000, episode_reward=1044.20 +/- 164.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=263000, episode_reward=886.80 +/- 265.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=264000, episode_reward=1131.40 +/- 224.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=265000, episode_reward=978.80 +/- 93.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=266000, episode_reward=1030.00 +/- 309.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=267000, episode_reward=1214.00 +/- 187.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=268000, episode_reward=812.40 +/- 276.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=269000, episode_reward=840.20 +/- 330.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=270000, episode_reward=775.20 +/- 429.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=271000, episode_reward=1146.40 +/- 345.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=272000, episode_reward=1002.80 +/- 168.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=273000, episode_reward=942.40 +/- 270.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=274000, episode_reward=948.00 +/- 209.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=275000, episode_reward=914.60 +/- 199.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=276000, episode_reward=970.60 +/- 328.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=277000, episode_reward=754.20 +/- 261.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=278000, episode_reward=1103.20 +/- 189.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=279000, episode_reward=1128.60 +/- 214.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=280000, episode_reward=689.60 +/- 413.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=281000, episode_reward=965.00 +/- 552.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=282000, episode_reward=991.20 +/- 182.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=283000, episode_reward=990.00 +/- 291.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=284000, episode_reward=959.00 +/- 266.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=285000, episode_reward=813.60 +/- 259.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=286000, episode_reward=775.20 +/- 305.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=287000, episode_reward=827.20 +/- 361.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=288000, episode_reward=1064.40 +/- 189.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=289000, episode_reward=646.40 +/- 355.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=290000, episode_reward=1201.80 +/- 217.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=291000, episode_reward=758.00 +/- 157.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=292000, episode_reward=711.20 +/- 258.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=293000, episode_reward=1182.40 +/- 300.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=294000, episode_reward=976.80 +/- 136.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=295000, episode_reward=822.20 +/- 319.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=296000, episode_reward=826.00 +/- 301.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=297000, episode_reward=656.00 +/- 340.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=298000, episode_reward=860.00 +/- 285.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=299000, episode_reward=937.60 +/- 352.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=300000, episode_reward=867.40 +/- 396.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=301000, episode_reward=1165.40 +/- 212.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=302000, episode_reward=1027.80 +/- 228.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=303000, episode_reward=890.00 +/- 273.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=304000, episode_reward=624.80 +/- 251.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=305000, episode_reward=835.80 +/- 201.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=306000, episode_reward=1043.20 +/- 196.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=307000, episode_reward=813.80 +/- 279.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=308000, episode_reward=1064.20 +/- 273.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=309000, episode_reward=865.00 +/- 335.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=310000, episode_reward=768.20 +/- 177.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=311000, episode_reward=1047.60 +/- 332.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=312000, episode_reward=1068.80 +/- 267.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=313000, episode_reward=1016.40 +/- 221.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=314000, episode_reward=948.40 +/- 271.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=315000, episode_reward=873.80 +/- 274.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=316000, episode_reward=1104.60 +/- 170.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=317000, episode_reward=932.00 +/- 251.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=318000, episode_reward=986.60 +/- 128.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=319000, episode_reward=700.80 +/- 279.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=320000, episode_reward=992.20 +/- 344.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=321000, episode_reward=918.80 +/- 376.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=322000, episode_reward=1029.80 +/- 172.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=323000, episode_reward=942.20 +/- 222.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=324000, episode_reward=1202.00 +/- 202.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=325000, episode_reward=947.40 +/- 128.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=326000, episode_reward=961.80 +/- 298.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=327000, episode_reward=996.60 +/- 236.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=328000, episode_reward=892.20 +/- 254.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=329000, episode_reward=808.00 +/- 233.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=330000, episode_reward=934.20 +/- 490.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=331000, episode_reward=1142.80 +/- 201.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=332000, episode_reward=1215.60 +/- 57.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=333000, episode_reward=1090.40 +/- 153.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=334000, episode_reward=1168.00 +/- 232.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=335000, episode_reward=1141.40 +/- 98.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=336000, episode_reward=1048.00 +/- 122.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=337000, episode_reward=1031.80 +/- 219.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=338000, episode_reward=1036.00 +/- 368.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=339000, episode_reward=1147.80 +/- 279.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=340000, episode_reward=1160.00 +/- 232.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=341000, episode_reward=853.80 +/- 244.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=342000, episode_reward=1040.60 +/- 246.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=343000, episode_reward=1222.40 +/- 237.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=344000, episode_reward=775.60 +/- 410.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=345000, episode_reward=953.00 +/- 174.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=346000, episode_reward=914.20 +/- 280.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=347000, episode_reward=1106.60 +/- 170.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=348000, episode_reward=970.40 +/- 251.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=349000, episode_reward=1140.60 +/- 269.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=350000, episode_reward=1099.20 +/- 253.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=351000, episode_reward=1018.40 +/- 325.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=352000, episode_reward=933.20 +/- 147.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=353000, episode_reward=1068.00 +/- 224.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=354000, episode_reward=1083.00 +/- 173.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=355000, episode_reward=1137.20 +/- 119.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=356000, episode_reward=1051.80 +/- 345.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=357000, episode_reward=1042.20 +/- 140.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=358000, episode_reward=791.40 +/- 255.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=359000, episode_reward=1196.40 +/- 208.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=360000, episode_reward=989.00 +/- 135.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=361000, episode_reward=1017.40 +/- 251.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=362000, episode_reward=1056.80 +/- 238.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=363000, episode_reward=942.00 +/- 136.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=364000, episode_reward=1131.80 +/- 266.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=365000, episode_reward=1126.20 +/- 251.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=366000, episode_reward=1015.40 +/- 101.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=367000, episode_reward=1093.20 +/- 139.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=368000, episode_reward=1226.40 +/- 78.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=369000, episode_reward=1195.00 +/- 132.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=370000, episode_reward=1171.20 +/- 290.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=371000, episode_reward=1112.80 +/- 320.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=372000, episode_reward=1057.40 +/- 287.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=373000, episode_reward=995.40 +/- 168.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=374000, episode_reward=1027.60 +/- 263.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=375000, episode_reward=911.20 +/- 401.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=376000, episode_reward=1021.80 +/- 328.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=377000, episode_reward=1010.80 +/- 134.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=378000, episode_reward=938.20 +/- 295.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=379000, episode_reward=1153.60 +/- 56.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=380000, episode_reward=1029.20 +/- 211.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=381000, episode_reward=1138.20 +/- 304.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=382000, episode_reward=1149.00 +/- 150.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=383000, episode_reward=973.00 +/- 135.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=384000, episode_reward=909.60 +/- 220.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=385000, episode_reward=1272.40 +/- 119.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=386000, episode_reward=1256.60 +/- 203.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=387000, episode_reward=986.00 +/- 163.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=388000, episode_reward=901.40 +/- 418.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=389000, episode_reward=1131.80 +/- 85.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=390000, episode_reward=1146.80 +/- 264.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=391000, episode_reward=1018.60 +/- 222.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=392000, episode_reward=784.40 +/- 227.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=393000, episode_reward=973.20 +/- 267.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=394000, episode_reward=1137.40 +/- 150.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=395000, episode_reward=903.20 +/- 193.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=396000, episode_reward=1076.80 +/- 333.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=397000, episode_reward=1146.80 +/- 265.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=398000, episode_reward=1110.60 +/- 142.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=399000, episode_reward=1011.20 +/- 179.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=400000, episode_reward=837.20 +/- 141.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=401000, episode_reward=809.60 +/- 365.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=402000, episode_reward=1175.80 +/- 360.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=403000, episode_reward=1153.00 +/- 187.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=404000, episode_reward=1121.00 +/- 195.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=405000, episode_reward=1090.60 +/- 259.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=406000, episode_reward=962.80 +/- 131.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=407000, episode_reward=1140.20 +/- 150.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=408000, episode_reward=1113.60 +/- 159.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=409000, episode_reward=1125.20 +/- 327.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=410000, episode_reward=1186.00 +/- 185.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=411000, episode_reward=1166.60 +/- 158.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=412000, episode_reward=1068.60 +/- 82.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=413000, episode_reward=1006.40 +/- 146.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=414000, episode_reward=1159.00 +/- 250.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=415000, episode_reward=921.40 +/- 233.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=416000, episode_reward=1086.60 +/- 175.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=417000, episode_reward=1016.00 +/- 526.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=418000, episode_reward=1226.00 +/- 218.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=419000, episode_reward=917.60 +/- 269.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=420000, episode_reward=1103.80 +/- 97.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=421000, episode_reward=1306.00 +/- 290.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=422000, episode_reward=895.80 +/- 314.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=423000, episode_reward=990.00 +/- 129.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=424000, episode_reward=1164.00 +/- 113.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=425000, episode_reward=948.80 +/- 228.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=426000, episode_reward=1194.80 +/- 213.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=427000, episode_reward=1085.20 +/- 308.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=428000, episode_reward=1080.80 +/- 163.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=429000, episode_reward=1088.60 +/- 146.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=430000, episode_reward=1182.20 +/- 280.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=431000, episode_reward=1163.40 +/- 177.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=432000, episode_reward=1174.80 +/- 276.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=433000, episode_reward=1209.60 +/- 246.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=434000, episode_reward=539.00 +/- 1304.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=435000, episode_reward=1079.80 +/- 154.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=436000, episode_reward=1039.20 +/- 191.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=437000, episode_reward=1264.20 +/- 275.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=438000, episode_reward=989.40 +/- 259.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=439000, episode_reward=1236.60 +/- 153.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=440000, episode_reward=1185.80 +/- 312.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=441000, episode_reward=1210.40 +/- 244.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=442000, episode_reward=1022.60 +/- 366.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=443000, episode_reward=978.00 +/- 180.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=444000, episode_reward=975.80 +/- 179.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=445000, episode_reward=1155.80 +/- 358.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=446000, episode_reward=1253.00 +/- 203.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=447000, episode_reward=1229.60 +/- 150.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=448000, episode_reward=949.00 +/- 249.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=449000, episode_reward=1114.00 +/- 332.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=450000, episode_reward=1212.80 +/- 221.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=451000, episode_reward=778.20 +/- 304.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=452000, episode_reward=998.20 +/- 261.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=453000, episode_reward=913.60 +/- 283.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=454000, episode_reward=1007.00 +/- 242.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=455000, episode_reward=1192.80 +/- 232.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=456000, episode_reward=1192.40 +/- 149.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=457000, episode_reward=1019.40 +/- 304.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=458000, episode_reward=1139.20 +/- 181.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=459000, episode_reward=1043.40 +/- 434.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=460000, episode_reward=1128.40 +/- 281.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=461000, episode_reward=1140.80 +/- 159.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=462000, episode_reward=1226.80 +/- 179.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=463000, episode_reward=884.20 +/- 321.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=464000, episode_reward=1126.00 +/- 249.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=465000, episode_reward=963.40 +/- 208.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=466000, episode_reward=934.40 +/- 384.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=467000, episode_reward=1112.60 +/- 314.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=468000, episode_reward=1092.20 +/- 224.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=469000, episode_reward=1150.40 +/- 126.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=470000, episode_reward=1210.80 +/- 247.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=471000, episode_reward=1203.40 +/- 168.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=472000, episode_reward=1279.80 +/- 226.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=473000, episode_reward=1133.00 +/- 207.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=474000, episode_reward=1169.00 +/- 252.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=475000, episode_reward=1137.20 +/- 138.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=476000, episode_reward=1169.60 +/- 292.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=477000, episode_reward=1236.40 +/- 236.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=478000, episode_reward=1143.20 +/- 154.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=479000, episode_reward=1152.80 +/- 305.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=480000, episode_reward=1289.60 +/- 340.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=481000, episode_reward=1363.60 +/- 281.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=482000, episode_reward=1380.40 +/- 231.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=483000, episode_reward=1089.20 +/- 459.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=484000, episode_reward=1192.80 +/- 161.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=485000, episode_reward=1088.20 +/- 274.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=486000, episode_reward=1439.20 +/- 136.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=487000, episode_reward=1185.60 +/- 135.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=488000, episode_reward=1308.60 +/- 133.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=489000, episode_reward=1053.80 +/- 197.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=490000, episode_reward=985.60 +/- 220.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=491000, episode_reward=1115.00 +/- 228.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=492000, episode_reward=969.60 +/- 282.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=493000, episode_reward=1216.80 +/- 285.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=494000, episode_reward=991.60 +/- 180.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=495000, episode_reward=1192.80 +/- 284.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=496000, episode_reward=1183.80 +/- 200.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=497000, episode_reward=1005.80 +/- 339.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=498000, episode_reward=1082.80 +/- 171.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=499000, episode_reward=937.20 +/- 534.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=500000, episode_reward=1124.80 +/- 279.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=501000, episode_reward=1126.60 +/- 324.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Gamma = 0.99, Learning Rate = 0.0003, Activation Function = ReLU, Neurons = 32, Process noise = 0.1, Measurement noise = 0.0\n",
      "Eval num_timesteps=1000, episode_reward=-1880.60 +/- 205.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=-1928.60 +/- 194.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=3000, episode_reward=-5.80 +/- 227.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4000, episode_reward=-375.40 +/- 484.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=5000, episode_reward=-469.00 +/- 411.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=6000, episode_reward=-84.60 +/- 336.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=7000, episode_reward=-305.60 +/- 529.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=8000, episode_reward=-909.20 +/- 770.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=9000, episode_reward=-314.40 +/- 288.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=10000, episode_reward=-540.00 +/- 843.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=11000, episode_reward=-6851.80 +/- 2277.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=12000, episode_reward=-6588.00 +/- 1463.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=13000, episode_reward=-808.80 +/- 390.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=14000, episode_reward=-1050.00 +/- 386.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=15000, episode_reward=-964.40 +/- 418.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=16000, episode_reward=-703.20 +/- 481.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=17000, episode_reward=-6509.40 +/- 1069.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=18000, episode_reward=-6526.00 +/- 2817.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=19000, episode_reward=-8699.40 +/- 2251.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=20000, episode_reward=-5956.80 +/- 1193.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=21000, episode_reward=-6216.20 +/- 1033.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=22000, episode_reward=-6606.60 +/- 2141.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=23000, episode_reward=-811.00 +/- 432.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=24000, episode_reward=-297.60 +/- 253.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=25000, episode_reward=-238.60 +/- 174.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=26000, episode_reward=-177.00 +/- 221.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=27000, episode_reward=-408.00 +/- 512.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=28000, episode_reward=-565.80 +/- 535.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=29000, episode_reward=101.40 +/- 228.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=30000, episode_reward=309.80 +/- 164.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=31000, episode_reward=-34.40 +/- 271.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=32000, episode_reward=-132.60 +/- 108.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=33000, episode_reward=-79.80 +/- 169.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=34000, episode_reward=-525.40 +/- 291.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=35000, episode_reward=-972.00 +/- 492.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=36000, episode_reward=-1398.60 +/- 291.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=37000, episode_reward=-204.80 +/- 343.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=38000, episode_reward=-209.20 +/- 123.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=39000, episode_reward=-610.00 +/- 308.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=40000, episode_reward=-358.20 +/- 196.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=41000, episode_reward=-3291.60 +/- 606.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=42000, episode_reward=-2822.40 +/- 401.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=43000, episode_reward=-3719.80 +/- 279.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=44000, episode_reward=-2283.40 +/- 845.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=45000, episode_reward=-2453.80 +/- 1083.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=46000, episode_reward=-1451.60 +/- 480.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=47000, episode_reward=-1689.00 +/- 1301.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=48000, episode_reward=-980.40 +/- 924.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=49000, episode_reward=-346.60 +/- 156.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=50000, episode_reward=-136.80 +/- 56.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=51000, episode_reward=-829.60 +/- 440.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=52000, episode_reward=-426.60 +/- 250.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=53000, episode_reward=-243.40 +/- 320.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=54000, episode_reward=70.20 +/- 254.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=55000, episode_reward=-63.80 +/- 367.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=56000, episode_reward=-102.40 +/- 137.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=57000, episode_reward=11.20 +/- 199.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=58000, episode_reward=3.40 +/- 284.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=59000, episode_reward=54.00 +/- 191.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=60000, episode_reward=-30.60 +/- 208.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=61000, episode_reward=127.40 +/- 228.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=62000, episode_reward=146.00 +/- 123.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=63000, episode_reward=205.20 +/- 171.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=64000, episode_reward=250.80 +/- 193.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=65000, episode_reward=289.20 +/- 146.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=66000, episode_reward=252.40 +/- 89.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=67000, episode_reward=283.20 +/- 267.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=68000, episode_reward=242.40 +/- 227.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=69000, episode_reward=225.40 +/- 226.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=70000, episode_reward=270.00 +/- 140.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=71000, episode_reward=272.80 +/- 176.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=72000, episode_reward=256.40 +/- 136.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=73000, episode_reward=494.60 +/- 163.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=74000, episode_reward=188.00 +/- 255.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=75000, episode_reward=294.40 +/- 172.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=76000, episode_reward=296.40 +/- 210.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=77000, episode_reward=268.20 +/- 268.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=78000, episode_reward=160.60 +/- 239.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=79000, episode_reward=260.20 +/- 193.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=80000, episode_reward=196.20 +/- 71.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=81000, episode_reward=344.20 +/- 229.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=82000, episode_reward=260.00 +/- 102.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=83000, episode_reward=242.20 +/- 134.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=84000, episode_reward=427.60 +/- 129.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=85000, episode_reward=119.40 +/- 182.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=86000, episode_reward=203.80 +/- 224.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=87000, episode_reward=277.00 +/- 213.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=88000, episode_reward=227.00 +/- 236.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=89000, episode_reward=366.20 +/- 197.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=90000, episode_reward=473.80 +/- 76.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=91000, episode_reward=192.00 +/- 140.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=92000, episode_reward=428.00 +/- 192.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=93000, episode_reward=433.60 +/- 51.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=94000, episode_reward=226.60 +/- 224.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=95000, episode_reward=214.00 +/- 179.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=96000, episode_reward=373.60 +/- 211.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=97000, episode_reward=192.60 +/- 258.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=98000, episode_reward=84.20 +/- 275.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=99000, episode_reward=246.60 +/- 203.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=100000, episode_reward=350.80 +/- 192.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=101000, episode_reward=413.20 +/- 138.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=102000, episode_reward=357.60 +/- 340.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=103000, episode_reward=70.60 +/- 89.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=104000, episode_reward=340.40 +/- 177.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=105000, episode_reward=197.00 +/- 193.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=106000, episode_reward=522.80 +/- 156.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=107000, episode_reward=296.80 +/- 203.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=108000, episode_reward=215.40 +/- 249.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=109000, episode_reward=436.20 +/- 271.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=110000, episode_reward=361.80 +/- 306.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=111000, episode_reward=483.40 +/- 230.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=112000, episode_reward=628.00 +/- 110.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=113000, episode_reward=366.00 +/- 248.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=114000, episode_reward=141.60 +/- 75.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=115000, episode_reward=280.40 +/- 95.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=116000, episode_reward=275.40 +/- 139.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=117000, episode_reward=279.60 +/- 135.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=118000, episode_reward=256.00 +/- 181.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=119000, episode_reward=615.40 +/- 196.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=120000, episode_reward=648.40 +/- 215.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=121000, episode_reward=554.20 +/- 184.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=122000, episode_reward=400.00 +/- 241.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=123000, episode_reward=374.80 +/- 189.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=124000, episode_reward=283.60 +/- 174.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=125000, episode_reward=129.20 +/- 155.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=126000, episode_reward=450.40 +/- 201.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=127000, episode_reward=580.40 +/- 142.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=128000, episode_reward=349.00 +/- 165.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=129000, episode_reward=414.20 +/- 194.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=130000, episode_reward=780.00 +/- 334.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=131000, episode_reward=379.00 +/- 246.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=132000, episode_reward=513.00 +/- 167.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=133000, episode_reward=517.80 +/- 87.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=134000, episode_reward=542.60 +/- 241.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=135000, episode_reward=540.20 +/- 189.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=136000, episode_reward=667.40 +/- 333.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=137000, episode_reward=358.80 +/- 194.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=138000, episode_reward=309.60 +/- 113.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=139000, episode_reward=295.00 +/- 282.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=140000, episode_reward=318.40 +/- 213.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=141000, episode_reward=409.80 +/- 347.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=142000, episode_reward=736.40 +/- 267.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=143000, episode_reward=679.00 +/- 260.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=144000, episode_reward=780.60 +/- 180.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=145000, episode_reward=741.80 +/- 132.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=146000, episode_reward=845.00 +/- 405.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=147000, episode_reward=619.40 +/- 368.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=148000, episode_reward=549.40 +/- 363.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=149000, episode_reward=707.20 +/- 325.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=150000, episode_reward=677.00 +/- 291.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=151000, episode_reward=707.40 +/- 238.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=152000, episode_reward=429.80 +/- 258.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=153000, episode_reward=495.80 +/- 179.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=154000, episode_reward=610.60 +/- 206.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=155000, episode_reward=545.00 +/- 150.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=156000, episode_reward=706.20 +/- 314.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=157000, episode_reward=700.80 +/- 186.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=158000, episode_reward=664.80 +/- 215.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=159000, episode_reward=568.60 +/- 371.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=160000, episode_reward=596.60 +/- 101.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=161000, episode_reward=807.00 +/- 195.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=162000, episode_reward=627.20 +/- 338.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=163000, episode_reward=758.00 +/- 273.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=164000, episode_reward=492.80 +/- 229.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=165000, episode_reward=612.40 +/- 337.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=166000, episode_reward=876.40 +/- 274.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=167000, episode_reward=853.40 +/- 95.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=168000, episode_reward=916.40 +/- 299.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=169000, episode_reward=948.80 +/- 276.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=170000, episode_reward=733.20 +/- 183.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=171000, episode_reward=847.00 +/- 165.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=172000, episode_reward=873.20 +/- 88.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=173000, episode_reward=752.00 +/- 326.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=174000, episode_reward=674.20 +/- 94.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=175000, episode_reward=854.20 +/- 42.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=176000, episode_reward=913.40 +/- 164.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=177000, episode_reward=920.60 +/- 160.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=178000, episode_reward=685.60 +/- 213.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=179000, episode_reward=794.20 +/- 160.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=180000, episode_reward=832.80 +/- 345.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=181000, episode_reward=875.20 +/- 180.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=182000, episode_reward=846.80 +/- 353.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=183000, episode_reward=977.60 +/- 242.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=184000, episode_reward=948.60 +/- 317.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=185000, episode_reward=925.00 +/- 271.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=186000, episode_reward=789.20 +/- 364.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=187000, episode_reward=998.80 +/- 90.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=188000, episode_reward=876.20 +/- 299.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=189000, episode_reward=909.40 +/- 191.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=190000, episode_reward=934.00 +/- 157.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=191000, episode_reward=805.20 +/- 83.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=192000, episode_reward=977.60 +/- 395.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=193000, episode_reward=532.20 +/- 199.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=194000, episode_reward=896.60 +/- 103.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=195000, episode_reward=862.20 +/- 399.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=196000, episode_reward=934.20 +/- 198.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=197000, episode_reward=848.20 +/- 87.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=198000, episode_reward=1110.60 +/- 91.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=199000, episode_reward=612.20 +/- 168.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=200000, episode_reward=782.00 +/- 296.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=201000, episode_reward=970.40 +/- 130.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=202000, episode_reward=805.20 +/- 475.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=203000, episode_reward=656.80 +/- 199.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=204000, episode_reward=741.20 +/- 202.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=205000, episode_reward=798.80 +/- 214.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=206000, episode_reward=754.00 +/- 189.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=207000, episode_reward=666.00 +/- 332.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=208000, episode_reward=571.60 +/- 216.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=209000, episode_reward=753.60 +/- 274.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=210000, episode_reward=894.20 +/- 427.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=211000, episode_reward=779.20 +/- 193.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=212000, episode_reward=917.20 +/- 107.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=213000, episode_reward=1050.80 +/- 157.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=214000, episode_reward=929.80 +/- 329.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=215000, episode_reward=719.60 +/- 195.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=216000, episode_reward=1005.60 +/- 253.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=217000, episode_reward=1048.00 +/- 236.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=218000, episode_reward=1081.60 +/- 97.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=219000, episode_reward=981.40 +/- 382.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=220000, episode_reward=852.20 +/- 199.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=221000, episode_reward=774.40 +/- 174.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=222000, episode_reward=705.60 +/- 356.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=223000, episode_reward=957.80 +/- 224.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=224000, episode_reward=890.00 +/- 122.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=225000, episode_reward=1001.20 +/- 160.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=226000, episode_reward=945.80 +/- 309.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=227000, episode_reward=1166.20 +/- 88.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=228000, episode_reward=1036.20 +/- 61.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=229000, episode_reward=818.00 +/- 315.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=230000, episode_reward=980.80 +/- 225.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=231000, episode_reward=813.80 +/- 150.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=232000, episode_reward=835.40 +/- 397.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=233000, episode_reward=1018.20 +/- 151.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=234000, episode_reward=846.80 +/- 85.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=235000, episode_reward=959.60 +/- 238.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=236000, episode_reward=896.80 +/- 340.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=237000, episode_reward=978.80 +/- 158.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=238000, episode_reward=1085.00 +/- 200.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=239000, episode_reward=938.00 +/- 335.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=240000, episode_reward=860.20 +/- 443.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=241000, episode_reward=813.20 +/- 443.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=242000, episode_reward=789.80 +/- 201.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=243000, episode_reward=1041.60 +/- 176.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=244000, episode_reward=847.60 +/- 427.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=245000, episode_reward=1151.60 +/- 192.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=246000, episode_reward=897.20 +/- 305.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=247000, episode_reward=1181.20 +/- 214.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=248000, episode_reward=966.80 +/- 175.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=249000, episode_reward=1026.80 +/- 124.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=250000, episode_reward=767.60 +/- 101.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=251000, episode_reward=1027.00 +/- 203.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=252000, episode_reward=911.80 +/- 207.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=253000, episode_reward=1301.80 +/- 221.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=254000, episode_reward=738.00 +/- 269.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=255000, episode_reward=820.40 +/- 371.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=256000, episode_reward=996.80 +/- 167.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=257000, episode_reward=1070.00 +/- 159.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=258000, episode_reward=886.60 +/- 181.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=259000, episode_reward=930.00 +/- 231.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=260000, episode_reward=634.80 +/- 92.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=261000, episode_reward=999.00 +/- 156.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=262000, episode_reward=736.80 +/- 353.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=263000, episode_reward=965.20 +/- 256.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=264000, episode_reward=917.00 +/- 319.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=265000, episode_reward=1078.20 +/- 143.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=266000, episode_reward=921.00 +/- 305.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=267000, episode_reward=944.60 +/- 215.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=268000, episode_reward=924.40 +/- 279.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=269000, episode_reward=1124.20 +/- 249.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=270000, episode_reward=925.40 +/- 132.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=271000, episode_reward=1152.60 +/- 214.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=272000, episode_reward=1072.20 +/- 213.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=273000, episode_reward=1083.40 +/- 268.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=274000, episode_reward=950.80 +/- 293.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=275000, episode_reward=989.00 +/- 154.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=276000, episode_reward=891.60 +/- 299.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=277000, episode_reward=1123.40 +/- 157.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=278000, episode_reward=1235.40 +/- 171.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=279000, episode_reward=854.20 +/- 164.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=280000, episode_reward=894.80 +/- 201.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=281000, episode_reward=1199.20 +/- 178.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=282000, episode_reward=987.00 +/- 270.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=283000, episode_reward=1001.40 +/- 274.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=284000, episode_reward=1032.40 +/- 235.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=285000, episode_reward=1241.60 +/- 75.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=286000, episode_reward=842.00 +/- 345.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=287000, episode_reward=1183.40 +/- 232.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=288000, episode_reward=1019.00 +/- 234.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=289000, episode_reward=969.00 +/- 233.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=290000, episode_reward=1094.00 +/- 402.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=291000, episode_reward=992.40 +/- 160.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=292000, episode_reward=1092.20 +/- 220.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=293000, episode_reward=1076.20 +/- 104.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=294000, episode_reward=1096.40 +/- 318.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=295000, episode_reward=1078.00 +/- 214.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=296000, episode_reward=1168.60 +/- 230.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=297000, episode_reward=1099.60 +/- 150.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=298000, episode_reward=1009.80 +/- 266.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=299000, episode_reward=944.00 +/- 404.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=300000, episode_reward=1113.40 +/- 90.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=301000, episode_reward=1007.60 +/- 374.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=302000, episode_reward=1158.40 +/- 190.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=303000, episode_reward=1134.40 +/- 228.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=304000, episode_reward=1208.80 +/- 121.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=305000, episode_reward=1131.80 +/- 258.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=306000, episode_reward=1119.40 +/- 162.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=307000, episode_reward=1019.60 +/- 204.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=308000, episode_reward=1146.40 +/- 132.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=309000, episode_reward=1052.60 +/- 324.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=310000, episode_reward=1068.20 +/- 160.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=311000, episode_reward=1102.60 +/- 157.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=312000, episode_reward=1120.00 +/- 180.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=313000, episode_reward=1229.00 +/- 133.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=314000, episode_reward=985.80 +/- 262.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=315000, episode_reward=925.00 +/- 250.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=316000, episode_reward=1051.60 +/- 82.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=317000, episode_reward=1141.60 +/- 278.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=318000, episode_reward=1087.60 +/- 239.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=319000, episode_reward=907.60 +/- 352.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=320000, episode_reward=1041.60 +/- 293.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=321000, episode_reward=1374.00 +/- 261.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=322000, episode_reward=1053.00 +/- 126.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=323000, episode_reward=1075.20 +/- 226.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=324000, episode_reward=1027.40 +/- 189.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=325000, episode_reward=1162.00 +/- 94.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=326000, episode_reward=1086.00 +/- 125.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=327000, episode_reward=1181.40 +/- 360.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=328000, episode_reward=1004.20 +/- 316.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=329000, episode_reward=1106.20 +/- 240.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=330000, episode_reward=1016.60 +/- 173.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=331000, episode_reward=1017.20 +/- 153.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=332000, episode_reward=1105.60 +/- 286.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=333000, episode_reward=1188.80 +/- 231.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=334000, episode_reward=1004.80 +/- 161.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=335000, episode_reward=1239.40 +/- 196.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=336000, episode_reward=917.00 +/- 132.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=337000, episode_reward=873.40 +/- 178.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=338000, episode_reward=983.00 +/- 332.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=339000, episode_reward=1180.80 +/- 253.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=340000, episode_reward=883.60 +/- 262.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=341000, episode_reward=924.80 +/- 141.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=342000, episode_reward=956.80 +/- 281.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=343000, episode_reward=897.80 +/- 107.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=344000, episode_reward=1102.20 +/- 180.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=345000, episode_reward=936.80 +/- 301.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=346000, episode_reward=1229.80 +/- 245.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=347000, episode_reward=1021.20 +/- 141.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=348000, episode_reward=1199.60 +/- 213.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=349000, episode_reward=1022.60 +/- 172.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=350000, episode_reward=1263.00 +/- 88.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=351000, episode_reward=974.00 +/- 111.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=352000, episode_reward=967.60 +/- 339.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=353000, episode_reward=1081.00 +/- 247.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=354000, episode_reward=1121.60 +/- 232.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=355000, episode_reward=947.60 +/- 204.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=356000, episode_reward=935.80 +/- 231.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=357000, episode_reward=929.20 +/- 328.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=358000, episode_reward=1106.40 +/- 273.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=359000, episode_reward=851.60 +/- 266.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=360000, episode_reward=1008.00 +/- 278.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=361000, episode_reward=977.00 +/- 290.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=362000, episode_reward=962.60 +/- 183.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=363000, episode_reward=1019.60 +/- 264.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=364000, episode_reward=1045.60 +/- 192.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=365000, episode_reward=1126.40 +/- 366.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=366000, episode_reward=805.20 +/- 199.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=367000, episode_reward=951.40 +/- 464.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=368000, episode_reward=1151.40 +/- 201.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=369000, episode_reward=925.60 +/- 142.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=370000, episode_reward=1021.60 +/- 277.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=371000, episode_reward=950.00 +/- 249.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=372000, episode_reward=990.80 +/- 266.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=373000, episode_reward=828.80 +/- 405.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=374000, episode_reward=813.00 +/- 334.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=375000, episode_reward=1218.20 +/- 194.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=376000, episode_reward=983.60 +/- 342.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=377000, episode_reward=1167.20 +/- 253.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=378000, episode_reward=1116.40 +/- 289.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=379000, episode_reward=896.00 +/- 71.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=380000, episode_reward=979.80 +/- 285.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=381000, episode_reward=1202.40 +/- 211.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=382000, episode_reward=827.60 +/- 284.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=383000, episode_reward=882.00 +/- 163.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=384000, episode_reward=1001.40 +/- 167.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=385000, episode_reward=879.80 +/- 284.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=386000, episode_reward=943.00 +/- 319.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=387000, episode_reward=856.80 +/- 339.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=388000, episode_reward=854.40 +/- 227.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=389000, episode_reward=979.00 +/- 216.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=390000, episode_reward=1156.60 +/- 198.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=391000, episode_reward=1083.00 +/- 265.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=392000, episode_reward=1045.20 +/- 332.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=393000, episode_reward=1015.40 +/- 384.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=394000, episode_reward=1088.00 +/- 180.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=395000, episode_reward=1008.40 +/- 354.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=396000, episode_reward=982.80 +/- 369.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=397000, episode_reward=926.80 +/- 381.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=398000, episode_reward=1107.00 +/- 254.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=399000, episode_reward=1051.20 +/- 300.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=400000, episode_reward=858.80 +/- 222.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=401000, episode_reward=959.40 +/- 215.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=402000, episode_reward=927.00 +/- 365.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=403000, episode_reward=1089.80 +/- 205.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=404000, episode_reward=971.80 +/- 187.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=405000, episode_reward=941.40 +/- 239.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=406000, episode_reward=-298.80 +/- 2807.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=407000, episode_reward=982.80 +/- 100.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=408000, episode_reward=1088.80 +/- 175.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=409000, episode_reward=1031.20 +/- 111.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=410000, episode_reward=1039.00 +/- 171.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=411000, episode_reward=1059.40 +/- 91.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=412000, episode_reward=1008.60 +/- 210.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=413000, episode_reward=828.80 +/- 341.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=414000, episode_reward=1055.40 +/- 202.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=415000, episode_reward=962.20 +/- 246.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=416000, episode_reward=1128.80 +/- 121.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=417000, episode_reward=1128.20 +/- 179.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=418000, episode_reward=1046.20 +/- 141.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=419000, episode_reward=1079.40 +/- 209.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=420000, episode_reward=1079.00 +/- 282.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=421000, episode_reward=1135.60 +/- 329.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=422000, episode_reward=941.00 +/- 143.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=423000, episode_reward=887.40 +/- 373.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=424000, episode_reward=1183.40 +/- 281.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=425000, episode_reward=1004.20 +/- 196.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=426000, episode_reward=1003.40 +/- 271.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=427000, episode_reward=1132.80 +/- 242.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=428000, episode_reward=1036.20 +/- 222.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=429000, episode_reward=1075.20 +/- 277.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=430000, episode_reward=1257.40 +/- 244.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=431000, episode_reward=1219.00 +/- 218.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=432000, episode_reward=1109.00 +/- 273.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=433000, episode_reward=1190.00 +/- 257.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=434000, episode_reward=1074.80 +/- 265.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=435000, episode_reward=1180.80 +/- 135.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=436000, episode_reward=1157.20 +/- 329.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=437000, episode_reward=1179.40 +/- 319.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=438000, episode_reward=1159.60 +/- 355.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=439000, episode_reward=1036.40 +/- 221.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=440000, episode_reward=1152.20 +/- 279.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=441000, episode_reward=1125.20 +/- 181.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=442000, episode_reward=1109.20 +/- 147.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=443000, episode_reward=1114.40 +/- 183.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=444000, episode_reward=983.80 +/- 325.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=445000, episode_reward=1306.00 +/- 281.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=446000, episode_reward=1140.20 +/- 154.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=447000, episode_reward=1145.80 +/- 184.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=448000, episode_reward=1019.20 +/- 238.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=449000, episode_reward=1228.60 +/- 195.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=450000, episode_reward=1025.60 +/- 110.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=451000, episode_reward=905.00 +/- 137.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=452000, episode_reward=840.00 +/- 256.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=453000, episode_reward=763.60 +/- 259.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=454000, episode_reward=1252.80 +/- 336.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=455000, episode_reward=1037.80 +/- 167.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=456000, episode_reward=951.20 +/- 138.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=457000, episode_reward=971.20 +/- 201.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=458000, episode_reward=1144.20 +/- 82.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=459000, episode_reward=931.20 +/- 422.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=460000, episode_reward=1082.60 +/- 152.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=461000, episode_reward=-1141.00 +/- 4309.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=462000, episode_reward=948.20 +/- 213.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=463000, episode_reward=984.40 +/- 167.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=464000, episode_reward=1269.00 +/- 200.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=465000, episode_reward=876.00 +/- 147.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=466000, episode_reward=1136.40 +/- 421.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=467000, episode_reward=1228.20 +/- 226.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=468000, episode_reward=1014.40 +/- 220.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=469000, episode_reward=1084.60 +/- 315.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=470000, episode_reward=990.80 +/- 246.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=471000, episode_reward=1122.20 +/- 318.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=472000, episode_reward=1053.20 +/- 202.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=473000, episode_reward=1047.00 +/- 234.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=474000, episode_reward=1250.60 +/- 270.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=475000, episode_reward=1212.80 +/- 214.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=476000, episode_reward=1207.00 +/- 296.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=477000, episode_reward=1226.80 +/- 167.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=478000, episode_reward=1032.80 +/- 183.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=479000, episode_reward=1076.80 +/- 312.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=480000, episode_reward=1026.60 +/- 216.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=481000, episode_reward=655.20 +/- 602.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=482000, episode_reward=1039.20 +/- 227.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=483000, episode_reward=965.40 +/- 330.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=484000, episode_reward=818.60 +/- 110.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=485000, episode_reward=1281.60 +/- 322.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=486000, episode_reward=1097.60 +/- 262.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=487000, episode_reward=1068.80 +/- 244.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=488000, episode_reward=1198.40 +/- 397.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=489000, episode_reward=1162.20 +/- 365.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=490000, episode_reward=1284.20 +/- 285.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=491000, episode_reward=1058.20 +/- 310.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=492000, episode_reward=857.80 +/- 316.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=493000, episode_reward=1238.20 +/- 243.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=494000, episode_reward=827.40 +/- 280.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=495000, episode_reward=828.80 +/- 418.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=496000, episode_reward=1050.40 +/- 224.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=497000, episode_reward=1370.40 +/- 208.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=498000, episode_reward=-1533.20 +/- 5474.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=499000, episode_reward=1047.00 +/- 369.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=500000, episode_reward=1236.60 +/- 111.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=501000, episode_reward=1087.40 +/- 220.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Gamma = 0.99, Learning Rate = 0.0003, Activation Function = ReLU, Neurons = 64, Process noise = 0.1, Measurement noise = 0.0\n",
      "Eval num_timesteps=1000, episode_reward=-963.40 +/- 155.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=-857.20 +/- 239.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3000, episode_reward=-6618.00 +/- 1395.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=4000, episode_reward=-7325.20 +/- 2403.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=5000, episode_reward=-9270.80 +/- 2174.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=6000, episode_reward=-5957.60 +/- 1977.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=7000, episode_reward=-8875.60 +/- 1976.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=8000, episode_reward=-6497.60 +/- 1133.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=9000, episode_reward=-8026.60 +/- 962.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=10000, episode_reward=-7627.60 +/- 1938.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=11000, episode_reward=305.20 +/- 163.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=12000, episode_reward=236.00 +/- 223.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=13000, episode_reward=310.60 +/- 214.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=14000, episode_reward=391.00 +/- 89.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=15000, episode_reward=-49.60 +/- 147.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=16000, episode_reward=206.60 +/- 263.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=17000, episode_reward=58.80 +/- 235.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=18000, episode_reward=129.20 +/- 259.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=19000, episode_reward=-391.80 +/- 953.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=20000, episode_reward=280.00 +/- 326.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=21000, episode_reward=-114.20 +/- 397.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=22000, episode_reward=-228.80 +/- 707.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=23000, episode_reward=-6704.20 +/- 711.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=24000, episode_reward=-6039.80 +/- 2390.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=25000, episode_reward=-5057.60 +/- 710.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=26000, episode_reward=-5493.60 +/- 1162.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=27000, episode_reward=47.60 +/- 100.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=28000, episode_reward=90.00 +/- 160.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=29000, episode_reward=427.20 +/- 112.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=30000, episode_reward=335.40 +/- 263.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=31000, episode_reward=-232.00 +/- 288.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=32000, episode_reward=-177.60 +/- 193.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=33000, episode_reward=-5581.60 +/- 1025.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=34000, episode_reward=-7337.20 +/- 1723.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=35000, episode_reward=-5869.80 +/- 964.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=36000, episode_reward=-5966.20 +/- 1632.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=37000, episode_reward=-7084.20 +/- 1266.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=38000, episode_reward=-6014.60 +/- 2030.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=39000, episode_reward=-8275.40 +/- 1462.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=40000, episode_reward=-6828.60 +/- 1670.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=41000, episode_reward=-6339.60 +/- 1540.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=42000, episode_reward=-5647.00 +/- 1963.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=43000, episode_reward=-6122.60 +/- 1667.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=44000, episode_reward=-5652.60 +/- 1477.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=45000, episode_reward=-5771.20 +/- 1682.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=46000, episode_reward=-5721.60 +/- 1915.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=47000, episode_reward=-6197.40 +/- 915.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=48000, episode_reward=-4020.80 +/- 187.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=49000, episode_reward=-4216.80 +/- 509.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=50000, episode_reward=-612.80 +/- 404.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=51000, episode_reward=-755.80 +/- 79.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=52000, episode_reward=-1333.00 +/- 224.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=53000, episode_reward=-1286.00 +/- 172.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=54000, episode_reward=53.60 +/- 163.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=55000, episode_reward=183.60 +/- 228.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=56000, episode_reward=147.80 +/- 217.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=57000, episode_reward=140.00 +/- 338.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=58000, episode_reward=389.40 +/- 214.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=59000, episode_reward=194.20 +/- 213.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=60000, episode_reward=250.60 +/- 191.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=61000, episode_reward=202.60 +/- 223.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=62000, episode_reward=227.60 +/- 161.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=63000, episode_reward=212.60 +/- 168.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=64000, episode_reward=361.20 +/- 151.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=65000, episode_reward=206.20 +/- 133.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=66000, episode_reward=307.40 +/- 37.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=67000, episode_reward=532.20 +/- 147.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=68000, episode_reward=344.40 +/- 90.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=69000, episode_reward=176.40 +/- 281.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=70000, episode_reward=409.60 +/- 73.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=71000, episode_reward=229.60 +/- 289.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=72000, episode_reward=174.20 +/- 162.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=73000, episode_reward=372.60 +/- 281.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=74000, episode_reward=288.00 +/- 274.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=75000, episode_reward=372.40 +/- 205.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=76000, episode_reward=249.00 +/- 340.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=77000, episode_reward=366.60 +/- 233.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=78000, episode_reward=335.20 +/- 235.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=79000, episode_reward=211.80 +/- 126.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=80000, episode_reward=371.00 +/- 229.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=81000, episode_reward=260.80 +/- 178.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=82000, episode_reward=409.00 +/- 192.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=83000, episode_reward=325.80 +/- 245.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=84000, episode_reward=264.40 +/- 144.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=85000, episode_reward=306.20 +/- 238.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=86000, episode_reward=361.00 +/- 154.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=87000, episode_reward=120.80 +/- 106.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=88000, episode_reward=454.80 +/- 269.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=89000, episode_reward=375.20 +/- 177.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=90000, episode_reward=371.40 +/- 236.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=91000, episode_reward=330.80 +/- 83.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=92000, episode_reward=297.40 +/- 212.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=93000, episode_reward=284.20 +/- 166.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=94000, episode_reward=177.80 +/- 171.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=95000, episode_reward=213.00 +/- 174.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=96000, episode_reward=128.80 +/- 135.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=97000, episode_reward=363.00 +/- 269.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=98000, episode_reward=303.40 +/- 104.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=99000, episode_reward=266.60 +/- 117.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=100000, episode_reward=434.00 +/- 195.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=101000, episode_reward=476.40 +/- 63.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=102000, episode_reward=431.80 +/- 43.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=103000, episode_reward=307.60 +/- 231.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=104000, episode_reward=241.60 +/- 118.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=105000, episode_reward=159.60 +/- 229.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=106000, episode_reward=474.00 +/- 154.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=107000, episode_reward=374.60 +/- 214.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=108000, episode_reward=343.20 +/- 102.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=109000, episode_reward=429.80 +/- 202.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=110000, episode_reward=390.40 +/- 175.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=111000, episode_reward=206.40 +/- 181.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=112000, episode_reward=294.00 +/- 87.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=113000, episode_reward=261.20 +/- 131.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=114000, episode_reward=348.00 +/- 154.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=115000, episode_reward=279.60 +/- 374.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=116000, episode_reward=328.80 +/- 93.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=117000, episode_reward=318.00 +/- 144.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=118000, episode_reward=354.80 +/- 134.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=119000, episode_reward=404.00 +/- 102.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=120000, episode_reward=402.40 +/- 197.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=121000, episode_reward=214.40 +/- 156.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=122000, episode_reward=292.20 +/- 244.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=123000, episode_reward=111.00 +/- 148.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=124000, episode_reward=411.00 +/- 335.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=125000, episode_reward=391.60 +/- 321.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=126000, episode_reward=224.40 +/- 142.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=127000, episode_reward=286.80 +/- 215.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=128000, episode_reward=340.60 +/- 57.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=129000, episode_reward=326.60 +/- 243.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=130000, episode_reward=282.00 +/- 138.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=131000, episode_reward=428.20 +/- 299.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=132000, episode_reward=332.80 +/- 130.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=133000, episode_reward=228.60 +/- 155.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=134000, episode_reward=394.40 +/- 173.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=135000, episode_reward=359.80 +/- 236.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=136000, episode_reward=467.60 +/- 132.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=137000, episode_reward=352.40 +/- 55.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=138000, episode_reward=223.60 +/- 112.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=139000, episode_reward=331.60 +/- 160.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=140000, episode_reward=316.60 +/- 304.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=141000, episode_reward=290.40 +/- 235.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=142000, episode_reward=391.20 +/- 197.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=143000, episode_reward=265.80 +/- 175.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=144000, episode_reward=247.00 +/- 242.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=145000, episode_reward=316.40 +/- 256.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=146000, episode_reward=260.80 +/- 180.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=147000, episode_reward=317.60 +/- 177.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=148000, episode_reward=397.60 +/- 225.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=149000, episode_reward=217.20 +/- 310.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=150000, episode_reward=414.80 +/- 129.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=151000, episode_reward=463.60 +/- 263.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=152000, episode_reward=190.60 +/- 117.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=153000, episode_reward=545.00 +/- 138.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=154000, episode_reward=377.80 +/- 206.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=155000, episode_reward=391.60 +/- 107.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=156000, episode_reward=241.00 +/- 60.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=157000, episode_reward=311.60 +/- 127.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=158000, episode_reward=366.80 +/- 123.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=159000, episode_reward=339.40 +/- 112.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=160000, episode_reward=263.00 +/- 252.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=161000, episode_reward=403.80 +/- 305.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=162000, episode_reward=257.60 +/- 103.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=163000, episode_reward=449.40 +/- 218.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=164000, episode_reward=293.00 +/- 252.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=165000, episode_reward=258.00 +/- 151.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=166000, episode_reward=327.00 +/- 147.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=167000, episode_reward=332.20 +/- 207.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=168000, episode_reward=339.20 +/- 204.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=169000, episode_reward=432.20 +/- 223.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=170000, episode_reward=499.20 +/- 173.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=171000, episode_reward=258.00 +/- 169.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=172000, episode_reward=395.40 +/- 225.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=173000, episode_reward=294.40 +/- 319.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=174000, episode_reward=-9.60 +/- 135.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=175000, episode_reward=393.40 +/- 217.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=176000, episode_reward=434.00 +/- 212.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=177000, episode_reward=500.00 +/- 182.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=178000, episode_reward=375.20 +/- 103.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=179000, episode_reward=421.00 +/- 123.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=180000, episode_reward=457.80 +/- 57.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=181000, episode_reward=342.60 +/- 283.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=182000, episode_reward=337.80 +/- 170.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=183000, episode_reward=405.60 +/- 269.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=184000, episode_reward=439.20 +/- 138.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=185000, episode_reward=230.00 +/- 115.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=186000, episode_reward=332.40 +/- 245.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=187000, episode_reward=393.80 +/- 85.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=188000, episode_reward=322.60 +/- 209.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=189000, episode_reward=282.00 +/- 112.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=190000, episode_reward=414.00 +/- 147.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=191000, episode_reward=358.40 +/- 111.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=192000, episode_reward=384.60 +/- 165.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=193000, episode_reward=336.20 +/- 70.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=194000, episode_reward=427.60 +/- 155.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=195000, episode_reward=258.80 +/- 158.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=196000, episode_reward=211.60 +/- 196.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=197000, episode_reward=361.80 +/- 143.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=198000, episode_reward=437.20 +/- 253.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=199000, episode_reward=195.00 +/- 128.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=200000, episode_reward=481.60 +/- 165.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=201000, episode_reward=316.00 +/- 151.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=202000, episode_reward=437.80 +/- 300.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=203000, episode_reward=355.00 +/- 94.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=204000, episode_reward=427.40 +/- 214.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=205000, episode_reward=320.00 +/- 129.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=206000, episode_reward=447.80 +/- 212.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=207000, episode_reward=187.40 +/- 239.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=208000, episode_reward=373.80 +/- 163.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=209000, episode_reward=261.80 +/- 170.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=210000, episode_reward=151.00 +/- 167.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=211000, episode_reward=479.20 +/- 157.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=212000, episode_reward=293.80 +/- 185.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=213000, episode_reward=539.20 +/- 109.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=214000, episode_reward=387.40 +/- 175.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=215000, episode_reward=308.20 +/- 170.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=216000, episode_reward=355.00 +/- 135.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=217000, episode_reward=335.20 +/- 154.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=218000, episode_reward=383.80 +/- 206.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=219000, episode_reward=576.20 +/- 65.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=220000, episode_reward=455.60 +/- 248.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=221000, episode_reward=718.60 +/- 263.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=222000, episode_reward=571.40 +/- 164.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=223000, episode_reward=576.40 +/- 275.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=224000, episode_reward=518.40 +/- 180.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=225000, episode_reward=607.00 +/- 177.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=226000, episode_reward=266.80 +/- 59.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=227000, episode_reward=631.60 +/- 194.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=228000, episode_reward=642.80 +/- 194.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=229000, episode_reward=461.80 +/- 297.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=230000, episode_reward=527.00 +/- 105.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=231000, episode_reward=513.00 +/- 225.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=232000, episode_reward=673.40 +/- 148.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=233000, episode_reward=887.00 +/- 425.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=234000, episode_reward=766.80 +/- 298.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=235000, episode_reward=1088.00 +/- 364.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=236000, episode_reward=712.00 +/- 316.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=237000, episode_reward=767.40 +/- 202.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=238000, episode_reward=548.00 +/- 195.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=239000, episode_reward=871.40 +/- 289.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=240000, episode_reward=809.40 +/- 321.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=241000, episode_reward=650.80 +/- 307.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=242000, episode_reward=775.80 +/- 281.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=243000, episode_reward=923.80 +/- 252.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=244000, episode_reward=865.80 +/- 136.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=245000, episode_reward=856.60 +/- 350.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=246000, episode_reward=1111.20 +/- 343.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=247000, episode_reward=1082.20 +/- 251.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=248000, episode_reward=908.20 +/- 46.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=249000, episode_reward=1027.80 +/- 347.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=250000, episode_reward=1067.00 +/- 285.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=251000, episode_reward=1223.80 +/- 291.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=252000, episode_reward=1242.60 +/- 344.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=253000, episode_reward=1033.00 +/- 303.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=254000, episode_reward=1111.20 +/- 328.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=255000, episode_reward=1078.60 +/- 127.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=256000, episode_reward=1084.40 +/- 104.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=257000, episode_reward=964.60 +/- 435.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=258000, episode_reward=981.40 +/- 313.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=259000, episode_reward=951.20 +/- 367.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=260000, episode_reward=1162.40 +/- 211.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=261000, episode_reward=1025.60 +/- 96.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=262000, episode_reward=1180.40 +/- 332.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=263000, episode_reward=859.00 +/- 124.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=264000, episode_reward=1203.40 +/- 164.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=265000, episode_reward=1261.00 +/- 104.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=266000, episode_reward=906.00 +/- 206.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=267000, episode_reward=1198.80 +/- 191.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=268000, episode_reward=865.20 +/- 268.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=269000, episode_reward=1161.40 +/- 279.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=270000, episode_reward=902.00 +/- 388.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=271000, episode_reward=1127.40 +/- 376.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=272000, episode_reward=1130.80 +/- 166.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=273000, episode_reward=1247.00 +/- 213.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=274000, episode_reward=1007.80 +/- 158.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=275000, episode_reward=1276.40 +/- 255.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=276000, episode_reward=1215.20 +/- 307.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=277000, episode_reward=-2290.60 +/- 6732.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=278000, episode_reward=1252.80 +/- 312.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=279000, episode_reward=1199.00 +/- 269.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=280000, episode_reward=1411.20 +/- 206.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=281000, episode_reward=1112.40 +/- 245.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=282000, episode_reward=1118.20 +/- 315.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=283000, episode_reward=958.00 +/- 281.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=284000, episode_reward=961.60 +/- 172.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=285000, episode_reward=1194.80 +/- 232.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=286000, episode_reward=1122.20 +/- 63.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=287000, episode_reward=1048.60 +/- 131.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=288000, episode_reward=1126.60 +/- 256.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=289000, episode_reward=1179.20 +/- 153.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=290000, episode_reward=1079.20 +/- 138.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=291000, episode_reward=1172.00 +/- 257.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=292000, episode_reward=-1635.40 +/- 5646.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=293000, episode_reward=1203.60 +/- 403.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=294000, episode_reward=1192.40 +/- 296.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=295000, episode_reward=1151.60 +/- 434.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=296000, episode_reward=1321.40 +/- 187.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=297000, episode_reward=1050.40 +/- 365.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=298000, episode_reward=861.80 +/- 306.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=299000, episode_reward=1243.40 +/- 258.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=300000, episode_reward=986.80 +/- 218.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=301000, episode_reward=1026.40 +/- 260.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=302000, episode_reward=-2417.20 +/- 6286.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=303000, episode_reward=1247.60 +/- 476.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=304000, episode_reward=1063.00 +/- 330.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=305000, episode_reward=1080.40 +/- 158.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=306000, episode_reward=868.80 +/- 271.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=307000, episode_reward=1327.20 +/- 150.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=308000, episode_reward=1160.20 +/- 260.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=309000, episode_reward=1157.80 +/- 125.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=310000, episode_reward=938.00 +/- 285.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=311000, episode_reward=1057.60 +/- 201.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=312000, episode_reward=1190.00 +/- 254.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=313000, episode_reward=1121.20 +/- 45.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=314000, episode_reward=1195.00 +/- 181.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=315000, episode_reward=964.00 +/- 368.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=316000, episode_reward=1250.00 +/- 299.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=317000, episode_reward=1351.00 +/- 187.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=318000, episode_reward=961.40 +/- 98.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=319000, episode_reward=1283.40 +/- 331.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=320000, episode_reward=965.80 +/- 187.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=321000, episode_reward=1165.40 +/- 124.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=322000, episode_reward=1142.60 +/- 185.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=323000, episode_reward=1341.20 +/- 176.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=324000, episode_reward=915.80 +/- 154.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=325000, episode_reward=1153.80 +/- 317.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=326000, episode_reward=1228.60 +/- 135.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=327000, episode_reward=1377.60 +/- 234.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=328000, episode_reward=1286.00 +/- 233.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=329000, episode_reward=1209.00 +/- 221.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=330000, episode_reward=920.40 +/- 382.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=331000, episode_reward=1024.20 +/- 427.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=332000, episode_reward=1170.80 +/- 324.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=333000, episode_reward=1142.80 +/- 207.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=334000, episode_reward=1230.00 +/- 149.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=335000, episode_reward=-2121.60 +/- 6791.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=336000, episode_reward=1293.80 +/- 212.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=337000, episode_reward=1186.20 +/- 215.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=338000, episode_reward=1357.60 +/- 265.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=339000, episode_reward=-1563.60 +/- 5528.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=340000, episode_reward=1121.60 +/- 359.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=341000, episode_reward=714.80 +/- 883.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=342000, episode_reward=1161.00 +/- 119.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=343000, episode_reward=1091.00 +/- 144.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=344000, episode_reward=1038.60 +/- 177.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=345000, episode_reward=1078.40 +/- 220.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=346000, episode_reward=969.40 +/- 341.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=347000, episode_reward=1215.00 +/- 109.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=348000, episode_reward=1184.40 +/- 156.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=349000, episode_reward=1392.00 +/- 83.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=350000, episode_reward=1041.20 +/- 309.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=351000, episode_reward=582.00 +/- 1048.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=352000, episode_reward=1137.40 +/- 349.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=353000, episode_reward=1256.60 +/- 126.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=354000, episode_reward=1151.20 +/- 123.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=355000, episode_reward=1405.60 +/- 238.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=356000, episode_reward=1469.80 +/- 210.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=357000, episode_reward=-1437.20 +/- 5490.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=358000, episode_reward=1259.40 +/- 405.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=359000, episode_reward=1192.80 +/- 285.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=360000, episode_reward=-2310.60 +/- 6919.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=361000, episode_reward=1203.40 +/- 143.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=362000, episode_reward=1148.80 +/- 341.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=363000, episode_reward=1329.60 +/- 110.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=364000, episode_reward=1070.80 +/- 163.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=365000, episode_reward=1100.80 +/- 255.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=366000, episode_reward=956.60 +/- 436.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=367000, episode_reward=1174.00 +/- 222.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=368000, episode_reward=1098.00 +/- 141.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=369000, episode_reward=1286.40 +/- 66.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=370000, episode_reward=1175.80 +/- 289.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=371000, episode_reward=1174.00 +/- 193.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=372000, episode_reward=1236.40 +/- 231.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=373000, episode_reward=1135.80 +/- 169.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=374000, episode_reward=274.80 +/- 1649.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=375000, episode_reward=1291.80 +/- 295.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=376000, episode_reward=1181.40 +/- 252.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=377000, episode_reward=-1261.20 +/- 5332.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=378000, episode_reward=-1531.80 +/- 3728.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=379000, episode_reward=1280.20 +/- 232.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=380000, episode_reward=1197.00 +/- 249.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=381000, episode_reward=1466.00 +/- 127.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=382000, episode_reward=-1238.00 +/- 4992.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=383000, episode_reward=1188.20 +/- 405.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=384000, episode_reward=1301.60 +/- 263.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=385000, episode_reward=1362.20 +/- 273.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=386000, episode_reward=1312.00 +/- 67.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=387000, episode_reward=1247.00 +/- 204.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=388000, episode_reward=1279.60 +/- 155.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=389000, episode_reward=1337.80 +/- 179.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=390000, episode_reward=1279.40 +/- 219.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=391000, episode_reward=1338.20 +/- 286.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=392000, episode_reward=1387.40 +/- 200.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=393000, episode_reward=1207.40 +/- 144.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=394000, episode_reward=1308.00 +/- 99.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=395000, episode_reward=1014.80 +/- 389.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=396000, episode_reward=1344.00 +/- 169.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=397000, episode_reward=1045.00 +/- 184.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=398000, episode_reward=1052.40 +/- 159.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=399000, episode_reward=1251.60 +/- 199.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=400000, episode_reward=1327.20 +/- 188.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=401000, episode_reward=1231.40 +/- 203.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=402000, episode_reward=1188.60 +/- 221.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=403000, episode_reward=1419.20 +/- 239.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=404000, episode_reward=1395.80 +/- 240.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=405000, episode_reward=1187.40 +/- 174.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=406000, episode_reward=1266.80 +/- 182.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=407000, episode_reward=1160.40 +/- 183.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=408000, episode_reward=1313.40 +/- 196.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=409000, episode_reward=1516.40 +/- 228.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=410000, episode_reward=-3807.20 +/- 6586.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=411000, episode_reward=1240.00 +/- 225.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=412000, episode_reward=1228.00 +/- 172.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=413000, episode_reward=1011.40 +/- 205.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=414000, episode_reward=1448.00 +/- 132.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=415000, episode_reward=-912.80 +/- 4355.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=416000, episode_reward=1232.00 +/- 213.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=417000, episode_reward=1110.20 +/- 136.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=418000, episode_reward=1262.60 +/- 268.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=419000, episode_reward=1205.00 +/- 155.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=420000, episode_reward=-122.40 +/- 2572.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=421000, episode_reward=1263.00 +/- 259.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=422000, episode_reward=1239.20 +/- 270.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=423000, episode_reward=1066.20 +/- 149.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=424000, episode_reward=1280.20 +/- 290.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=425000, episode_reward=988.80 +/- 203.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=426000, episode_reward=1130.40 +/- 153.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=427000, episode_reward=1285.00 +/- 225.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=428000, episode_reward=1325.40 +/- 103.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=429000, episode_reward=1272.60 +/- 283.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=430000, episode_reward=1285.40 +/- 154.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=431000, episode_reward=1251.80 +/- 255.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=432000, episode_reward=1220.00 +/- 226.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=433000, episode_reward=1238.20 +/- 145.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=434000, episode_reward=1248.20 +/- 199.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=435000, episode_reward=-848.80 +/- 4094.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=436000, episode_reward=992.60 +/- 201.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=437000, episode_reward=1097.80 +/- 362.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=438000, episode_reward=960.60 +/- 205.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=439000, episode_reward=1306.20 +/- 123.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=440000, episode_reward=520.40 +/- 1760.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=441000, episode_reward=1201.20 +/- 199.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=442000, episode_reward=1207.80 +/- 224.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=443000, episode_reward=1203.00 +/- 178.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=444000, episode_reward=1138.00 +/- 119.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=445000, episode_reward=1165.40 +/- 253.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=446000, episode_reward=1212.60 +/- 304.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=447000, episode_reward=-618.60 +/- 3577.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=448000, episode_reward=1110.00 +/- 296.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=449000, episode_reward=1094.40 +/- 106.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=450000, episode_reward=-2326.20 +/- 7264.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=451000, episode_reward=1256.40 +/- 140.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=452000, episode_reward=1039.00 +/- 230.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=453000, episode_reward=452.80 +/- 1511.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=454000, episode_reward=1012.20 +/- 361.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=455000, episode_reward=1063.20 +/- 259.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=456000, episode_reward=1135.00 +/- 131.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=457000, episode_reward=1006.60 +/- 239.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=458000, episode_reward=1182.80 +/- 172.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=459000, episode_reward=1423.80 +/- 206.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=460000, episode_reward=-1001.00 +/- 4383.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=461000, episode_reward=1189.00 +/- 288.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=462000, episode_reward=1291.00 +/- 218.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=463000, episode_reward=1005.80 +/- 157.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=464000, episode_reward=1199.80 +/- 133.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=465000, episode_reward=1171.00 +/- 279.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=466000, episode_reward=1353.40 +/- 197.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=467000, episode_reward=683.60 +/- 1471.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=468000, episode_reward=1184.40 +/- 307.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=469000, episode_reward=1256.60 +/- 150.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=470000, episode_reward=1404.40 +/- 203.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=471000, episode_reward=214.80 +/- 2423.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=472000, episode_reward=1411.00 +/- 64.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=473000, episode_reward=1221.80 +/- 176.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=474000, episode_reward=1363.80 +/- 118.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=475000, episode_reward=-48.00 +/- 2742.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=476000, episode_reward=151.40 +/- 2353.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=477000, episode_reward=1216.60 +/- 228.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=478000, episode_reward=1045.20 +/- 137.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=479000, episode_reward=1288.60 +/- 212.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=480000, episode_reward=1178.80 +/- 308.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=481000, episode_reward=1065.60 +/- 195.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=482000, episode_reward=1238.60 +/- 96.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=483000, episode_reward=1203.20 +/- 93.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=484000, episode_reward=1344.20 +/- 163.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=485000, episode_reward=-401.00 +/- 3487.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=486000, episode_reward=1421.00 +/- 221.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=487000, episode_reward=-813.40 +/- 4133.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=488000, episode_reward=958.00 +/- 622.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=489000, episode_reward=-946.20 +/- 4206.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=490000, episode_reward=-165.80 +/- 2860.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=491000, episode_reward=-3917.40 +/- 6428.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=492000, episode_reward=1236.00 +/- 224.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=493000, episode_reward=1140.40 +/- 150.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=494000, episode_reward=1181.40 +/- 104.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=495000, episode_reward=1151.00 +/- 284.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=496000, episode_reward=1251.40 +/- 354.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=497000, episode_reward=-202.80 +/- 2909.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=498000, episode_reward=1254.60 +/- 301.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=499000, episode_reward=1408.40 +/- 124.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=500000, episode_reward=-311.60 +/- 2837.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=501000, episode_reward=1317.00 +/- 281.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Gamma = 0.99, Learning Rate = 3e-05, Activation Function = Tanh, Neurons = 8, Process noise = 0.1, Measurement noise = 0.0\n",
      "Eval num_timesteps=1000, episode_reward=-304.20 +/- 158.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=-292.00 +/- 170.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3000, episode_reward=248.20 +/- 98.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4000, episode_reward=144.00 +/- 83.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=5000, episode_reward=177.20 +/- 246.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=6000, episode_reward=427.20 +/- 237.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=7000, episode_reward=372.60 +/- 157.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=8000, episode_reward=159.60 +/- 221.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=9000, episode_reward=526.40 +/- 195.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=10000, episode_reward=447.20 +/- 322.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=11000, episode_reward=397.80 +/- 157.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=12000, episode_reward=419.40 +/- 188.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=13000, episode_reward=326.00 +/- 261.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=14000, episode_reward=420.80 +/- 165.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=15000, episode_reward=178.20 +/- 219.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=16000, episode_reward=308.20 +/- 197.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=17000, episode_reward=449.00 +/- 113.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=18000, episode_reward=108.40 +/- 328.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=19000, episode_reward=402.40 +/- 60.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=20000, episode_reward=325.80 +/- 91.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=21000, episode_reward=242.00 +/- 52.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=22000, episode_reward=490.80 +/- 215.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=23000, episode_reward=279.00 +/- 149.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=24000, episode_reward=404.40 +/- 326.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=25000, episode_reward=403.40 +/- 196.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=26000, episode_reward=163.60 +/- 293.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=27000, episode_reward=222.60 +/- 258.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=28000, episode_reward=466.60 +/- 208.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=29000, episode_reward=189.00 +/- 143.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=30000, episode_reward=275.40 +/- 184.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=31000, episode_reward=393.20 +/- 205.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=32000, episode_reward=-12.60 +/- 50.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=33000, episode_reward=67.40 +/- 179.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=34000, episode_reward=-46.20 +/- 210.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=35000, episode_reward=18.00 +/- 174.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=36000, episode_reward=275.40 +/- 195.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=37000, episode_reward=137.40 +/- 150.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=38000, episode_reward=186.20 +/- 189.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=39000, episode_reward=149.00 +/- 209.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=40000, episode_reward=34.80 +/- 194.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=41000, episode_reward=76.60 +/- 149.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=42000, episode_reward=300.80 +/- 232.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=43000, episode_reward=246.80 +/- 276.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=44000, episode_reward=96.80 +/- 198.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=45000, episode_reward=120.00 +/- 189.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=46000, episode_reward=62.20 +/- 430.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=47000, episode_reward=97.80 +/- 171.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=48000, episode_reward=117.20 +/- 97.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=49000, episode_reward=30.80 +/- 216.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=50000, episode_reward=-14.40 +/- 214.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=51000, episode_reward=42.20 +/- 288.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=52000, episode_reward=71.60 +/- 158.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=53000, episode_reward=201.20 +/- 178.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=54000, episode_reward=256.00 +/- 322.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=55000, episode_reward=166.80 +/- 157.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=56000, episode_reward=-33.00 +/- 143.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=57000, episode_reward=-22.00 +/- 211.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=58000, episode_reward=25.40 +/- 140.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=59000, episode_reward=242.40 +/- 123.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=60000, episode_reward=21.60 +/- 119.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=61000, episode_reward=-25.40 +/- 158.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=62000, episode_reward=312.20 +/- 190.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=63000, episode_reward=181.80 +/- 209.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=64000, episode_reward=-42.80 +/- 174.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=65000, episode_reward=224.40 +/- 135.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=66000, episode_reward=62.60 +/- 135.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=67000, episode_reward=149.60 +/- 204.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=68000, episode_reward=164.80 +/- 248.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=69000, episode_reward=65.60 +/- 123.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=70000, episode_reward=120.40 +/- 175.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=71000, episode_reward=140.60 +/- 114.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=72000, episode_reward=145.40 +/- 169.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=73000, episode_reward=40.80 +/- 215.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=74000, episode_reward=49.40 +/- 95.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=75000, episode_reward=-66.00 +/- 215.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=76000, episode_reward=2.60 +/- 170.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=77000, episode_reward=50.20 +/- 154.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=78000, episode_reward=-27.00 +/- 183.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=79000, episode_reward=-0.20 +/- 325.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=80000, episode_reward=49.00 +/- 153.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=81000, episode_reward=135.40 +/- 193.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=82000, episode_reward=110.60 +/- 139.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=83000, episode_reward=13.80 +/- 295.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=84000, episode_reward=22.40 +/- 104.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=85000, episode_reward=140.00 +/- 121.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=86000, episode_reward=-13.40 +/- 150.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=87000, episode_reward=-44.40 +/- 286.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=88000, episode_reward=151.20 +/- 162.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=89000, episode_reward=90.60 +/- 226.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=90000, episode_reward=184.60 +/- 166.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=91000, episode_reward=-35.20 +/- 290.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=92000, episode_reward=13.20 +/- 188.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=93000, episode_reward=19.20 +/- 301.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=94000, episode_reward=2.00 +/- 151.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=95000, episode_reward=-96.80 +/- 215.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=96000, episode_reward=82.80 +/- 182.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=97000, episode_reward=1.40 +/- 269.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=98000, episode_reward=175.80 +/- 226.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=99000, episode_reward=120.40 +/- 244.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=100000, episode_reward=88.40 +/- 247.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=101000, episode_reward=-58.00 +/- 242.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=102000, episode_reward=-106.80 +/- 55.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=103000, episode_reward=22.20 +/- 310.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=104000, episode_reward=8.00 +/- 48.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=105000, episode_reward=94.60 +/- 71.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=106000, episode_reward=-62.00 +/- 168.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=107000, episode_reward=119.40 +/- 176.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=108000, episode_reward=-6.20 +/- 378.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=109000, episode_reward=175.40 +/- 97.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=110000, episode_reward=97.80 +/- 174.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=111000, episode_reward=38.40 +/- 170.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=112000, episode_reward=9.20 +/- 216.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=113000, episode_reward=100.00 +/- 199.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=114000, episode_reward=82.40 +/- 116.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=115000, episode_reward=-32.40 +/- 136.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=116000, episode_reward=94.80 +/- 308.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=117000, episode_reward=-107.80 +/- 119.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=118000, episode_reward=158.80 +/- 196.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=119000, episode_reward=230.40 +/- 203.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=120000, episode_reward=207.60 +/- 314.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=121000, episode_reward=139.00 +/- 241.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=122000, episode_reward=37.00 +/- 200.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=123000, episode_reward=130.40 +/- 242.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=124000, episode_reward=-13.60 +/- 200.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=125000, episode_reward=-42.40 +/- 260.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=126000, episode_reward=57.20 +/- 272.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=127000, episode_reward=95.40 +/- 94.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=128000, episode_reward=141.00 +/- 127.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=129000, episode_reward=47.80 +/- 100.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=130000, episode_reward=214.60 +/- 172.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=131000, episode_reward=206.80 +/- 212.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=132000, episode_reward=13.80 +/- 201.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=133000, episode_reward=101.00 +/- 182.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=134000, episode_reward=74.40 +/- 242.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=135000, episode_reward=99.60 +/- 153.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=136000, episode_reward=316.20 +/- 188.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=137000, episode_reward=123.40 +/- 349.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=138000, episode_reward=365.80 +/- 324.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=139000, episode_reward=229.60 +/- 275.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=140000, episode_reward=415.80 +/- 209.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=141000, episode_reward=108.00 +/- 240.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=142000, episode_reward=223.20 +/- 127.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=143000, episode_reward=528.00 +/- 230.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=144000, episode_reward=309.60 +/- 144.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=145000, episode_reward=331.80 +/- 143.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=146000, episode_reward=441.40 +/- 240.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=147000, episode_reward=421.40 +/- 174.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=148000, episode_reward=401.80 +/- 120.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=149000, episode_reward=651.60 +/- 187.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=150000, episode_reward=341.40 +/- 255.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=151000, episode_reward=569.40 +/- 482.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=152000, episode_reward=537.40 +/- 188.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=153000, episode_reward=544.00 +/- 149.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=154000, episode_reward=606.60 +/- 264.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=155000, episode_reward=405.60 +/- 280.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=156000, episode_reward=271.20 +/- 158.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=157000, episode_reward=599.80 +/- 134.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=158000, episode_reward=725.40 +/- 211.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=159000, episode_reward=418.60 +/- 117.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=160000, episode_reward=579.60 +/- 168.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=161000, episode_reward=501.00 +/- 126.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=162000, episode_reward=383.20 +/- 155.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=163000, episode_reward=406.20 +/- 159.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=164000, episode_reward=415.80 +/- 210.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=165000, episode_reward=327.20 +/- 240.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=166000, episode_reward=497.80 +/- 234.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=167000, episode_reward=462.60 +/- 195.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=168000, episode_reward=269.00 +/- 437.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=169000, episode_reward=412.20 +/- 246.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=170000, episode_reward=433.60 +/- 214.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=171000, episode_reward=561.00 +/- 214.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=172000, episode_reward=421.40 +/- 373.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=173000, episode_reward=616.40 +/- 297.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=174000, episode_reward=725.60 +/- 196.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=175000, episode_reward=608.40 +/- 94.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=176000, episode_reward=572.20 +/- 310.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=177000, episode_reward=503.00 +/- 94.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=178000, episode_reward=574.20 +/- 369.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=179000, episode_reward=774.60 +/- 260.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=180000, episode_reward=645.80 +/- 145.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=181000, episode_reward=557.40 +/- 219.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=182000, episode_reward=262.00 +/- 289.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=183000, episode_reward=640.80 +/- 312.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=184000, episode_reward=567.80 +/- 70.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=185000, episode_reward=558.00 +/- 346.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=186000, episode_reward=670.60 +/- 293.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=187000, episode_reward=692.20 +/- 251.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=188000, episode_reward=426.60 +/- 196.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=189000, episode_reward=722.00 +/- 309.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=190000, episode_reward=573.00 +/- 214.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=191000, episode_reward=650.80 +/- 162.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=192000, episode_reward=498.80 +/- 260.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=193000, episode_reward=664.00 +/- 213.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=194000, episode_reward=526.00 +/- 221.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=195000, episode_reward=554.00 +/- 313.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=196000, episode_reward=638.40 +/- 307.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=197000, episode_reward=672.40 +/- 432.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=198000, episode_reward=614.80 +/- 145.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=199000, episode_reward=723.40 +/- 117.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=200000, episode_reward=635.00 +/- 383.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=201000, episode_reward=724.00 +/- 272.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=202000, episode_reward=601.40 +/- 218.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=203000, episode_reward=816.40 +/- 126.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=204000, episode_reward=636.80 +/- 268.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=205000, episode_reward=535.80 +/- 115.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=206000, episode_reward=686.80 +/- 150.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=207000, episode_reward=615.40 +/- 234.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=208000, episode_reward=522.20 +/- 187.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=209000, episode_reward=673.00 +/- 424.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=210000, episode_reward=683.20 +/- 126.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=211000, episode_reward=596.80 +/- 286.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=212000, episode_reward=659.20 +/- 167.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=213000, episode_reward=540.80 +/- 177.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=214000, episode_reward=558.20 +/- 221.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=215000, episode_reward=753.20 +/- 106.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=216000, episode_reward=604.20 +/- 408.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=217000, episode_reward=818.00 +/- 205.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=218000, episode_reward=561.40 +/- 365.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=219000, episode_reward=506.00 +/- 209.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=220000, episode_reward=663.00 +/- 165.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=221000, episode_reward=543.40 +/- 155.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=222000, episode_reward=835.60 +/- 255.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=223000, episode_reward=595.80 +/- 278.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=224000, episode_reward=507.00 +/- 331.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=225000, episode_reward=757.00 +/- 308.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=226000, episode_reward=623.00 +/- 198.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=227000, episode_reward=594.00 +/- 317.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=228000, episode_reward=601.60 +/- 389.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=229000, episode_reward=639.40 +/- 173.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=230000, episode_reward=565.20 +/- 212.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=231000, episode_reward=532.20 +/- 280.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=232000, episode_reward=502.00 +/- 304.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=233000, episode_reward=868.40 +/- 124.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=234000, episode_reward=456.00 +/- 354.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=235000, episode_reward=696.20 +/- 398.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=236000, episode_reward=538.20 +/- 233.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=237000, episode_reward=565.00 +/- 170.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=238000, episode_reward=269.40 +/- 249.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=239000, episode_reward=658.40 +/- 78.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=240000, episode_reward=448.40 +/- 357.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=241000, episode_reward=434.40 +/- 505.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=242000, episode_reward=655.00 +/- 216.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=243000, episode_reward=727.20 +/- 355.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=244000, episode_reward=604.80 +/- 266.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=245000, episode_reward=618.80 +/- 171.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=246000, episode_reward=539.60 +/- 155.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=247000, episode_reward=500.80 +/- 475.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=248000, episode_reward=463.80 +/- 264.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=249000, episode_reward=461.80 +/- 291.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=250000, episode_reward=571.80 +/- 212.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=251000, episode_reward=368.80 +/- 285.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=252000, episode_reward=568.80 +/- 273.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=253000, episode_reward=760.80 +/- 324.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=254000, episode_reward=719.40 +/- 220.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=255000, episode_reward=685.80 +/- 341.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=256000, episode_reward=717.40 +/- 193.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=257000, episode_reward=563.40 +/- 178.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=258000, episode_reward=531.80 +/- 337.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=259000, episode_reward=518.20 +/- 263.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=260000, episode_reward=418.00 +/- 598.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=261000, episode_reward=655.20 +/- 435.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=262000, episode_reward=430.20 +/- 130.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=263000, episode_reward=590.80 +/- 313.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=264000, episode_reward=853.00 +/- 306.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=265000, episode_reward=465.40 +/- 168.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=266000, episode_reward=511.20 +/- 96.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=267000, episode_reward=562.20 +/- 438.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=268000, episode_reward=525.80 +/- 214.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=269000, episode_reward=697.60 +/- 290.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=270000, episode_reward=975.80 +/- 249.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=271000, episode_reward=495.20 +/- 453.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=272000, episode_reward=277.80 +/- 206.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=273000, episode_reward=428.60 +/- 295.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=274000, episode_reward=273.60 +/- 245.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=275000, episode_reward=456.20 +/- 178.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=276000, episode_reward=438.80 +/- 343.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=277000, episode_reward=292.80 +/- 177.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=278000, episode_reward=659.20 +/- 227.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=279000, episode_reward=639.80 +/- 252.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=280000, episode_reward=474.20 +/- 444.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=281000, episode_reward=761.40 +/- 349.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=282000, episode_reward=708.00 +/- 268.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=283000, episode_reward=273.00 +/- 251.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=284000, episode_reward=637.60 +/- 319.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=285000, episode_reward=728.00 +/- 166.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=286000, episode_reward=530.00 +/- 291.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=287000, episode_reward=313.00 +/- 265.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=288000, episode_reward=502.80 +/- 293.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=289000, episode_reward=325.40 +/- 196.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=290000, episode_reward=446.60 +/- 224.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=291000, episode_reward=417.40 +/- 201.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=292000, episode_reward=557.60 +/- 476.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=293000, episode_reward=375.60 +/- 241.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=294000, episode_reward=382.80 +/- 346.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=295000, episode_reward=370.40 +/- 200.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=296000, episode_reward=261.20 +/- 147.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=297000, episode_reward=488.80 +/- 275.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=298000, episode_reward=584.20 +/- 213.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=299000, episode_reward=733.40 +/- 294.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=300000, episode_reward=563.40 +/- 85.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=301000, episode_reward=286.60 +/- 448.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=302000, episode_reward=270.60 +/- 463.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=303000, episode_reward=491.80 +/- 214.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=304000, episode_reward=552.00 +/- 296.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=305000, episode_reward=603.80 +/- 406.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=306000, episode_reward=741.00 +/- 294.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=307000, episode_reward=273.20 +/- 98.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=308000, episode_reward=401.80 +/- 216.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=309000, episode_reward=767.00 +/- 257.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=310000, episode_reward=461.80 +/- 164.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=311000, episode_reward=715.40 +/- 191.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=312000, episode_reward=477.60 +/- 416.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=313000, episode_reward=359.60 +/- 431.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=314000, episode_reward=458.20 +/- 448.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=315000, episode_reward=351.00 +/- 387.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=316000, episode_reward=702.80 +/- 399.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=317000, episode_reward=867.20 +/- 206.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=318000, episode_reward=724.20 +/- 234.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=319000, episode_reward=549.60 +/- 239.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=320000, episode_reward=604.80 +/- 192.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=321000, episode_reward=676.20 +/- 464.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=322000, episode_reward=637.20 +/- 165.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=323000, episode_reward=716.20 +/- 316.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=324000, episode_reward=687.40 +/- 213.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=325000, episode_reward=883.80 +/- 221.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=326000, episode_reward=588.20 +/- 311.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=327000, episode_reward=620.80 +/- 211.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=328000, episode_reward=794.00 +/- 262.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=329000, episode_reward=421.00 +/- 373.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=330000, episode_reward=803.60 +/- 179.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=331000, episode_reward=382.60 +/- 257.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=332000, episode_reward=660.00 +/- 202.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=333000, episode_reward=868.20 +/- 295.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=334000, episode_reward=673.40 +/- 283.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=335000, episode_reward=922.60 +/- 150.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=336000, episode_reward=492.20 +/- 232.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=337000, episode_reward=679.60 +/- 166.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=338000, episode_reward=798.60 +/- 323.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=339000, episode_reward=553.00 +/- 266.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=340000, episode_reward=653.20 +/- 424.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=341000, episode_reward=598.20 +/- 208.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=342000, episode_reward=686.20 +/- 368.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=343000, episode_reward=768.40 +/- 283.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=344000, episode_reward=506.80 +/- 433.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=345000, episode_reward=763.40 +/- 167.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=346000, episode_reward=829.00 +/- 243.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=347000, episode_reward=769.20 +/- 462.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=348000, episode_reward=735.60 +/- 287.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=349000, episode_reward=790.40 +/- 345.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=350000, episode_reward=744.80 +/- 125.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=351000, episode_reward=982.20 +/- 223.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=352000, episode_reward=688.40 +/- 316.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=353000, episode_reward=516.00 +/- 335.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=354000, episode_reward=613.60 +/- 207.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=355000, episode_reward=813.20 +/- 236.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=356000, episode_reward=748.00 +/- 316.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=357000, episode_reward=839.60 +/- 234.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=358000, episode_reward=1022.00 +/- 304.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=359000, episode_reward=915.80 +/- 242.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=360000, episode_reward=925.00 +/- 262.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=361000, episode_reward=910.60 +/- 152.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=362000, episode_reward=771.00 +/- 225.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=363000, episode_reward=627.00 +/- 327.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=364000, episode_reward=759.80 +/- 138.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=365000, episode_reward=515.60 +/- 335.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=366000, episode_reward=790.80 +/- 166.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=367000, episode_reward=699.40 +/- 251.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=368000, episode_reward=773.80 +/- 195.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=369000, episode_reward=897.00 +/- 200.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=370000, episode_reward=742.60 +/- 131.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=371000, episode_reward=820.00 +/- 319.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=372000, episode_reward=608.00 +/- 475.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=373000, episode_reward=816.40 +/- 367.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=374000, episode_reward=798.40 +/- 156.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=375000, episode_reward=1091.40 +/- 98.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=376000, episode_reward=932.80 +/- 322.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=377000, episode_reward=863.00 +/- 116.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=378000, episode_reward=1049.80 +/- 145.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=379000, episode_reward=612.20 +/- 211.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=380000, episode_reward=877.00 +/- 216.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=381000, episode_reward=789.00 +/- 354.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=382000, episode_reward=770.20 +/- 279.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=383000, episode_reward=699.60 +/- 345.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=384000, episode_reward=682.80 +/- 146.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=385000, episode_reward=818.80 +/- 179.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=386000, episode_reward=796.80 +/- 252.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=387000, episode_reward=632.00 +/- 144.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=388000, episode_reward=651.60 +/- 194.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=389000, episode_reward=895.80 +/- 335.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=390000, episode_reward=667.80 +/- 173.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=391000, episode_reward=791.20 +/- 284.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=392000, episode_reward=547.80 +/- 404.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=393000, episode_reward=883.00 +/- 354.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=394000, episode_reward=1055.60 +/- 221.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=395000, episode_reward=550.80 +/- 270.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=396000, episode_reward=743.80 +/- 242.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=397000, episode_reward=1008.00 +/- 172.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=398000, episode_reward=795.20 +/- 187.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=399000, episode_reward=800.40 +/- 212.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=400000, episode_reward=835.80 +/- 147.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=401000, episode_reward=671.40 +/- 325.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=402000, episode_reward=843.00 +/- 229.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=403000, episode_reward=931.40 +/- 156.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=404000, episode_reward=970.20 +/- 174.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=405000, episode_reward=723.60 +/- 509.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=406000, episode_reward=788.40 +/- 221.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=407000, episode_reward=858.60 +/- 211.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=408000, episode_reward=735.60 +/- 206.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=409000, episode_reward=701.20 +/- 495.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=410000, episode_reward=656.00 +/- 212.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=411000, episode_reward=715.60 +/- 179.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=412000, episode_reward=978.80 +/- 219.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=413000, episode_reward=585.40 +/- 244.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=414000, episode_reward=784.00 +/- 128.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=415000, episode_reward=480.00 +/- 194.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=416000, episode_reward=614.20 +/- 271.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=417000, episode_reward=898.60 +/- 357.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=418000, episode_reward=781.40 +/- 151.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=419000, episode_reward=897.20 +/- 251.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=420000, episode_reward=657.00 +/- 382.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=421000, episode_reward=907.00 +/- 239.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=422000, episode_reward=655.40 +/- 216.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=423000, episode_reward=815.40 +/- 154.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=424000, episode_reward=677.00 +/- 230.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=425000, episode_reward=719.60 +/- 317.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=426000, episode_reward=731.40 +/- 105.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=427000, episode_reward=880.60 +/- 112.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=428000, episode_reward=997.80 +/- 213.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=429000, episode_reward=566.60 +/- 198.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=430000, episode_reward=626.60 +/- 310.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=431000, episode_reward=694.40 +/- 369.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=432000, episode_reward=751.00 +/- 174.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=433000, episode_reward=854.80 +/- 349.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=434000, episode_reward=1026.20 +/- 235.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=435000, episode_reward=647.80 +/- 414.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=436000, episode_reward=1028.60 +/- 238.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=437000, episode_reward=715.00 +/- 114.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=438000, episode_reward=782.40 +/- 232.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=439000, episode_reward=817.20 +/- 119.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=440000, episode_reward=781.80 +/- 325.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=441000, episode_reward=728.60 +/- 215.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=442000, episode_reward=763.60 +/- 344.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=443000, episode_reward=786.60 +/- 234.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=444000, episode_reward=710.00 +/- 248.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=445000, episode_reward=683.40 +/- 230.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=446000, episode_reward=781.60 +/- 206.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=447000, episode_reward=788.60 +/- 386.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=448000, episode_reward=772.60 +/- 198.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=449000, episode_reward=693.80 +/- 183.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=450000, episode_reward=858.20 +/- 239.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=451000, episode_reward=780.40 +/- 292.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=452000, episode_reward=582.00 +/- 288.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=453000, episode_reward=926.40 +/- 241.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=454000, episode_reward=981.40 +/- 207.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=455000, episode_reward=620.40 +/- 431.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=456000, episode_reward=928.20 +/- 269.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=457000, episode_reward=574.40 +/- 436.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=458000, episode_reward=750.20 +/- 316.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=459000, episode_reward=830.20 +/- 143.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=460000, episode_reward=681.40 +/- 188.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=461000, episode_reward=918.40 +/- 260.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=462000, episode_reward=647.00 +/- 249.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=463000, episode_reward=532.60 +/- 308.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=464000, episode_reward=671.20 +/- 298.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=465000, episode_reward=623.20 +/- 262.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=466000, episode_reward=736.40 +/- 178.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=467000, episode_reward=684.00 +/- 319.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=468000, episode_reward=675.60 +/- 325.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=469000, episode_reward=686.60 +/- 224.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=470000, episode_reward=616.00 +/- 250.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=471000, episode_reward=789.00 +/- 191.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=472000, episode_reward=456.80 +/- 176.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=473000, episode_reward=831.40 +/- 263.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=474000, episode_reward=539.80 +/- 349.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=475000, episode_reward=630.80 +/- 252.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=476000, episode_reward=531.40 +/- 304.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=477000, episode_reward=825.80 +/- 223.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=478000, episode_reward=615.60 +/- 166.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=479000, episode_reward=681.00 +/- 171.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=480000, episode_reward=371.40 +/- 241.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=481000, episode_reward=640.60 +/- 123.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=482000, episode_reward=480.60 +/- 327.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=483000, episode_reward=556.40 +/- 446.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=484000, episode_reward=343.60 +/- 167.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=485000, episode_reward=600.80 +/- 212.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=486000, episode_reward=471.40 +/- 300.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=487000, episode_reward=528.40 +/- 206.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=488000, episode_reward=391.20 +/- 376.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=489000, episode_reward=765.40 +/- 254.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=490000, episode_reward=415.40 +/- 356.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=491000, episode_reward=623.80 +/- 171.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=492000, episode_reward=724.20 +/- 181.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=493000, episode_reward=298.80 +/- 212.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=494000, episode_reward=454.40 +/- 434.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=495000, episode_reward=650.00 +/- 425.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=496000, episode_reward=684.00 +/- 304.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=497000, episode_reward=505.60 +/- 152.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=498000, episode_reward=429.80 +/- 181.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=499000, episode_reward=893.00 +/- 227.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=500000, episode_reward=582.40 +/- 287.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=501000, episode_reward=524.00 +/- 346.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Gamma = 0.99, Learning Rate = 3e-05, Activation Function = Tanh, Neurons = 32, Process noise = 0.1, Measurement noise = 0.0\n",
      "Eval num_timesteps=1000, episode_reward=-19027.00 +/- 74.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=-18910.00 +/- 97.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3000, episode_reward=-1192.80 +/- 349.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4000, episode_reward=-1054.60 +/- 184.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=5000, episode_reward=209.00 +/- 230.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=6000, episode_reward=160.80 +/- 123.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=7000, episode_reward=-5089.80 +/- 1993.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=8000, episode_reward=-3531.60 +/- 1276.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=9000, episode_reward=-5134.60 +/- 1531.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=10000, episode_reward=-3374.80 +/- 1085.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=11000, episode_reward=-5495.20 +/- 2735.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=12000, episode_reward=-6353.20 +/- 1573.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=13000, episode_reward=-4110.40 +/- 1192.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=14000, episode_reward=-6316.40 +/- 1497.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=15000, episode_reward=-7007.60 +/- 1968.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=16000, episode_reward=-7265.80 +/- 1356.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=17000, episode_reward=-6929.20 +/- 1733.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=18000, episode_reward=-6950.00 +/- 2044.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=19000, episode_reward=-7748.00 +/- 1228.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=20000, episode_reward=-7272.60 +/- 1532.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=21000, episode_reward=-5285.60 +/- 1297.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=22000, episode_reward=-6796.80 +/- 1715.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=23000, episode_reward=-6146.20 +/- 1408.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=24000, episode_reward=-6009.80 +/- 1068.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=25000, episode_reward=-6046.20 +/- 1224.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=26000, episode_reward=-5675.00 +/- 1798.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=27000, episode_reward=-6227.80 +/- 1900.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=28000, episode_reward=-6865.60 +/- 1635.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=29000, episode_reward=-6083.40 +/- 1877.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=30000, episode_reward=-6585.40 +/- 1315.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=31000, episode_reward=-6049.60 +/- 1539.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=32000, episode_reward=-5020.20 +/- 1284.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=33000, episode_reward=-6431.80 +/- 1741.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=34000, episode_reward=-7348.00 +/- 2148.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=35000, episode_reward=-4722.20 +/- 1900.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=36000, episode_reward=-7608.20 +/- 2611.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=37000, episode_reward=-5446.20 +/- 1674.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=38000, episode_reward=-8530.00 +/- 849.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=39000, episode_reward=-4870.80 +/- 1962.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=40000, episode_reward=-7832.20 +/- 3001.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=41000, episode_reward=-7643.20 +/- 1651.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=42000, episode_reward=-6221.00 +/- 1362.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=43000, episode_reward=-6024.80 +/- 2268.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=44000, episode_reward=-5488.20 +/- 937.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=45000, episode_reward=-6832.20 +/- 1212.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=46000, episode_reward=-6264.40 +/- 2353.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=47000, episode_reward=-6682.80 +/- 2187.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=48000, episode_reward=-8475.80 +/- 1885.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=49000, episode_reward=-6971.00 +/- 1524.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=50000, episode_reward=-6319.40 +/- 655.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=51000, episode_reward=-5793.00 +/- 748.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=52000, episode_reward=-4475.80 +/- 1283.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=53000, episode_reward=-5216.40 +/- 1719.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=54000, episode_reward=-6717.40 +/- 1659.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=55000, episode_reward=-4805.20 +/- 2533.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=56000, episode_reward=-5920.60 +/- 1632.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=57000, episode_reward=-6208.60 +/- 2378.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=58000, episode_reward=-6012.40 +/- 1508.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=59000, episode_reward=-3156.80 +/- 2706.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=60000, episode_reward=-6875.40 +/- 1097.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=61000, episode_reward=-6081.80 +/- 2703.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=62000, episode_reward=-4966.20 +/- 2081.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=63000, episode_reward=-6075.60 +/- 1133.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=64000, episode_reward=-7363.40 +/- 2150.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=65000, episode_reward=-5730.80 +/- 1667.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=66000, episode_reward=-6672.60 +/- 1831.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=67000, episode_reward=-6194.20 +/- 2106.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=68000, episode_reward=-6898.80 +/- 1949.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=69000, episode_reward=-5793.40 +/- 2615.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=70000, episode_reward=-5637.80 +/- 2182.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=71000, episode_reward=-6911.60 +/- 1925.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=72000, episode_reward=-7853.80 +/- 2157.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=73000, episode_reward=-5902.00 +/- 3129.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=74000, episode_reward=-7917.20 +/- 2126.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=75000, episode_reward=-4922.80 +/- 2197.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=76000, episode_reward=-6964.80 +/- 1594.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=77000, episode_reward=-7040.60 +/- 990.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=78000, episode_reward=-6458.20 +/- 907.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=79000, episode_reward=-5864.20 +/- 1041.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=80000, episode_reward=-5425.80 +/- 2159.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=81000, episode_reward=-7444.00 +/- 2086.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=82000, episode_reward=-6823.00 +/- 2230.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=83000, episode_reward=-6063.20 +/- 2484.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=84000, episode_reward=-7413.40 +/- 1944.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=85000, episode_reward=-6669.80 +/- 2196.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=86000, episode_reward=-6472.00 +/- 1622.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=87000, episode_reward=-1768.80 +/- 316.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=88000, episode_reward=-1942.80 +/- 218.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=89000, episode_reward=-1188.20 +/- 417.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=90000, episode_reward=-1354.60 +/- 388.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=91000, episode_reward=-814.40 +/- 257.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=92000, episode_reward=-642.00 +/- 483.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=93000, episode_reward=-456.80 +/- 342.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=94000, episode_reward=-502.00 +/- 183.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=95000, episode_reward=-543.60 +/- 205.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=96000, episode_reward=-692.00 +/- 245.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=97000, episode_reward=-219.80 +/- 352.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=98000, episode_reward=-149.80 +/- 390.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=99000, episode_reward=-128.40 +/- 364.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=100000, episode_reward=-119.00 +/- 401.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=101000, episode_reward=-395.80 +/- 360.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=102000, episode_reward=-521.00 +/- 551.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=103000, episode_reward=-233.20 +/- 212.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=104000, episode_reward=-681.60 +/- 418.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=105000, episode_reward=-119.40 +/- 343.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=106000, episode_reward=-575.00 +/- 711.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=107000, episode_reward=79.00 +/- 364.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=108000, episode_reward=-442.60 +/- 420.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=109000, episode_reward=38.20 +/- 221.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=110000, episode_reward=193.80 +/- 152.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=111000, episode_reward=-23.20 +/- 308.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=112000, episode_reward=136.00 +/- 206.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=113000, episode_reward=6.60 +/- 351.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=114000, episode_reward=82.20 +/- 107.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=115000, episode_reward=-93.00 +/- 118.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=116000, episode_reward=-96.00 +/- 142.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=117000, episode_reward=-142.60 +/- 240.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=118000, episode_reward=-151.40 +/- 295.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=119000, episode_reward=-27.60 +/- 250.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=120000, episode_reward=16.80 +/- 325.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=121000, episode_reward=-95.20 +/- 236.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=122000, episode_reward=-45.40 +/- 295.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=123000, episode_reward=142.60 +/- 215.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=124000, episode_reward=231.80 +/- 205.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=125000, episode_reward=-148.20 +/- 115.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=126000, episode_reward=45.60 +/- 172.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=127000, episode_reward=127.60 +/- 189.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=128000, episode_reward=-4.80 +/- 117.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=129000, episode_reward=45.80 +/- 274.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=130000, episode_reward=151.60 +/- 312.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=131000, episode_reward=228.20 +/- 291.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=132000, episode_reward=-2.40 +/- 158.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=133000, episode_reward=-53.60 +/- 243.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=134000, episode_reward=-55.40 +/- 84.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=135000, episode_reward=88.00 +/- 103.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=136000, episode_reward=128.00 +/- 241.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=137000, episode_reward=332.40 +/- 166.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=138000, episode_reward=109.40 +/- 113.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=139000, episode_reward=215.00 +/- 187.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=140000, episode_reward=223.20 +/- 224.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=141000, episode_reward=139.20 +/- 182.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=142000, episode_reward=127.00 +/- 339.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=143000, episode_reward=240.40 +/- 99.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=144000, episode_reward=-51.60 +/- 226.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=145000, episode_reward=151.00 +/- 116.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=146000, episode_reward=195.60 +/- 181.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=147000, episode_reward=169.80 +/- 309.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=148000, episode_reward=169.00 +/- 247.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=149000, episode_reward=176.00 +/- 119.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=150000, episode_reward=89.00 +/- 158.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=151000, episode_reward=98.00 +/- 241.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=152000, episode_reward=32.40 +/- 99.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=153000, episode_reward=14.00 +/- 209.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=154000, episode_reward=148.00 +/- 168.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=155000, episode_reward=104.80 +/- 96.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=156000, episode_reward=-10.40 +/- 225.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=157000, episode_reward=132.20 +/- 73.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=158000, episode_reward=121.40 +/- 160.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=159000, episode_reward=348.40 +/- 279.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=160000, episode_reward=448.40 +/- 121.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=161000, episode_reward=264.40 +/- 241.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=162000, episode_reward=316.80 +/- 294.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=163000, episode_reward=36.00 +/- 250.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=164000, episode_reward=97.80 +/- 205.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=165000, episode_reward=180.60 +/- 176.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=166000, episode_reward=385.00 +/- 126.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=167000, episode_reward=309.60 +/- 167.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=168000, episode_reward=314.00 +/- 90.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=169000, episode_reward=360.60 +/- 327.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=170000, episode_reward=182.60 +/- 113.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=171000, episode_reward=306.20 +/- 163.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=172000, episode_reward=412.40 +/- 300.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=173000, episode_reward=232.60 +/- 179.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=174000, episode_reward=352.20 +/- 206.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=175000, episode_reward=380.80 +/- 142.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=176000, episode_reward=228.20 +/- 203.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=177000, episode_reward=320.40 +/- 189.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=178000, episode_reward=160.80 +/- 337.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=179000, episode_reward=411.20 +/- 310.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=180000, episode_reward=518.80 +/- 101.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=181000, episode_reward=330.00 +/- 57.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=182000, episode_reward=403.40 +/- 203.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=183000, episode_reward=226.40 +/- 213.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=184000, episode_reward=481.20 +/- 247.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=185000, episode_reward=262.20 +/- 183.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=186000, episode_reward=368.20 +/- 281.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=187000, episode_reward=646.60 +/- 112.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=188000, episode_reward=484.20 +/- 139.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=189000, episode_reward=463.00 +/- 235.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=190000, episode_reward=544.00 +/- 78.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=191000, episode_reward=239.20 +/- 296.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=192000, episode_reward=395.00 +/- 190.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=193000, episode_reward=641.80 +/- 173.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=194000, episode_reward=463.60 +/- 375.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=195000, episode_reward=548.40 +/- 287.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=196000, episode_reward=337.40 +/- 163.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=197000, episode_reward=596.00 +/- 165.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=198000, episode_reward=432.40 +/- 272.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=199000, episode_reward=340.00 +/- 227.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=200000, episode_reward=377.60 +/- 252.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=201000, episode_reward=486.20 +/- 282.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=202000, episode_reward=556.80 +/- 181.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=203000, episode_reward=275.60 +/- 277.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=204000, episode_reward=528.20 +/- 208.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=205000, episode_reward=714.60 +/- 296.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=206000, episode_reward=511.20 +/- 134.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=207000, episode_reward=489.20 +/- 160.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=208000, episode_reward=593.80 +/- 209.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=209000, episode_reward=550.80 +/- 210.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=210000, episode_reward=479.40 +/- 123.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=211000, episode_reward=530.20 +/- 326.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=212000, episode_reward=581.80 +/- 142.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=213000, episode_reward=654.80 +/- 174.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=214000, episode_reward=547.00 +/- 262.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=215000, episode_reward=600.40 +/- 235.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=216000, episode_reward=587.80 +/- 224.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=217000, episode_reward=462.40 +/- 206.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=218000, episode_reward=608.40 +/- 259.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=219000, episode_reward=370.80 +/- 246.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=220000, episode_reward=400.80 +/- 191.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=221000, episode_reward=303.20 +/- 342.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=222000, episode_reward=312.80 +/- 101.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=223000, episode_reward=492.40 +/- 297.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=224000, episode_reward=258.20 +/- 109.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=225000, episode_reward=576.60 +/- 114.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=226000, episode_reward=331.40 +/- 241.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=227000, episode_reward=505.80 +/- 468.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=228000, episode_reward=455.40 +/- 235.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=229000, episode_reward=620.20 +/- 253.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=230000, episode_reward=684.80 +/- 223.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=231000, episode_reward=611.40 +/- 166.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=232000, episode_reward=759.00 +/- 265.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=233000, episode_reward=680.40 +/- 162.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=234000, episode_reward=407.60 +/- 168.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=235000, episode_reward=568.80 +/- 140.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=236000, episode_reward=772.80 +/- 336.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=237000, episode_reward=558.00 +/- 279.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=238000, episode_reward=139.60 +/- 107.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=239000, episode_reward=493.40 +/- 181.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=240000, episode_reward=464.80 +/- 434.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=241000, episode_reward=396.20 +/- 185.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=242000, episode_reward=645.60 +/- 426.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=243000, episode_reward=480.80 +/- 222.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=244000, episode_reward=543.00 +/- 159.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=245000, episode_reward=621.40 +/- 124.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=246000, episode_reward=604.20 +/- 243.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=247000, episode_reward=872.80 +/- 176.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=248000, episode_reward=412.40 +/- 396.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=249000, episode_reward=489.40 +/- 341.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=250000, episode_reward=669.00 +/- 257.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=251000, episode_reward=667.20 +/- 194.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=252000, episode_reward=600.40 +/- 113.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=253000, episode_reward=596.40 +/- 163.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=254000, episode_reward=345.20 +/- 136.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=255000, episode_reward=562.40 +/- 228.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=256000, episode_reward=613.80 +/- 235.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=257000, episode_reward=616.20 +/- 281.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=258000, episode_reward=596.60 +/- 280.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=259000, episode_reward=614.60 +/- 165.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=260000, episode_reward=618.60 +/- 164.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=261000, episode_reward=670.80 +/- 289.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=262000, episode_reward=670.60 +/- 300.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=263000, episode_reward=400.00 +/- 150.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=264000, episode_reward=590.60 +/- 112.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=265000, episode_reward=523.60 +/- 171.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=266000, episode_reward=283.00 +/- 374.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=267000, episode_reward=399.80 +/- 79.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=268000, episode_reward=564.20 +/- 271.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=269000, episode_reward=441.40 +/- 231.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=270000, episode_reward=392.60 +/- 309.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=271000, episode_reward=461.60 +/- 348.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=272000, episode_reward=302.40 +/- 282.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=273000, episode_reward=560.40 +/- 184.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=274000, episode_reward=365.20 +/- 116.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=275000, episode_reward=425.60 +/- 226.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=276000, episode_reward=293.40 +/- 103.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=277000, episode_reward=190.60 +/- 221.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=278000, episode_reward=289.00 +/- 181.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=279000, episode_reward=341.20 +/- 73.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=280000, episode_reward=619.20 +/- 228.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=281000, episode_reward=486.60 +/- 354.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=282000, episode_reward=350.40 +/- 103.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=283000, episode_reward=560.80 +/- 216.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=284000, episode_reward=478.40 +/- 339.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=285000, episode_reward=437.40 +/- 219.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=286000, episode_reward=487.00 +/- 163.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=287000, episode_reward=594.00 +/- 366.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=288000, episode_reward=492.40 +/- 153.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=289000, episode_reward=537.60 +/- 86.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=290000, episode_reward=544.00 +/- 233.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=291000, episode_reward=655.20 +/- 174.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=292000, episode_reward=797.20 +/- 143.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=293000, episode_reward=340.60 +/- 244.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=294000, episode_reward=634.40 +/- 182.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=295000, episode_reward=362.00 +/- 226.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=296000, episode_reward=252.60 +/- 200.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=297000, episode_reward=413.20 +/- 250.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=298000, episode_reward=410.20 +/- 141.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=299000, episode_reward=329.00 +/- 193.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=300000, episode_reward=331.20 +/- 263.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=301000, episode_reward=479.20 +/- 274.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=302000, episode_reward=734.20 +/- 250.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=303000, episode_reward=896.60 +/- 247.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=304000, episode_reward=805.20 +/- 318.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=305000, episode_reward=377.80 +/- 223.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=306000, episode_reward=535.60 +/- 157.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=307000, episode_reward=448.20 +/- 191.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=308000, episode_reward=356.40 +/- 196.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=309000, episode_reward=458.80 +/- 296.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=310000, episode_reward=655.00 +/- 305.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=311000, episode_reward=464.40 +/- 422.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=312000, episode_reward=601.20 +/- 228.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=313000, episode_reward=539.20 +/- 164.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=314000, episode_reward=486.40 +/- 367.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=315000, episode_reward=608.80 +/- 210.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=316000, episode_reward=361.80 +/- 109.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=317000, episode_reward=482.20 +/- 192.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=318000, episode_reward=602.20 +/- 268.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=319000, episode_reward=470.00 +/- 435.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=320000, episode_reward=924.00 +/- 238.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=321000, episode_reward=677.20 +/- 178.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=322000, episode_reward=745.00 +/- 308.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=323000, episode_reward=906.00 +/- 173.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=324000, episode_reward=556.80 +/- 144.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=325000, episode_reward=513.20 +/- 230.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=326000, episode_reward=501.20 +/- 216.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=327000, episode_reward=466.80 +/- 192.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=328000, episode_reward=455.80 +/- 101.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=329000, episode_reward=603.80 +/- 124.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=330000, episode_reward=493.40 +/- 215.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=331000, episode_reward=637.80 +/- 288.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=332000, episode_reward=18.80 +/- 198.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=333000, episode_reward=373.80 +/- 277.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=334000, episode_reward=591.20 +/- 315.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=335000, episode_reward=868.00 +/- 217.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=336000, episode_reward=421.60 +/- 277.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=337000, episode_reward=521.80 +/- 288.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=338000, episode_reward=490.00 +/- 239.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=339000, episode_reward=486.60 +/- 269.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=340000, episode_reward=464.60 +/- 105.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=341000, episode_reward=282.20 +/- 111.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=342000, episode_reward=622.40 +/- 318.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=343000, episode_reward=691.60 +/- 262.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=344000, episode_reward=700.40 +/- 385.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=345000, episode_reward=692.40 +/- 286.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=346000, episode_reward=934.20 +/- 198.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=347000, episode_reward=434.40 +/- 95.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=348000, episode_reward=792.20 +/- 342.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=349000, episode_reward=431.80 +/- 301.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=350000, episode_reward=549.60 +/- 209.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=351000, episode_reward=595.00 +/- 122.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=352000, episode_reward=398.60 +/- 176.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=353000, episode_reward=431.80 +/- 223.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=354000, episode_reward=622.40 +/- 381.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=355000, episode_reward=399.40 +/- 278.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=356000, episode_reward=499.40 +/- 251.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=357000, episode_reward=466.20 +/- 325.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=358000, episode_reward=617.40 +/- 200.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=359000, episode_reward=502.40 +/- 255.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=360000, episode_reward=590.60 +/- 181.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=361000, episode_reward=607.60 +/- 476.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=362000, episode_reward=531.20 +/- 169.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=363000, episode_reward=482.00 +/- 262.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=364000, episode_reward=560.20 +/- 228.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=365000, episode_reward=300.60 +/- 206.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=366000, episode_reward=528.40 +/- 286.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=367000, episode_reward=674.00 +/- 212.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=368000, episode_reward=740.60 +/- 210.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=369000, episode_reward=377.80 +/- 222.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=370000, episode_reward=970.00 +/- 201.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=371000, episode_reward=437.40 +/- 454.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=372000, episode_reward=777.80 +/- 256.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=373000, episode_reward=319.60 +/- 237.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=374000, episode_reward=564.80 +/- 231.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=375000, episode_reward=642.20 +/- 332.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=376000, episode_reward=671.40 +/- 126.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=377000, episode_reward=776.00 +/- 174.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=378000, episode_reward=758.20 +/- 372.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=379000, episode_reward=724.60 +/- 190.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=380000, episode_reward=633.60 +/- 316.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=381000, episode_reward=503.60 +/- 251.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=382000, episode_reward=593.80 +/- 528.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=383000, episode_reward=510.40 +/- 185.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=384000, episode_reward=574.40 +/- 190.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=385000, episode_reward=494.20 +/- 320.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=386000, episode_reward=595.80 +/- 67.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=387000, episode_reward=764.00 +/- 287.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=388000, episode_reward=612.20 +/- 244.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=389000, episode_reward=450.80 +/- 167.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=390000, episode_reward=534.20 +/- 247.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=391000, episode_reward=398.00 +/- 342.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=392000, episode_reward=594.80 +/- 209.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=393000, episode_reward=465.00 +/- 179.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=394000, episode_reward=877.00 +/- 147.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=395000, episode_reward=510.60 +/- 311.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=396000, episode_reward=798.00 +/- 320.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=397000, episode_reward=916.20 +/- 159.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=398000, episode_reward=564.00 +/- 195.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=399000, episode_reward=847.80 +/- 196.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=400000, episode_reward=693.40 +/- 427.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=401000, episode_reward=962.60 +/- 25.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=402000, episode_reward=466.40 +/- 86.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=403000, episode_reward=1002.00 +/- 153.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=404000, episode_reward=611.40 +/- 286.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=405000, episode_reward=664.20 +/- 183.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=406000, episode_reward=720.80 +/- 285.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=407000, episode_reward=674.00 +/- 152.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=408000, episode_reward=612.00 +/- 335.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=409000, episode_reward=825.80 +/- 264.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=410000, episode_reward=641.20 +/- 92.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=411000, episode_reward=747.60 +/- 477.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=412000, episode_reward=909.00 +/- 402.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=413000, episode_reward=828.00 +/- 185.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=414000, episode_reward=725.20 +/- 287.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=415000, episode_reward=577.60 +/- 411.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=416000, episode_reward=588.40 +/- 195.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=417000, episode_reward=534.00 +/- 277.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=418000, episode_reward=485.00 +/- 228.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=419000, episode_reward=807.80 +/- 502.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=420000, episode_reward=652.80 +/- 239.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=421000, episode_reward=410.60 +/- 243.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=422000, episode_reward=898.80 +/- 511.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=423000, episode_reward=607.00 +/- 204.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=424000, episode_reward=861.80 +/- 339.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=425000, episode_reward=817.40 +/- 129.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=426000, episode_reward=969.80 +/- 185.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=427000, episode_reward=904.00 +/- 289.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=428000, episode_reward=850.00 +/- 173.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=429000, episode_reward=724.40 +/- 286.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=430000, episode_reward=857.40 +/- 139.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=431000, episode_reward=667.20 +/- 252.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=432000, episode_reward=741.00 +/- 421.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=433000, episode_reward=637.00 +/- 282.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=434000, episode_reward=536.40 +/- 395.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=435000, episode_reward=908.40 +/- 323.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=436000, episode_reward=813.00 +/- 237.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=437000, episode_reward=937.80 +/- 239.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=438000, episode_reward=774.80 +/- 320.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=439000, episode_reward=820.00 +/- 248.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=440000, episode_reward=958.40 +/- 370.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=441000, episode_reward=819.00 +/- 191.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=442000, episode_reward=603.40 +/- 179.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=443000, episode_reward=792.40 +/- 264.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=444000, episode_reward=696.40 +/- 217.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=445000, episode_reward=809.00 +/- 410.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=446000, episode_reward=764.80 +/- 347.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=447000, episode_reward=745.80 +/- 397.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=448000, episode_reward=877.80 +/- 169.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=449000, episode_reward=570.60 +/- 264.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=450000, episode_reward=613.20 +/- 290.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=451000, episode_reward=740.80 +/- 335.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=452000, episode_reward=1190.20 +/- 289.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=453000, episode_reward=1002.20 +/- 156.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=454000, episode_reward=957.60 +/- 261.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=455000, episode_reward=992.20 +/- 234.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=456000, episode_reward=748.80 +/- 320.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=457000, episode_reward=1051.00 +/- 208.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=458000, episode_reward=800.40 +/- 116.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=459000, episode_reward=643.80 +/- 507.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=460000, episode_reward=741.80 +/- 247.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=461000, episode_reward=872.40 +/- 233.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=462000, episode_reward=731.20 +/- 241.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=463000, episode_reward=1106.60 +/- 340.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=464000, episode_reward=908.80 +/- 139.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=465000, episode_reward=982.60 +/- 227.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=466000, episode_reward=787.00 +/- 223.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=467000, episode_reward=906.60 +/- 136.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=468000, episode_reward=604.60 +/- 438.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=469000, episode_reward=786.20 +/- 341.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=470000, episode_reward=768.80 +/- 493.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=471000, episode_reward=874.20 +/- 366.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=472000, episode_reward=763.40 +/- 301.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=473000, episode_reward=920.60 +/- 309.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=474000, episode_reward=788.20 +/- 257.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=475000, episode_reward=821.00 +/- 312.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=476000, episode_reward=791.00 +/- 247.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=477000, episode_reward=584.20 +/- 214.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=478000, episode_reward=618.20 +/- 177.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=479000, episode_reward=602.60 +/- 153.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=480000, episode_reward=942.20 +/- 178.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=481000, episode_reward=896.80 +/- 309.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=482000, episode_reward=930.80 +/- 205.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=483000, episode_reward=865.00 +/- 371.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=484000, episode_reward=829.80 +/- 228.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=485000, episode_reward=871.80 +/- 298.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=486000, episode_reward=1005.60 +/- 165.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=487000, episode_reward=924.20 +/- 265.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=488000, episode_reward=846.40 +/- 479.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=489000, episode_reward=1034.60 +/- 190.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=490000, episode_reward=1012.60 +/- 303.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=491000, episode_reward=1221.40 +/- 353.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=492000, episode_reward=901.00 +/- 275.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=493000, episode_reward=1065.80 +/- 200.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=494000, episode_reward=1111.60 +/- 283.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=495000, episode_reward=874.00 +/- 322.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=496000, episode_reward=1332.40 +/- 205.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=497000, episode_reward=1212.80 +/- 336.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=498000, episode_reward=888.80 +/- 217.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=499000, episode_reward=1027.60 +/- 196.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=500000, episode_reward=1005.20 +/- 207.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=501000, episode_reward=1111.00 +/- 250.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Gamma = 0.99, Learning Rate = 3e-05, Activation Function = Tanh, Neurons = 64, Process noise = 0.1, Measurement noise = 0.0\n",
      "Eval num_timesteps=1000, episode_reward=-1385.00 +/- 114.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=-1259.40 +/- 81.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3000, episode_reward=-339.80 +/- 88.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4000, episode_reward=-469.20 +/- 164.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=5000, episode_reward=371.40 +/- 58.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=6000, episode_reward=409.80 +/- 177.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=7000, episode_reward=318.80 +/- 243.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=8000, episode_reward=202.40 +/- 89.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=9000, episode_reward=260.40 +/- 146.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=10000, episode_reward=463.20 +/- 116.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=11000, episode_reward=-214.60 +/- 302.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=12000, episode_reward=67.20 +/- 204.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=13000, episode_reward=-657.20 +/- 661.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=14000, episode_reward=-133.20 +/- 445.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=15000, episode_reward=-165.00 +/- 382.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=16000, episode_reward=13.40 +/- 259.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=17000, episode_reward=131.00 +/- 182.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=18000, episode_reward=-611.00 +/- 844.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=19000, episode_reward=-275.20 +/- 509.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=20000, episode_reward=-485.60 +/- 779.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=21000, episode_reward=-1079.60 +/- 1177.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=22000, episode_reward=-343.20 +/- 976.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=23000, episode_reward=-219.80 +/- 500.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=24000, episode_reward=-343.80 +/- 695.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=25000, episode_reward=-564.00 +/- 467.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=26000, episode_reward=-337.00 +/- 576.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=27000, episode_reward=38.00 +/- 146.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=28000, episode_reward=17.80 +/- 89.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=29000, episode_reward=103.20 +/- 228.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=30000, episode_reward=309.00 +/- 340.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=31000, episode_reward=-327.00 +/- 318.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=32000, episode_reward=181.80 +/- 222.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=33000, episode_reward=-47.00 +/- 138.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=34000, episode_reward=66.00 +/- 346.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=35000, episode_reward=-191.40 +/- 175.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=36000, episode_reward=-502.60 +/- 490.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=37000, episode_reward=21.80 +/- 407.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=38000, episode_reward=-88.60 +/- 459.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=39000, episode_reward=-277.80 +/- 395.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=40000, episode_reward=-681.20 +/- 1222.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=41000, episode_reward=-198.60 +/- 551.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=42000, episode_reward=-492.20 +/- 275.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=43000, episode_reward=-489.60 +/- 533.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=44000, episode_reward=-352.80 +/- 597.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=45000, episode_reward=-264.20 +/- 610.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=46000, episode_reward=-43.80 +/- 312.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=47000, episode_reward=-541.20 +/- 764.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=48000, episode_reward=-159.40 +/- 324.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=49000, episode_reward=-109.60 +/- 122.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=50000, episode_reward=-326.40 +/- 441.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=51000, episode_reward=-604.60 +/- 317.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=52000, episode_reward=-242.60 +/- 717.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=53000, episode_reward=-779.00 +/- 718.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=54000, episode_reward=-666.40 +/- 720.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=55000, episode_reward=-449.00 +/- 369.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=56000, episode_reward=-1614.00 +/- 365.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=57000, episode_reward=-1606.00 +/- 437.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=58000, episode_reward=-1687.40 +/- 1015.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=59000, episode_reward=-1562.80 +/- 295.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=60000, episode_reward=-394.80 +/- 564.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=61000, episode_reward=-323.60 +/- 317.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=62000, episode_reward=-670.60 +/- 227.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=63000, episode_reward=-931.60 +/- 615.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=64000, episode_reward=-490.80 +/- 434.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=65000, episode_reward=-34.60 +/- 253.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=66000, episode_reward=-856.80 +/- 220.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=67000, episode_reward=-274.60 +/- 719.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=68000, episode_reward=-165.00 +/- 441.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=69000, episode_reward=-726.80 +/- 391.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=70000, episode_reward=-134.40 +/- 666.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=71000, episode_reward=-402.60 +/- 268.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=72000, episode_reward=-568.40 +/- 242.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=73000, episode_reward=-242.00 +/- 393.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=74000, episode_reward=74.00 +/- 387.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=75000, episode_reward=69.80 +/- 473.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=76000, episode_reward=-15.00 +/- 190.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=77000, episode_reward=64.60 +/- 422.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=78000, episode_reward=301.60 +/- 148.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=79000, episode_reward=162.20 +/- 91.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=80000, episode_reward=214.60 +/- 116.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=81000, episode_reward=105.20 +/- 135.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=82000, episode_reward=252.40 +/- 178.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=83000, episode_reward=87.00 +/- 159.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=84000, episode_reward=341.20 +/- 123.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=85000, episode_reward=429.40 +/- 182.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=86000, episode_reward=397.00 +/- 77.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=87000, episode_reward=259.60 +/- 182.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=88000, episode_reward=266.40 +/- 118.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=89000, episode_reward=442.20 +/- 258.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=90000, episode_reward=371.00 +/- 186.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=91000, episode_reward=306.40 +/- 107.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=92000, episode_reward=83.80 +/- 79.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=93000, episode_reward=424.00 +/- 114.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=94000, episode_reward=315.80 +/- 112.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=95000, episode_reward=238.00 +/- 299.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=96000, episode_reward=323.80 +/- 159.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=97000, episode_reward=318.80 +/- 189.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=98000, episode_reward=320.80 +/- 269.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=99000, episode_reward=215.60 +/- 366.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=100000, episode_reward=291.20 +/- 65.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=101000, episode_reward=355.60 +/- 258.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=102000, episode_reward=312.40 +/- 358.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=103000, episode_reward=274.60 +/- 196.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=104000, episode_reward=342.60 +/- 166.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=105000, episode_reward=361.40 +/- 269.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=106000, episode_reward=318.80 +/- 240.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=107000, episode_reward=448.60 +/- 160.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=108000, episode_reward=220.00 +/- 146.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=109000, episode_reward=189.20 +/- 319.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=110000, episode_reward=357.20 +/- 203.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=111000, episode_reward=146.40 +/- 121.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=112000, episode_reward=289.60 +/- 249.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=113000, episode_reward=307.60 +/- 226.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=114000, episode_reward=329.00 +/- 159.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=115000, episode_reward=475.00 +/- 133.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=116000, episode_reward=204.40 +/- 186.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=117000, episode_reward=347.80 +/- 234.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=118000, episode_reward=224.80 +/- 115.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=119000, episode_reward=450.40 +/- 219.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=120000, episode_reward=345.20 +/- 213.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=121000, episode_reward=338.20 +/- 200.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=122000, episode_reward=315.00 +/- 158.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=123000, episode_reward=501.60 +/- 195.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=124000, episode_reward=231.60 +/- 264.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=125000, episode_reward=221.80 +/- 224.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=126000, episode_reward=362.60 +/- 294.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=127000, episode_reward=368.80 +/- 216.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=128000, episode_reward=375.80 +/- 215.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=129000, episode_reward=377.20 +/- 136.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=130000, episode_reward=258.60 +/- 169.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=131000, episode_reward=458.40 +/- 94.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=132000, episode_reward=490.40 +/- 129.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=133000, episode_reward=257.00 +/- 261.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=134000, episode_reward=320.60 +/- 152.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=135000, episode_reward=167.40 +/- 149.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=136000, episode_reward=335.00 +/- 105.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=137000, episode_reward=436.20 +/- 272.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=138000, episode_reward=317.80 +/- 226.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=139000, episode_reward=271.00 +/- 267.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=140000, episode_reward=283.60 +/- 174.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=141000, episode_reward=456.20 +/- 130.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=142000, episode_reward=176.00 +/- 268.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=143000, episode_reward=278.20 +/- 194.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=144000, episode_reward=214.00 +/- 292.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=145000, episode_reward=200.80 +/- 215.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=146000, episode_reward=299.40 +/- 140.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=147000, episode_reward=327.80 +/- 278.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=148000, episode_reward=277.40 +/- 103.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=149000, episode_reward=442.80 +/- 98.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=150000, episode_reward=434.60 +/- 73.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=151000, episode_reward=324.00 +/- 152.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=152000, episode_reward=425.60 +/- 135.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=153000, episode_reward=242.60 +/- 281.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=154000, episode_reward=343.80 +/- 131.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=155000, episode_reward=577.80 +/- 145.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=156000, episode_reward=339.80 +/- 154.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=157000, episode_reward=318.60 +/- 165.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=158000, episode_reward=423.20 +/- 260.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=159000, episode_reward=192.60 +/- 247.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=160000, episode_reward=414.00 +/- 156.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=161000, episode_reward=324.20 +/- 159.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=162000, episode_reward=262.00 +/- 205.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=163000, episode_reward=244.00 +/- 264.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=164000, episode_reward=347.60 +/- 89.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=165000, episode_reward=435.40 +/- 175.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=166000, episode_reward=411.60 +/- 200.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=167000, episode_reward=398.40 +/- 177.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=168000, episode_reward=386.40 +/- 157.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=169000, episode_reward=203.00 +/- 196.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=170000, episode_reward=508.80 +/- 113.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=171000, episode_reward=341.00 +/- 253.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=172000, episode_reward=296.20 +/- 171.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=173000, episode_reward=260.80 +/- 187.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=174000, episode_reward=317.60 +/- 215.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=175000, episode_reward=404.80 +/- 316.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=176000, episode_reward=352.00 +/- 234.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=177000, episode_reward=341.40 +/- 190.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=178000, episode_reward=258.80 +/- 136.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=179000, episode_reward=272.40 +/- 162.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=180000, episode_reward=450.20 +/- 156.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=181000, episode_reward=446.20 +/- 216.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=182000, episode_reward=401.00 +/- 226.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=183000, episode_reward=221.60 +/- 158.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=184000, episode_reward=420.60 +/- 190.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=185000, episode_reward=325.60 +/- 98.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=186000, episode_reward=226.00 +/- 120.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=187000, episode_reward=364.80 +/- 140.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=188000, episode_reward=227.80 +/- 235.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=189000, episode_reward=302.60 +/- 173.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=190000, episode_reward=443.00 +/- 245.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=191000, episode_reward=342.20 +/- 115.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=192000, episode_reward=257.00 +/- 316.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=193000, episode_reward=306.80 +/- 298.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=194000, episode_reward=418.40 +/- 181.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=195000, episode_reward=320.60 +/- 158.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=196000, episode_reward=327.20 +/- 112.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=197000, episode_reward=348.80 +/- 113.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=198000, episode_reward=440.80 +/- 97.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=199000, episode_reward=145.60 +/- 90.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=200000, episode_reward=517.20 +/- 169.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=201000, episode_reward=332.20 +/- 118.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=202000, episode_reward=271.00 +/- 209.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=203000, episode_reward=363.60 +/- 142.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=204000, episode_reward=254.60 +/- 128.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=205000, episode_reward=266.20 +/- 216.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=206000, episode_reward=318.40 +/- 132.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=207000, episode_reward=276.00 +/- 219.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=208000, episode_reward=402.60 +/- 169.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=209000, episode_reward=362.40 +/- 241.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=210000, episode_reward=341.20 +/- 158.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=211000, episode_reward=268.00 +/- 202.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=212000, episode_reward=318.20 +/- 229.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=213000, episode_reward=471.60 +/- 146.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=214000, episode_reward=414.20 +/- 210.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=215000, episode_reward=346.60 +/- 78.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=216000, episode_reward=181.60 +/- 223.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=217000, episode_reward=308.20 +/- 175.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=218000, episode_reward=365.20 +/- 186.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=219000, episode_reward=366.80 +/- 133.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=220000, episode_reward=252.80 +/- 84.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=221000, episode_reward=247.60 +/- 146.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=222000, episode_reward=352.60 +/- 113.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=223000, episode_reward=323.00 +/- 230.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=224000, episode_reward=371.40 +/- 66.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=225000, episode_reward=286.80 +/- 144.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=226000, episode_reward=339.60 +/- 94.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=227000, episode_reward=292.60 +/- 174.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=228000, episode_reward=277.60 +/- 236.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=229000, episode_reward=333.60 +/- 178.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=230000, episode_reward=392.20 +/- 106.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=231000, episode_reward=218.20 +/- 161.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=232000, episode_reward=419.60 +/- 109.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=233000, episode_reward=482.00 +/- 58.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=234000, episode_reward=290.40 +/- 206.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=235000, episode_reward=401.60 +/- 154.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=236000, episode_reward=356.20 +/- 81.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=237000, episode_reward=277.60 +/- 165.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=238000, episode_reward=401.60 +/- 250.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=239000, episode_reward=360.20 +/- 75.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=240000, episode_reward=303.00 +/- 116.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=241000, episode_reward=180.40 +/- 328.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=242000, episode_reward=304.80 +/- 249.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=243000, episode_reward=315.20 +/- 183.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=244000, episode_reward=461.00 +/- 205.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=245000, episode_reward=331.40 +/- 215.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=246000, episode_reward=234.20 +/- 96.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=247000, episode_reward=309.20 +/- 376.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=248000, episode_reward=334.00 +/- 278.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=249000, episode_reward=182.80 +/- 223.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=250000, episode_reward=373.40 +/- 285.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=251000, episode_reward=313.00 +/- 52.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=252000, episode_reward=207.20 +/- 50.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=253000, episode_reward=368.40 +/- 177.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=254000, episode_reward=324.20 +/- 245.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=255000, episode_reward=270.40 +/- 84.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=256000, episode_reward=225.00 +/- 167.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=257000, episode_reward=356.80 +/- 107.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=258000, episode_reward=378.60 +/- 63.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=259000, episode_reward=81.40 +/- 106.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=260000, episode_reward=365.80 +/- 139.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=261000, episode_reward=331.20 +/- 152.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=262000, episode_reward=392.80 +/- 111.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=263000, episode_reward=352.00 +/- 203.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=264000, episode_reward=194.80 +/- 355.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=265000, episode_reward=322.00 +/- 117.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=266000, episode_reward=509.80 +/- 105.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=267000, episode_reward=379.00 +/- 217.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=268000, episode_reward=297.40 +/- 255.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=269000, episode_reward=345.20 +/- 185.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=270000, episode_reward=364.20 +/- 201.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=271000, episode_reward=222.60 +/- 101.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=272000, episode_reward=442.40 +/- 259.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=273000, episode_reward=328.80 +/- 167.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=274000, episode_reward=416.20 +/- 223.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=275000, episode_reward=451.80 +/- 103.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=276000, episode_reward=415.60 +/- 184.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=277000, episode_reward=385.80 +/- 194.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=278000, episode_reward=256.40 +/- 173.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=279000, episode_reward=254.20 +/- 263.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=280000, episode_reward=304.60 +/- 227.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=281000, episode_reward=378.00 +/- 207.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=282000, episode_reward=383.80 +/- 87.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=283000, episode_reward=390.20 +/- 160.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=284000, episode_reward=262.60 +/- 179.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=285000, episode_reward=510.80 +/- 167.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=286000, episode_reward=332.60 +/- 318.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=287000, episode_reward=306.40 +/- 130.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=288000, episode_reward=331.00 +/- 146.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=289000, episode_reward=229.20 +/- 145.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=290000, episode_reward=328.40 +/- 210.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=291000, episode_reward=10.40 +/- 141.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=292000, episode_reward=188.00 +/- 265.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=293000, episode_reward=551.60 +/- 139.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=294000, episode_reward=430.80 +/- 103.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=295000, episode_reward=279.80 +/- 162.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=296000, episode_reward=231.40 +/- 145.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=297000, episode_reward=265.60 +/- 193.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=298000, episode_reward=461.00 +/- 104.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=299000, episode_reward=302.20 +/- 289.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=300000, episode_reward=435.20 +/- 199.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=301000, episode_reward=426.00 +/- 198.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=302000, episode_reward=348.20 +/- 185.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=303000, episode_reward=202.40 +/- 160.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=304000, episode_reward=198.40 +/- 89.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=305000, episode_reward=120.40 +/- 186.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=306000, episode_reward=428.20 +/- 122.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=307000, episode_reward=377.80 +/- 221.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=308000, episode_reward=450.40 +/- 236.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=309000, episode_reward=417.00 +/- 228.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=310000, episode_reward=312.80 +/- 166.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=311000, episode_reward=360.00 +/- 36.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=312000, episode_reward=392.20 +/- 167.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=313000, episode_reward=521.00 +/- 166.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=314000, episode_reward=327.40 +/- 227.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=315000, episode_reward=414.20 +/- 206.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=316000, episode_reward=239.80 +/- 149.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=317000, episode_reward=317.20 +/- 136.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=318000, episode_reward=220.20 +/- 305.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=319000, episode_reward=502.20 +/- 181.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=320000, episode_reward=317.40 +/- 225.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=321000, episode_reward=238.40 +/- 246.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=322000, episode_reward=502.80 +/- 148.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=323000, episode_reward=246.80 +/- 163.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=324000, episode_reward=365.80 +/- 200.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=325000, episode_reward=345.60 +/- 212.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=326000, episode_reward=412.40 +/- 195.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=327000, episode_reward=289.80 +/- 118.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=328000, episode_reward=288.80 +/- 200.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=329000, episode_reward=517.40 +/- 115.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=330000, episode_reward=227.00 +/- 110.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=331000, episode_reward=471.40 +/- 240.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=332000, episode_reward=294.40 +/- 126.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=333000, episode_reward=245.60 +/- 252.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=334000, episode_reward=273.00 +/- 189.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=335000, episode_reward=119.80 +/- 168.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=336000, episode_reward=400.80 +/- 115.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=337000, episode_reward=358.60 +/- 249.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=338000, episode_reward=214.20 +/- 223.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=339000, episode_reward=364.60 +/- 189.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=340000, episode_reward=349.00 +/- 140.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=341000, episode_reward=277.00 +/- 105.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=342000, episode_reward=187.00 +/- 286.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=343000, episode_reward=458.40 +/- 154.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=344000, episode_reward=448.40 +/- 208.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=345000, episode_reward=378.00 +/- 186.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=346000, episode_reward=322.80 +/- 191.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=347000, episode_reward=253.40 +/- 248.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=348000, episode_reward=450.20 +/- 129.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=349000, episode_reward=324.00 +/- 217.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=350000, episode_reward=387.20 +/- 166.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=351000, episode_reward=236.00 +/- 372.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=352000, episode_reward=69.80 +/- 184.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=353000, episode_reward=264.20 +/- 205.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=354000, episode_reward=155.40 +/- 258.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=355000, episode_reward=253.60 +/- 247.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=356000, episode_reward=359.20 +/- 136.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=357000, episode_reward=114.80 +/- 305.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=358000, episode_reward=193.80 +/- 57.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=359000, episode_reward=295.60 +/- 151.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=360000, episode_reward=378.00 +/- 182.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=361000, episode_reward=434.00 +/- 31.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=362000, episode_reward=413.80 +/- 157.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=363000, episode_reward=235.40 +/- 223.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=364000, episode_reward=335.40 +/- 150.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=365000, episode_reward=320.20 +/- 83.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=366000, episode_reward=321.00 +/- 262.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=367000, episode_reward=116.00 +/- 201.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=368000, episode_reward=361.40 +/- 252.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=369000, episode_reward=302.40 +/- 213.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=370000, episode_reward=356.20 +/- 166.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=371000, episode_reward=170.80 +/- 166.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=372000, episode_reward=249.80 +/- 102.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=373000, episode_reward=224.20 +/- 119.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=374000, episode_reward=465.40 +/- 132.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=375000, episode_reward=256.00 +/- 180.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=376000, episode_reward=392.60 +/- 106.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=377000, episode_reward=364.40 +/- 231.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=378000, episode_reward=287.60 +/- 150.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=379000, episode_reward=311.00 +/- 272.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=380000, episode_reward=242.40 +/- 198.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=381000, episode_reward=365.40 +/- 174.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=382000, episode_reward=379.80 +/- 151.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=383000, episode_reward=344.60 +/- 198.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=384000, episode_reward=507.40 +/- 174.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=385000, episode_reward=491.60 +/- 115.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=386000, episode_reward=434.60 +/- 319.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=387000, episode_reward=145.00 +/- 63.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=388000, episode_reward=334.60 +/- 203.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=389000, episode_reward=315.00 +/- 219.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=390000, episode_reward=395.40 +/- 177.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=391000, episode_reward=360.20 +/- 169.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=392000, episode_reward=457.40 +/- 198.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=393000, episode_reward=215.40 +/- 198.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=394000, episode_reward=441.60 +/- 188.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=395000, episode_reward=260.20 +/- 364.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=396000, episode_reward=376.80 +/- 159.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=397000, episode_reward=382.00 +/- 189.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=398000, episode_reward=177.20 +/- 127.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=399000, episode_reward=331.20 +/- 123.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=400000, episode_reward=408.40 +/- 210.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=401000, episode_reward=380.40 +/- 231.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=402000, episode_reward=330.40 +/- 208.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=403000, episode_reward=463.20 +/- 150.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=404000, episode_reward=306.20 +/- 170.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=405000, episode_reward=362.00 +/- 315.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=406000, episode_reward=394.80 +/- 272.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=407000, episode_reward=460.60 +/- 282.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=408000, episode_reward=324.40 +/- 127.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=409000, episode_reward=618.80 +/- 118.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=410000, episode_reward=262.00 +/- 171.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=411000, episode_reward=315.40 +/- 130.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=412000, episode_reward=397.80 +/- 352.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=413000, episode_reward=293.00 +/- 346.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=414000, episode_reward=450.00 +/- 115.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=415000, episode_reward=207.40 +/- 310.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=416000, episode_reward=132.20 +/- 193.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=417000, episode_reward=296.40 +/- 264.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=418000, episode_reward=318.00 +/- 200.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=419000, episode_reward=444.60 +/- 230.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=420000, episode_reward=377.60 +/- 256.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=421000, episode_reward=250.00 +/- 293.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=422000, episode_reward=340.60 +/- 61.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=423000, episode_reward=211.00 +/- 190.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=424000, episode_reward=382.00 +/- 260.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=425000, episode_reward=495.20 +/- 199.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=426000, episode_reward=480.80 +/- 283.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=427000, episode_reward=488.20 +/- 223.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=428000, episode_reward=283.40 +/- 149.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=429000, episode_reward=326.00 +/- 163.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=430000, episode_reward=326.00 +/- 359.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=431000, episode_reward=275.20 +/- 283.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=432000, episode_reward=325.20 +/- 247.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=433000, episode_reward=391.80 +/- 203.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=434000, episode_reward=333.40 +/- 328.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=435000, episode_reward=492.40 +/- 223.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=436000, episode_reward=470.00 +/- 130.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=437000, episode_reward=400.80 +/- 171.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=438000, episode_reward=360.20 +/- 162.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=439000, episode_reward=384.80 +/- 339.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=440000, episode_reward=476.00 +/- 278.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=441000, episode_reward=335.40 +/- 214.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=442000, episode_reward=291.20 +/- 85.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=443000, episode_reward=287.40 +/- 168.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=444000, episode_reward=412.80 +/- 167.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=445000, episode_reward=388.20 +/- 220.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=446000, episode_reward=385.20 +/- 94.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=447000, episode_reward=396.00 +/- 128.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=448000, episode_reward=214.80 +/- 197.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=449000, episode_reward=557.40 +/- 125.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=450000, episode_reward=302.80 +/- 109.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=451000, episode_reward=431.60 +/- 230.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=452000, episode_reward=433.00 +/- 280.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=453000, episode_reward=461.00 +/- 150.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=454000, episode_reward=310.60 +/- 129.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=455000, episode_reward=487.40 +/- 224.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=456000, episode_reward=461.00 +/- 134.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=457000, episode_reward=296.20 +/- 352.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=458000, episode_reward=279.20 +/- 239.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=459000, episode_reward=498.60 +/- 272.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=460000, episode_reward=458.80 +/- 218.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=461000, episode_reward=606.20 +/- 234.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=462000, episode_reward=449.20 +/- 142.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=463000, episode_reward=368.80 +/- 437.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=464000, episode_reward=719.60 +/- 501.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=465000, episode_reward=357.60 +/- 148.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=466000, episode_reward=402.40 +/- 255.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=467000, episode_reward=532.40 +/- 315.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=468000, episode_reward=330.40 +/- 150.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=469000, episode_reward=471.60 +/- 233.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=470000, episode_reward=697.00 +/- 216.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=471000, episode_reward=688.80 +/- 152.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=472000, episode_reward=705.00 +/- 252.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=473000, episode_reward=800.40 +/- 219.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=474000, episode_reward=498.40 +/- 304.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=475000, episode_reward=429.40 +/- 316.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=476000, episode_reward=639.00 +/- 326.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=477000, episode_reward=527.80 +/- 310.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=478000, episode_reward=444.00 +/- 249.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=479000, episode_reward=334.40 +/- 104.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=480000, episode_reward=927.20 +/- 150.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=481000, episode_reward=590.80 +/- 274.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=482000, episode_reward=641.60 +/- 203.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=483000, episode_reward=960.00 +/- 276.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=484000, episode_reward=832.00 +/- 392.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=485000, episode_reward=515.20 +/- 390.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=486000, episode_reward=567.00 +/- 327.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=487000, episode_reward=765.40 +/- 247.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=488000, episode_reward=547.80 +/- 251.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=489000, episode_reward=643.40 +/- 210.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=490000, episode_reward=689.60 +/- 360.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=491000, episode_reward=759.20 +/- 357.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=492000, episode_reward=665.40 +/- 449.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=493000, episode_reward=656.60 +/- 205.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=494000, episode_reward=731.60 +/- 243.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=495000, episode_reward=517.80 +/- 97.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=496000, episode_reward=610.20 +/- 181.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=497000, episode_reward=420.00 +/- 239.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=498000, episode_reward=587.20 +/- 317.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=499000, episode_reward=428.20 +/- 276.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=500000, episode_reward=709.00 +/- 357.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=501000, episode_reward=650.40 +/- 369.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Gamma = 0.99, Learning Rate = 3e-05, Activation Function = ReLU, Neurons = 8, Process noise = 0.1, Measurement noise = 0.0\n",
      "Eval num_timesteps=1000, episode_reward=-1434.00 +/- 70.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=-1530.00 +/- 69.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=3000, episode_reward=198.60 +/- 397.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4000, episode_reward=-851.60 +/- 719.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=5000, episode_reward=196.60 +/- 143.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=6000, episode_reward=232.20 +/- 286.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=7000, episode_reward=-63.20 +/- 172.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=8000, episode_reward=-58.60 +/- 40.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=9000, episode_reward=-692.60 +/- 870.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=10000, episode_reward=-768.20 +/- 1073.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=11000, episode_reward=-101.00 +/- 625.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=12000, episode_reward=-270.20 +/- 521.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=13000, episode_reward=112.40 +/- 338.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=14000, episode_reward=-33.80 +/- 490.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=15000, episode_reward=37.20 +/- 274.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=16000, episode_reward=-17.20 +/- 306.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=17000, episode_reward=-145.60 +/- 381.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=18000, episode_reward=-501.40 +/- 1004.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=19000, episode_reward=52.80 +/- 243.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=20000, episode_reward=-85.00 +/- 480.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=21000, episode_reward=-505.00 +/- 1094.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=22000, episode_reward=-139.00 +/- 359.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=23000, episode_reward=-48.80 +/- 290.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=24000, episode_reward=105.20 +/- 613.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=25000, episode_reward=-585.60 +/- 1093.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=26000, episode_reward=-152.80 +/- 600.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=27000, episode_reward=-71.40 +/- 266.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=28000, episode_reward=-75.80 +/- 450.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=29000, episode_reward=-108.40 +/- 310.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=30000, episode_reward=221.80 +/- 269.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=31000, episode_reward=-155.40 +/- 309.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=32000, episode_reward=-477.20 +/- 163.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=33000, episode_reward=-139.40 +/- 346.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=34000, episode_reward=-133.40 +/- 476.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=35000, episode_reward=146.80 +/- 315.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=36000, episode_reward=-200.60 +/- 427.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=37000, episode_reward=-13.80 +/- 226.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=38000, episode_reward=-113.80 +/- 721.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=39000, episode_reward=-228.00 +/- 680.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=40000, episode_reward=-965.20 +/- 1234.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=41000, episode_reward=-335.40 +/- 760.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=42000, episode_reward=-328.20 +/- 447.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=43000, episode_reward=-419.20 +/- 386.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=44000, episode_reward=-256.40 +/- 872.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=45000, episode_reward=114.40 +/- 215.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=46000, episode_reward=-254.60 +/- 329.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=47000, episode_reward=-544.80 +/- 791.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=48000, episode_reward=-248.60 +/- 791.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=49000, episode_reward=-120.20 +/- 411.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=50000, episode_reward=-125.00 +/- 318.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=51000, episode_reward=-279.20 +/- 875.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=52000, episode_reward=-46.60 +/- 449.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=53000, episode_reward=-267.60 +/- 349.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=54000, episode_reward=119.80 +/- 312.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=55000, episode_reward=-197.80 +/- 279.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=56000, episode_reward=-14.20 +/- 262.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=57000, episode_reward=37.00 +/- 629.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=58000, episode_reward=-210.00 +/- 418.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=59000, episode_reward=-52.20 +/- 281.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=60000, episode_reward=-843.40 +/- 788.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=61000, episode_reward=-173.80 +/- 266.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=62000, episode_reward=-252.60 +/- 292.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=63000, episode_reward=-318.60 +/- 667.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=64000, episode_reward=-199.60 +/- 464.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=65000, episode_reward=166.00 +/- 200.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=66000, episode_reward=-173.40 +/- 474.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=67000, episode_reward=-79.60 +/- 155.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=68000, episode_reward=-478.20 +/- 774.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=69000, episode_reward=-371.40 +/- 329.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=70000, episode_reward=-160.60 +/- 432.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=71000, episode_reward=132.00 +/- 200.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=72000, episode_reward=32.00 +/- 229.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=73000, episode_reward=-937.00 +/- 1055.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=74000, episode_reward=-307.60 +/- 355.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=75000, episode_reward=-33.40 +/- 229.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=76000, episode_reward=-266.40 +/- 572.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=77000, episode_reward=-850.60 +/- 489.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=78000, episode_reward=-330.80 +/- 462.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=79000, episode_reward=105.80 +/- 158.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=80000, episode_reward=65.40 +/- 383.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=81000, episode_reward=26.80 +/- 323.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=82000, episode_reward=196.00 +/- 441.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=83000, episode_reward=-425.00 +/- 519.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=84000, episode_reward=73.40 +/- 245.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=85000, episode_reward=-327.80 +/- 593.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=86000, episode_reward=-186.40 +/- 262.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=87000, episode_reward=-191.40 +/- 607.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=88000, episode_reward=-272.60 +/- 524.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=89000, episode_reward=-186.00 +/- 482.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=90000, episode_reward=-153.20 +/- 380.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=91000, episode_reward=-540.60 +/- 664.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=92000, episode_reward=-538.00 +/- 672.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=93000, episode_reward=-28.60 +/- 202.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=94000, episode_reward=-473.80 +/- 476.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=95000, episode_reward=-591.80 +/- 938.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=96000, episode_reward=-736.80 +/- 901.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=97000, episode_reward=-125.20 +/- 259.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=98000, episode_reward=-117.80 +/- 247.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=99000, episode_reward=-133.80 +/- 235.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=100000, episode_reward=-595.00 +/- 760.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=101000, episode_reward=-75.20 +/- 173.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=102000, episode_reward=-186.40 +/- 434.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=103000, episode_reward=-242.40 +/- 397.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=104000, episode_reward=-528.40 +/- 592.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=105000, episode_reward=-178.80 +/- 457.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=106000, episode_reward=-747.20 +/- 1049.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=107000, episode_reward=-532.60 +/- 193.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=108000, episode_reward=-300.60 +/- 586.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=109000, episode_reward=-282.60 +/- 353.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=110000, episode_reward=-208.60 +/- 276.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=111000, episode_reward=-199.20 +/- 444.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=112000, episode_reward=-340.00 +/- 800.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=113000, episode_reward=0.00 +/- 389.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=114000, episode_reward=-582.00 +/- 376.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=115000, episode_reward=-560.20 +/- 872.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=116000, episode_reward=-281.20 +/- 220.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=117000, episode_reward=-218.80 +/- 263.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=118000, episode_reward=102.80 +/- 181.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=119000, episode_reward=-424.60 +/- 1027.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=120000, episode_reward=-166.00 +/- 352.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=121000, episode_reward=-337.60 +/- 401.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=122000, episode_reward=-149.80 +/- 354.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=123000, episode_reward=-441.40 +/- 451.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=124000, episode_reward=-51.60 +/- 605.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=125000, episode_reward=-244.60 +/- 534.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=126000, episode_reward=-150.00 +/- 218.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=127000, episode_reward=25.40 +/- 393.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=128000, episode_reward=95.60 +/- 157.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=129000, episode_reward=-179.00 +/- 516.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=130000, episode_reward=-453.60 +/- 277.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=131000, episode_reward=103.60 +/- 226.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=132000, episode_reward=-357.80 +/- 665.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=133000, episode_reward=-505.60 +/- 1308.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=134000, episode_reward=-338.80 +/- 494.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=135000, episode_reward=-765.80 +/- 687.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=136000, episode_reward=-494.20 +/- 190.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=137000, episode_reward=-86.60 +/- 273.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=138000, episode_reward=59.40 +/- 292.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=139000, episode_reward=-380.40 +/- 326.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=140000, episode_reward=-226.40 +/- 358.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=141000, episode_reward=-635.60 +/- 723.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=142000, episode_reward=-342.80 +/- 150.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=143000, episode_reward=-168.60 +/- 167.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=144000, episode_reward=-591.20 +/- 572.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=145000, episode_reward=162.20 +/- 291.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=146000, episode_reward=-32.60 +/- 511.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=147000, episode_reward=-103.40 +/- 301.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=148000, episode_reward=-1092.00 +/- 1475.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=149000, episode_reward=-812.00 +/- 959.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=150000, episode_reward=-119.00 +/- 386.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=151000, episode_reward=-227.40 +/- 542.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=152000, episode_reward=-275.80 +/- 258.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=153000, episode_reward=-918.40 +/- 613.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=154000, episode_reward=-327.40 +/- 928.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=155000, episode_reward=-158.00 +/- 406.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=156000, episode_reward=-64.00 +/- 145.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=157000, episode_reward=-529.40 +/- 778.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=158000, episode_reward=-14.20 +/- 236.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=159000, episode_reward=-404.20 +/- 826.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=160000, episode_reward=-839.60 +/- 1455.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=161000, episode_reward=-114.00 +/- 457.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=162000, episode_reward=-498.60 +/- 700.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=163000, episode_reward=-697.80 +/- 922.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=164000, episode_reward=104.60 +/- 329.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=165000, episode_reward=-137.00 +/- 641.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=166000, episode_reward=-810.60 +/- 1036.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=167000, episode_reward=8.40 +/- 689.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=168000, episode_reward=-143.60 +/- 300.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=169000, episode_reward=-664.80 +/- 279.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=170000, episode_reward=-143.80 +/- 397.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=171000, episode_reward=-582.60 +/- 1086.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=172000, episode_reward=-54.40 +/- 456.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=173000, episode_reward=-375.40 +/- 811.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=174000, episode_reward=-144.60 +/- 343.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=175000, episode_reward=-264.60 +/- 396.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=176000, episode_reward=-461.00 +/- 605.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=177000, episode_reward=-747.60 +/- 817.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=178000, episode_reward=-193.40 +/- 342.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=179000, episode_reward=-186.00 +/- 159.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=180000, episode_reward=278.60 +/- 62.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=181000, episode_reward=-724.60 +/- 587.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=182000, episode_reward=-979.60 +/- 1308.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=183000, episode_reward=-27.20 +/- 184.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=184000, episode_reward=-385.20 +/- 415.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=185000, episode_reward=-195.60 +/- 325.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=186000, episode_reward=-64.60 +/- 421.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=187000, episode_reward=-441.00 +/- 581.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=188000, episode_reward=-214.20 +/- 365.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=189000, episode_reward=-376.60 +/- 302.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=190000, episode_reward=-469.00 +/- 738.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=191000, episode_reward=-256.40 +/- 196.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=192000, episode_reward=-514.20 +/- 831.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=193000, episode_reward=-785.60 +/- 447.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=194000, episode_reward=-204.40 +/- 140.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=195000, episode_reward=-44.40 +/- 235.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=196000, episode_reward=-494.40 +/- 969.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=197000, episode_reward=-399.40 +/- 327.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=198000, episode_reward=-645.20 +/- 985.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=199000, episode_reward=-177.80 +/- 97.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=200000, episode_reward=-78.20 +/- 264.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=201000, episode_reward=-347.60 +/- 549.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=202000, episode_reward=-807.80 +/- 1319.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=203000, episode_reward=-5.80 +/- 434.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=204000, episode_reward=-259.60 +/- 405.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=205000, episode_reward=-626.80 +/- 264.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=206000, episode_reward=-200.20 +/- 565.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=207000, episode_reward=-193.60 +/- 466.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=208000, episode_reward=-247.40 +/- 267.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=209000, episode_reward=-37.80 +/- 265.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=210000, episode_reward=-334.60 +/- 628.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=211000, episode_reward=-319.00 +/- 361.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=212000, episode_reward=-1119.40 +/- 1035.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=213000, episode_reward=-166.60 +/- 326.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=214000, episode_reward=-280.40 +/- 333.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=215000, episode_reward=-114.80 +/- 323.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=216000, episode_reward=-998.20 +/- 794.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=217000, episode_reward=-239.40 +/- 386.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=218000, episode_reward=-773.00 +/- 413.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=219000, episode_reward=-976.20 +/- 1367.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=220000, episode_reward=-438.20 +/- 522.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=221000, episode_reward=-386.40 +/- 259.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=222000, episode_reward=-781.80 +/- 748.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=223000, episode_reward=-696.80 +/- 278.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=224000, episode_reward=-424.00 +/- 332.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=225000, episode_reward=-489.80 +/- 305.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=226000, episode_reward=-629.40 +/- 759.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=227000, episode_reward=-545.00 +/- 432.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=228000, episode_reward=-287.60 +/- 325.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=229000, episode_reward=-217.00 +/- 371.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=230000, episode_reward=-1642.80 +/- 1887.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=231000, episode_reward=-179.60 +/- 373.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=232000, episode_reward=-357.40 +/- 453.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=233000, episode_reward=-1100.40 +/- 774.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=234000, episode_reward=-642.80 +/- 677.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=235000, episode_reward=-298.40 +/- 185.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=236000, episode_reward=-449.20 +/- 606.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=237000, episode_reward=-374.60 +/- 373.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=238000, episode_reward=-587.00 +/- 619.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=239000, episode_reward=-767.20 +/- 449.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=240000, episode_reward=-654.80 +/- 864.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=241000, episode_reward=-529.80 +/- 521.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=242000, episode_reward=-809.20 +/- 686.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=243000, episode_reward=-47.40 +/- 570.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=244000, episode_reward=-778.80 +/- 613.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=245000, episode_reward=-370.80 +/- 923.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=246000, episode_reward=-469.40 +/- 1342.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=247000, episode_reward=-856.00 +/- 1171.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=248000, episode_reward=-287.20 +/- 529.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=249000, episode_reward=-97.40 +/- 199.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=250000, episode_reward=-972.60 +/- 402.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=251000, episode_reward=-228.60 +/- 401.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=252000, episode_reward=-216.80 +/- 265.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=253000, episode_reward=-38.40 +/- 160.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=254000, episode_reward=-245.80 +/- 324.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=255000, episode_reward=-447.40 +/- 363.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=256000, episode_reward=-106.00 +/- 318.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=257000, episode_reward=135.80 +/- 380.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=258000, episode_reward=-619.60 +/- 736.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=259000, episode_reward=-239.20 +/- 195.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=260000, episode_reward=-596.00 +/- 598.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=261000, episode_reward=-610.00 +/- 663.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=262000, episode_reward=-44.40 +/- 393.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=263000, episode_reward=-137.20 +/- 388.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=264000, episode_reward=-92.00 +/- 225.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=265000, episode_reward=-288.40 +/- 396.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=266000, episode_reward=-311.80 +/- 181.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=267000, episode_reward=-165.80 +/- 243.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=268000, episode_reward=-384.60 +/- 396.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=269000, episode_reward=-485.80 +/- 530.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=270000, episode_reward=117.80 +/- 285.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=271000, episode_reward=-486.60 +/- 303.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=272000, episode_reward=-342.20 +/- 593.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=273000, episode_reward=-63.60 +/- 542.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=274000, episode_reward=-600.80 +/- 373.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=275000, episode_reward=-380.80 +/- 505.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=276000, episode_reward=-539.60 +/- 301.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=277000, episode_reward=-552.20 +/- 448.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=278000, episode_reward=60.00 +/- 217.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=279000, episode_reward=-118.60 +/- 381.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=280000, episode_reward=-453.80 +/- 467.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=281000, episode_reward=-114.80 +/- 330.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=282000, episode_reward=-700.20 +/- 396.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=283000, episode_reward=-434.00 +/- 172.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=284000, episode_reward=-638.40 +/- 528.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=285000, episode_reward=-214.00 +/- 461.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=286000, episode_reward=-409.00 +/- 669.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=287000, episode_reward=-467.00 +/- 608.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=288000, episode_reward=-342.60 +/- 447.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=289000, episode_reward=-685.20 +/- 769.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=290000, episode_reward=-380.80 +/- 193.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=291000, episode_reward=-416.40 +/- 452.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=292000, episode_reward=-379.60 +/- 150.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=293000, episode_reward=-286.20 +/- 303.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=294000, episode_reward=-137.00 +/- 375.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=295000, episode_reward=-143.80 +/- 145.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=296000, episode_reward=-689.60 +/- 405.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=297000, episode_reward=-362.40 +/- 470.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=298000, episode_reward=-440.20 +/- 347.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=299000, episode_reward=-285.40 +/- 377.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=300000, episode_reward=-132.60 +/- 428.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=301000, episode_reward=-153.80 +/- 186.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=302000, episode_reward=-555.60 +/- 291.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=303000, episode_reward=-592.40 +/- 203.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=304000, episode_reward=-593.40 +/- 374.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=305000, episode_reward=-123.80 +/- 318.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=306000, episode_reward=-357.80 +/- 374.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=307000, episode_reward=-415.80 +/- 495.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=308000, episode_reward=-153.00 +/- 174.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=309000, episode_reward=-268.20 +/- 288.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=310000, episode_reward=-245.80 +/- 324.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=311000, episode_reward=10.60 +/- 232.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=312000, episode_reward=-256.20 +/- 166.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=313000, episode_reward=-39.40 +/- 222.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=314000, episode_reward=-218.80 +/- 379.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=315000, episode_reward=-482.60 +/- 268.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=316000, episode_reward=-113.00 +/- 143.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=317000, episode_reward=-331.40 +/- 309.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=318000, episode_reward=47.00 +/- 154.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=319000, episode_reward=-48.00 +/- 297.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=320000, episode_reward=-215.00 +/- 226.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=321000, episode_reward=-65.20 +/- 419.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=322000, episode_reward=-123.80 +/- 320.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=323000, episode_reward=-200.00 +/- 365.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=324000, episode_reward=33.40 +/- 365.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=325000, episode_reward=-269.40 +/- 182.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=326000, episode_reward=-268.00 +/- 284.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=327000, episode_reward=-72.40 +/- 120.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=328000, episode_reward=33.60 +/- 213.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=329000, episode_reward=58.40 +/- 417.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=330000, episode_reward=-335.00 +/- 261.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=331000, episode_reward=-15.60 +/- 187.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=332000, episode_reward=-495.60 +/- 365.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=333000, episode_reward=-281.20 +/- 404.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=334000, episode_reward=-438.60 +/- 781.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=335000, episode_reward=-274.80 +/- 384.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=336000, episode_reward=-414.80 +/- 470.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=337000, episode_reward=-241.40 +/- 161.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=338000, episode_reward=-323.80 +/- 356.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=339000, episode_reward=-377.80 +/- 548.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=340000, episode_reward=-353.00 +/- 500.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=341000, episode_reward=-766.60 +/- 578.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=342000, episode_reward=-200.00 +/- 553.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=343000, episode_reward=-620.20 +/- 613.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=344000, episode_reward=-832.80 +/- 875.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=345000, episode_reward=-429.60 +/- 685.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=346000, episode_reward=46.80 +/- 195.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=347000, episode_reward=-431.00 +/- 788.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=348000, episode_reward=-215.00 +/- 747.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=349000, episode_reward=-289.00 +/- 305.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=350000, episode_reward=-156.20 +/- 217.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=351000, episode_reward=-378.60 +/- 288.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=352000, episode_reward=55.00 +/- 202.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=353000, episode_reward=-85.80 +/- 247.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=354000, episode_reward=-146.60 +/- 80.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=355000, episode_reward=-369.20 +/- 467.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=356000, episode_reward=-88.60 +/- 282.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=357000, episode_reward=-58.40 +/- 380.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=358000, episode_reward=-432.40 +/- 574.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=359000, episode_reward=-376.20 +/- 436.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=360000, episode_reward=-100.60 +/- 391.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=361000, episode_reward=-550.00 +/- 462.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=362000, episode_reward=-302.40 +/- 249.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=363000, episode_reward=-667.20 +/- 589.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=364000, episode_reward=-460.80 +/- 358.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=365000, episode_reward=-503.00 +/- 338.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=366000, episode_reward=-331.60 +/- 239.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=367000, episode_reward=-470.60 +/- 570.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=368000, episode_reward=-205.80 +/- 346.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=369000, episode_reward=-205.20 +/- 243.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=370000, episode_reward=-574.40 +/- 440.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=371000, episode_reward=-353.20 +/- 283.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=372000, episode_reward=-841.00 +/- 474.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=373000, episode_reward=-555.60 +/- 224.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=374000, episode_reward=-1017.80 +/- 574.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=375000, episode_reward=-302.20 +/- 102.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=376000, episode_reward=-475.20 +/- 677.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=377000, episode_reward=-278.40 +/- 433.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=378000, episode_reward=-518.00 +/- 599.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=379000, episode_reward=-448.00 +/- 351.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=380000, episode_reward=-500.80 +/- 223.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=381000, episode_reward=-340.20 +/- 411.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=382000, episode_reward=-343.40 +/- 224.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=383000, episode_reward=-520.80 +/- 329.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=384000, episode_reward=-533.00 +/- 491.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=385000, episode_reward=-550.40 +/- 554.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=386000, episode_reward=-293.60 +/- 466.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=387000, episode_reward=-338.80 +/- 641.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=388000, episode_reward=60.40 +/- 314.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=389000, episode_reward=-145.40 +/- 208.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=390000, episode_reward=47.00 +/- 341.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=391000, episode_reward=-121.40 +/- 371.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=392000, episode_reward=-95.20 +/- 367.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=393000, episode_reward=-192.20 +/- 262.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=394000, episode_reward=-234.00 +/- 355.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=395000, episode_reward=-355.40 +/- 454.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=396000, episode_reward=-487.20 +/- 361.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=397000, episode_reward=-167.40 +/- 376.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=398000, episode_reward=-73.60 +/- 177.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=399000, episode_reward=-242.80 +/- 348.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=400000, episode_reward=-96.20 +/- 311.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=401000, episode_reward=-267.20 +/- 129.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=402000, episode_reward=-76.20 +/- 432.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=403000, episode_reward=-14.20 +/- 96.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=404000, episode_reward=-176.00 +/- 239.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=405000, episode_reward=-37.40 +/- 343.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=406000, episode_reward=-60.00 +/- 178.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=407000, episode_reward=17.00 +/- 153.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=408000, episode_reward=-8.60 +/- 180.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=409000, episode_reward=-284.80 +/- 184.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=410000, episode_reward=-91.40 +/- 266.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=411000, episode_reward=-179.00 +/- 361.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=412000, episode_reward=110.40 +/- 267.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=413000, episode_reward=34.20 +/- 106.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=414000, episode_reward=82.60 +/- 71.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=415000, episode_reward=-78.60 +/- 210.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=416000, episode_reward=-253.60 +/- 404.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=417000, episode_reward=-228.20 +/- 274.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=418000, episode_reward=-162.20 +/- 348.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=419000, episode_reward=-30.00 +/- 163.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=420000, episode_reward=-161.60 +/- 306.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=421000, episode_reward=-632.20 +/- 526.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=422000, episode_reward=-560.20 +/- 957.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=423000, episode_reward=-92.00 +/- 480.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=424000, episode_reward=-60.00 +/- 376.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=425000, episode_reward=-207.20 +/- 400.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=426000, episode_reward=-236.40 +/- 344.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=427000, episode_reward=-279.40 +/- 300.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=428000, episode_reward=-461.00 +/- 467.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=429000, episode_reward=-267.20 +/- 603.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=430000, episode_reward=-87.40 +/- 235.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=431000, episode_reward=-236.60 +/- 243.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=432000, episode_reward=18.20 +/- 452.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=433000, episode_reward=-122.40 +/- 279.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=434000, episode_reward=-49.20 +/- 264.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=435000, episode_reward=-284.40 +/- 297.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=436000, episode_reward=-327.80 +/- 468.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=437000, episode_reward=-310.80 +/- 628.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=438000, episode_reward=-662.40 +/- 202.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=439000, episode_reward=-573.60 +/- 730.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=440000, episode_reward=-609.20 +/- 715.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=441000, episode_reward=-68.40 +/- 56.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=442000, episode_reward=-715.20 +/- 385.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=443000, episode_reward=235.20 +/- 187.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=444000, episode_reward=-210.00 +/- 192.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=445000, episode_reward=-193.60 +/- 209.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=446000, episode_reward=-421.60 +/- 624.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=447000, episode_reward=-378.40 +/- 271.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=448000, episode_reward=-1049.40 +/- 922.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=449000, episode_reward=-721.40 +/- 282.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=450000, episode_reward=-292.40 +/- 323.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=451000, episode_reward=-123.00 +/- 474.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=452000, episode_reward=-61.60 +/- 211.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=453000, episode_reward=-409.80 +/- 721.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=454000, episode_reward=-15.20 +/- 490.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=455000, episode_reward=-88.20 +/- 262.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=456000, episode_reward=-580.60 +/- 331.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=457000, episode_reward=-340.80 +/- 389.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=458000, episode_reward=-305.80 +/- 548.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=459000, episode_reward=-53.80 +/- 544.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=460000, episode_reward=-202.80 +/- 110.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=461000, episode_reward=-585.80 +/- 321.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=462000, episode_reward=-75.20 +/- 334.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=463000, episode_reward=-721.40 +/- 926.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=464000, episode_reward=-190.80 +/- 466.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=465000, episode_reward=-225.00 +/- 275.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=466000, episode_reward=-139.80 +/- 485.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=467000, episode_reward=-603.60 +/- 528.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=468000, episode_reward=-415.80 +/- 420.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=469000, episode_reward=-322.80 +/- 518.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=470000, episode_reward=-116.20 +/- 136.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=471000, episode_reward=-124.00 +/- 299.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=472000, episode_reward=-185.40 +/- 162.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=473000, episode_reward=-455.80 +/- 544.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=474000, episode_reward=-82.80 +/- 201.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=475000, episode_reward=-631.20 +/- 533.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=476000, episode_reward=-18.40 +/- 361.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=477000, episode_reward=-618.40 +/- 401.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=478000, episode_reward=-474.20 +/- 302.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=479000, episode_reward=-490.80 +/- 978.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=480000, episode_reward=-972.20 +/- 719.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=481000, episode_reward=-659.00 +/- 551.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=482000, episode_reward=-1105.80 +/- 617.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=483000, episode_reward=-75.00 +/- 192.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=484000, episode_reward=-584.40 +/- 626.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=485000, episode_reward=-154.40 +/- 184.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=486000, episode_reward=-435.60 +/- 315.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=487000, episode_reward=-549.80 +/- 746.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=488000, episode_reward=-465.60 +/- 647.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=489000, episode_reward=-161.60 +/- 320.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=490000, episode_reward=-167.80 +/- 212.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=491000, episode_reward=-607.80 +/- 335.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=492000, episode_reward=-159.20 +/- 286.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=493000, episode_reward=-789.20 +/- 812.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=494000, episode_reward=-105.60 +/- 429.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=495000, episode_reward=-244.00 +/- 349.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=496000, episode_reward=-416.80 +/- 566.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=497000, episode_reward=-603.80 +/- 512.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=498000, episode_reward=-459.00 +/- 453.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=499000, episode_reward=-244.40 +/- 375.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=500000, episode_reward=-76.80 +/- 254.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=501000, episode_reward=-590.00 +/- 487.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Gamma = 0.99, Learning Rate = 3e-05, Activation Function = ReLU, Neurons = 32, Process noise = 0.1, Measurement noise = 0.0\n",
      "Eval num_timesteps=1000, episode_reward=-1586.40 +/- 50.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=-1639.40 +/- 129.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=3000, episode_reward=-7484.80 +/- 1920.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=4000, episode_reward=-7997.80 +/- 1159.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=5000, episode_reward=-6822.40 +/- 1688.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=6000, episode_reward=-8013.20 +/- 1370.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=7000, episode_reward=14.80 +/- 251.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=8000, episode_reward=-177.20 +/- 457.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=9000, episode_reward=-90.60 +/- 319.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=10000, episode_reward=-219.60 +/- 615.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=11000, episode_reward=-108.20 +/- 386.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=12000, episode_reward=-235.20 +/- 327.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=13000, episode_reward=-137.00 +/- 654.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=14000, episode_reward=-898.60 +/- 717.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=15000, episode_reward=-77.40 +/- 369.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=16000, episode_reward=133.80 +/- 167.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=17000, episode_reward=-469.00 +/- 430.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=18000, episode_reward=-399.20 +/- 519.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=19000, episode_reward=-811.00 +/- 683.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=20000, episode_reward=-90.00 +/- 205.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=21000, episode_reward=-993.60 +/- 435.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=22000, episode_reward=-1002.60 +/- 619.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=23000, episode_reward=-888.40 +/- 461.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=24000, episode_reward=-614.20 +/- 352.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=25000, episode_reward=-517.20 +/- 543.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=26000, episode_reward=-476.40 +/- 363.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=27000, episode_reward=-7280.80 +/- 2275.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=28000, episode_reward=-6182.60 +/- 2560.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=29000, episode_reward=-1292.20 +/- 489.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=30000, episode_reward=-1441.60 +/- 162.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=31000, episode_reward=-681.00 +/- 384.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=32000, episode_reward=-902.00 +/- 391.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=33000, episode_reward=-5044.00 +/- 1766.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=34000, episode_reward=-6493.60 +/- 1178.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=35000, episode_reward=-4334.00 +/- 743.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=36000, episode_reward=-3725.60 +/- 1093.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=37000, episode_reward=-6117.20 +/- 1453.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=38000, episode_reward=-5650.60 +/- 3133.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=39000, episode_reward=-7485.00 +/- 2286.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=40000, episode_reward=-4992.40 +/- 1814.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=41000, episode_reward=-5439.20 +/- 2336.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=42000, episode_reward=-6242.60 +/- 1472.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=43000, episode_reward=-4678.60 +/- 1318.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=44000, episode_reward=-6299.80 +/- 2581.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=45000, episode_reward=-6262.60 +/- 1355.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=46000, episode_reward=-6062.80 +/- 1146.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=47000, episode_reward=-6323.60 +/- 1677.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=48000, episode_reward=-7556.40 +/- 2818.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=49000, episode_reward=-5568.60 +/- 1835.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=50000, episode_reward=-6674.60 +/- 1461.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=51000, episode_reward=-7800.60 +/- 1390.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=52000, episode_reward=-4857.20 +/- 2073.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=53000, episode_reward=-5199.80 +/- 1121.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=54000, episode_reward=-6391.00 +/- 1136.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=55000, episode_reward=-5095.40 +/- 685.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=56000, episode_reward=-4917.00 +/- 893.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=57000, episode_reward=-6088.80 +/- 1228.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=58000, episode_reward=-5752.80 +/- 2665.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=59000, episode_reward=-7647.60 +/- 1229.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=60000, episode_reward=-7960.00 +/- 1576.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=61000, episode_reward=-5391.40 +/- 2005.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=62000, episode_reward=-5639.00 +/- 1265.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=63000, episode_reward=-5929.20 +/- 1354.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=64000, episode_reward=-5411.40 +/- 3009.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=65000, episode_reward=-7598.20 +/- 1241.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=66000, episode_reward=-6133.00 +/- 831.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=67000, episode_reward=-7609.20 +/- 1154.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=68000, episode_reward=-5463.00 +/- 716.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=69000, episode_reward=-7020.20 +/- 1342.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=70000, episode_reward=-2574.00 +/- 737.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=71000, episode_reward=-2683.40 +/- 565.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=72000, episode_reward=-1702.80 +/- 101.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=73000, episode_reward=-1995.00 +/- 396.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=74000, episode_reward=-1618.60 +/- 419.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=75000, episode_reward=-1424.00 +/- 358.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=76000, episode_reward=-1047.80 +/- 231.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=77000, episode_reward=-1925.60 +/- 1110.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=78000, episode_reward=-728.40 +/- 690.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=79000, episode_reward=-861.80 +/- 931.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=80000, episode_reward=-237.00 +/- 237.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=81000, episode_reward=-508.20 +/- 326.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=82000, episode_reward=-284.60 +/- 298.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=83000, episode_reward=-165.80 +/- 242.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=84000, episode_reward=-92.00 +/- 874.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=85000, episode_reward=-424.80 +/- 707.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=86000, episode_reward=-465.20 +/- 410.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=87000, episode_reward=-526.60 +/- 173.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=88000, episode_reward=-696.00 +/- 727.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=89000, episode_reward=-260.60 +/- 398.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=90000, episode_reward=-376.20 +/- 324.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=91000, episode_reward=-219.80 +/- 320.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=92000, episode_reward=-920.80 +/- 1141.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=93000, episode_reward=-456.80 +/- 553.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=94000, episode_reward=-1592.60 +/- 1092.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=95000, episode_reward=-291.40 +/- 473.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=96000, episode_reward=-789.20 +/- 868.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=97000, episode_reward=-268.00 +/- 346.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=98000, episode_reward=-320.60 +/- 452.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=99000, episode_reward=-433.80 +/- 453.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=100000, episode_reward=-493.40 +/- 545.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=101000, episode_reward=-448.60 +/- 540.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=102000, episode_reward=-147.60 +/- 142.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=103000, episode_reward=-611.40 +/- 744.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=104000, episode_reward=-672.20 +/- 435.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=105000, episode_reward=55.40 +/- 380.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=106000, episode_reward=-866.80 +/- 794.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=107000, episode_reward=-594.40 +/- 873.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=108000, episode_reward=-840.60 +/- 672.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=109000, episode_reward=-647.80 +/- 455.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=110000, episode_reward=-1226.00 +/- 919.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=111000, episode_reward=-563.60 +/- 214.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=112000, episode_reward=-556.40 +/- 348.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=113000, episode_reward=-913.80 +/- 493.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=114000, episode_reward=-645.60 +/- 278.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=115000, episode_reward=-911.60 +/- 395.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=116000, episode_reward=-909.60 +/- 296.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=117000, episode_reward=-1162.40 +/- 347.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=118000, episode_reward=-1193.40 +/- 331.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=119000, episode_reward=-955.20 +/- 710.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=120000, episode_reward=-829.80 +/- 560.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=121000, episode_reward=-284.80 +/- 577.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=122000, episode_reward=-275.00 +/- 467.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=123000, episode_reward=-894.00 +/- 473.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=124000, episode_reward=-417.40 +/- 452.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=125000, episode_reward=-262.00 +/- 273.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=126000, episode_reward=-638.80 +/- 717.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=127000, episode_reward=-759.20 +/- 487.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=128000, episode_reward=-951.80 +/- 918.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=129000, episode_reward=-70.20 +/- 429.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=130000, episode_reward=-699.60 +/- 541.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=131000, episode_reward=-736.60 +/- 614.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=132000, episode_reward=-694.60 +/- 398.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=133000, episode_reward=-778.80 +/- 531.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=134000, episode_reward=-599.20 +/- 380.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=135000, episode_reward=-298.40 +/- 495.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=136000, episode_reward=-53.00 +/- 260.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=137000, episode_reward=-735.20 +/- 772.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=138000, episode_reward=15.80 +/- 281.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=139000, episode_reward=-273.40 +/- 325.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=140000, episode_reward=-68.60 +/- 239.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=141000, episode_reward=-278.80 +/- 320.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=142000, episode_reward=-618.80 +/- 430.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=143000, episode_reward=-56.60 +/- 211.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=144000, episode_reward=-851.80 +/- 618.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=145000, episode_reward=-234.40 +/- 464.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=146000, episode_reward=-134.00 +/- 415.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=147000, episode_reward=-477.20 +/- 436.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=148000, episode_reward=-252.20 +/- 381.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=149000, episode_reward=-241.20 +/- 285.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=150000, episode_reward=-44.40 +/- 525.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=151000, episode_reward=-473.00 +/- 488.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=152000, episode_reward=-338.00 +/- 405.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=153000, episode_reward=-36.60 +/- 156.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=154000, episode_reward=-332.40 +/- 356.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=155000, episode_reward=-398.00 +/- 147.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=156000, episode_reward=-137.60 +/- 280.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=157000, episode_reward=-205.60 +/- 463.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=158000, episode_reward=-407.00 +/- 574.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=159000, episode_reward=-82.40 +/- 205.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=160000, episode_reward=-40.20 +/- 286.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=161000, episode_reward=-415.20 +/- 412.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=162000, episode_reward=-166.80 +/- 379.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=163000, episode_reward=-239.20 +/- 701.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=164000, episode_reward=-45.20 +/- 376.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=165000, episode_reward=-419.20 +/- 514.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=166000, episode_reward=-458.80 +/- 615.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=167000, episode_reward=-460.20 +/- 711.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=168000, episode_reward=-346.20 +/- 687.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=169000, episode_reward=-305.80 +/- 262.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=170000, episode_reward=-135.20 +/- 346.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=171000, episode_reward=-65.20 +/- 421.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=172000, episode_reward=-374.80 +/- 198.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=173000, episode_reward=-436.80 +/- 394.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=174000, episode_reward=81.40 +/- 214.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=175000, episode_reward=-747.80 +/- 769.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=176000, episode_reward=-330.00 +/- 471.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=177000, episode_reward=46.60 +/- 169.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=178000, episode_reward=-61.60 +/- 253.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=179000, episode_reward=-443.00 +/- 515.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=180000, episode_reward=-647.20 +/- 803.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=181000, episode_reward=-342.40 +/- 532.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=182000, episode_reward=-132.80 +/- 281.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=183000, episode_reward=52.00 +/- 169.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=184000, episode_reward=75.80 +/- 116.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=185000, episode_reward=-510.40 +/- 623.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=186000, episode_reward=-181.40 +/- 240.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=187000, episode_reward=-549.40 +/- 802.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=188000, episode_reward=120.80 +/- 428.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=189000, episode_reward=-61.00 +/- 424.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=190000, episode_reward=188.60 +/- 230.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=191000, episode_reward=-218.40 +/- 394.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=192000, episode_reward=-303.60 +/- 680.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=193000, episode_reward=-1053.80 +/- 1429.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=194000, episode_reward=-658.00 +/- 434.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=195000, episode_reward=-99.60 +/- 124.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=196000, episode_reward=-1058.80 +/- 1437.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=197000, episode_reward=17.40 +/- 433.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=198000, episode_reward=-324.60 +/- 840.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=199000, episode_reward=-822.60 +/- 985.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=200000, episode_reward=-66.80 +/- 148.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=201000, episode_reward=-177.60 +/- 356.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=202000, episode_reward=-105.00 +/- 274.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=203000, episode_reward=144.60 +/- 246.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=204000, episode_reward=-161.80 +/- 737.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=205000, episode_reward=-324.20 +/- 197.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=206000, episode_reward=-761.20 +/- 878.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=207000, episode_reward=-279.40 +/- 429.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=208000, episode_reward=-449.60 +/- 585.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=209000, episode_reward=-581.80 +/- 367.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=210000, episode_reward=-259.20 +/- 291.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=211000, episode_reward=100.00 +/- 367.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=212000, episode_reward=-235.60 +/- 580.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=213000, episode_reward=-450.20 +/- 576.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=214000, episode_reward=-167.00 +/- 508.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=215000, episode_reward=-322.40 +/- 579.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=216000, episode_reward=-278.80 +/- 488.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=217000, episode_reward=169.00 +/- 188.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=218000, episode_reward=-116.00 +/- 488.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=219000, episode_reward=-282.20 +/- 219.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=220000, episode_reward=92.40 +/- 155.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=221000, episode_reward=-295.80 +/- 862.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=222000, episode_reward=-119.60 +/- 359.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=223000, episode_reward=184.20 +/- 191.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=224000, episode_reward=69.00 +/- 173.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=225000, episode_reward=-240.80 +/- 520.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=226000, episode_reward=-494.40 +/- 264.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=227000, episode_reward=-210.20 +/- 433.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=228000, episode_reward=117.80 +/- 384.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=229000, episode_reward=-959.60 +/- 1105.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=230000, episode_reward=-429.20 +/- 870.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=231000, episode_reward=-574.00 +/- 957.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=232000, episode_reward=-336.00 +/- 490.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=233000, episode_reward=-123.60 +/- 368.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=234000, episode_reward=-783.40 +/- 810.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=235000, episode_reward=-13.00 +/- 262.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=236000, episode_reward=-15.00 +/- 396.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=237000, episode_reward=-172.20 +/- 335.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=238000, episode_reward=-79.00 +/- 89.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=239000, episode_reward=16.80 +/- 132.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=240000, episode_reward=-9.80 +/- 503.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=241000, episode_reward=-1197.40 +/- 919.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=242000, episode_reward=-104.40 +/- 48.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=243000, episode_reward=-879.40 +/- 787.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=244000, episode_reward=-476.40 +/- 281.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=245000, episode_reward=-700.80 +/- 395.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=246000, episode_reward=-734.40 +/- 918.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=247000, episode_reward=-509.60 +/- 363.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=248000, episode_reward=-567.00 +/- 367.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=249000, episode_reward=-502.40 +/- 467.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=250000, episode_reward=-221.60 +/- 367.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=251000, episode_reward=16.40 +/- 244.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=252000, episode_reward=-728.60 +/- 1266.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=253000, episode_reward=-738.20 +/- 466.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=254000, episode_reward=-182.00 +/- 442.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=255000, episode_reward=-720.40 +/- 1014.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=256000, episode_reward=-168.60 +/- 272.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=257000, episode_reward=-361.80 +/- 575.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=258000, episode_reward=-124.00 +/- 466.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=259000, episode_reward=-612.00 +/- 437.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=260000, episode_reward=-376.80 +/- 475.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=261000, episode_reward=-591.40 +/- 454.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=262000, episode_reward=-433.80 +/- 638.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=263000, episode_reward=-98.60 +/- 387.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=264000, episode_reward=-290.20 +/- 242.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=265000, episode_reward=-775.80 +/- 1169.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=266000, episode_reward=-539.20 +/- 411.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=267000, episode_reward=-501.20 +/- 256.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=268000, episode_reward=74.80 +/- 332.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=269000, episode_reward=-510.40 +/- 536.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=270000, episode_reward=-500.00 +/- 215.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=271000, episode_reward=-568.80 +/- 697.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=272000, episode_reward=-538.80 +/- 546.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=273000, episode_reward=-305.40 +/- 341.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=274000, episode_reward=-608.00 +/- 423.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=275000, episode_reward=-74.80 +/- 351.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=276000, episode_reward=-746.60 +/- 865.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=277000, episode_reward=-530.60 +/- 466.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=278000, episode_reward=-1086.40 +/- 370.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=279000, episode_reward=-863.60 +/- 416.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=280000, episode_reward=-717.00 +/- 298.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=281000, episode_reward=-232.00 +/- 353.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=282000, episode_reward=-637.00 +/- 601.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=283000, episode_reward=-169.40 +/- 359.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=284000, episode_reward=-397.00 +/- 679.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=285000, episode_reward=-548.40 +/- 507.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=286000, episode_reward=-242.80 +/- 628.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=287000, episode_reward=-249.60 +/- 380.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=288000, episode_reward=-234.40 +/- 365.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=289000, episode_reward=-146.80 +/- 220.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=290000, episode_reward=-347.40 +/- 340.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=291000, episode_reward=-337.40 +/- 522.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=292000, episode_reward=-297.60 +/- 312.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=293000, episode_reward=-424.60 +/- 449.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=294000, episode_reward=-81.00 +/- 333.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=295000, episode_reward=25.60 +/- 212.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=296000, episode_reward=-404.80 +/- 610.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=297000, episode_reward=-130.20 +/- 286.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=298000, episode_reward=-499.60 +/- 290.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=299000, episode_reward=-455.00 +/- 1001.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=300000, episode_reward=-759.80 +/- 1101.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=301000, episode_reward=-401.80 +/- 653.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=302000, episode_reward=-267.40 +/- 326.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=303000, episode_reward=-439.40 +/- 528.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=304000, episode_reward=-659.60 +/- 713.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=305000, episode_reward=-595.80 +/- 608.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=306000, episode_reward=-828.80 +/- 1113.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=307000, episode_reward=-193.80 +/- 227.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=308000, episode_reward=-796.80 +/- 711.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=309000, episode_reward=-345.80 +/- 232.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=310000, episode_reward=-461.20 +/- 557.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=311000, episode_reward=-541.80 +/- 296.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=312000, episode_reward=-787.60 +/- 528.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=313000, episode_reward=-1193.80 +/- 508.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=314000, episode_reward=-1057.00 +/- 430.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=315000, episode_reward=-447.00 +/- 397.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=316000, episode_reward=-519.20 +/- 473.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=317000, episode_reward=-426.80 +/- 505.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=318000, episode_reward=-531.80 +/- 397.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=319000, episode_reward=-307.60 +/- 574.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=320000, episode_reward=-459.20 +/- 620.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=321000, episode_reward=-310.40 +/- 220.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=322000, episode_reward=-689.20 +/- 341.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=323000, episode_reward=-222.40 +/- 395.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=324000, episode_reward=-1124.80 +/- 724.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=325000, episode_reward=-440.20 +/- 215.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=326000, episode_reward=-549.40 +/- 618.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=327000, episode_reward=-798.00 +/- 370.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=328000, episode_reward=-749.20 +/- 599.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=329000, episode_reward=-456.20 +/- 265.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=330000, episode_reward=-596.80 +/- 286.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=331000, episode_reward=-1500.40 +/- 567.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=332000, episode_reward=-1449.40 +/- 345.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=333000, episode_reward=-1588.80 +/- 1230.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=334000, episode_reward=-5452.40 +/- 570.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=335000, episode_reward=-6191.00 +/- 1343.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=336000, episode_reward=-5865.40 +/- 952.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=337000, episode_reward=-5415.40 +/- 158.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=338000, episode_reward=-1859.80 +/- 475.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=339000, episode_reward=-2141.40 +/- 307.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=340000, episode_reward=-2684.20 +/- 1098.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=341000, episode_reward=-1856.40 +/- 336.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=342000, episode_reward=-2337.80 +/- 511.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=343000, episode_reward=-4318.20 +/- 417.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=344000, episode_reward=-4318.80 +/- 367.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=345000, episode_reward=-2333.20 +/- 859.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=346000, episode_reward=-1995.80 +/- 605.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=347000, episode_reward=-1802.60 +/- 260.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=348000, episode_reward=-2162.00 +/- 900.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=349000, episode_reward=-1089.40 +/- 1294.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=350000, episode_reward=-722.80 +/- 299.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=351000, episode_reward=-1123.20 +/- 213.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=352000, episode_reward=-865.00 +/- 428.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=353000, episode_reward=-1631.00 +/- 555.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=354000, episode_reward=-916.40 +/- 616.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=355000, episode_reward=-592.60 +/- 325.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=356000, episode_reward=-699.40 +/- 313.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=357000, episode_reward=-516.00 +/- 336.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=358000, episode_reward=-424.60 +/- 243.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=359000, episode_reward=-224.80 +/- 313.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=360000, episode_reward=-207.00 +/- 422.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=361000, episode_reward=-547.60 +/- 477.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=362000, episode_reward=-611.00 +/- 492.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=363000, episode_reward=-805.20 +/- 888.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=364000, episode_reward=-793.00 +/- 392.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=365000, episode_reward=-231.80 +/- 571.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=366000, episode_reward=-722.40 +/- 722.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=367000, episode_reward=-562.80 +/- 295.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=368000, episode_reward=-485.40 +/- 231.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=369000, episode_reward=-519.60 +/- 818.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=370000, episode_reward=-591.00 +/- 448.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=371000, episode_reward=-165.60 +/- 461.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=372000, episode_reward=-504.00 +/- 505.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=373000, episode_reward=-559.60 +/- 276.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=374000, episode_reward=-636.00 +/- 358.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=375000, episode_reward=-310.00 +/- 553.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=376000, episode_reward=-1371.00 +/- 1009.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=377000, episode_reward=-70.40 +/- 580.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=378000, episode_reward=-362.40 +/- 383.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=379000, episode_reward=-682.80 +/- 575.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=380000, episode_reward=-338.60 +/- 541.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=381000, episode_reward=-477.60 +/- 149.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=382000, episode_reward=106.20 +/- 346.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=383000, episode_reward=-676.00 +/- 914.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=384000, episode_reward=-236.80 +/- 441.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=385000, episode_reward=-120.60 +/- 417.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=386000, episode_reward=-543.60 +/- 521.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=387000, episode_reward=-271.40 +/- 310.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=388000, episode_reward=-227.40 +/- 368.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=389000, episode_reward=-189.00 +/- 428.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=390000, episode_reward=-251.40 +/- 402.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=391000, episode_reward=-529.00 +/- 494.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=392000, episode_reward=-229.80 +/- 290.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=393000, episode_reward=-597.60 +/- 292.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=394000, episode_reward=-590.00 +/- 441.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=395000, episode_reward=-156.80 +/- 587.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=396000, episode_reward=-63.20 +/- 555.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=397000, episode_reward=-213.20 +/- 90.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=398000, episode_reward=-382.00 +/- 427.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=399000, episode_reward=75.80 +/- 195.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=400000, episode_reward=-98.00 +/- 414.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=401000, episode_reward=-103.60 +/- 122.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=402000, episode_reward=164.80 +/- 263.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=403000, episode_reward=-252.60 +/- 532.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=404000, episode_reward=8.60 +/- 485.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=405000, episode_reward=180.40 +/- 261.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=406000, episode_reward=-1176.80 +/- 1138.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=407000, episode_reward=135.60 +/- 220.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=408000, episode_reward=-284.00 +/- 597.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=409000, episode_reward=-324.40 +/- 485.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=410000, episode_reward=143.20 +/- 217.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=411000, episode_reward=-149.40 +/- 502.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=412000, episode_reward=116.60 +/- 312.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=413000, episode_reward=-499.60 +/- 898.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=414000, episode_reward=-88.20 +/- 326.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=415000, episode_reward=-3.60 +/- 272.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=416000, episode_reward=71.40 +/- 277.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=417000, episode_reward=-269.20 +/- 529.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=418000, episode_reward=-238.80 +/- 530.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=419000, episode_reward=-336.60 +/- 525.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=420000, episode_reward=-228.20 +/- 413.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=421000, episode_reward=-375.00 +/- 978.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=422000, episode_reward=-144.40 +/- 275.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=423000, episode_reward=98.20 +/- 410.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=424000, episode_reward=-1454.40 +/- 1842.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=425000, episode_reward=-246.00 +/- 750.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=426000, episode_reward=-231.60 +/- 539.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=427000, episode_reward=-581.60 +/- 919.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=428000, episode_reward=-111.40 +/- 323.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=429000, episode_reward=-162.00 +/- 291.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=430000, episode_reward=-338.20 +/- 268.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=431000, episode_reward=60.60 +/- 344.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=432000, episode_reward=-112.80 +/- 393.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=433000, episode_reward=-212.60 +/- 441.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=434000, episode_reward=-278.00 +/- 326.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=435000, episode_reward=-393.80 +/- 829.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=436000, episode_reward=-152.60 +/- 534.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=437000, episode_reward=-124.60 +/- 588.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=438000, episode_reward=130.20 +/- 254.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=439000, episode_reward=-23.00 +/- 217.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=440000, episode_reward=-165.60 +/- 354.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=441000, episode_reward=-317.40 +/- 380.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=442000, episode_reward=-328.80 +/- 348.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=443000, episode_reward=-363.40 +/- 268.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=444000, episode_reward=219.00 +/- 169.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=445000, episode_reward=-112.20 +/- 334.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=446000, episode_reward=-598.20 +/- 772.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=447000, episode_reward=-156.00 +/- 650.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=448000, episode_reward=-499.20 +/- 552.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=449000, episode_reward=-646.20 +/- 1008.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=450000, episode_reward=-116.20 +/- 525.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=451000, episode_reward=147.20 +/- 269.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=452000, episode_reward=-258.60 +/- 560.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=453000, episode_reward=-219.20 +/- 591.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=454000, episode_reward=-70.40 +/- 267.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=455000, episode_reward=-315.00 +/- 684.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=456000, episode_reward=-268.40 +/- 401.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=457000, episode_reward=130.60 +/- 468.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=458000, episode_reward=122.00 +/- 128.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=459000, episode_reward=-64.40 +/- 310.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=460000, episode_reward=-324.00 +/- 730.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=461000, episode_reward=11.80 +/- 278.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=462000, episode_reward=-215.60 +/- 299.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=463000, episode_reward=-683.60 +/- 728.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=464000, episode_reward=-287.40 +/- 275.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=465000, episode_reward=-105.00 +/- 318.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=466000, episode_reward=-561.80 +/- 527.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=467000, episode_reward=-365.80 +/- 613.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=468000, episode_reward=-383.40 +/- 901.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=469000, episode_reward=33.20 +/- 254.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=470000, episode_reward=15.40 +/- 285.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=471000, episode_reward=-482.60 +/- 332.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=472000, episode_reward=-685.00 +/- 1178.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=473000, episode_reward=-167.20 +/- 410.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=474000, episode_reward=-61.00 +/- 315.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=475000, episode_reward=-99.40 +/- 174.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=476000, episode_reward=-97.80 +/- 332.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=477000, episode_reward=-106.40 +/- 407.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=478000, episode_reward=-705.80 +/- 802.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=479000, episode_reward=-331.40 +/- 438.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=480000, episode_reward=-904.40 +/- 1166.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=481000, episode_reward=-273.20 +/- 518.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=482000, episode_reward=-323.40 +/- 668.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=483000, episode_reward=-197.60 +/- 306.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=484000, episode_reward=-495.20 +/- 279.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=485000, episode_reward=-683.60 +/- 913.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=486000, episode_reward=-55.20 +/- 233.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=487000, episode_reward=-16.80 +/- 216.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=488000, episode_reward=74.40 +/- 280.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=489000, episode_reward=-272.40 +/- 383.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=490000, episode_reward=28.60 +/- 376.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=491000, episode_reward=-55.20 +/- 307.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=492000, episode_reward=-156.40 +/- 456.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=493000, episode_reward=-296.00 +/- 242.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=494000, episode_reward=2.20 +/- 316.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=495000, episode_reward=-414.40 +/- 521.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=496000, episode_reward=17.00 +/- 357.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=497000, episode_reward=-282.60 +/- 805.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=498000, episode_reward=-352.00 +/- 280.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=499000, episode_reward=-212.00 +/- 158.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=500000, episode_reward=-197.20 +/- 401.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=501000, episode_reward=-324.60 +/- 744.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Gamma = 0.99, Learning Rate = 3e-05, Activation Function = ReLU, Neurons = 64, Process noise = 0.1, Measurement noise = 0.0\n",
      "Eval num_timesteps=1000, episode_reward=-19013.00 +/- 45.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=-18978.00 +/- 36.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3000, episode_reward=-7112.20 +/- 2495.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4000, episode_reward=-5265.00 +/- 1058.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=5000, episode_reward=-6255.60 +/- 2044.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=6000, episode_reward=-7028.20 +/- 2286.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=7000, episode_reward=-6523.60 +/- 2052.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=8000, episode_reward=-5668.60 +/- 1487.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=9000, episode_reward=-7074.80 +/- 1216.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=10000, episode_reward=-6468.20 +/- 1815.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=11000, episode_reward=-30.20 +/- 181.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=12000, episode_reward=68.60 +/- 209.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=13000, episode_reward=-6594.80 +/- 1838.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=14000, episode_reward=-5650.60 +/- 809.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=15000, episode_reward=-5804.00 +/- 2028.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=16000, episode_reward=-5881.20 +/- 2454.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=17000, episode_reward=-7535.60 +/- 1235.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=18000, episode_reward=-6337.20 +/- 1056.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=19000, episode_reward=-6357.40 +/- 1873.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=20000, episode_reward=-6721.60 +/- 1577.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=21000, episode_reward=-6021.60 +/- 1750.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=22000, episode_reward=-7415.60 +/- 1680.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=23000, episode_reward=241.40 +/- 102.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=24000, episode_reward=369.80 +/- 162.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=25000, episode_reward=396.60 +/- 145.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=26000, episode_reward=105.60 +/- 228.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=27000, episode_reward=430.60 +/- 166.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=28000, episode_reward=485.80 +/- 154.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=29000, episode_reward=222.20 +/- 148.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=30000, episode_reward=293.40 +/- 217.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=31000, episode_reward=320.00 +/- 111.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=32000, episode_reward=257.20 +/- 200.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=33000, episode_reward=313.20 +/- 103.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=34000, episode_reward=180.00 +/- 65.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=35000, episode_reward=213.60 +/- 221.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=36000, episode_reward=201.80 +/- 317.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=37000, episode_reward=118.80 +/- 95.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=38000, episode_reward=421.00 +/- 260.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=39000, episode_reward=38.20 +/- 104.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=40000, episode_reward=127.60 +/- 249.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=41000, episode_reward=-103.00 +/- 184.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=42000, episode_reward=-118.40 +/- 102.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=43000, episode_reward=85.60 +/- 172.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=44000, episode_reward=98.80 +/- 230.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=45000, episode_reward=143.00 +/- 219.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=46000, episode_reward=130.20 +/- 220.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=47000, episode_reward=207.40 +/- 218.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=48000, episode_reward=116.80 +/- 290.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=49000, episode_reward=215.80 +/- 120.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=50000, episode_reward=30.60 +/- 256.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=51000, episode_reward=211.60 +/- 301.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=52000, episode_reward=170.60 +/- 212.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=53000, episode_reward=96.60 +/- 152.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=54000, episode_reward=294.60 +/- 118.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=55000, episode_reward=146.40 +/- 177.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=56000, episode_reward=189.00 +/- 146.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=57000, episode_reward=373.00 +/- 209.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=58000, episode_reward=274.40 +/- 204.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=59000, episode_reward=187.80 +/- 178.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=60000, episode_reward=505.20 +/- 43.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=61000, episode_reward=264.60 +/- 129.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=62000, episode_reward=141.40 +/- 207.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=63000, episode_reward=270.80 +/- 183.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=64000, episode_reward=221.20 +/- 220.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=65000, episode_reward=407.40 +/- 96.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=66000, episode_reward=323.20 +/- 166.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=67000, episode_reward=449.00 +/- 114.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=68000, episode_reward=306.40 +/- 146.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=69000, episode_reward=264.00 +/- 168.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=70000, episode_reward=521.00 +/- 120.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=71000, episode_reward=444.60 +/- 174.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=72000, episode_reward=338.60 +/- 312.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=73000, episode_reward=214.40 +/- 199.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=74000, episode_reward=293.80 +/- 125.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=75000, episode_reward=370.20 +/- 118.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=76000, episode_reward=196.60 +/- 115.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=77000, episode_reward=410.40 +/- 314.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=78000, episode_reward=373.00 +/- 120.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=79000, episode_reward=342.20 +/- 214.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=80000, episode_reward=248.80 +/- 212.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=81000, episode_reward=234.40 +/- 116.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=82000, episode_reward=307.60 +/- 187.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=83000, episode_reward=256.20 +/- 201.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=84000, episode_reward=358.40 +/- 252.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=85000, episode_reward=279.20 +/- 260.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=86000, episode_reward=253.60 +/- 218.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=87000, episode_reward=253.40 +/- 250.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=88000, episode_reward=373.80 +/- 237.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=89000, episode_reward=261.20 +/- 72.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=90000, episode_reward=295.60 +/- 288.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=91000, episode_reward=353.40 +/- 178.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=92000, episode_reward=402.20 +/- 67.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=93000, episode_reward=298.80 +/- 229.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=94000, episode_reward=257.20 +/- 203.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=95000, episode_reward=508.20 +/- 156.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=96000, episode_reward=452.20 +/- 131.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=97000, episode_reward=479.80 +/- 171.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=98000, episode_reward=411.80 +/- 162.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=99000, episode_reward=441.00 +/- 85.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=100000, episode_reward=313.60 +/- 171.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=101000, episode_reward=83.40 +/- 188.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=102000, episode_reward=309.40 +/- 153.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=103000, episode_reward=477.40 +/- 153.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=104000, episode_reward=257.40 +/- 259.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=105000, episode_reward=265.20 +/- 250.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=106000, episode_reward=153.80 +/- 175.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=107000, episode_reward=580.00 +/- 155.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=108000, episode_reward=368.60 +/- 105.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=109000, episode_reward=155.20 +/- 253.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=110000, episode_reward=488.60 +/- 158.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=111000, episode_reward=398.20 +/- 300.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=112000, episode_reward=320.60 +/- 200.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=113000, episode_reward=351.20 +/- 139.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=114000, episode_reward=294.80 +/- 244.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=115000, episode_reward=413.80 +/- 174.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=116000, episode_reward=432.00 +/- 81.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=117000, episode_reward=415.20 +/- 297.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=118000, episode_reward=285.60 +/- 245.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=119000, episode_reward=326.40 +/- 58.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=120000, episode_reward=557.20 +/- 86.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=121000, episode_reward=331.20 +/- 133.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=122000, episode_reward=239.40 +/- 191.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=123000, episode_reward=313.40 +/- 122.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=124000, episode_reward=394.80 +/- 215.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=125000, episode_reward=216.80 +/- 224.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=126000, episode_reward=253.80 +/- 170.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=127000, episode_reward=398.20 +/- 152.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=128000, episode_reward=331.40 +/- 186.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=129000, episode_reward=342.00 +/- 273.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=130000, episode_reward=369.00 +/- 223.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=131000, episode_reward=447.20 +/- 145.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=132000, episode_reward=187.00 +/- 278.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=133000, episode_reward=279.20 +/- 68.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=134000, episode_reward=472.80 +/- 133.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=135000, episode_reward=418.00 +/- 240.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=136000, episode_reward=245.20 +/- 215.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=137000, episode_reward=201.20 +/- 269.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=138000, episode_reward=254.20 +/- 233.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=139000, episode_reward=279.60 +/- 292.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=140000, episode_reward=125.40 +/- 267.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=141000, episode_reward=325.40 +/- 175.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=142000, episode_reward=300.20 +/- 197.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=143000, episode_reward=199.60 +/- 122.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=144000, episode_reward=398.20 +/- 103.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=145000, episode_reward=499.80 +/- 195.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=146000, episode_reward=480.80 +/- 168.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=147000, episode_reward=320.20 +/- 249.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=148000, episode_reward=279.20 +/- 105.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=149000, episode_reward=288.60 +/- 197.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=150000, episode_reward=276.80 +/- 244.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=151000, episode_reward=278.60 +/- 97.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=152000, episode_reward=341.80 +/- 223.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=153000, episode_reward=517.20 +/- 134.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=154000, episode_reward=224.80 +/- 82.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=155000, episode_reward=354.60 +/- 250.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=156000, episode_reward=398.20 +/- 181.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=157000, episode_reward=320.20 +/- 191.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=158000, episode_reward=307.40 +/- 173.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=159000, episode_reward=325.40 +/- 229.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=160000, episode_reward=180.20 +/- 274.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=161000, episode_reward=235.20 +/- 201.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=162000, episode_reward=183.40 +/- 271.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=163000, episode_reward=328.00 +/- 115.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=164000, episode_reward=127.00 +/- 127.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=165000, episode_reward=322.60 +/- 96.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=166000, episode_reward=220.20 +/- 200.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=167000, episode_reward=266.80 +/- 108.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=168000, episode_reward=263.40 +/- 179.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=169000, episode_reward=338.80 +/- 249.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=170000, episode_reward=240.80 +/- 153.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=171000, episode_reward=62.40 +/- 163.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=172000, episode_reward=132.60 +/- 123.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=173000, episode_reward=379.00 +/- 136.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=174000, episode_reward=240.00 +/- 192.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=175000, episode_reward=255.20 +/- 300.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=176000, episode_reward=327.80 +/- 160.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=177000, episode_reward=87.00 +/- 211.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=178000, episode_reward=229.80 +/- 196.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=179000, episode_reward=413.40 +/- 117.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=180000, episode_reward=369.20 +/- 112.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=181000, episode_reward=478.60 +/- 103.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=182000, episode_reward=321.00 +/- 243.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=183000, episode_reward=390.40 +/- 204.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=184000, episode_reward=302.40 +/- 107.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=185000, episode_reward=311.00 +/- 341.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=186000, episode_reward=399.60 +/- 135.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=187000, episode_reward=408.20 +/- 237.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=188000, episode_reward=298.60 +/- 242.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=189000, episode_reward=431.00 +/- 120.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=190000, episode_reward=347.20 +/- 72.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=191000, episode_reward=274.40 +/- 108.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=192000, episode_reward=275.00 +/- 127.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=193000, episode_reward=330.40 +/- 219.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=194000, episode_reward=491.60 +/- 248.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=195000, episode_reward=251.60 +/- 115.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=196000, episode_reward=405.40 +/- 141.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=197000, episode_reward=265.20 +/- 148.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=198000, episode_reward=321.60 +/- 61.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=199000, episode_reward=384.20 +/- 98.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=200000, episode_reward=458.40 +/- 188.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=201000, episode_reward=267.80 +/- 182.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=202000, episode_reward=505.40 +/- 74.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=203000, episode_reward=315.40 +/- 204.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=204000, episode_reward=480.60 +/- 100.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=205000, episode_reward=373.60 +/- 138.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=206000, episode_reward=246.80 +/- 218.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=207000, episode_reward=212.60 +/- 92.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=208000, episode_reward=384.80 +/- 137.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=209000, episode_reward=371.20 +/- 37.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=210000, episode_reward=406.00 +/- 185.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=211000, episode_reward=277.60 +/- 142.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=212000, episode_reward=492.40 +/- 39.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=213000, episode_reward=499.60 +/- 279.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=214000, episode_reward=291.20 +/- 85.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=215000, episode_reward=511.80 +/- 105.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=216000, episode_reward=274.60 +/- 123.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=217000, episode_reward=296.80 +/- 120.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=218000, episode_reward=295.00 +/- 230.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=219000, episode_reward=341.80 +/- 108.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=220000, episode_reward=231.40 +/- 176.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=221000, episode_reward=472.00 +/- 152.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=222000, episode_reward=332.40 +/- 182.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=223000, episode_reward=394.60 +/- 182.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=224000, episode_reward=379.80 +/- 91.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=225000, episode_reward=309.40 +/- 171.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=226000, episode_reward=208.20 +/- 210.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=227000, episode_reward=491.80 +/- 232.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=228000, episode_reward=238.60 +/- 218.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=229000, episode_reward=333.00 +/- 228.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=230000, episode_reward=348.00 +/- 64.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=231000, episode_reward=540.60 +/- 131.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=232000, episode_reward=406.60 +/- 177.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=233000, episode_reward=396.20 +/- 127.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=234000, episode_reward=430.40 +/- 103.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=235000, episode_reward=373.20 +/- 203.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=236000, episode_reward=216.80 +/- 179.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=237000, episode_reward=335.40 +/- 116.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=238000, episode_reward=190.80 +/- 226.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=239000, episode_reward=381.60 +/- 242.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=240000, episode_reward=422.60 +/- 230.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=241000, episode_reward=407.00 +/- 200.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=242000, episode_reward=313.00 +/- 204.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=243000, episode_reward=305.80 +/- 231.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=244000, episode_reward=347.40 +/- 98.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=245000, episode_reward=333.60 +/- 182.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=246000, episode_reward=276.80 +/- 211.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=247000, episode_reward=312.20 +/- 210.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=248000, episode_reward=457.40 +/- 204.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=249000, episode_reward=364.80 +/- 194.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=250000, episode_reward=489.40 +/- 152.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=251000, episode_reward=188.80 +/- 123.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=252000, episode_reward=132.40 +/- 135.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=253000, episode_reward=429.80 +/- 142.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=254000, episode_reward=336.00 +/- 159.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=255000, episode_reward=390.20 +/- 134.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=256000, episode_reward=372.00 +/- 329.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=257000, episode_reward=190.60 +/- 313.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=258000, episode_reward=207.00 +/- 162.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=259000, episode_reward=420.40 +/- 137.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=260000, episode_reward=310.40 +/- 240.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=261000, episode_reward=462.00 +/- 246.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=262000, episode_reward=330.20 +/- 134.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=263000, episode_reward=186.40 +/- 142.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=264000, episode_reward=354.00 +/- 290.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=265000, episode_reward=385.80 +/- 61.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=266000, episode_reward=404.20 +/- 183.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=267000, episode_reward=290.00 +/- 193.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=268000, episode_reward=330.20 +/- 182.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=269000, episode_reward=315.80 +/- 135.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=270000, episode_reward=407.40 +/- 265.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=271000, episode_reward=367.60 +/- 138.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=272000, episode_reward=543.80 +/- 193.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=273000, episode_reward=440.80 +/- 179.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=274000, episode_reward=468.80 +/- 297.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=275000, episode_reward=498.40 +/- 108.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=276000, episode_reward=292.40 +/- 144.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=277000, episode_reward=391.20 +/- 178.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=278000, episode_reward=303.60 +/- 288.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=279000, episode_reward=469.20 +/- 174.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=280000, episode_reward=341.60 +/- 188.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=281000, episode_reward=267.00 +/- 132.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=282000, episode_reward=287.40 +/- 178.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=283000, episode_reward=311.80 +/- 240.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=284000, episode_reward=456.20 +/- 44.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=285000, episode_reward=489.60 +/- 190.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=286000, episode_reward=267.40 +/- 180.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=287000, episode_reward=459.00 +/- 204.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=288000, episode_reward=348.40 +/- 163.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=289000, episode_reward=387.00 +/- 167.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=290000, episode_reward=94.20 +/- 88.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=291000, episode_reward=299.20 +/- 189.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=292000, episode_reward=271.80 +/- 304.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=293000, episode_reward=251.20 +/- 235.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=294000, episode_reward=349.20 +/- 96.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=295000, episode_reward=425.40 +/- 170.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=296000, episode_reward=186.80 +/- 225.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=297000, episode_reward=306.60 +/- 35.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=298000, episode_reward=321.00 +/- 167.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=299000, episode_reward=432.40 +/- 107.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=300000, episode_reward=308.00 +/- 65.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=301000, episode_reward=584.20 +/- 119.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=302000, episode_reward=373.00 +/- 145.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=303000, episode_reward=513.20 +/- 190.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=304000, episode_reward=487.00 +/- 176.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=305000, episode_reward=363.00 +/- 194.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=306000, episode_reward=300.40 +/- 220.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=307000, episode_reward=283.40 +/- 115.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=308000, episode_reward=277.20 +/- 247.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=309000, episode_reward=348.00 +/- 152.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=310000, episode_reward=321.40 +/- 140.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=311000, episode_reward=341.20 +/- 210.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=312000, episode_reward=368.20 +/- 229.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=313000, episode_reward=391.80 +/- 175.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=314000, episode_reward=302.60 +/- 163.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=315000, episode_reward=605.20 +/- 268.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=316000, episode_reward=498.60 +/- 151.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=317000, episode_reward=274.00 +/- 95.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=318000, episode_reward=355.00 +/- 153.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=319000, episode_reward=199.60 +/- 52.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=320000, episode_reward=405.20 +/- 150.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=321000, episode_reward=432.60 +/- 194.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=322000, episode_reward=125.60 +/- 115.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=323000, episode_reward=459.60 +/- 172.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=324000, episode_reward=343.00 +/- 200.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=325000, episode_reward=253.60 +/- 162.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=326000, episode_reward=528.80 +/- 161.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=327000, episode_reward=452.00 +/- 175.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=328000, episode_reward=182.20 +/- 123.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=329000, episode_reward=376.20 +/- 239.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=330000, episode_reward=377.80 +/- 180.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=331000, episode_reward=339.60 +/- 140.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=332000, episode_reward=385.60 +/- 117.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=333000, episode_reward=304.40 +/- 130.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=334000, episode_reward=259.40 +/- 71.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=335000, episode_reward=291.80 +/- 162.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=336000, episode_reward=494.80 +/- 160.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=337000, episode_reward=438.00 +/- 193.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=338000, episode_reward=213.40 +/- 267.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=339000, episode_reward=415.40 +/- 40.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=340000, episode_reward=274.80 +/- 124.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=341000, episode_reward=509.20 +/- 140.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=342000, episode_reward=3.20 +/- 251.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=343000, episode_reward=460.40 +/- 248.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=344000, episode_reward=308.40 +/- 217.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=345000, episode_reward=445.20 +/- 98.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=346000, episode_reward=368.20 +/- 37.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=347000, episode_reward=456.20 +/- 155.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=348000, episode_reward=328.00 +/- 230.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=349000, episode_reward=400.00 +/- 171.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=350000, episode_reward=291.20 +/- 200.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=351000, episode_reward=375.40 +/- 154.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=352000, episode_reward=403.60 +/- 174.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=353000, episode_reward=355.60 +/- 134.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=354000, episode_reward=370.80 +/- 239.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=355000, episode_reward=275.20 +/- 154.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=356000, episode_reward=469.20 +/- 87.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=357000, episode_reward=304.40 +/- 146.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=358000, episode_reward=457.00 +/- 252.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=359000, episode_reward=411.60 +/- 140.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=360000, episode_reward=360.00 +/- 168.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=361000, episode_reward=311.60 +/- 301.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=362000, episode_reward=176.00 +/- 279.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=363000, episode_reward=382.60 +/- 90.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=364000, episode_reward=331.00 +/- 163.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=365000, episode_reward=431.20 +/- 191.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=366000, episode_reward=326.20 +/- 111.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=367000, episode_reward=443.80 +/- 167.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=368000, episode_reward=365.20 +/- 327.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=369000, episode_reward=234.80 +/- 93.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=370000, episode_reward=433.40 +/- 101.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=371000, episode_reward=370.00 +/- 137.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=372000, episode_reward=405.20 +/- 159.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=373000, episode_reward=288.00 +/- 274.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=374000, episode_reward=385.40 +/- 202.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=375000, episode_reward=356.00 +/- 130.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=376000, episode_reward=509.60 +/- 176.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=377000, episode_reward=556.00 +/- 122.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=378000, episode_reward=519.20 +/- 175.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=379000, episode_reward=273.00 +/- 338.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=380000, episode_reward=231.00 +/- 116.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=381000, episode_reward=219.40 +/- 174.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=382000, episode_reward=195.40 +/- 163.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=383000, episode_reward=388.80 +/- 178.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=384000, episode_reward=330.00 +/- 100.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=385000, episode_reward=466.60 +/- 151.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=386000, episode_reward=355.80 +/- 118.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=387000, episode_reward=250.60 +/- 83.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=388000, episode_reward=435.40 +/- 275.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=389000, episode_reward=285.60 +/- 264.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=390000, episode_reward=322.60 +/- 296.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=391000, episode_reward=104.40 +/- 197.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=392000, episode_reward=350.80 +/- 170.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=393000, episode_reward=325.00 +/- 244.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=394000, episode_reward=294.20 +/- 128.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=395000, episode_reward=355.80 +/- 222.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=396000, episode_reward=317.40 +/- 268.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=397000, episode_reward=405.40 +/- 51.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=398000, episode_reward=303.20 +/- 195.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=399000, episode_reward=301.20 +/- 160.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=400000, episode_reward=305.80 +/- 101.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=401000, episode_reward=399.00 +/- 109.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=402000, episode_reward=307.80 +/- 242.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=403000, episode_reward=382.00 +/- 184.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=404000, episode_reward=298.20 +/- 276.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=405000, episode_reward=181.00 +/- 189.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=406000, episode_reward=380.80 +/- 134.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=407000, episode_reward=332.20 +/- 188.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=408000, episode_reward=393.00 +/- 134.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=409000, episode_reward=425.80 +/- 124.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=410000, episode_reward=444.40 +/- 174.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=411000, episode_reward=330.00 +/- 140.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=412000, episode_reward=279.00 +/- 99.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=413000, episode_reward=278.60 +/- 191.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=414000, episode_reward=288.40 +/- 81.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=415000, episode_reward=429.00 +/- 151.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=416000, episode_reward=417.00 +/- 213.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=417000, episode_reward=453.00 +/- 186.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=418000, episode_reward=224.20 +/- 158.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=419000, episode_reward=535.80 +/- 216.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=420000, episode_reward=269.00 +/- 212.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=421000, episode_reward=231.20 +/- 181.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=422000, episode_reward=388.40 +/- 251.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=423000, episode_reward=285.00 +/- 176.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=424000, episode_reward=307.00 +/- 265.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=425000, episode_reward=334.00 +/- 116.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=426000, episode_reward=365.80 +/- 90.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=427000, episode_reward=389.00 +/- 153.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=428000, episode_reward=223.00 +/- 243.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=429000, episode_reward=439.00 +/- 243.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=430000, episode_reward=374.60 +/- 198.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=431000, episode_reward=401.20 +/- 124.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=432000, episode_reward=273.20 +/- 366.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=433000, episode_reward=487.80 +/- 148.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=434000, episode_reward=373.80 +/- 116.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=435000, episode_reward=392.00 +/- 160.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=436000, episode_reward=271.00 +/- 44.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=437000, episode_reward=306.20 +/- 108.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=438000, episode_reward=486.40 +/- 154.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=439000, episode_reward=337.00 +/- 222.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=440000, episode_reward=384.20 +/- 155.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=441000, episode_reward=366.40 +/- 103.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=442000, episode_reward=200.40 +/- 158.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=443000, episode_reward=411.40 +/- 116.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=444000, episode_reward=408.40 +/- 329.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=445000, episode_reward=362.20 +/- 288.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=446000, episode_reward=318.80 +/- 195.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=447000, episode_reward=480.40 +/- 233.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=448000, episode_reward=507.00 +/- 79.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=449000, episode_reward=323.80 +/- 94.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=450000, episode_reward=473.80 +/- 175.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=451000, episode_reward=73.40 +/- 209.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=452000, episode_reward=285.80 +/- 272.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=453000, episode_reward=355.00 +/- 120.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=454000, episode_reward=161.00 +/- 108.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=455000, episode_reward=379.80 +/- 313.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=456000, episode_reward=312.20 +/- 207.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=457000, episode_reward=411.20 +/- 160.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=458000, episode_reward=318.20 +/- 132.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=459000, episode_reward=547.00 +/- 116.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=460000, episode_reward=292.60 +/- 234.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=461000, episode_reward=315.20 +/- 71.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=462000, episode_reward=386.80 +/- 84.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=463000, episode_reward=284.40 +/- 127.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=464000, episode_reward=425.00 +/- 100.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=465000, episode_reward=389.40 +/- 110.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=466000, episode_reward=410.60 +/- 196.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=467000, episode_reward=553.80 +/- 105.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=468000, episode_reward=216.00 +/- 221.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=469000, episode_reward=322.60 +/- 137.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=470000, episode_reward=351.00 +/- 223.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=471000, episode_reward=182.40 +/- 181.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=472000, episode_reward=330.00 +/- 117.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=473000, episode_reward=249.80 +/- 107.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=474000, episode_reward=241.20 +/- 200.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=475000, episode_reward=305.60 +/- 119.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=476000, episode_reward=382.60 +/- 248.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=477000, episode_reward=287.80 +/- 161.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=478000, episode_reward=299.80 +/- 88.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=479000, episode_reward=356.00 +/- 194.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=480000, episode_reward=297.60 +/- 176.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=481000, episode_reward=326.20 +/- 210.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=482000, episode_reward=382.00 +/- 73.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=483000, episode_reward=276.80 +/- 190.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=484000, episode_reward=384.00 +/- 62.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=485000, episode_reward=455.80 +/- 213.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=486000, episode_reward=379.40 +/- 158.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=487000, episode_reward=300.60 +/- 265.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=488000, episode_reward=438.60 +/- 216.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=489000, episode_reward=376.60 +/- 142.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=490000, episode_reward=260.20 +/- 218.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=491000, episode_reward=281.60 +/- 192.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=492000, episode_reward=371.40 +/- 82.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=493000, episode_reward=349.80 +/- 186.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=494000, episode_reward=358.80 +/- 89.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=495000, episode_reward=294.00 +/- 233.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=496000, episode_reward=470.60 +/- 88.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=497000, episode_reward=461.00 +/- 99.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=498000, episode_reward=149.80 +/- 222.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=499000, episode_reward=440.40 +/- 148.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=500000, episode_reward=562.20 +/- 95.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=501000, episode_reward=398.00 +/- 109.86\n",
      "Episode length: 100.00 +/- 0.00\n"
     ]
    }
   ],
   "source": [
    "### REINFORCEMENT LEARNING I ###\n",
    "### TRAIN, SAVE, EVALUATE MODEL - HYPERPARAMETER TUNING, NOISE LEVELS ###\n",
    "\n",
    "import gym\n",
    "import stable_baselines3 as sb\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "import pickle\n",
    "import torch as th\n",
    "\n",
    "total_timesteps = 5e5\n",
    "for gamma in [0.8, 0.9, 0.99]: #[0.8, 0.9, 0.99]: # Default PPO = 0.99 / DQN = 0.99 / A2C = 0.99\n",
    "        for learningrate in [0.003, 0.0003, 0.00003]: #[0.003, 0.0003, 0.00003]: # Default PPO = 0.0003 / DQN = 0.0001 / A2C = 0.0007\n",
    "            for actfn in [th.nn.Tanh, th.nn.ReLU]: #[th.nn.Tanh, th.nn.ReLU]: # Default PPO = Tanh / DQN = ReLU / A2C = Tanh\n",
    "                # The ``net_arch`` parameter allows to specify the amount and size of the hidden layers and how many\n",
    "                # of them are shared between the policy network and the value network. It is assumed to be a list with the following\n",
    "                # structure:\n",
    "                # 1. An arbitrary length (zero allowed) number of integers each specifying the number of units in a shared layer.\n",
    "                #    If the number of ints is zero, there will be no shared layers.\n",
    "                # 2. An optional dict, to specify the following non-shared layers for the value network and the policy network.\n",
    "                #    It is formatted like ``dict(vf=[<value layer sizes>], pi=[<policy layer sizes>])``.\n",
    "                #    If it is missing any of the keys (pi or vf), no non-shared layers (empty list) is assumed.\n",
    "                # For example to construct a network with one shared layer of size 55 followed by two non-shared layers for the value\n",
    "                # network of size 255 and a single non-shared layer of size 128 for the policy network, the following layers_spec\n",
    "                # would be used: ``[55, dict(vf=[255, 255], pi=[128])]``. A simple shared network topology with two layers of size 128\n",
    "                # would be specified as [128, 128].\n",
    "                for neurons in [8, 32, 64]: #[8, 32, 64]: # Default PPO = net_arch = [dict(vf=[64, 64], pi=[64, 64])] / DQN = net_arch = [64, 64] / A2C = net_arch = [dict(vf=[64, 64], pi=[64, 64])]\n",
    "                    for pn in [1]: #[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]: # Default = 1\n",
    "                        for mn in [0]: #[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]: # Default = 0\n",
    "                            print('Gamma = ' + str(gamma) + ', Learning Rate = ' + str(learningrate) + ', Activation Function = ' + str(actfn.__name__) +\n",
    "                            ', Neurons = ' + str(neurons) + ', Process noise = ' + str(0.1*pn) + ', Measurement noise = ' + str(0.1*mn))\n",
    "                            diag_model = pickle.load(open('diagnostics/model_' + 'pn' + str(pn) + '_mn' + str(mn), 'rb'))\n",
    "                            env = gym.make('Production-v0', diag_model = diag_model, process_noise = 0.1*pn, prod_levels = 5, forecast = 0)\n",
    "                            # Callback for best model\n",
    "                            best_callback = EvalCallback(env, best_model_save_path='./REV1/callback/A2C_fc0_l5_g' + str(gamma) + '_lr' + str(learningrate) + '_act' + str(actfn.__name__) + '_nn' + str(neurons) + '_pn' + str(pn) + '_mn' + str(mn),\n",
    "                                                                        log_path='./REV1/callback/A2C_fc0_l5_g' + str(gamma) + '_lr' + str(learningrate) + '_act' + str(actfn.__name__) + '_nn' + str(neurons) + '_pn' + str(pn) + '_mn' + str(mn),\n",
    "                                                    eval_freq=1000, deterministic=True, render=False)\n",
    "                            model = sb.A2C('MlpPolicy', env, gamma = gamma, learning_rate = learningrate, policy_kwargs = dict(activation_fn=actfn, net_arch=[dict(pi=[neurons, neurons], vf=[neurons, neurons])]), tensorboard_log=\"./REV1/tensorboard/\")\n",
    "                            #model = sb.PPO('MlpPolicy', env, gamma = gamma, learning_rate = learningrate, tensorboard_log=\"./REV1/tensorboard/\")\n",
    "                            model.learn(total_timesteps=total_timesteps, tb_log_name='A2C_fc0_l5_g' + str(gamma) + '_lr' + str(learningrate) + '_act' + str(actfn.__name__) + '_nn' + str(neurons) + '_pn' + str(pn) + '_mn' + str(mn),\n",
    "                                        callback = best_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gamma = 0.9, Learning Rate = 0.003, Activation Function = Tanh, Neurons = 8, Process noise = 0.1, Measurement noise = 0.0\n",
      "The average number of reactive maintenance interventions per episode is:  0.91\n",
      "The average number of preventive maintenance interventions per episode is:  6.64\n",
      "The average sum of inventory per episode is:  194.08\n",
      "The average sum of spare parts inventory per episode is:  7.61\n",
      "The average reward per episode is:  1666.9\n",
      "The average upper bound per episode is:  3005.5\n"
     ]
    }
   ],
   "source": [
    "### REINFORCEMENT LEARNING II ###\n",
    "### TRY AND EVALUATE MY MODEL ###\n",
    "import pandas as pd\n",
    "from stable_baselines3 import PPO, DQN\n",
    "import gym\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "import torch as th\n",
    "\n",
    "for gamma in [0.9]: #[0.8, 0.9, 0.99]: # Default = 0.99\n",
    "        for learningrate in [0.003]: #[0.003, 0.0003, 0.00003]: # Default = 3e-4 = 0.0003\n",
    "            for actfn in [th.nn.Tanh]: #[th.nn.Tanh, th.nn.ReLU]: # Default = Tanh\n",
    "                for neurons in [8]: #[8, 32, 64]: # Default = 64\n",
    "                    for pn in [1]: #[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]: # Default = 1\n",
    "                        for mn in [0]: #[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]: # Default = 0\n",
    "                            print('Gamma = ' + str(gamma) + ', Learning Rate = ' + str(learningrate) + ', Activation Function = ' + str(actfn.__name__) +\n",
    "                                                ', Neurons = ' + str(neurons) + ', Process noise = ' + str(0.1*pn) + ', Measurement noise = ' + str(0.1*mn))\n",
    "                            diag_model = pickle.load(open('diagnostics/model_' + 'pn' + str(pn) + '_mn' + str(mn), 'rb'))\n",
    "                            env = gym.make('Production-v0', diag_model = diag_model, process_noise = 0.1*pn, prod_levels = 5, forecast = 0)\n",
    "                            model = PPO.load('./REV1/callback/PPO_fc0_l5_g' + str(gamma) + '_lr' + str(learningrate) + '_act' + str(actfn.__name__) + '_nn' + str(neurons) + '_pn' + str(pn) + '_mn' + str(mn) + '/best_model', env = env)\n",
    "                            \n",
    "                            # Set iterations\n",
    "                            iterations = 100\n",
    "                            # Initilaize Reward\n",
    "                            result_df = pd.DataFrame(np.nan, index=range(0,iterations), columns=['RM', 'PM', 'Inventory', 'Spare Parts Inventory', 'Reward', 'Upper'])\n",
    "                            # Calculate reward with no costs and fulfillment of all orders\n",
    "                            \n",
    "                            for i in range(iterations):\n",
    "                                # Initialize episode\n",
    "                                store = []\n",
    "                                obs = env.reset()\n",
    "                                done = False\n",
    "                                store.append([0, obs[0], env.breakdown, obs[1], obs[2], 0, done, env.old_order])\n",
    "                                # Compute one episode\n",
    "                                while not done:\n",
    "                                    # Get best action for state\n",
    "                                    action, _state = model.predict(obs, deterministic=True)\n",
    "                                    # Compute next state\n",
    "                                    #action = random.choice([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
    "                                    obs, reward, done, info = env.step(action)\n",
    "                                    # Store results of this episode\n",
    "                                    store.append([action, obs[0], env.breakdown, obs[1], obs[2], reward, done, env.old_order])\n",
    "                                eps_df = pd.DataFrame(store, columns=['action', 'health_rul', 'breakdown', 'inventory', 'sp_inventory', 'reward', 'done', 'old_order'])\n",
    "                                # Calculate nr. of reactive maintenance interventions by counting health 'resets' and substracting PM actions\n",
    "                                result_df.iloc[i]['RM'] = sum(eps_df['breakdown']==True)\n",
    "                                # Calculate nr. of preventive maintenance interventions\n",
    "                                result_df.iloc[i]['PM'] = sum(eps_df['action']== env.actions-1)\n",
    "                                # Calculate inventory\n",
    "                                result_df.iloc[i]['Inventory'] = sum(eps_df['inventory'])\n",
    "                                # Calculate spare parts inventory per period\n",
    "                                result_df.iloc[i]['Spare Parts Inventory'] = sum(eps_df['sp_inventory'])\n",
    "                                # Calculate reward\n",
    "                                result_df.iloc[i]['Reward'] = sum(eps_df['reward'])\n",
    "                                # Calculate reward with no costs and fulfillment of all orders\n",
    "                                result_df.iloc[i]['Upper'] = sum(eps_df['old_order']) * env.order_r\n",
    "\n",
    "                            print(\"The average number of reactive maintenance interventions per episode is: \", result_df['RM'].mean())\n",
    "                            print(\"The average number of preventive maintenance interventions per episode is: \", result_df['PM'].mean())\n",
    "                            print(\"The average sum of inventory per episode is: \", result_df['Inventory'].mean())\n",
    "                            print(\"The average sum of spare parts inventory per episode is: \", result_df['Spare Parts Inventory'].mean())\n",
    "                            print(\"The average reward per episode is: \", result_df['Reward'].mean())\n",
    "                            print(\"The average upper bound per episode is: \", result_df['Upper'].mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x900 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize Exponential Degradation\n",
    "import gym\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import random\n",
    "store = []\n",
    "diag_model = pickle.load(open('diagnostics/model_' + 'pn' + str(0) + '_mn' + str(0), 'rb'))\n",
    "env = gym.make('Production-v0', diag_model = diag_model, prod_levels = 5)\n",
    "obs = env.reset()\n",
    "done = False\n",
    "store.append([1])\n",
    "#while not done:\n",
    "for i in range(100):\n",
    "    # Compute next state\n",
    "    #action = random.choice([0, 1, 2, 3, 4])\n",
    "    obs, reward, done, info = env.step(4)\n",
    "    # Store results of this episode\n",
    "    store.append([env.true_health])\n",
    "eps_df = pd.DataFrame(store, columns=['health'])\n",
    "eps_df.plot(y='health')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x900 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "eps_df.plot(y='health_rul')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average number of reactive maintenance interventions per episode is:  5.92\n",
      "The average number of preventive maintenance interventions per episode is:  0.0\n",
      "The average mean time between failure per episode is:  14.887095238095235\n",
      "The standard deviation of the mean time between failure per episode is:  1.5405320836510183\n",
      "The average sum of inventory per episode is:  114.34\n",
      "The average sum of spare parts inventory per episode is:  0.0\n",
      "The average reward per episode is:  317.37\n",
      "The average upper bound per episode is:  2991.5\n"
     ]
    }
   ],
   "source": [
    "### REINFORCEMENT LEARNING III ###\n",
    "### EVALUATE REACTIVE MODEL ###\n",
    "\n",
    "import pandas as pd\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3 import PPO\n",
    "import numpy as np\n",
    "import math\n",
    "import gym\n",
    "\n",
    "env = gym.make('Production-v0', reactive_mode = True, forecast = 0, prod_levels = 5)\n",
    "# Initialize Reward\n",
    "# Set iterations\n",
    "iterations = 100\n",
    "result_df = pd.DataFrame(np.nan, index=range(0,iterations), columns=['RM', 'PM', 'MTBF', 'Inventory', 'Spare Parts Inventory', 'Reward', 'Upper'])\n",
    "\n",
    "for i in range(iterations):\n",
    "    # Initialize episode\n",
    "    store = []\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    store.append([0, env.true_health, env.breakdown, obs[0], obs[1], 0, done, env.old_order])\n",
    "    # Compute one episode\n",
    "    while not done:\n",
    "        # Get best action for state\n",
    "        action = min(env.prod_levels-1, max(0, round(env.old_order)))\n",
    "        # Compute next state\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        # Store results of this episode\n",
    "        store.append([action, env.true_health, env.breakdown, obs[0], obs[1], reward, done, env.old_order])\n",
    "    eps_df = pd.DataFrame(store, columns=['action', 'health', 'breakdown', 'inventory', 'sp_inventory', 'reward', 'done', 'old_order'])\n",
    "    # Calculate nr. of reactive maintenance interventions by counting health 'resets' and substracting PM actions\n",
    "    result_df.iloc[i]['RM'] = sum(eps_df['breakdown']==True)\n",
    "    # Calculate nr. of preventive maintenance interventions\n",
    "    result_df.iloc[i]['PM'] = sum(eps_df['action']== env.actions-1)\n",
    "    # Calculate mean time between failures\n",
    "    # Cut df after last breakdown\n",
    "    eps_df_trim = eps_df.iloc[:(np.where(eps_df['breakdown'].eq(True), eps_df.index, 0).max()+1)]\n",
    "    # Calculate MTBF by dividing periods where machine is running / breakdowns\n",
    "    result_df.iloc[i]['MTBF'] = (len(eps_df_trim) - sum(eps_df_trim['breakdown'] == True)) / sum(eps_df_trim['breakdown'] == True)\n",
    "    # Calculate inventory\n",
    "    result_df.iloc[i]['Inventory'] = sum(eps_df['inventory'])\n",
    "    # Calculate spare parts inventory per period\n",
    "    result_df.iloc[i]['Spare Parts Inventory'] = sum(eps_df['sp_inventory'])\n",
    "    # Calculate reward\n",
    "    result_df.iloc[i]['Reward'] = sum(eps_df['reward'])\n",
    "    # Calculate reward with no costs and fulfillment of all orders\n",
    "    result_df.iloc[i]['Upper'] = sum(eps_df['old_order']) * env.order_r\n",
    "\n",
    "print(\"The average number of reactive maintenance interventions per episode is: \", result_df['RM'].mean())\n",
    "print(\"The average number of preventive maintenance interventions per episode is: \", result_df['PM'].mean())\n",
    "print(\"The average mean time between failure per episode is: \", result_df['MTBF'].mean())\n",
    "print(\"The standard deviation of the mean time between failure per episode is: \", result_df['MTBF'].std())\n",
    "print(\"The average sum of inventory per episode is: \", result_df['Inventory'].mean())\n",
    "print(\"The average sum of spare parts inventory per episode is: \", result_df['Spare Parts Inventory'].mean())\n",
    "print(\"The average reward per episode is: \", result_df['Reward'].mean())\n",
    "print(\"The average upper bound per episode is: \", result_df['Upper'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Maintenance Interval:  11 Coefficient:  0.55\n",
      "The average number of reactive maintenance interventions per episode is:  1.65\n",
      "The average number of preventive maintenance interventions per episode is:  6.57\n",
      "The average sum of inventory per episode is:  156.97\n",
      "The average sum of spare parts inventory per episode is:  6.63\n",
      "The average reward per episode is:  1221.5\n",
      "The average upper bound per episode is:  3010.1\n"
     ]
    }
   ],
   "source": [
    "### REINFORCEMENT LEARNING IV ###\n",
    "### EVALUATE TIME-BASED PREVENTIVE MODEL ###\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gym\n",
    "import math\n",
    "\n",
    "env = gym.make('Production-v0', forecast = 0, prod_levels = 6)\n",
    "interval = range (11, 12)\n",
    "# Set iterations\n",
    "iterations = 100\n",
    "\n",
    "for k in interval:\n",
    "    # Initilaize Reward\n",
    "    result_df = pd.DataFrame(np.nan, index=range(0,100), columns=['RM', 'PM', 'Inventory', 'Spare Parts Inventory', 'Reward', 'Upper'])\n",
    "\n",
    "    for i in range(iterations):\n",
    "        # Initialize episode\n",
    "        store = []\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        store.append([0, obs[0], env.breakdown, obs[1], obs[2], 0, done, env.old_order])\n",
    "        # Compute one episode\n",
    "        while not done:\n",
    "            # One period before maintenance: action = order + spare part order\n",
    "            if env.scheduled_maintenance_counter == k-1:\n",
    "                action = min(env.prod_levels-1, max(0, round(env.old_order))) + env.prod_levels\n",
    "            # At period of mtbf: maintain\n",
    "            elif env.scheduled_maintenance_counter == k:\n",
    "                action = env.actions-1\n",
    "            # Else: action = order    \n",
    "            else:             \n",
    "                action = min(env.prod_levels-1, max(0, round(env.old_order)))\n",
    "            # Compute next state\n",
    "            obs, reward, done, info = env.step(action)\n",
    "            # Store results of this episode\n",
    "            store.append([action, obs[0], env.breakdown, obs[1], obs[2], reward, done, env.old_order])\n",
    "        eps_df = pd.DataFrame(store, columns=['action', 'health', 'breakdown', 'inventory', 'sp_inventory', 'reward', 'done', 'old_order'])\n",
    "        # Calculate nr. of reactive maintenance interventions by counting health 'resets' and substracting PM actions\n",
    "        result_df.iloc[i]['RM'] = sum(eps_df['breakdown']==True)\n",
    "        # Calculate nr. of preventive maintenance interventions\n",
    "        result_df.iloc[i]['PM'] = sum(eps_df['action']==env.actions-1)\n",
    "        # Calculate inventory\n",
    "        result_df.iloc[i]['Inventory'] = sum(eps_df['inventory'])\n",
    "        # Calculate spare parts inventory per period\n",
    "        result_df.iloc[i]['Spare Parts Inventory'] = sum(eps_df['sp_inventory'])\n",
    "        # Calculate reward\n",
    "        result_df.iloc[i]['Reward'] = sum(eps_df['reward'])\n",
    "        # Calculate reward with no costs and fulfillment of all orders\n",
    "        result_df.iloc[i]['Upper'] = sum(eps_df['old_order']) * env.order_r\n",
    "\n",
    "    print(\"\\n\", \"Maintenance Interval: \", k, \"Coefficient: \", 0+0.05*k)\n",
    "    print(\"The average number of reactive maintenance interventions per episode is: \", result_df['RM'].mean())\n",
    "    print(\"The average number of preventive maintenance interventions per episode is: \", result_df['PM'].mean())\n",
    "    print(\"The average sum of inventory per episode is: \", result_df['Inventory'].mean())\n",
    "    print(\"The average sum of spare parts inventory per episode is: \", result_df['Spare Parts Inventory'].mean())\n",
    "    print(\"The average reward per episode is: \", result_df['Reward'].mean())\n",
    "    print(\"The average upper bound per episode is: \", result_df['Upper'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "### REINFORCEMENT LEARNING Va ###\n",
    "### VISUALIZE STATE-ACTION ###\n",
    "import numpy as np\n",
    "state_action = []\n",
    "\n",
    "# Define observation grid\n",
    "grid_health = np.arange(0.0, 1.01, 0.01)\n",
    "grid_order = range(0, 5)\n",
    "grid_inventory = range(0, 10)\n",
    "grid_sp_inventory = [0, 1]\n",
    "\n",
    "# Loop through grid and store best action for each state\n",
    "for hlt in grid_health:\n",
    "    for ord in grid_order:\n",
    "        for inv in grid_inventory:\n",
    "            for sin in grid_sp_inventory:\n",
    "                # Predict\n",
    "                action, _state = model.predict((hlt, ord, inv, sin), deterministic=True)\n",
    "                state_action.append([hlt, ord, inv, sin, action])\n",
    "\n",
    "state_action_df = pd.DataFrame(state_action, columns=['health', 'order', 'inventory', 'sp_inventory', 'action'])\n",
    "state_action_df.to_excel(\"visuals/state_action.xlsx\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### REINFORCEMENT LEARNING Vb ###\n",
    "### VISUALIZE STATE-VALUE ###\n",
    "import numpy as np\n",
    "state_action = []\n",
    "\n",
    "# Define observation grid\n",
    "grid_health = np.arange(0.0, 1.01, 0.01)\n",
    "grid_order = range(0, 5)\n",
    "grid_inventory = range(0, 10)\n",
    "grid_sp_inventory = [0, 1]\n",
    "\n",
    "# Loop through grid and store best action for each state\n",
    "for hlt in grid_health:\n",
    "    for ord in grid_order:\n",
    "        for inv in grid_inventory:\n",
    "            for sin in grid_sp_inventory:\n",
    "                # Predict\n",
    "                obs, _ = model.policy.obs_to_tensor(hlt, ord, inv, sin)\n",
    "                value = model.policy.predict_values(obs).item()\n",
    "                state_action.append([hlt, ord, inv, sin, value])\n",
    "\n",
    "state_action_df = pd.DataFrame(state_action, columns=['health', 'order', 'inventory', 'sp_inventory', 'value'])\n",
    "state_action_df.to_excel(\"visuals/state_value.xlsx\") \n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6f69c5940b32a5cbabe45c9825076a627c6cdb9ede58cf4d0fa74ca6057ffe74"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
