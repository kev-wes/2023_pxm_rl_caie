{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process noise = 0.0, Measurement noise = 0.0\n",
      "Process noise = 0.1, Measurement noise = 0.0\n",
      "Process noise = 0.2, Measurement noise = 0.0\n",
      "Process noise = 0.30000000000000004, Measurement noise = 0.0\n",
      "Process noise = 0.4, Measurement noise = 0.0\n",
      "Process noise = 0.5, Measurement noise = 0.0\n",
      "Process noise = 0.6000000000000001, Measurement noise = 0.0\n",
      "Process noise = 0.7000000000000001, Measurement noise = 0.0\n",
      "Process noise = 0.8, Measurement noise = 0.0\n",
      "Process noise = 0.9, Measurement noise = 0.0\n",
      "Process noise = 1.0, Measurement noise = 0.0\n"
     ]
    }
   ],
   "source": [
    "### DATA-DRIVEN DIAGNOSTICS I ###\n",
    "### GENERATE DATA FOR DATA-DRIVEN MODEL ###\n",
    "\n",
    "import random\n",
    "from prog_models.models import BatteryCircuit\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "# Ignore warnings when machine exceeds its end of life\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\"\"\" Method that uses a physical machine model from the prog_models package and a current (health) state of the model and\n",
    "an action (i.e., intensity), which is performed for 100 time steps\n",
    "    Parameter:\n",
    "        machine             machine model from the prog_models package\n",
    "        state               current (health) state of the model\n",
    "        action              loading of the machine for the next 100 time steps\n",
    "    Return:\n",
    "        health                               \n",
    "    \"\"\"\n",
    "def produce_model(machine, states, action):\n",
    "        \n",
    "        # Define load of battery\n",
    "        def future_loading(t, x=None):\n",
    "            return {'i': action}\n",
    "\n",
    "        # Set current state of machine\n",
    "        machine.parameters['x0'] = states\n",
    "        # Simulate 100 steps\n",
    "        options = {\n",
    "            'save_freq': 100,  # Frequency at which results are saved\n",
    "            'dt': 2  # Timestep\n",
    "        }\n",
    "        (_, _, states, outputs, event_states) = machine.simulate_to(100, future_loading, **options)\n",
    "        health = event_states[-1]['EOD']\n",
    "        return(round(health, 2), states[-1], outputs[-1]['t'], outputs[-1]['v'])\n",
    "def reset_states(machine):\n",
    "    # Returns initial states of machine, e.g., {'tb': 18.95, 'qb': 7856.3254, 'qcp': 0, 'qcs': 0} for Battery\n",
    "    return(machine.default_parameters['x0'])\n",
    "   \n",
    "for pn in [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]: \n",
    "    for mn in [0]:\n",
    "        print('Process noise = ' + str(0.1*pn) + ', Measurement noise = ' + str(0.1*mn))\n",
    "        battery = BatteryCircuit(process_noise = 0.1*pn, measurement_noise = 0.1*mn)\n",
    "        states = reset_states(battery)\n",
    "        reset_counter = 0\n",
    "        dataset = []\n",
    "        for i in range(int(1e4)):\n",
    "            # If asset failed last period, reset all historical values\n",
    "            if reset_counter == 0: t = v = t_1 = v_1 = t_2 = v_2 = t_3 = v_3 = 0 \n",
    "            # Shift history by one time period\n",
    "            v_3 = v_2\n",
    "            t_3 = t_2\n",
    "            v_2 = v_1\n",
    "            t_2 = t_1\n",
    "            v_1 = v\n",
    "            t_1 = t\n",
    "\n",
    "            # Increment reset_counter\n",
    "            reset_counter = reset_counter + 1\n",
    "            # Compute new health, states, t, and v using last battery state and a random new action\n",
    "            health, states, t, v = produce_model(machine=battery, states=states, action=random.sample((0, 1, 2, 3, 4), 1)[0])\n",
    "            \n",
    "            if health <= 0: \n",
    "                # Reset battery states to initialize battery for next produce_model call\n",
    "                states = reset_states(battery)\n",
    "                # Initialize reset_counter\n",
    "                reset_counter = 0\n",
    "                # Sometimes produce_model returns weird or negative values as the end of life is exceeded\n",
    "                # Here, we just simply set it to zero to not confuse a later learner \n",
    "                health = 0\n",
    "\n",
    "            # append to two-dimensional list\n",
    "            dataset.append([t, v, t_1, v_1, t_2, v_2, t_3, v_3, health])\n",
    "\n",
    "        # Transform two-dim list to dataframe\n",
    "        dataset = pd.DataFrame(dataset, columns=['t', 'v', 't_1', 'v_1', 't_2', 'v_2', 't_3', 'v_3', 'health'])\n",
    "        # Denote machine runs to-failure with incrementing id\n",
    "        k = 0\n",
    "        for i in range(dataset.shape[0]):\n",
    "            if (dataset.iloc[i]['t_1'] == 0 and dataset.iloc[i]['v_1'] == 0):\n",
    "                j = 1\n",
    "                k = k + 1\n",
    "            dataset.loc[i, 'time'] = j\n",
    "            j = j + 1\n",
    "            dataset.loc[i, 'ID'] = k\n",
    "        dataset = dataset.sort_values(['ID', 'time'], ascending=[True, False])\n",
    "        # Assign RUL by counting upwards per serial number in the descended data frame\n",
    "        dataset['RUL'] = dataset.groupby((dataset['ID'] != dataset['ID'].shift(1)).cumsum()).cumcount()\n",
    "        dataset = dataset.sort_values(['ID', 'time'], ascending=True)\n",
    "\n",
    "        # Save it as pickle\n",
    "        dataset.to_pickle('diagnostics/data_' + 'pn' + str(pn) + '_mn' + str(mn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model based on RUL (prognostics = True) or health (diagnostics, prognostics = False)\n",
    "prognostics = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process noise = 0.0, Measurement noise = 0.0\n",
      "R2: [0.98346924 0.97869884 0.98271119 0.98084096 0.98029931]\n",
      "mae: [-0.0186088  -0.02097155 -0.01949765 -0.0201391  -0.0208858 ]\n",
      "mape: [-7.86658635e-02 -3.23583633e+11 -9.00719925e+09 -9.20286148e-02\n",
      " -1.71587146e+11]\n",
      "rmse: [-0.03669848 -0.04135376 -0.03768967 -0.03960828 -0.04014499]\n",
      "Process noise = 0.1, Measurement noise = 0.0\n",
      "R2: [0.94511725 0.94338909 0.94308702 0.95269309 0.92781427]\n",
      "mae: [-0.04355035 -0.0430732  -0.0434886  -0.0409953  -0.04892035]\n",
      "mape: [-1.57625987e+09 -6.75539944e+09 -2.31935381e+10 -3.03992975e+10\n",
      " -3.69295169e+11]\n",
      "rmse: [-0.06585241 -0.06812947 -0.06765209 -0.06181947 -0.07649472]\n",
      "Process noise = 0.2, Measurement noise = 0.0\n",
      "R2: [0.89160295 0.90719555 0.90106071 0.89986382 0.8983238 ]\n",
      "mae: [-0.06726765 -0.0624331  -0.0637716  -0.0658622  -0.0643069 ]\n",
      "mape: [-3.90912448e+11 -3.67493730e+11 -1.03605309e+12 -7.60657977e+11\n",
      " -1.41413028e+11]\n",
      "rmse: [-0.09406841 -0.08643758 -0.08901846 -0.09017457 -0.0908437 ]\n",
      "Process noise = 0.30000000000000004, Measurement noise = 0.0\n",
      "R2: [0.87037827 0.84902257 0.84585148 0.85446821 0.85283189]\n",
      "mae: [-0.07610885 -0.0816426  -0.0808236  -0.0798498  -0.0796984 ]\n",
      "mape: [-8.21906932e+10 -2.44320280e+11 -3.61413870e+11 -2.58281439e+11\n",
      " -2.56705179e+10]\n",
      "rmse: [-0.10279177 -0.11007041 -0.11067439 -0.10907084 -0.10878756]\n",
      "Process noise = 0.4, Measurement noise = 0.0\n",
      "R2: [0.83456832 0.83012227 0.80835482 0.83903277 0.81974217]\n",
      "mae: [-0.0846392  -0.0875222  -0.09059015 -0.08526315 -0.090239  ]\n",
      "mape: [-1.02456892e+11 -4.00820367e+11 -2.39816680e+11 -2.95661316e+11\n",
      " -6.73288144e+10]\n",
      "rmse: [-0.11565337 -0.11667704 -0.12263368 -0.11445298 -0.12049927]\n",
      "Process noise = 0.5, Measurement noise = 0.0\n",
      "R2: [0.82839126 0.80480565 0.82500873 0.8179684  0.817267  ]\n",
      "mae: [-0.09095045 -0.096345   -0.0892272  -0.09109585 -0.09089715]\n",
      "mape: [-4.76480841e+11 -2.50850499e+11 -2.85978576e+11 -7.34086739e+10\n",
      " -4.02171447e+11]\n",
      "rmse: [-0.11899326 -0.12706747 -0.11847659 -0.12179301 -0.12175318]\n",
      "Process noise = 0.6000000000000001, Measurement noise = 0.0\n",
      "R2: [0.78807567 0.79536683 0.79253112 0.78549367 0.7681572 ]\n",
      "mae: [-0.09885245 -0.09946715 -0.09709435 -0.1021844  -0.1052449 ]\n",
      "mape: [-3.47452711e+11 -4.54638382e+11 -8.87209127e+11 -2.08516663e+11\n",
      " -3.28762773e+11]\n",
      "rmse: [-0.13075345 -0.13061024 -0.12871161 -0.13304715 -0.13793793]\n",
      "Process noise = 0.7000000000000001, Measurement noise = 0.0\n",
      "R2: [0.77073703 0.76979583 0.75657293 0.74444239 0.7908058 ]\n",
      "mae: [-0.1031984  -0.1025792  -0.10822995 -0.11202985 -0.09902865]\n",
      "mape: [-2.76746197e+11 -4.12529726e+11 -1.64156206e+11 -5.88395291e+11\n",
      " -3.12099454e+11]\n",
      "rmse: [-0.13438963 -0.1351357  -0.14146253 -0.14412065 -0.1310333 ]\n",
      "Process noise = 0.8, Measurement noise = 0.0\n",
      "R2: [0.75441041 0.73836646 0.72956504 0.7436836  0.74272255]\n",
      "mae: [-0.1075912  -0.1102882  -0.11299515 -0.10926115 -0.1102889 ]\n",
      "mape: [-3.29888673e+11 -1.32315757e+12 -3.39571412e+11 -9.10177485e+11\n",
      " -5.11158558e+11]\n",
      "rmse: [-0.14059851 -0.14411492 -0.14701622 -0.1434903  -0.14296051]\n",
      "Process noise = 0.9, Measurement noise = 0.0\n",
      "R2: [0.7459057  0.72225387 0.71386912 0.73079549 0.71229312]\n",
      "mae: [-0.1088923  -0.115251   -0.11461225 -0.1123348  -0.1146464 ]\n",
      "mape: [-2.86203756e+11 -1.43439648e+11 -3.62990130e+11 -8.26410532e+10\n",
      " -6.39285967e+11]\n",
      "rmse: [-0.14232725 -0.14946078 -0.14982241 -0.14679103 -0.14974282]\n",
      "Process noise = 1.0, Measurement noise = 0.0\n",
      "R2: [0.70404433 0.7015158  0.69136757 0.70469365 0.71885872]\n",
      "mae: [-0.1154981  -0.1165566  -0.11575255 -0.11541465 -0.11638255]\n",
      "mape: [-1.49744688e+11 -5.21967197e+11 -5.96051411e+11 -2.15925084e+12\n",
      " -3.37769972e+11]\n",
      "rmse: [-0.15095345 -0.15119491 -0.15424514 -0.15225114 -0.15056938]\n"
     ]
    }
   ],
   "source": [
    "### DATA-DRIVEN DIAGNOSTICS II ###\n",
    "### FIT AND TEST MODEL ###\n",
    "from sklearn import tree, linear_model, kernel_ridge, svm, neighbors, gaussian_process, ensemble, neural_network\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_validate\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "score_data = []\n",
    "for pn in [0, 1, 2, 3, 4, 5, 6, 7 , 8, 9, 10]: \n",
    "    for mn in [0]:\n",
    "        print('Process noise = ' + str(0.1*pn) + ', Measurement noise = ' + str(0.1*mn))\n",
    "        dataset = pd.read_pickle('diagnostics/data_' + 'pn' + str(pn) + '_mn' + str(mn))\n",
    "        X = dataset[['t', 'v', 't_1', 'v_1', 't_2', 'v_2', 't_3', 'v_3']]\n",
    "        if prognostics:\n",
    "            y = dataset['RUL']\n",
    "        else:\n",
    "            y = dataset['health']\n",
    "        learner = ensemble.RandomForestRegressor()\n",
    "        scoring = {'r2': 'r2',\n",
    "                    'mae': 'neg_mean_absolute_error',\n",
    "                    'mape': 'neg_mean_absolute_percentage_error',\n",
    "                    'rmse': 'neg_root_mean_squared_error'}\n",
    "        scores = cross_validate(learner, X, y, scoring=scoring, cv=5) # default scoring R2\n",
    "        print(\"R2: \" + str(scores['test_r2']))\n",
    "        print(\"mae: \" + str(scores['test_mae']))\n",
    "        print(\"mape: \" + str(scores['test_mape']))\n",
    "        print(\"rmse: \" + str(scores['test_rmse']))\n",
    "        # Store measures\n",
    "        score_data.append(['model_' + 'pn' + str(pn) + '_mn' + str(mn), np.mean(scores['test_r2']),\n",
    "                            np.mean(scores['test_mae']), np.mean(scores['test_mape']), np.mean(scores['test_rmse'])])\n",
    "\n",
    "        # Fit on all data\n",
    "        model = ensemble.RandomForestRegressor().fit(X, y)\n",
    "        if prognostics:\n",
    "            pickle.dump(model, open('prognostics/model_' + 'pn' + str(pn) + '_mn' + str(mn), 'wb'))\n",
    "        else:\n",
    "            pickle.dump(model, open('diagnostics/model_' + 'pn' + str(pn) + '_mn' + str(mn), 'wb'))\n",
    "# Transform two-dim list to dataframe\n",
    "score_df = pd.DataFrame(score_data, columns=['model', 'r2', 'mae', 'mape', 'rmse'])\n",
    "if prognostics:\n",
    "    score_df.to_excel(\"prognostics/scores.xlsx\") \n",
    "else:\n",
    "    score_df.to_excel(\"diagnostics/scores.xlsx\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process noise = 0.1, Measurement noise = 0.0\n"
     ]
    }
   ],
   "source": [
    "### DATA-DRIVEN DIAGNOSTICS IIIa ###\n",
    "### FIT, TEST, VISUALIZE MODEL USING TRAIN AND TEST SETS ###\n",
    "\n",
    "for pn in [1]: #[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "    for mn in [0]:\n",
    "        print('Process noise = ' + str(0+0.1*pn) + ', Measurement noise = ' + str(0+0.1*mn))\n",
    "        if prognostics:\n",
    "            dataset = pd.read_pickle('prognostics/data_' + 'pn' + str(pn) + '_mn' + str(mn))\n",
    "            y = dataset['RUL']\n",
    "        else:\n",
    "            dataset = pd.read_pickle('diagnostics/data_' + 'pn' + str(pn) + '_mn' + str(mn))\n",
    "            y = dataset['health']\n",
    "        X = dataset[['t', 'v', 't_1', 'v_1', 't_2', 'v_2', 't_3', 'v_3']]\n",
    "        # Find index of healthy machines\n",
    "        index_df = X.index[(X['t_1'] == 0) & (X['v_1'] == 0) & (X['t_2'] == 0) & (X['v_2'] == 0) & (X['t_3'] == 0) & (X['v_3'] == 0)].tolist()\n",
    "        index_test = round(len(index_df)*0.8)\n",
    "\n",
    "        # Create train and test set without disrupting machine runs to-failure\n",
    "        X_train = X.iloc[0:(index_df[index_test])]\n",
    "        y_train = y.iloc[0:(index_df[index_test])]\n",
    "        X_test = X.iloc[index_df[index_test]:(len(X))]\n",
    "        y_test = y.iloc[index_df[index_test]:(len(y))]\n",
    "\n",
    "        ## Train\n",
    "        #model = ensemble.RandomForestRegressor().fit(X_train, y_train)\n",
    "        # Load\n",
    "        model = pickle.load(open('diagnostics/model_' + 'pn' + str(pn) + '_mn' + str(mn), 'rb'))\n",
    "        ## Predict\n",
    "        y_pred = pd.DataFrame(model.predict(X_test), columns=['Pred'])\n",
    "        ## Analyze\n",
    "        #reset index of each DataFrame\n",
    "        X_test.reset_index(drop=True, inplace=True)\n",
    "        y_test.reset_index(drop=True, inplace=True)\n",
    "        # Concat dataframes\n",
    "        test_df = pd.concat([X_test, y_test, y_pred], axis=1)\n",
    "        # Print for visualization (e.g., in R)\n",
    "        if prognostics:\n",
    "            test_df.to_excel(\"prognostics/test_results_\" + 'pn' + str(pn) + '_mn' + str(mn) + \".xlsx\") \n",
    "        else:\n",
    "            test_df.to_excel(\"diagnostics/test_results_\" + 'pn' + str(pn) + '_mn' + str(mn) + \".xlsx\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### REINFORCEMENT LEARNING I ###\n",
    "### TRAIN, SAVE, EVALUATE MODEL - HYPERPARAMETER TUNING, NOISE LEVELS ###\n",
    "\n",
    "import gym\n",
    "import stable_baselines3 as sb\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "import pickle\n",
    "import torch as th\n",
    "\n",
    "total_timesteps = 1e6\n",
    "for gamma in [0.9]: #[0.8, 0.9, 0.99]: # Default PPO = 0.99 / DQN = 0.99 / A2C = 0.99\n",
    "        for learningrate in [0.003]: #[0.003, 0.0003, 0.00003]: # Default PPO = 0.0003 / DQN = 0.0001 / A2C = 0.0007\n",
    "            for actfn in [th.nn.ReLU]: #[th.nn.Tanh, th.nn.ReLU]: # Default PPO = Tanh / DQN = ReLU / A2C = Tanh\n",
    "                # The ``net_arch`` parameter allows to specify the amount and size of the hidden layers and how many\n",
    "                # of them are shared between the policy network and the value network. It is assumed to be a list with the following\n",
    "                # structure:\n",
    "                # 1. An arbitrary length (zero allowed) number of integers each specifying the number of units in a shared layer.\n",
    "                #    If the number of ints is zero, there will be no shared layers.\n",
    "                # 2. An optional dict, to specify the following non-shared layers for the value network and the policy network.\n",
    "                #    It is formatted like ``dict(vf=[<value layer sizes>], pi=[<policy layer sizes>])``.\n",
    "                #    If it is missing any of the keys (pi or vf), no non-shared layers (empty list) is assumed.\n",
    "                # For example to construct a network with one shared layer of size 55 followed by two non-shared layers for the value\n",
    "                # network of size 255 and a single non-shared layer of size 128 for the policy network, the following layers_spec\n",
    "                # would be used: ``[55, dict(vf=[255, 255], pi=[128])]``. A simple shared network topology with two layers of size 128\n",
    "                # would be specified as [128, 128].\n",
    "                for neurons in [8]: #[8, 32, 64]: # Default PPO = net_arch = [dict(vf=[64, 64], pi=[64, 64])] / DQN = net_arch = [64, 64] / A2C = net_arch = [dict(vf=[64, 64], pi=[64, 64])]\n",
    "                    for pn in [0]: #[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]: # Default = 1\n",
    "                        for mn in [0]: #[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]: # Default = 0\n",
    "                            print('Gamma = ' + str(gamma) + ', Learning Rate = ' + str(learningrate) + ', Activation Function = ' + str(actfn.__name__) +\n",
    "                            ', Neurons = ' + str(neurons) + ', Process noise = ' + str(0.1*pn) + ', Measurement noise = ' + str(0.1*mn))\n",
    "                            diag_model = pickle.load(open('diagnostics/model_' + 'pn' + str(pn) + '_mn' + str(mn), 'rb'))\n",
    "                            env = gym.make('Production-v0', diag_model = None, process_noise = 0.1*pn, prod_levels = 5, forecast = 0)\n",
    "                            # Callback for best model\n",
    "                            best_callback = EvalCallback(env, best_model_save_path='./REV1/callback/PPOTRUE_fc0_l5_g' + str(gamma) + '_lr' + str(learningrate) + '_act' + str(actfn.__name__) + '_nn' + str(neurons) + '_pn' + str(pn) + '_mn' + str(mn),\n",
    "                                                                        log_path='./REV1/callback/PPOTRUE_fc0_l5_g' + str(gamma) + '_lr' + str(learningrate) + '_act' + str(actfn.__name__) + '_nn' + str(neurons) + '_pn' + str(pn) + '_mn' + str(mn),\n",
    "                                                    eval_freq=1000, deterministic=True, render=False)\n",
    "                            model = sb.PPO('MlpPolicy', env, gamma = gamma, learning_rate = learningrate, policy_kwargs = dict(activation_fn=actfn, net_arch=[dict(vf=[neurons, neurons], pi=[neurons, neurons])]), tensorboard_log=\"./REV1/tensorboard/\")\n",
    "                            model.learn(total_timesteps=total_timesteps, tb_log_name='PPOTRUE_fc0_l5_g' + str(gamma) + '_lr' + str(learningrate) + '_act' + str(actfn.__name__) + '_nn' + str(neurons) + '_pn' + str(pn) + '_mn' + str(mn),\n",
    "                                        callback = best_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Costs = [50, 100, 2.5, 5, 5, 5, 25, 1], Gamma = 0.9, Learning Rate = 0.003, Activation Function = ReLU, Neurons = 8, Process noise = 0.1, Measurement noise = 0.0\n",
      "The average reward per episode is:  848.085\n",
      "The average upper bound per episode is:  1517.8\n",
      "The performance in percent of UB:  0.558759388588747\n",
      "Costs = [50, 100, 2.5, 5, 10, 10, 75, 2], Gamma = 0.9, Learning Rate = 0.003, Activation Function = ReLU, Neurons = 8, Process noise = 0.1, Measurement noise = 0.0\n"
     ]
    }
   ],
   "source": [
    "### REINFORCEMENT LEARNING II ###\n",
    "### TRY AND EVALUATE MY MODEL ###\n",
    "import pandas as pd\n",
    "from stable_baselines3 import PPO, DQN\n",
    "import gym\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "import torch as th\n",
    "\n",
    "eval = []\n",
    "# [maintenance_c, repair_c, backorder_c, order_r, spare_part_order_c, spare_part_holding_c, sp_emergency_order_c, holding_c]\n",
    "for costs_maintenance in [[50, 100], [100, 200], [200, 400]]: #[50, 100], [100, 200], [200, 400]\n",
    "    for costs_market in [[2.5, 5], [5, 10], [10, 20]]: #[2.5, 5], [5, 10], [10, 20]\n",
    "        for costs_logistics in [[5, 5, 25, 1], [10, 10, 75, 2], [20, 20, 125, 4]]: #[5, 5, 25, 1], [10, 10, 75, 2], [20, 20, 125, 4]\n",
    "            costs = [costs_maintenance[0], costs_maintenance[1], costs_market[0], costs_market[1], costs_logistics[0], costs_logistics[1], costs_logistics[2], costs_logistics[3]]\n",
    "            for gamma in [0.9]: #[0.8, 0.9, 0.99]: # Default = 0.99\n",
    "                    for learningrate in [0.003]: #[0.003, 0.0003, 0.00003]: # Default = 3e-4 = 0.0003\n",
    "                        for actfn in [th.nn.ReLU]: #[th.nn.Tanh, th.nn.ReLU]: # Default = Tanh\n",
    "                            for neurons in [8]: #[8, 32, 64]: # Default = 64\n",
    "                                for pn in [1]: #[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]: # Default = 1\n",
    "                                    for mn in [0]: #[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]: # Default = 0\n",
    "                                        print('Costs = ' + str(costs) + ', Gamma = ' + str(gamma) + ', Learning Rate = ' + str(learningrate) + ', Activation Function = ' + str(actfn.__name__) +\n",
    "                                                            ', Neurons = ' + str(neurons) + ', Process noise = ' + str(0.1*pn) + ', Measurement noise = ' + str(0.1*mn))\n",
    "                                        diag_model = pickle.load(open('diagnostics/model_' + 'pn' + str(pn) + '_mn' + str(mn), 'rb'))\n",
    "                                        # Set iterations\n",
    "                                        iterations = 100\n",
    "                                        # Initilaize Reward\n",
    "                                        result_df = pd.DataFrame(np.nan, index=range(0,iterations), columns=['RM', 'PM', 'Inventory', 'Spare Parts Inventory', 'Reward', 'Upper', \n",
    "                                        'Revenue', 'Stockout Cost', 'Spare Parts Order Cost', 'Emergency Order Cost', 'Inventory Cost', 'Spare Parts Inventory Cost', \n",
    "                                        'PM Cost', 'RM Cost'])\n",
    "                                        # Calculate reward with no costs and fulfillment of all orders\n",
    "                                        \n",
    "                                        for i in range(iterations):\n",
    "                                            # Initialize episode\n",
    "                                            store = []\n",
    "                                            np.random.seed(i)\n",
    "                                            random.seed(i)\n",
    "                                            env = gym.make('Production-v0', diag_model = diag_model, process_noise = 0.1*pn, prod_levels = 5, forecast = 0, costs = costs)\n",
    "                                            obs = env.reset() \n",
    "                                            env.seed(i)\n",
    "                                            model = PPO.load('./REV1/callback/PPO_fc0_l5_g' + str(gamma) + '_lr' + str(learningrate) + '_act' + str(actfn.__name__) + '_nn' + str(neurons) + '_pn' + str(pn) + '_mn' + str(mn) + '/best_model', env = env)\n",
    "\n",
    "                                            done = False\n",
    "                                            store.append([0, obs[0], env.breakdown, obs[1], obs[2], 0, done, env.old_order, 1])\n",
    "                                            # Compute one episode\n",
    "                                            while not done:\n",
    "                                                # Get best action for state\n",
    "                                                action, _state = model.predict(obs, deterministic=True)\n",
    "                                                # Compute next state\n",
    "                                                #action = random.choice([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
    "                                                obs, reward, done, info = env.step(action)\n",
    "                                                # Store results of this episode\n",
    "                                                store.append([action, obs[0], env.breakdown, obs[1], obs[2], reward, done, env.old_order, env.true_health])\n",
    "                                            eps_df = pd.DataFrame(store, columns=['action', 'health_rul', 'breakdown', 'inventory', 'sp_inventory', 'reward', 'done', 'old_order', 'true_health'])\n",
    "                                            # Calculate nr. of reactive maintenance interventions by counting health 'resets' and substracting PM actions\n",
    "                                            result_df.iloc[i]['RM'] = sum(eps_df['breakdown']==True)\n",
    "                                            # Calculate nr. of preventive maintenance interventions\n",
    "                                            result_df.iloc[i]['PM'] = sum(eps_df['action']== env.actions-1)\n",
    "                                            # Calculate inventory\n",
    "                                            result_df.iloc[i]['Inventory'] = sum(eps_df['inventory'])\n",
    "                                            # Calculate spare parts inventory per period\n",
    "                                            result_df.iloc[i]['Spare Parts Inventory'] = sum(eps_df['sp_inventory'])\n",
    "                                            # Calculate reward\n",
    "                                            result_df.iloc[i]['Reward'] = sum(eps_df['reward'])\n",
    "                                            # Calculate reward with no costs and fulfillment of all orders\n",
    "                                            result_df.iloc[i]['Upper'] = sum(eps_df['old_order']) * env.order_r\n",
    "\n",
    "                                            # Total costs\n",
    "                                            result_df.iloc[i]['Revenue'] = env.total_revenue\n",
    "                                            result_df.iloc[i]['Stockout Cost'] = env.total_stockout_cost\n",
    "                                            result_df.iloc[i]['Spare Parts Order Cost'] = env.total_sp_order_cost\n",
    "                                            result_df.iloc[i]['Emergency Order Cost'] = env.total_emergency_order_cost\n",
    "                                            result_df.iloc[i]['Inventory Cost'] = env.total_fginv_cost\n",
    "                                            result_df.iloc[i]['Spare Parts Inventory Cost'] = env.total_spinv_cost\n",
    "                                            result_df.iloc[i]['PM Cost'] = env.total_pm_cost\n",
    "                                            result_df.iloc[i]['RM Cost'] = env.total_rm_cost\n",
    "                                        \n",
    "                                        \n",
    "                                        eval.append([('Costs = ' + str(costs) + ', Gamma = ' + str(gamma) + ', Learning Rate = ' + str(learningrate) + ', Activation Function = ' + str(actfn.__name__) +\n",
    "                                                            ', Neurons = ' + str(neurons) + ', Process noise = ' + str(0.1*pn) + ', Measurement noise = ' + str(0.1*mn)),\n",
    "                                                        result_df['RM'].mean(), result_df['PM'].mean(), result_df['Inventory'].mean(),result_df['Spare Parts Inventory'].mean(),\n",
    "                                                        result_df['Reward'].mean(), result_df['Upper'].mean(), result_df.iloc[i]['Revenue'].mean(),\n",
    "                                                        result_df.iloc[i]['Stockout Cost'].mean(), result_df.iloc[i]['Spare Parts Order Cost'].mean(),\n",
    "                                                        result_df.iloc[i]['Emergency Order Cost'].mean(), result_df.iloc[i]['Inventory Cost'].mean(),\n",
    "                                                        result_df.iloc[i]['Spare Parts Inventory Cost'].mean(), result_df.iloc[i]['PM Cost'].mean(), result_df.iloc[i]['RM Cost'].mean()])\n",
    "\n",
    "                                        print(\"The average reward per episode is: \", result_df['Reward'].mean())\n",
    "                                        print(\"The average upper bound per episode is: \", result_df['Upper'].mean())\n",
    "                                        print(\"The performance in percent of UB: \", result_df['Reward'].mean()/result_df['Upper'].mean())\n",
    "eval_df = pd.DataFrame(eval, columns=['Config', 'RM', 'PM', 'Inventory', 'Spare Parts Inventory', 'Reward', 'Upper', 'Revenue', 'Stockout Cost', 'Spare Parts Order Cost',\n",
    "                                        'Emergency Order Cost', 'Inventory Cost', 'Spare Parts Inventory Cost', 'PM Cost', 'RM Cost'])\n",
    "eval_df.to_excel(\"./Rev1/visuals/rl_evaluation.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Costs = [50, 100, 2.5, 5, 5, 5, 25, 1]\n",
      "The average mean time between failure per episode is:  15.833333333333334\n",
      "The standard deviation of the mean time between failure per episode is:  nan\n",
      "The average reward per episode is:  258.0\n",
      "The average upper bound per episode is:  1505.0\n",
      "Costs = [50, 100, 2.5, 5, 10, 10, 75, 2]\n",
      "The average mean time between failure per episode is:  15.833333333333334\n",
      "The standard deviation of the mean time between failure per episode is:  nan\n",
      "The average reward per episode is:  -149.0\n",
      "The average upper bound per episode is:  1505.0\n",
      "Costs = [50, 100, 2.5, 5, 20, 20, 125, 4]\n",
      "The average mean time between failure per episode is:  15.833333333333334\n",
      "The standard deviation of the mean time between failure per episode is:  nan\n",
      "The average reward per episode is:  -663.0\n",
      "The average upper bound per episode is:  1505.0\n",
      "Costs = [50, 100, 5, 10, 5, 5, 25, 1]\n",
      "The average mean time between failure per episode is:  15.833333333333334\n",
      "The standard deviation of the mean time between failure per episode is:  nan\n",
      "The average reward per episode is:  1373.0\n",
      "The average upper bound per episode is:  3010.0\n",
      "Costs = [50, 100, 5, 10, 10, 10, 75, 2]\n",
      "The average mean time between failure per episode is:  15.833333333333334\n",
      "The standard deviation of the mean time between failure per episode is:  nan\n",
      "The average reward per episode is:  966.0\n",
      "The average upper bound per episode is:  3010.0\n",
      "Costs = [50, 100, 5, 10, 20, 20, 125, 4]\n",
      "The average mean time between failure per episode is:  15.833333333333334\n",
      "The standard deviation of the mean time between failure per episode is:  nan\n",
      "The average reward per episode is:  452.0\n",
      "The average upper bound per episode is:  3010.0\n",
      "Costs = [50, 100, 10, 20, 5, 5, 25, 1]\n",
      "The average mean time between failure per episode is:  15.833333333333334\n",
      "The standard deviation of the mean time between failure per episode is:  nan\n",
      "The average reward per episode is:  3603.0\n",
      "The average upper bound per episode is:  6020.0\n",
      "Costs = [50, 100, 10, 20, 10, 10, 75, 2]\n",
      "The average mean time between failure per episode is:  15.833333333333334\n",
      "The standard deviation of the mean time between failure per episode is:  nan\n",
      "The average reward per episode is:  3196.0\n",
      "The average upper bound per episode is:  6020.0\n",
      "Costs = [50, 100, 10, 20, 20, 20, 125, 4]\n",
      "The average mean time between failure per episode is:  15.833333333333334\n",
      "The standard deviation of the mean time between failure per episode is:  nan\n",
      "The average reward per episode is:  2682.0\n",
      "The average upper bound per episode is:  6020.0\n",
      "Costs = [100, 200, 2.5, 5, 5, 5, 25, 1]\n",
      "The average mean time between failure per episode is:  15.833333333333334\n",
      "The standard deviation of the mean time between failure per episode is:  nan\n",
      "The average reward per episode is:  -342.0\n",
      "The average upper bound per episode is:  1505.0\n",
      "Costs = [100, 200, 2.5, 5, 10, 10, 75, 2]\n",
      "The average mean time between failure per episode is:  15.833333333333334\n",
      "The standard deviation of the mean time between failure per episode is:  nan\n",
      "The average reward per episode is:  -749.0\n",
      "The average upper bound per episode is:  1505.0\n",
      "Costs = [100, 200, 2.5, 5, 20, 20, 125, 4]\n",
      "The average mean time between failure per episode is:  15.833333333333334\n",
      "The standard deviation of the mean time between failure per episode is:  nan\n",
      "The average reward per episode is:  -1263.0\n",
      "The average upper bound per episode is:  1505.0\n",
      "Costs = [100, 200, 5, 10, 5, 5, 25, 1]\n",
      "The average mean time between failure per episode is:  15.833333333333334\n",
      "The standard deviation of the mean time between failure per episode is:  nan\n",
      "The average reward per episode is:  773.0\n",
      "The average upper bound per episode is:  3010.0\n",
      "Costs = [100, 200, 5, 10, 10, 10, 75, 2]\n",
      "The average mean time between failure per episode is:  15.833333333333334\n",
      "The standard deviation of the mean time between failure per episode is:  nan\n",
      "The average reward per episode is:  366.0\n",
      "The average upper bound per episode is:  3010.0\n",
      "Costs = [100, 200, 5, 10, 20, 20, 125, 4]\n",
      "The average mean time between failure per episode is:  15.833333333333334\n",
      "The standard deviation of the mean time between failure per episode is:  nan\n",
      "The average reward per episode is:  -148.0\n",
      "The average upper bound per episode is:  3010.0\n",
      "Costs = [100, 200, 10, 20, 5, 5, 25, 1]\n",
      "The average mean time between failure per episode is:  15.833333333333334\n",
      "The standard deviation of the mean time between failure per episode is:  nan\n",
      "The average reward per episode is:  3003.0\n",
      "The average upper bound per episode is:  6020.0\n",
      "Costs = [100, 200, 10, 20, 10, 10, 75, 2]\n",
      "The average mean time between failure per episode is:  15.833333333333334\n",
      "The standard deviation of the mean time between failure per episode is:  nan\n",
      "The average reward per episode is:  2596.0\n",
      "The average upper bound per episode is:  6020.0\n",
      "Costs = [100, 200, 10, 20, 20, 20, 125, 4]\n",
      "The average mean time between failure per episode is:  15.833333333333334\n",
      "The standard deviation of the mean time between failure per episode is:  nan\n",
      "The average reward per episode is:  2082.0\n",
      "The average upper bound per episode is:  6020.0\n",
      "Costs = [200, 400, 2.5, 5, 5, 5, 25, 1]\n",
      "The average mean time between failure per episode is:  15.833333333333334\n",
      "The standard deviation of the mean time between failure per episode is:  nan\n",
      "The average reward per episode is:  -1542.0\n",
      "The average upper bound per episode is:  1505.0\n",
      "Costs = [200, 400, 2.5, 5, 10, 10, 75, 2]\n",
      "The average mean time between failure per episode is:  15.833333333333334\n",
      "The standard deviation of the mean time between failure per episode is:  nan\n",
      "The average reward per episode is:  -1949.0\n",
      "The average upper bound per episode is:  1505.0\n",
      "Costs = [200, 400, 2.5, 5, 20, 20, 125, 4]\n",
      "The average mean time between failure per episode is:  15.833333333333334\n",
      "The standard deviation of the mean time between failure per episode is:  nan\n",
      "The average reward per episode is:  -2463.0\n",
      "The average upper bound per episode is:  1505.0\n",
      "Costs = [200, 400, 5, 10, 5, 5, 25, 1]\n",
      "The average mean time between failure per episode is:  15.833333333333334\n",
      "The standard deviation of the mean time between failure per episode is:  nan\n",
      "The average reward per episode is:  -427.0\n",
      "The average upper bound per episode is:  3010.0\n",
      "Costs = [200, 400, 5, 10, 10, 10, 75, 2]\n",
      "The average mean time between failure per episode is:  15.833333333333334\n",
      "The standard deviation of the mean time between failure per episode is:  nan\n",
      "The average reward per episode is:  -834.0\n",
      "The average upper bound per episode is:  3010.0\n",
      "Costs = [200, 400, 5, 10, 20, 20, 125, 4]\n",
      "The average mean time between failure per episode is:  15.833333333333334\n",
      "The standard deviation of the mean time between failure per episode is:  nan\n",
      "The average reward per episode is:  -1348.0\n",
      "The average upper bound per episode is:  3010.0\n",
      "Costs = [200, 400, 10, 20, 5, 5, 25, 1]\n",
      "The average mean time between failure per episode is:  15.833333333333334\n",
      "The standard deviation of the mean time between failure per episode is:  nan\n",
      "The average reward per episode is:  1803.0\n",
      "The average upper bound per episode is:  6020.0\n",
      "Costs = [200, 400, 10, 20, 10, 10, 75, 2]\n",
      "The average mean time between failure per episode is:  15.833333333333334\n",
      "The standard deviation of the mean time between failure per episode is:  nan\n",
      "The average reward per episode is:  1396.0\n",
      "The average upper bound per episode is:  6020.0\n",
      "Costs = [200, 400, 10, 20, 20, 20, 125, 4]\n",
      "The average mean time between failure per episode is:  15.833333333333334\n",
      "The standard deviation of the mean time between failure per episode is:  nan\n",
      "The average reward per episode is:  882.0\n",
      "The average upper bound per episode is:  6020.0\n"
     ]
    }
   ],
   "source": [
    "### REINFORCEMENT LEARNING III ###\n",
    "### EVALUATE REACTIVE MODEL ###\n",
    "\n",
    "import pandas as pd\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3 import PPO\n",
    "import numpy as np\n",
    "import random\n",
    "import gym\n",
    "\n",
    "eval = []\n",
    "# [maintenance_c, repair_c, backorder_c, order_r, spare_part_order_c, spare_part_holding_c, sp_emergency_order_c, holding_c]\n",
    "for costs_maintenance in [[50, 100], [100, 200], [200, 400]]: #[50, 100], [100, 200], [200, 400]\n",
    "    for costs_market in [[2.5, 5], [5, 10], [10, 20]]: #[2.5, 5], [5, 10], [10, 20]\n",
    "        for costs_logistics in [[5, 5, 25, 1], [10, 10, 75, 2], [20, 20, 125, 4]]: #[5, 5, 25, 1], [10, 10, 75, 2], [20, 20, 125, 4]\n",
    "            costs = [costs_maintenance[0], costs_maintenance[1], costs_market[0], costs_market[1], costs_logistics[0], costs_logistics[1], costs_logistics[2], costs_logistics[3]]\n",
    "            print('Costs = ' + str(costs))\n",
    "            # Set iterations\n",
    "            iterations = 100\n",
    "            result_df = pd.DataFrame(np.nan, index=range(0,iterations), columns=['RM', 'PM', 'MTBF', 'Inventory', 'Spare Parts Inventory', 'Reward', 'Upper', \n",
    "                                        'Revenue', 'Stockout Cost', 'Spare Parts Order Cost', 'Emergency Order Cost', 'Inventory Cost', 'Spare Parts Inventory Cost', \n",
    "                                        'PM Cost', 'RM Cost'])\n",
    "            for i in range(iterations):\n",
    "                # Initialize episode\n",
    "                store = []\n",
    "                random.seed(i)\n",
    "                np.random.seed(i)\n",
    "                env = gym.make('Production-v0', reactive_mode = True, forecast = 0, prod_levels = 5, costs = costs)\n",
    "                obs = env.reset() \n",
    "                env.seed(i)\n",
    "                done = False\n",
    "                store.append([0, env.true_health, env.breakdown, obs[0], obs[1], 0, done, env.old_order])\n",
    "                # Compute one episode\n",
    "                while not done:\n",
    "                    # Get best action for state\n",
    "                    action = min(env.prod_levels-1, max(0, round(env.old_order)))\n",
    "                    # Compute next state\n",
    "                    obs, reward, done, info = env.step(action)\n",
    "                    # Store results of this episode\n",
    "                    store.append([action, env.true_health, env.breakdown, obs[0], obs[1], reward, done, env.old_order])\n",
    "                eps_df = pd.DataFrame(store, columns=['action', 'health', 'breakdown', 'inventory', 'sp_inventory', 'reward', 'done', 'old_order'])\n",
    "                # Calculate nr. of reactive maintenance interventions by counting health 'resets' and substracting PM actions\n",
    "                result_df.iloc[i]['RM'] = sum(eps_df['breakdown']==True)\n",
    "                # Calculate nr. of preventive maintenance interventions\n",
    "                result_df.iloc[i]['PM'] = sum(eps_df['action']== env.actions-1)\n",
    "                # Calculate mean time between failures\n",
    "                # Cut df after last breakdown\n",
    "                eps_df_trim = eps_df.iloc[:(np.where(eps_df['breakdown'].eq(True), eps_df.index, 0).max()+1)]\n",
    "                # Calculate MTBF by dividing periods where machine is running / breakdowns\n",
    "                result_df.iloc[i]['MTBF'] = (len(eps_df_trim) - sum(eps_df_trim['breakdown'] == True)) / sum(eps_df_trim['breakdown'] == True)\n",
    "                # Calculate inventory\n",
    "                result_df.iloc[i]['Inventory'] = sum(eps_df['inventory'])\n",
    "                # Calculate spare parts inventory per period\n",
    "                result_df.iloc[i]['Spare Parts Inventory'] = sum(eps_df['sp_inventory'])\n",
    "                # Calculate reward\n",
    "                result_df.iloc[i]['Reward'] = sum(eps_df['reward'])\n",
    "                # Calculate reward with no costs and fulfillment of all orders\n",
    "                result_df.iloc[i]['Upper'] = sum(eps_df['old_order']) * env.order_r\n",
    "\n",
    "                # Total costs\n",
    "                result_df.iloc[i]['Revenue'] = env.total_revenue\n",
    "                result_df.iloc[i]['Stockout Cost'] = env.total_stockout_cost\n",
    "                result_df.iloc[i]['Spare Parts Order Cost'] = env.total_sp_order_cost\n",
    "                result_df.iloc[i]['Emergency Order Cost'] = env.total_emergency_order_cost\n",
    "                result_df.iloc[i]['Inventory Cost'] = env.total_fginv_cost\n",
    "                result_df.iloc[i]['Spare Parts Inventory Cost'] = env.total_spinv_cost\n",
    "                result_df.iloc[i]['PM Cost'] = env.total_pm_cost\n",
    "                result_df.iloc[i]['RM Cost'] = env.total_rm_cost\n",
    "\n",
    "            eval.append([('Costs = ' + str(costs)),\n",
    "                        result_df['RM'].mean(), result_df['PM'].mean(), result_df['Inventory'].mean(),result_df['Spare Parts Inventory'].mean(),\n",
    "                        result_df['Reward'].mean(), result_df['Upper'].mean(), result_df.iloc[i]['Revenue'].mean(),\n",
    "                        result_df.iloc[i]['Stockout Cost'].mean(), result_df.iloc[i]['Spare Parts Order Cost'].mean(),\n",
    "                        result_df.iloc[i]['Emergency Order Cost'].mean(), result_df.iloc[i]['Inventory Cost'].mean(),\n",
    "                        result_df.iloc[i]['Spare Parts Inventory Cost'].mean(), result_df.iloc[i]['PM Cost'].mean(), result_df.iloc[i]['RM Cost'].mean()])\n",
    "\n",
    "\n",
    "            print(\"The average mean time between failure per episode is: \", result_df['MTBF'].mean())\n",
    "            print(\"The standard deviation of the mean time between failure per episode is: \", result_df['MTBF'].std())\n",
    "            print(\"The average reward per episode is: \", result_df['Reward'].mean())\n",
    "            print(\"The average upper bound per episode is: \", result_df['Upper'].mean())\n",
    "\n",
    "eval_df = pd.DataFrame(eval, columns=['Config', 'RM', 'PM', 'Inventory', 'Spare Parts Inventory', 'Reward', 'Upper', 'Revenue', 'Stockout Cost', 'Spare Parts Order Cost',\n",
    "                                        'Emergency Order Cost', 'Inventory Cost', 'Spare Parts Inventory Cost', 'PM Cost', 'RM Cost'])\n",
    "eval_df.to_excel(\"./Rev1/visuals/rm_evaluation.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Costs = [50, 100, 2.5, 5, 5, 5, 25, 1]\n",
      "\n",
      " Maintenance Interval:  11 Coefficient:  0.55\n",
      "The average reward per episode is:  758.0\n",
      "The average upper bound per episode is:  1505.0\n",
      "Costs = [50, 100, 2.5, 5, 10, 10, 75, 2]\n",
      "\n",
      " Maintenance Interval:  11 Coefficient:  0.55\n",
      "The average reward per episode is:  574.0\n",
      "The average upper bound per episode is:  1505.0\n",
      "Costs = [50, 100, 2.5, 5, 20, 20, 125, 4]\n",
      "\n",
      " Maintenance Interval:  11 Coefficient:  0.55\n",
      "The average reward per episode is:  206.0\n",
      "The average upper bound per episode is:  1505.0\n",
      "Costs = [50, 100, 5, 10, 5, 5, 25, 1]\n",
      "\n",
      " Maintenance Interval:  11 Coefficient:  0.55\n",
      "The average reward per episode is:  1873.0\n",
      "The average upper bound per episode is:  3010.0\n",
      "Costs = [50, 100, 5, 10, 10, 10, 75, 2]\n",
      "\n",
      " Maintenance Interval:  11 Coefficient:  0.55\n",
      "The average reward per episode is:  1689.0\n",
      "The average upper bound per episode is:  3010.0\n",
      "Costs = [50, 100, 5, 10, 20, 20, 125, 4]\n",
      "\n",
      " Maintenance Interval:  11 Coefficient:  0.55\n",
      "The average reward per episode is:  1321.0\n",
      "The average upper bound per episode is:  3010.0\n",
      "Costs = [50, 100, 10, 20, 5, 5, 25, 1]\n",
      "\n",
      " Maintenance Interval:  11 Coefficient:  0.55\n",
      "The average reward per episode is:  4103.0\n",
      "The average upper bound per episode is:  6020.0\n",
      "Costs = [50, 100, 10, 20, 10, 10, 75, 2]\n",
      "\n",
      " Maintenance Interval:  11 Coefficient:  0.55\n",
      "The average reward per episode is:  3919.0\n",
      "The average upper bound per episode is:  6020.0\n",
      "Costs = [50, 100, 10, 20, 20, 20, 125, 4]\n",
      "\n",
      " Maintenance Interval:  11 Coefficient:  0.55\n",
      "The average reward per episode is:  3551.0\n",
      "The average upper bound per episode is:  6020.0\n",
      "Costs = [100, 200, 2.5, 5, 5, 5, 25, 1]\n",
      "\n",
      " Maintenance Interval:  11 Coefficient:  0.55\n",
      "The average reward per episode is:  585.0\n",
      "The average upper bound per episode is:  1505.0\n",
      "Costs = [100, 200, 2.5, 5, 10, 10, 75, 2]\n",
      "\n",
      " Maintenance Interval:  11 Coefficient:  0.55\n",
      "The average reward per episode is:  401.0\n",
      "The average upper bound per episode is:  1505.0\n",
      "Costs = [100, 200, 2.5, 5, 20, 20, 125, 4]\n",
      "\n",
      " Maintenance Interval:  11 Coefficient:  0.55\n",
      "The average reward per episode is:  33.0\n",
      "The average upper bound per episode is:  1505.0\n",
      "Costs = [100, 200, 5, 10, 5, 5, 25, 1]\n",
      "\n",
      " Maintenance Interval:  11 Coefficient:  0.55\n",
      "The average reward per episode is:  1700.0\n",
      "The average upper bound per episode is:  3010.0\n",
      "Costs = [100, 200, 5, 10, 10, 10, 75, 2]\n",
      "\n",
      " Maintenance Interval:  11 Coefficient:  0.55\n",
      "The average reward per episode is:  1516.0\n",
      "The average upper bound per episode is:  3010.0\n",
      "Costs = [100, 200, 5, 10, 20, 20, 125, 4]\n",
      "\n",
      " Maintenance Interval:  11 Coefficient:  0.55\n",
      "The average reward per episode is:  1148.0\n",
      "The average upper bound per episode is:  3010.0\n",
      "Costs = [100, 200, 10, 20, 5, 5, 25, 1]\n",
      "\n",
      " Maintenance Interval:  11 Coefficient:  0.55\n",
      "The average reward per episode is:  3930.0\n",
      "The average upper bound per episode is:  6020.0\n",
      "Costs = [100, 200, 10, 20, 10, 10, 75, 2]\n",
      "\n",
      " Maintenance Interval:  11 Coefficient:  0.55\n",
      "The average reward per episode is:  3746.0\n",
      "The average upper bound per episode is:  6020.0\n",
      "Costs = [100, 200, 10, 20, 20, 20, 125, 4]\n",
      "\n",
      " Maintenance Interval:  11 Coefficient:  0.55\n",
      "The average reward per episode is:  3378.0\n",
      "The average upper bound per episode is:  6020.0\n",
      "Costs = [200, 400, 2.5, 5, 5, 5, 25, 1]\n",
      "\n",
      " Maintenance Interval:  11 Coefficient:  0.55\n",
      "The average reward per episode is:  239.0\n",
      "The average upper bound per episode is:  1505.0\n",
      "Costs = [200, 400, 2.5, 5, 10, 10, 75, 2]\n",
      "\n",
      " Maintenance Interval:  11 Coefficient:  0.55\n",
      "The average reward per episode is:  55.0\n",
      "The average upper bound per episode is:  1505.0\n",
      "Costs = [200, 400, 2.5, 5, 20, 20, 125, 4]\n",
      "\n",
      " Maintenance Interval:  11 Coefficient:  0.55\n",
      "The average reward per episode is:  -313.0\n",
      "The average upper bound per episode is:  1505.0\n",
      "Costs = [200, 400, 5, 10, 5, 5, 25, 1]\n",
      "\n",
      " Maintenance Interval:  11 Coefficient:  0.55\n",
      "The average reward per episode is:  1354.0\n",
      "The average upper bound per episode is:  3010.0\n",
      "Costs = [200, 400, 5, 10, 10, 10, 75, 2]\n",
      "\n",
      " Maintenance Interval:  11 Coefficient:  0.55\n",
      "The average reward per episode is:  1170.0\n",
      "The average upper bound per episode is:  3010.0\n",
      "Costs = [200, 400, 5, 10, 20, 20, 125, 4]\n",
      "\n",
      " Maintenance Interval:  11 Coefficient:  0.55\n",
      "The average reward per episode is:  802.0\n",
      "The average upper bound per episode is:  3010.0\n",
      "Costs = [200, 400, 10, 20, 5, 5, 25, 1]\n",
      "\n",
      " Maintenance Interval:  11 Coefficient:  0.55\n",
      "The average reward per episode is:  3584.0\n",
      "The average upper bound per episode is:  6020.0\n",
      "Costs = [200, 400, 10, 20, 10, 10, 75, 2]\n",
      "\n",
      " Maintenance Interval:  11 Coefficient:  0.55\n",
      "The average reward per episode is:  3400.0\n",
      "The average upper bound per episode is:  6020.0\n",
      "Costs = [200, 400, 10, 20, 20, 20, 125, 4]\n",
      "\n",
      " Maintenance Interval:  11 Coefficient:  0.55\n",
      "The average reward per episode is:  3032.0\n",
      "The average upper bound per episode is:  6020.0\n"
     ]
    }
   ],
   "source": [
    "### REINFORCEMENT LEARNING IV ###\n",
    "### EVALUATE TIME-BASED PREVENTIVE MODEL ###\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "\n",
    "\n",
    "interval = range (11, 12)\n",
    "# Set iterations\n",
    "iterations = 100\n",
    "eval = []\n",
    "# [maintenance_c, repair_c, backorder_c, order_r, spare_part_order_c, spare_part_holding_c, sp_emergency_order_c, holding_c]\n",
    "for costs_maintenance in [[50, 100], [100, 200], [200, 400]]: #[50, 100], [100, 200], [200, 400]\n",
    "    for costs_market in [[2.5, 5], [5, 10], [10, 20]]: #[2.5, 5], [5, 10], [10, 20]\n",
    "        for costs_logistics in [[5, 5, 25, 1], [10, 10, 75, 2], [20, 20, 125, 4]]: #[5, 5, 25, 1], [10, 10, 75, 2], [20, 20, 125, 4]\n",
    "            costs = [costs_maintenance[0], costs_maintenance[1], costs_market[0], costs_market[1], costs_logistics[0], costs_logistics[1], costs_logistics[2], costs_logistics[3]]\n",
    "            print('Costs = ' + str(costs))\n",
    "            for k in interval:\n",
    "                # Initilaize Reward\n",
    "                result_df = pd.DataFrame(np.nan, index=range(0,iterations), columns=['RM', 'PM', 'Inventory', 'Spare Parts Inventory', 'Reward', 'Upper', \n",
    "                                        'Revenue', 'Stockout Cost', 'Spare Parts Order Cost', 'Emergency Order Cost', 'Inventory Cost', 'Spare Parts Inventory Cost', \n",
    "                                        'PM Cost', 'RM Cost'])\n",
    "\n",
    "                for i in range(iterations):\n",
    "                    # Initialize episode\n",
    "                    store = []\n",
    "                    random.seed(i)\n",
    "                    np.random.seed(i)\n",
    "                    env = gym.make('Production-v0', forecast = 0, prod_levels = 5, costs = costs)\n",
    "                    obs = env.reset()\n",
    "                    env.seed(i)\n",
    "                    done = False\n",
    "                    store.append([0, obs[0], env.breakdown, obs[1], obs[2], 0, done, env.old_order])\n",
    "                    # Compute one episode\n",
    "                    while not done:\n",
    "                        # One period before maintenance: action = order + spare part order\n",
    "                        if env.scheduled_maintenance_counter == k-1:\n",
    "                            action = min(env.prod_levels-1, max(0, round(env.old_order))) + env.prod_levels\n",
    "                        # At period of mtbf: maintain\n",
    "                        elif env.scheduled_maintenance_counter == k:\n",
    "                            action = env.actions-1\n",
    "                        # Else: action = order    \n",
    "                        else:             \n",
    "                            action = min(env.prod_levels-1, max(0, round(env.old_order)))\n",
    "                        # Compute next state\n",
    "                        obs, reward, done, info = env.step(action)\n",
    "                        # Store results of this episode\n",
    "                        store.append([action, obs[0], env.breakdown, obs[1], obs[2], reward, done, env.old_order])\n",
    "                    eps_df = pd.DataFrame(store, columns=['action', 'health', 'breakdown', 'inventory', 'sp_inventory', 'reward', 'done', 'old_order'])\n",
    "                    # Calculate nr. of reactive maintenance interventions by counting health 'resets' and substracting PM actions\n",
    "                    result_df.iloc[i]['RM'] = sum(eps_df['breakdown']==True)\n",
    "                    # Calculate nr. of preventive maintenance interventions\n",
    "                    result_df.iloc[i]['PM'] = sum(eps_df['action']==env.actions-1)\n",
    "                    # Calculate inventory\n",
    "                    result_df.iloc[i]['Inventory'] = sum(eps_df['inventory'])\n",
    "                    # Calculate spare parts inventory per period\n",
    "                    result_df.iloc[i]['Spare Parts Inventory'] = sum(eps_df['sp_inventory'])\n",
    "                    # Calculate reward\n",
    "                    result_df.iloc[i]['Reward'] = sum(eps_df['reward'])\n",
    "                    # Calculate reward with no costs and fulfillment of all orders\n",
    "                    result_df.iloc[i]['Upper'] = sum(eps_df['old_order']) * env.order_r\n",
    "\n",
    "                    # Total costs\n",
    "                    result_df.iloc[i]['Revenue'] = env.total_revenue\n",
    "                    result_df.iloc[i]['Stockout Cost'] = env.total_stockout_cost\n",
    "                    result_df.iloc[i]['Spare Parts Order Cost'] = env.total_sp_order_cost\n",
    "                    result_df.iloc[i]['Emergency Order Cost'] = env.total_emergency_order_cost\n",
    "                    result_df.iloc[i]['Inventory Cost'] = env.total_fginv_cost\n",
    "                    result_df.iloc[i]['Spare Parts Inventory Cost'] = env.total_spinv_cost\n",
    "                    result_df.iloc[i]['PM Cost'] = env.total_pm_cost\n",
    "                    result_df.iloc[i]['RM Cost'] = env.total_rm_cost\n",
    "\n",
    "                eval.append([('Costs = ' + str(costs) + ', Maintenance Interval = ' + str(k)),\n",
    "                            result_df['RM'].mean(), result_df['PM'].mean(), result_df['Inventory'].mean(),result_df['Spare Parts Inventory'].mean(),\n",
    "                            result_df['Reward'].mean(), result_df['Upper'].mean(), result_df.iloc[i]['Revenue'].mean(),\n",
    "                            result_df.iloc[i]['Stockout Cost'].mean(), result_df.iloc[i]['Spare Parts Order Cost'].mean(),\n",
    "                            result_df.iloc[i]['Emergency Order Cost'].mean(), result_df.iloc[i]['Inventory Cost'].mean(),\n",
    "                            result_df.iloc[i]['Spare Parts Inventory Cost'].mean(), result_df.iloc[i]['PM Cost'].mean(), result_df.iloc[i]['RM Cost'].mean()])\n",
    "\n",
    "                print(\"\\n\", \"Maintenance Interval: \", k, \"Coefficient: \", 0+0.05*k)\n",
    "                print(\"The average reward per episode is: \", result_df['Reward'].mean())\n",
    "                print(\"The average upper bound per episode is: \", result_df['Upper'].mean())\n",
    "\n",
    "eval_df = pd.DataFrame(eval, columns=['Config', 'RM', 'PM', 'Inventory', 'Spare Parts Inventory', 'Reward', 'Upper', 'Revenue', 'Stockout Cost', 'Spare Parts Order Cost',\n",
    "                                        'Emergency Order Cost', 'Inventory Cost', 'Spare Parts Inventory Cost', 'PM Cost', 'RM Cost'])\n",
    "eval_df.to_excel(\"./Rev1/visuals/pm_evaluation.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### REINFORCEMENT LEARNING Va ###\n",
    "### VISUALIZE STATE-ACTION ###\n",
    "import numpy as np\n",
    "state_action = []\n",
    "\n",
    "# Define observation grid\n",
    "grid_health = np.arange(0.0, 1.01, 0.01)\n",
    "#grid_order = range(0, 5)\n",
    "grid_inventory = range(0, 10)\n",
    "grid_sp_inventory = [0, 1]\n",
    "\n",
    "# Loop through grid and store best action for each state\n",
    "for hlt in grid_health:\n",
    "    #for ord in grid_order:\n",
    "    for inv in grid_inventory:\n",
    "        for sin in grid_sp_inventory:\n",
    "            # Predict\n",
    "            #action, _state = model.predict((hlt, ord, inv, sin), deterministic=True)\n",
    "            #state_action.append([hlt, ord, inv, sin, action])\n",
    "            action, _state = model.predict((hlt, inv, sin), deterministic=True)\n",
    "            state_action.append([hlt, inv, sin, action])\n",
    "\n",
    "#state_action_df = pd.DataFrame(state_action, columns=['health', 'order', 'inventory', 'sp_inventory', 'action'])\n",
    "state_action_df = pd.DataFrame(state_action, columns=['health', 'inventory', 'sp_inventory', 'action'])\n",
    "state_action_df.to_excel(\"./Rev1/visuals/state_action.xlsx\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "### REINFORCEMENT LEARNING Vb ###\n",
    "### VISUALIZE STATE-VALUE ###\n",
    "import numpy as np\n",
    "state_value = []\n",
    "\n",
    "# Define observation grid\n",
    "grid_health = np.arange(0.0, 1.01, 0.01)\n",
    "#grid_order = range(0, 5)\n",
    "grid_inventory = range(0, 10)\n",
    "grid_sp_inventory = [0, 1]\n",
    "\n",
    "# Loop through grid and store best action for each state\n",
    "for hlt in grid_health:\n",
    "    #for ord in grid_order:\n",
    "    for inv in grid_inventory:\n",
    "        for sin in grid_sp_inventory:\n",
    "            # Predict\n",
    "            #obs, _ = model.policy.obs_to_tensor(hlt, ord, inv, sin)\n",
    "            obs, _ = model.policy.obs_to_tensor(observation=(hlt, inv, sin))\n",
    "            value = model.policy.predict_values(obs).item()\n",
    "            #state_value.append([hlt, ord, inv, sin, value])\n",
    "            state_value.append([hlt, inv, sin, value])\n",
    "\n",
    "#state_value_df = pd.DataFrame(state_value, columns=['health', 'order', 'inventory', 'sp_inventory', 'value'])\n",
    "state_value_df = pd.DataFrame(state_value, columns=['health', 'inventory', 'sp_inventory', 'value'])\n",
    "state_value_df.to_excel(\"./Rev1/visuals/state_value.xlsx\") \n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6f69c5940b32a5cbabe45c9825076a627c6cdb9ede58cf4d0fa74ca6057ffe74"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
