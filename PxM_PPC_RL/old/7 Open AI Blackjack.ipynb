{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_mc(env, num_episodes):\n",
    "    '''\n",
    "    observation_space[0] is the 18 possible player values. (3 through 20)\n",
    "    observation_space[1] is the 10 possible dealer upcards. (2 through 11)\n",
    "\n",
    "    Combining these together yields all possible states.\n",
    "\n",
    "    Multiplying this with hit/stand yields all possible state/action pairs.\n",
    "\n",
    "    This is the Q map.\n",
    "    '''\n",
    "    Q = np.zeros([env.observation_space[0].n * env.observation_space[1].n, env.action_space.n], dtype=np.float16)\n",
    "\n",
    "\n",
    "    # This map contains the probability distributions for each action (hit or stand) given a state.\n",
    "    # The state (combo of player hand value and dealer upcard value) index in this array yields a 2-element array\n",
    "    # The 0th index of this 2-element array refers to the probability of \"hit\", and the 1st index is the probability of \"stand\"\n",
    "    prob = np.zeros([env.observation_space[0].n * env.observation_space[1].n, env.action_space.n], dtype=np.float16) + 0.5\n",
    "\n",
    "    # The learning rate. Very small to avoid making quick, large changes in our policy.\n",
    "    alpha = 0.001\n",
    "\n",
    "    epsilon = 1\n",
    "    \n",
    "    # The rate by which epsilon will decay over time.\n",
    "    # Since the probability we take the option with the highest Q-value is 1-epsilon + probability,\n",
    "    # this decay will make sure we are the taking the better option more often in the longrun.\n",
    "    # This allows the algorithm to explore in the early stages, and exploit in the later stages.\n",
    "    decay = 0.9999\n",
    "    \n",
    "    # The lowest value that epsilon can go to.\n",
    "    # Although the decay seems slow, it actually grows exponentially, and this is magnified when\n",
    "    # running thousands of episodes.\n",
    "    epsilon_min = 0.9\n",
    "\n",
    "    # may have to be tweaked later.\n",
    "    gamma = 0.8\n",
    "    \n",
    "    for _ in range(num_episodes):\n",
    "        episode = play_game(env, Q, prob)\n",
    "        \n",
    "        epsilon = max(epsilon * decay, epsilon_min)\n",
    "        \n",
    "        Q = update_Q(env, episode, Q, alpha, gamma)\n",
    "        prob = update_prob(env, episode, Q, prob, epsilon)\n",
    "        \n",
    "    return Q, prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_game(env, Q, prob):\n",
    "    # Can contain numerous state->action->reward tuples because a round of \n",
    "    # Blackjack is not always resolved in one turn.\n",
    "    # However, there will be no state that has a player hand value that exceeds 20, since only initial\n",
    "    # states BEFORE actions are made are used when storing state->action->reward tuples.\n",
    "    episode = []\n",
    "    \n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    \n",
    "    while done == False:\n",
    "        if state[0] == 19: #Player was dealt Blackjack, player_value already subtracted by 2 to get state[0]\n",
    "            # don't do any episode analysis for this episode. This is a useless episode.\n",
    "            next_state, reward, done, info = env.step(1) # doesn't matter what action is taken.\n",
    "        else:\n",
    "            # Get the index in Q that corresponds to the current state\n",
    "            Q_state_index = get_Q_state_index(state)\n",
    "            \n",
    "            # Use the index to get the possible actions, and use np.argmax()\n",
    "            # to get the index of the action that has the highest current Q\n",
    "            # value. Index 0 is hit, index 1 is stand.\n",
    "            best_action = np.argmax(Q[Q_state_index])\n",
    "            \n",
    "            # Go to the prob table to retrieve the probability of this action.\n",
    "            # This uses the same Q_state_index used for finding the state index\n",
    "            # of the Q-array.\n",
    "            prob_of_best_action = get_prob_of_best_action(env, state, Q, prob)\n",
    "\n",
    "            action_to_take = None\n",
    "\n",
    "            if random.uniform(0,1) < prob_of_best_action: # Take the best action\n",
    "                action_to_take = best_action\n",
    "            else: # Take the other action\n",
    "                action_to_take = 1 if best_action == 0 else 0\n",
    "            \n",
    "            # The agent does the action, and we get the next state, the rewards,\n",
    "            # and whether the game is now done.\n",
    "            next_state, reward, done, info = env.step(action_to_take)\n",
    "            \n",
    "            # We now have a state->action->reward sequence we can log\n",
    "            # in `episode`\n",
    "            episode.append((state, action_to_take, reward))\n",
    "            \n",
    "            # update the state for the next decision made by the agent.\n",
    "            state = next_state\n",
    "        \n",
    "    return episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_Q(env, episode, Q, alpha, gamma):\n",
    "    '''\n",
    "    THIS IS WHERE THE ALGORITHM HINGES ON BEING FIRST VISIT OR EVERY VISIT.\n",
    "    I AM GOING TO USE FIRST-VISIT, AND HERE'S WHY.\n",
    "    \n",
    "    If you want first-visit, you need to use the cumulative reward of the entire\n",
    "    episode when updating a Q-value for ALL of the state/action pairs in the\n",
    "    episode, even the first state/action pair. In this algorithm, an episode\n",
    "    is a round of Blackjack. Although the bulk of the reward may come from the\n",
    "    2nd or 3rd decision, deciding to hit on the 1st decision is what enabled\n",
    "    the future situations to even occur, so it is important to include the\n",
    "    entire cumulative reward. We can reduce the impact of the rewards of the\n",
    "    future decisions by lowering gamma, which will lower the G value for our\n",
    "    early state/action pair in which we hit and did not get any immediate rewards.\n",
    "    This will make our agent consider future rewards, and not just look at \n",
    "    each state in isolation despite having hit previously.\n",
    "     \n",
    "    If you want Every-Visit MC, do not use the cumulative rewards when updating Q-values,\n",
    "    and just use the immediate reward in this episode for each state/action pair.\n",
    "    '''\n",
    "    step = 0\n",
    "    for state, action, reward in episode:\n",
    "        # calculate the cumulative reward of taking this action in this state.\n",
    "        # Start from the immediate rewards, and use all the rewards from the\n",
    "        # subsequent states. Do not use rewards from previous states.\n",
    "        total_reward = 0\n",
    "        gamma_exp = 0\n",
    "        for curr_step in range(step, len(episode)):\n",
    "            curr_reward = episode[curr_step][2]\n",
    "            total_reward += (gamma ** gamma_exp) * curr_reward\n",
    "            gamma_exp += 1\n",
    "        \n",
    "        # Update the Q-value\n",
    "        Q_state_index = get_Q_state_index(state)\n",
    "        curr_Q_value = Q[Q_state_index][action]\n",
    "        Q[Q_state_index][action] = curr_Q_value + alpha * (total_reward - curr_Q_value)\n",
    "        \n",
    "        # update step to start further down the episode next time.\n",
    "        step += 1\n",
    "        \n",
    "        \n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_prob(env, episode, Q, prob, epsilon):\n",
    "    for state, action, reward in episode:\n",
    "        # Update the probabilities of the actions that can be taken given the current\n",
    "        # state. The goal is that the new update in Q has changed what the best action\n",
    "        # is, and epsilon will be used to create a small increase in the probability\n",
    "        # that the new, better action is chosen.\n",
    "        prob = update_prob_of_best_action(env, state, Q, prob, epsilon)\n",
    "        \n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given a state, derive the corresponding index in the Q-array.\n",
    "# The state is a player hand value + dealer upcard pair,\n",
    "# so a \"hashing\" formula must be used to allocate the\n",
    "# indices of the Q-array properly.\n",
    "def get_Q_state_index(state):\n",
    "    # the player value is already subtracted by 1 in the env when it returns the state.\n",
    "    # subtract by 1 again to fit with the array indexing that starts at 0\n",
    "    initial_player_value = state[0] - 1\n",
    "    # the upcard value is already subtracted by 1 in the env when it returns the state.\n",
    "    # dealer_upcard will be subtracted by 1 to fit with the array indexing that starts at 0\n",
    "    dealer_upcard = state[1] - 1\n",
    "\n",
    "    return (env.observation_space[1].n * (initial_player_value)) + (dealer_upcard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prob_of_best_action(env, state, Q, prob):\n",
    "    # Use the mapping function to figure out which index of Q corresponds to \n",
    "    # the player hand value + dealer upcard value that defines each state.\n",
    "    Q_state_index = get_Q_state_index(state)\n",
    "    \n",
    "    # Use this index in the Q 2-D array to get a 2-element array that yield\n",
    "    # the current Q-values for hitting (index 0) and standing (index 1) in this state.\n",
    "    # Use the np.argmax() function to find the index of the action that yields the\n",
    "    # rewards i.e. the best action we are looking for.\n",
    "    best_action = np.argmax(Q[Q_state_index])\n",
    "    \n",
    "    # Retrieve the probability of the best action using the \n",
    "    # state/action pair as indices for the `prob` array,\n",
    "    # which stores the probability of taking an action (hit or stand)\n",
    "    # for a given state/action pair.\n",
    "    return prob[Q_state_index][best_action]\n",
    "    \n",
    "def update_prob_of_best_action(env, state, Q, prob, epsilon):\n",
    "\n",
    "    Q_state_index = get_Q_state_index(state)\n",
    "    \n",
    "    best_action = np.argmax(Q[Q_state_index])\n",
    "    \n",
    "    # Slightly alter the probability of this best action being taken by using epsilon\n",
    "    # Epsilon starts at 1.0, and slowly decays over time.\n",
    "    # Therefore, as per the equation below, the AI agent will use the probability listed \n",
    "    # for the best action in the `prob` array during the beginning of the algorithm.\n",
    "    # As time goes on, the likelihood that the best action is taken is increased from\n",
    "    # what is listed in the `prob` array.\n",
    "    # This allows for exploration of other moves in the beginning of the algorithm,\n",
    "    # but exploitation later for a greater reward.\n",
    "    #prob[Q_state_index][best_action] = prob[Q_state_index][best_action] + ((1 - epsilon) * (1 - prob[Q_state_index][best_action]))\n",
    "    prob[Q_state_index][best_action] = min(1, prob[Q_state_index][best_action] + 1 - epsilon)\n",
    "    \n",
    "    other_action = 1 if best_action == 0 else 0\n",
    "    prob[Q_state_index][other_action] = 1 - prob[Q_state_index][best_action]\n",
    "    \n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Time for Learning: 10.195332288742065\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "env = gym.make('Blackjack-v0')\n",
    "\n",
    "start_time = time.time()\n",
    "new_Q, new_prob = run_mc(env, 100000)\n",
    "end_time = time.time()\n",
    "\n",
    "print(\"Total Time for Learning: \" + str(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.0000e+00,  0.0000e+00],\n",
       "       [ 0.0000e+00,  0.0000e+00],\n",
       "       [ 0.0000e+00,  0.0000e+00],\n",
       "       [ 0.0000e+00,  0.0000e+00],\n",
       "       [ 0.0000e+00,  0.0000e+00],\n",
       "       [ 0.0000e+00,  0.0000e+00],\n",
       "       [ 0.0000e+00,  0.0000e+00],\n",
       "       [ 0.0000e+00,  0.0000e+00],\n",
       "       [ 0.0000e+00,  0.0000e+00],\n",
       "       [ 0.0000e+00,  0.0000e+00],\n",
       "       [ 0.0000e+00,  0.0000e+00],\n",
       "       [ 0.0000e+00,  0.0000e+00],\n",
       "       [ 0.0000e+00,  0.0000e+00],\n",
       "       [ 0.0000e+00,  0.0000e+00],\n",
       "       [ 0.0000e+00,  0.0000e+00],\n",
       "       [ 0.0000e+00,  0.0000e+00],\n",
       "       [ 0.0000e+00,  0.0000e+00],\n",
       "       [ 0.0000e+00,  0.0000e+00],\n",
       "       [ 0.0000e+00,  0.0000e+00],\n",
       "       [ 0.0000e+00,  0.0000e+00],\n",
       "       [ 0.0000e+00,  0.0000e+00],\n",
       "       [ 0.0000e+00,  0.0000e+00],\n",
       "       [ 0.0000e+00,  0.0000e+00],\n",
       "       [ 0.0000e+00,  0.0000e+00],\n",
       "       [ 0.0000e+00,  0.0000e+00],\n",
       "       [ 0.0000e+00,  0.0000e+00],\n",
       "       [ 0.0000e+00,  0.0000e+00],\n",
       "       [ 0.0000e+00,  0.0000e+00],\n",
       "       [ 0.0000e+00,  0.0000e+00],\n",
       "       [ 0.0000e+00,  0.0000e+00],\n",
       "       [ 0.0000e+00,  0.0000e+00],\n",
       "       [ 0.0000e+00,  0.0000e+00],\n",
       "       [ 0.0000e+00,  0.0000e+00],\n",
       "       [-1.0948e-02, -1.1230e-02],\n",
       "       [-2.9411e-03, -4.6272e-03],\n",
       "       [-2.0447e-03, -2.1160e-05],\n",
       "       [ 9.7036e-04, -3.9635e-03],\n",
       "       [-4.9667e-03, -3.9062e-03],\n",
       "       [-1.0061e-03, -2.0542e-03],\n",
       "       [-9.9335e-03, -2.2564e-03],\n",
       "       [-6.9084e-03, -6.4507e-03],\n",
       "       [-1.2894e-02, -1.0986e-02],\n",
       "       [-3.5980e-02, -3.6072e-02],\n",
       "       [ 0.0000e+00,  0.0000e+00],\n",
       "       [-1.7776e-02, -1.6327e-02],\n",
       "       [-1.9989e-03,  4.6501e-03],\n",
       "       [-1.2886e-02, -1.2367e-02],\n",
       "       [-1.9951e-03,  8.7204e-03],\n",
       "       [-8.9645e-03, -6.7787e-03],\n",
       "       [ 1.9894e-03, -3.2482e-03],\n",
       "       [-1.4717e-02, -1.4473e-02],\n",
       "       [-1.9501e-02, -1.8341e-02],\n",
       "       [-1.0902e-02, -8.6288e-03],\n",
       "       [-7.9529e-02, -7.9834e-02],\n",
       "       [ 0.0000e+00,  0.0000e+00],\n",
       "       [-3.2440e-02, -3.4271e-02],\n",
       "       [-1.6434e-02, -1.6434e-02],\n",
       "       [-1.1864e-02, -1.2955e-02],\n",
       "       [ 4.0321e-03, -4.9744e-03],\n",
       "       [-1.4801e-02, -9.1705e-03],\n",
       "       [-6.1874e-03, -4.6158e-03],\n",
       "       [-2.0554e-02, -1.9714e-02],\n",
       "       [-2.4109e-02, -2.1469e-02],\n",
       "       [-2.6260e-02, -2.6184e-02],\n",
       "       [-8.1299e-02, -8.0872e-02],\n",
       "       [ 0.0000e+00,  0.0000e+00],\n",
       "       [-4.5471e-02, -4.1840e-02],\n",
       "       [-1.6464e-02, -1.3611e-02],\n",
       "       [-1.1909e-02, -9.0027e-03],\n",
       "       [-8.3084e-03, -1.5961e-02],\n",
       "       [-1.6678e-02, -1.0239e-02],\n",
       "       [-1.0773e-02, -1.2802e-02],\n",
       "       [-2.2797e-02, -2.1866e-02],\n",
       "       [-3.7354e-02, -3.7537e-02],\n",
       "       [-3.4607e-02, -3.7598e-02],\n",
       "       [-1.2457e-01, -1.2262e-01],\n",
       "       [ 0.0000e+00,  0.0000e+00],\n",
       "       [-5.5725e-02, -5.5908e-02],\n",
       "       [-1.3702e-02, -8.4076e-03],\n",
       "       [-5.9624e-03,  8.1100e-03],\n",
       "       [-9.7961e-03, -7.8430e-03],\n",
       "       [-8.5907e-03, -1.0971e-02],\n",
       "       [-1.0696e-02, -5.3825e-03],\n",
       "       [-1.6830e-02, -1.3664e-02],\n",
       "       [-2.6230e-02, -2.3209e-02],\n",
       "       [-3.8971e-02, -3.8635e-02],\n",
       "       [-1.3196e-01, -1.2964e-01],\n",
       "       [ 0.0000e+00,  0.0000e+00],\n",
       "       [-5.9540e-02, -5.8624e-02],\n",
       "       [-1.8570e-02, -1.0796e-02],\n",
       "       [-1.3962e-02, -1.7441e-02],\n",
       "       [-1.2871e-02, -7.6523e-03],\n",
       "       [-9.8495e-03, -1.1864e-02],\n",
       "       [-5.0011e-03,  3.6621e-03],\n",
       "       [-2.0676e-02, -1.8341e-02],\n",
       "       [-3.2654e-02, -3.1952e-02],\n",
       "       [-3.8177e-02, -3.3447e-02],\n",
       "       [-1.0449e-01, -1.0278e-01],\n",
       "       [ 0.0000e+00,  0.0000e+00],\n",
       "       [-5.1331e-02, -5.0751e-02],\n",
       "       [-2.9964e-03,  1.8906e-02],\n",
       "       [-8.9417e-03,  3.6346e-02],\n",
       "       [ 1.5497e-06,  4.3671e-02],\n",
       "       [-1.9989e-03,  3.5492e-02],\n",
       "       [ 9.9564e-04,  4.8004e-02],\n",
       "       [-1.0004e-03,  4.4525e-02],\n",
       "       [-1.0933e-02,  2.5146e-02],\n",
       "       [-2.9926e-03,  1.6718e-03],\n",
       "       [-5.3497e-02, -3.8849e-02],\n",
       "       [ 0.0000e+00,  0.0000e+00],\n",
       "       [-3.2196e-02, -3.0304e-02],\n",
       "       [ 1.5497e-06,  4.4983e-02],\n",
       "       [-4.9896e-03,  4.7729e-02],\n",
       "       [-2.9926e-03,  4.8218e-02],\n",
       "       [-3.0003e-03,  6.7993e-02],\n",
       "       [-9.9659e-04,  8.5754e-02],\n",
       "       [-1.9989e-03,  4.4769e-02],\n",
       "       [-1.9989e-03,  4.9011e-02],\n",
       "       [-5.9776e-03,  1.1940e-02],\n",
       "       [-6.9733e-03,  3.2013e-02],\n",
       "       [ 0.0000e+00,  0.0000e+00],\n",
       "       [-2.0337e-01, -1.9727e-01],\n",
       "       [-1.0889e-01, -1.1133e-01],\n",
       "       [-9.1125e-02, -8.3862e-02],\n",
       "       [-8.7402e-02, -8.4839e-02],\n",
       "       [-4.5776e-02, -5.5542e-02],\n",
       "       [-3.8513e-02, -4.3610e-02],\n",
       "       [-1.5088e-01, -1.4990e-01],\n",
       "       [-1.2976e-01, -1.2915e-01],\n",
       "       [-1.5039e-01, -1.5112e-01],\n",
       "       [-3.7842e-01, -3.7378e-01],\n",
       "       [ 0.0000e+00,  0.0000e+00],\n",
       "       [-2.0874e-01, -2.0654e-01],\n",
       "       [-1.2115e-01, -1.2372e-01],\n",
       "       [-9.4727e-02, -8.8562e-02],\n",
       "       [-9.1309e-02, -8.9783e-02],\n",
       "       [-7.7942e-02, -8.1238e-02],\n",
       "       [-4.9164e-02, -6.6833e-02],\n",
       "       [-1.3770e-01, -1.3379e-01],\n",
       "       [-1.3574e-01, -1.3513e-01],\n",
       "       [-1.6028e-01, -1.6199e-01],\n",
       "       [-3.9062e-01, -3.9990e-01],\n",
       "       [ 0.0000e+00,  0.0000e+00],\n",
       "       [-2.3438e-01, -2.3450e-01],\n",
       "       [-1.2140e-01, -1.2549e-01],\n",
       "       [-1.3696e-01, -1.4197e-01],\n",
       "       [-8.9233e-02, -8.7830e-02],\n",
       "       [-6.9824e-02, -6.8176e-02],\n",
       "       [-6.3782e-02, -8.1970e-02],\n",
       "       [-1.6663e-01, -1.6602e-01],\n",
       "       [-1.6150e-01, -1.5979e-01],\n",
       "       [-1.8042e-01, -1.7542e-01],\n",
       "       [-4.1895e-01, -4.2749e-01],\n",
       "       [ 0.0000e+00,  0.0000e+00],\n",
       "       [-2.5073e-01, -2.5024e-01],\n",
       "       [-1.0773e-01, -1.0944e-01],\n",
       "       [-9.4788e-02, -1.0022e-01],\n",
       "       [-7.4585e-02, -7.4646e-02],\n",
       "       [-8.7708e-02, -8.8501e-02],\n",
       "       [-7.0068e-02, -7.6538e-02],\n",
       "       [-1.8054e-01, -1.8030e-01],\n",
       "       [-1.6882e-01, -1.6980e-01],\n",
       "       [-1.9116e-01, -1.9043e-01],\n",
       "       [-4.2090e-01, -4.2163e-01],\n",
       "       [ 0.0000e+00,  0.0000e+00],\n",
       "       [-2.3279e-01, -2.3364e-01],\n",
       "       [-1.4539e-01, -1.4807e-01],\n",
       "       [-1.1890e-01, -1.2561e-01],\n",
       "       [-9.5337e-02, -9.9915e-02],\n",
       "       [-8.6365e-02, -8.5449e-02],\n",
       "       [-9.9426e-02, -9.7778e-02],\n",
       "       [-1.6919e-01, -1.6663e-01],\n",
       "       [-1.8762e-01, -1.8982e-01],\n",
       "       [-2.1362e-01, -2.1326e-01],\n",
       "       [-4.4141e-01, -4.4165e-01],\n",
       "       [ 0.0000e+00,  0.0000e+00],\n",
       "       [-2.1790e-01, -2.1704e-01],\n",
       "       [-7.1777e-02, -7.2510e-02],\n",
       "       [-5.7373e-02, -6.1859e-02],\n",
       "       [-4.3457e-02, -4.5227e-02],\n",
       "       [-2.9770e-02, -2.9739e-02],\n",
       "       [-5.0087e-03, -1.4771e-02],\n",
       "       [-4.9835e-02, -5.1086e-02],\n",
       "       [-1.3135e-01, -1.3196e-01],\n",
       "       [-1.8408e-01, -1.8506e-01],\n",
       "       [-3.9624e-01, -3.9868e-01],\n",
       "       [ 0.0000e+00,  0.0000e+00],\n",
       "       [-1.6064e-01, -1.6077e-01],\n",
       "       [ 7.3181e-02, -2.9926e-03],\n",
       "       [ 8.7585e-02, -4.7913e-03],\n",
       "       [ 9.4666e-02, -3.7956e-03],\n",
       "       [ 7.8796e-02, -6.1798e-03],\n",
       "       [ 1.1096e-01,  0.0000e+00],\n",
       "       [ 2.1448e-01, -2.9964e-03],\n",
       "       [ 3.1464e-02, -1.1734e-02],\n",
       "       [-8.6914e-02, -9.7961e-02],\n",
       "       [-2.2852e-01, -2.3010e-01],\n",
       "       [ 0.0000e+00,  0.0000e+00],\n",
       "       [ 0.0000e+00,  0.0000e+00],\n",
       "       [ 0.0000e+00,  0.0000e+00],\n",
       "       [ 0.0000e+00,  0.0000e+00],\n",
       "       [ 0.0000e+00,  0.0000e+00],\n",
       "       [ 0.0000e+00,  0.0000e+00],\n",
       "       [ 0.0000e+00,  0.0000e+00],\n",
       "       [ 0.0000e+00,  0.0000e+00],\n",
       "       [ 0.0000e+00,  0.0000e+00],\n",
       "       [ 0.0000e+00,  0.0000e+00],\n",
       "       [ 0.0000e+00,  0.0000e+00],\n",
       "       [ 0.0000e+00,  0.0000e+00],\n",
       "       [ 9.8877e-02, -4.5929e-03],\n",
       "       [ 4.1113e-01, -1.9989e-03],\n",
       "       [ 4.2261e-01, -4.7913e-03],\n",
       "       [ 4.3799e-01, -2.9964e-03],\n",
       "       [ 4.4214e-01, -4.7913e-03],\n",
       "       [ 4.4238e-01, -3.9940e-03],\n",
       "       [ 5.2783e-01, -1.9944e-04],\n",
       "       [ 5.2490e-01, -3.3569e-03],\n",
       "       [ 5.0488e-01, -2.9964e-03],\n",
       "       [ 3.8965e-01, -9.5596e-03],\n",
       "       [ 0.0000e+00,  0.0000e+00],\n",
       "       [ 3.7207e-01, -4.6310e-03],\n",
       "       [ 4.3604e-01, -1.5993e-03],\n",
       "       [ 4.4092e-01, -8.0013e-04],\n",
       "       [ 4.2554e-01, -4.4250e-03],\n",
       "       [ 4.0088e-01, -3.4351e-03],\n",
       "       [ 4.2407e-01, -1.1606e-03],\n",
       "       [ 4.7217e-01, -1.7996e-03],\n",
       "       [ 4.9072e-01, -1.0004e-03],\n",
       "       [ 4.9268e-01, -2.9964e-03],\n",
       "       [ 7.5488e-01, -2.7905e-03],\n",
       "       [ 0.0000e+00,  0.0000e+00],\n",
       "       [ 0.0000e+00,  0.0000e+00],\n",
       "       [ 0.0000e+00,  0.0000e+00],\n",
       "       [ 0.0000e+00,  0.0000e+00],\n",
       "       [ 0.0000e+00,  0.0000e+00],\n",
       "       [ 0.0000e+00,  0.0000e+00],\n",
       "       [ 0.0000e+00,  0.0000e+00],\n",
       "       [ 0.0000e+00,  0.0000e+00],\n",
       "       [ 0.0000e+00,  0.0000e+00],\n",
       "       [ 0.0000e+00,  0.0000e+00],\n",
       "       [ 0.0000e+00,  0.0000e+00],\n",
       "       [ 0.0000e+00,  0.0000e+00],\n",
       "       [ 0.0000e+00,  0.0000e+00],\n",
       "       [ 0.0000e+00,  0.0000e+00],\n",
       "       [ 0.0000e+00,  0.0000e+00],\n",
       "       [ 0.0000e+00,  0.0000e+00],\n",
       "       [ 0.0000e+00,  0.0000e+00],\n",
       "       [ 0.0000e+00,  0.0000e+00],\n",
       "       [ 0.0000e+00,  0.0000e+00],\n",
       "       [ 0.0000e+00,  0.0000e+00],\n",
       "       [ 0.0000e+00,  0.0000e+00],\n",
       "       [ 0.0000e+00,  0.0000e+00],\n",
       "       [ 0.0000e+00,  0.0000e+00],\n",
       "       [ 0.0000e+00,  0.0000e+00],\n",
       "       [ 0.0000e+00,  0.0000e+00],\n",
       "       [ 0.0000e+00,  0.0000e+00],\n",
       "       [ 0.0000e+00,  0.0000e+00],\n",
       "       [ 0.0000e+00,  0.0000e+00],\n",
       "       [ 0.0000e+00,  0.0000e+00],\n",
       "       [ 0.0000e+00,  0.0000e+00],\n",
       "       [ 0.0000e+00,  0.0000e+00],\n",
       "       [ 0.0000e+00,  0.0000e+00],\n",
       "       [ 0.0000e+00,  0.0000e+00],\n",
       "       [ 0.0000e+00,  0.0000e+00],\n",
       "       [ 0.0000e+00,  0.0000e+00],\n",
       "       [ 0.0000e+00,  0.0000e+00],\n",
       "       [ 0.0000e+00,  0.0000e+00],\n",
       "       [ 0.0000e+00,  0.0000e+00],\n",
       "       [ 0.0000e+00,  0.0000e+00],\n",
       "       [ 0.0000e+00,  0.0000e+00],\n",
       "       [ 0.0000e+00,  0.0000e+00],\n",
       "       [ 0.0000e+00,  0.0000e+00],\n",
       "       [ 0.0000e+00,  0.0000e+00],\n",
       "       [ 0.0000e+00,  0.0000e+00],\n",
       "       [ 0.0000e+00,  0.0000e+00],\n",
       "       [ 0.0000e+00,  0.0000e+00],\n",
       "       [ 0.0000e+00,  0.0000e+00],\n",
       "       [ 0.0000e+00,  0.0000e+00],\n",
       "       [ 0.0000e+00,  0.0000e+00],\n",
       "       [ 0.0000e+00,  0.0000e+00],\n",
       "       [ 0.0000e+00,  0.0000e+00],\n",
       "       [ 0.0000e+00,  0.0000e+00],\n",
       "       [ 0.0000e+00,  0.0000e+00],\n",
       "       [ 0.0000e+00,  0.0000e+00],\n",
       "       [ 0.0000e+00,  0.0000e+00],\n",
       "       [ 0.0000e+00,  0.0000e+00],\n",
       "       [ 0.0000e+00,  0.0000e+00],\n",
       "       [ 0.0000e+00,  0.0000e+00],\n",
       "       [ 0.0000e+00,  0.0000e+00],\n",
       "       [ 0.0000e+00,  0.0000e+00],\n",
       "       [ 0.0000e+00,  0.0000e+00],\n",
       "       [ 0.0000e+00,  0.0000e+00],\n",
       "       [ 0.0000e+00,  0.0000e+00],\n",
       "       [ 0.0000e+00,  0.0000e+00],\n",
       "       [ 0.0000e+00,  0.0000e+00],\n",
       "       [ 0.0000e+00,  0.0000e+00],\n",
       "       [ 0.0000e+00,  0.0000e+00],\n",
       "       [ 0.0000e+00,  0.0000e+00],\n",
       "       [ 0.0000e+00,  0.0000e+00],\n",
       "       [ 0.0000e+00,  0.0000e+00],\n",
       "       [ 0.0000e+00,  0.0000e+00],\n",
       "       [ 0.0000e+00,  0.0000e+00],\n",
       "       [ 0.0000e+00,  0.0000e+00],\n",
       "       [ 0.0000e+00,  0.0000e+00],\n",
       "       [ 0.0000e+00,  0.0000e+00],\n",
       "       [ 0.0000e+00,  0.0000e+00],\n",
       "       [ 0.0000e+00,  0.0000e+00],\n",
       "       [ 0.0000e+00,  0.0000e+00],\n",
       "       [ 0.0000e+00,  0.0000e+00],\n",
       "       [ 0.0000e+00,  0.0000e+00],\n",
       "       [ 0.0000e+00,  0.0000e+00],\n",
       "       [ 0.0000e+00,  0.0000e+00],\n",
       "       [ 0.0000e+00,  0.0000e+00],\n",
       "       [ 0.0000e+00,  0.0000e+00],\n",
       "       [ 0.0000e+00,  0.0000e+00],\n",
       "       [ 0.0000e+00,  0.0000e+00],\n",
       "       [ 0.0000e+00,  0.0000e+00],\n",
       "       [ 0.0000e+00,  0.0000e+00],\n",
       "       [ 0.0000e+00,  0.0000e+00],\n",
       "       [ 0.0000e+00,  0.0000e+00],\n",
       "       [ 0.0000e+00,  0.0000e+00],\n",
       "       [ 0.0000e+00,  0.0000e+00],\n",
       "       [ 0.0000e+00,  0.0000e+00],\n",
       "       [ 0.0000e+00,  0.0000e+00],\n",
       "       [ 0.0000e+00,  0.0000e+00],\n",
       "       [ 0.0000e+00,  0.0000e+00],\n",
       "       [ 0.0000e+00,  0.0000e+00],\n",
       "       [ 0.0000e+00,  0.0000e+00],\n",
       "       [ 0.0000e+00,  0.0000e+00],\n",
       "       [ 0.0000e+00,  0.0000e+00],\n",
       "       [ 0.0000e+00,  0.0000e+00],\n",
       "       [ 0.0000e+00,  0.0000e+00],\n",
       "       [ 0.0000e+00,  0.0000e+00],\n",
       "       [ 0.0000e+00,  0.0000e+00],\n",
       "       [ 0.0000e+00,  0.0000e+00],\n",
       "       [ 0.0000e+00,  0.0000e+00],\n",
       "       [ 0.0000e+00,  0.0000e+00],\n",
       "       [ 0.0000e+00,  0.0000e+00],\n",
       "       [ 0.0000e+00,  0.0000e+00],\n",
       "       [ 0.0000e+00,  0.0000e+00],\n",
       "       [ 0.0000e+00,  0.0000e+00],\n",
       "       [ 0.0000e+00,  0.0000e+00],\n",
       "       [ 0.0000e+00,  0.0000e+00],\n",
       "       [ 0.0000e+00,  0.0000e+00],\n",
       "       [ 0.0000e+00,  0.0000e+00],\n",
       "       [ 0.0000e+00,  0.0000e+00],\n",
       "       [ 0.0000e+00,  0.0000e+00],\n",
       "       [ 0.0000e+00,  0.0000e+00],\n",
       "       [ 0.0000e+00,  0.0000e+00],\n",
       "       [ 0.0000e+00,  0.0000e+00],\n",
       "       [ 0.0000e+00,  0.0000e+00],\n",
       "       [ 0.0000e+00,  0.0000e+00]], dtype=float16)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_Q\n",
    "#new_prob"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6f69c5940b32a5cbabe45c9825076a627c6cdb9ede58cf4d0fa74ca6057ffe74"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
