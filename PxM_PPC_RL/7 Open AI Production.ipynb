{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Set 1. Process noise = 0.0, Measurement noise = 0.0\n",
      "Data Set 1. Process noise = 0.0, Measurement noise = 0.1\n",
      "Data Set 1. Process noise = 0.0, Measurement noise = 0.2\n",
      "Data Set 1. Process noise = 0.0, Measurement noise = 0.30000000000000004\n",
      "Data Set 1. Process noise = 0.0, Measurement noise = 0.4\n",
      "Data Set 1. Process noise = 0.0, Measurement noise = 0.5\n",
      "Data Set 1. Process noise = 0.1, Measurement noise = 0.0\n",
      "Data Set 1. Process noise = 0.1, Measurement noise = 0.1\n",
      "Data Set 1. Process noise = 0.1, Measurement noise = 0.2\n",
      "Data Set 1. Process noise = 0.1, Measurement noise = 0.30000000000000004\n",
      "Data Set 1. Process noise = 0.1, Measurement noise = 0.4\n",
      "Data Set 1. Process noise = 0.1, Measurement noise = 0.5\n",
      "Data Set 1. Process noise = 0.2, Measurement noise = 0.0\n",
      "Data Set 1. Process noise = 0.2, Measurement noise = 0.1\n",
      "Data Set 1. Process noise = 0.2, Measurement noise = 0.2\n",
      "Data Set 1. Process noise = 0.2, Measurement noise = 0.30000000000000004\n",
      "Data Set 1. Process noise = 0.2, Measurement noise = 0.4\n",
      "Data Set 1. Process noise = 0.2, Measurement noise = 0.5\n",
      "Data Set 1. Process noise = 0.30000000000000004, Measurement noise = 0.0\n",
      "Data Set 1. Process noise = 0.30000000000000004, Measurement noise = 0.1\n",
      "Data Set 1. Process noise = 0.30000000000000004, Measurement noise = 0.2\n",
      "Data Set 1. Process noise = 0.30000000000000004, Measurement noise = 0.30000000000000004\n",
      "Data Set 1. Process noise = 0.30000000000000004, Measurement noise = 0.4\n",
      "Data Set 1. Process noise = 0.30000000000000004, Measurement noise = 0.5\n",
      "Data Set 1. Process noise = 0.4, Measurement noise = 0.0\n",
      "Data Set 1. Process noise = 0.4, Measurement noise = 0.1\n",
      "Data Set 1. Process noise = 0.4, Measurement noise = 0.2\n",
      "Data Set 1. Process noise = 0.4, Measurement noise = 0.30000000000000004\n",
      "Data Set 1. Process noise = 0.4, Measurement noise = 0.4\n",
      "Data Set 1. Process noise = 0.4, Measurement noise = 0.5\n",
      "Data Set 1. Process noise = 0.5, Measurement noise = 0.0\n",
      "Data Set 1. Process noise = 0.5, Measurement noise = 0.1\n",
      "Data Set 1. Process noise = 0.5, Measurement noise = 0.2\n",
      "Data Set 1. Process noise = 0.5, Measurement noise = 0.30000000000000004\n",
      "Data Set 1. Process noise = 0.5, Measurement noise = 0.4\n",
      "Data Set 1. Process noise = 0.5, Measurement noise = 0.5\n"
     ]
    }
   ],
   "source": [
    "### DATA-DRIVEN PROGNOSTICS I ###\n",
    "### GENERATE DATA FOR DATA-DRIVEN MODEL ###\n",
    "\n",
    "import random\n",
    "from prog_models.models import BatteryCircuit\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "# Ignore warnings when machine exceeds its end of life\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\"\"\" Method that uses a physical machine model from the prog_models package and a current (health) state of the model and\n",
    "an action (i.e., intensity), which is performed for 100 time steps\n",
    "    Parameter:\n",
    "        machine             machine model from the prog_models package\n",
    "        state               current (health) state of the model\n",
    "        action              loading of the machine for the next 100 time steps\n",
    "    Return:\n",
    "        health                               \n",
    "    \"\"\"\n",
    "def produce_model(machine, states, action):\n",
    "        \n",
    "        # Define load of battery\n",
    "        def future_loading(t, x=None):\n",
    "            return {'i': action}\n",
    "\n",
    "        # Set current state of machine\n",
    "        machine.parameters['x0'] = states\n",
    "        # Simulate 100 steps\n",
    "        options = {\n",
    "            'save_freq': 100,  # Frequency at which results are saved\n",
    "            'dt': 2  # Timestep\n",
    "        }\n",
    "        (_, _, states, outputs, event_states) = machine.simulate_to(100, future_loading, **options)\n",
    "        health = event_states[-1]['EOD']\n",
    "        return(round(health, 2), states[-1], outputs[-1]['t'], outputs[-1]['v'])\n",
    "def reset_states(machine):\n",
    "    # Returns initial states of machine, e.g., {'tb': 18.95, 'qb': 7856.3254, 'qcp': 0, 'qcs': 0} for Battery\n",
    "    return(machine.default_parameters['x0'])\n",
    "for pn in range (6): \n",
    "    for mn in range(6):\n",
    "        print('Process noise = ' + str(0+0.1*pn) + ', Measurement noise = ' + str(0+0.1*mn))\n",
    "        battery = BatteryCircuit(process_noise = 0+0.1*pn, measurement_noise = 0+0.1*mn)\n",
    "        states = reset_states(battery)\n",
    "        reset_counter = 0\n",
    "        dataset = []\n",
    "        for i in range(int(1e4)):\n",
    "            # If asset failed last period, reset all historical values\n",
    "            if reset_counter == 0: t = v = t_1 = v_1 = t_2 = v_2 = t_3 = v_3 = 0 \n",
    "            # Shift history by one time period\n",
    "            v_3 = v_2\n",
    "            t_3 = t_2\n",
    "            v_2 = v_1\n",
    "            t_2 = t_1\n",
    "            v_1 = v\n",
    "            t_1 = t\n",
    "\n",
    "            # Increment reset_counter\n",
    "            reset_counter = reset_counter + 1\n",
    "            # Compute new health, states, t, and v using last battery state and a random new action\n",
    "            health, states, t, v = produce_model(machine=battery, states=states, action=random.sample((0, 1, 2, 3, 4), 1)[0])\n",
    "            \n",
    "            if health <= 0: \n",
    "                # Reset battery states to initialize battery for next produce_model call\n",
    "                states = reset_states(battery)\n",
    "                # Initialize reset_counter\n",
    "                reset_counter = 0\n",
    "                # Sometimes produce_model returns weird or negative values as the end of life is exceeded\n",
    "                # Here, we just simply set it to zero to not confuse a later learner \n",
    "                health = 0\n",
    "\n",
    "            # append to two-dimensional list\n",
    "            dataset.append([t, v, t_1, v_1, t_2, v_2, t_3, v_3, health])\n",
    "\n",
    "        # Transform two-dim list to dataframe\n",
    "        dataset = pd.DataFrame(dataset, columns=['t', 'v', 't_1', 'v_1', 't_2', 'v_2', 't_3', 'v_3', 'health'])\n",
    "        # Save it as pickle\n",
    "        dataset.to_pickle('diagnostics/data_' + 'pn' + str(pn) + '_mn' + str(mn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process noise = 0.0, Measurement noise = 0.0\n",
      "RandomForestRegressor() : [0.98197883 0.97730528 0.97722026 0.98224411 0.98033069]\n",
      "Process noise = 0.0, Measurement noise = 0.1\n",
      "RandomForestRegressor() : [0.9864139  0.97676802 0.98183423 0.97782142 0.9793493 ]\n",
      "Process noise = 0.0, Measurement noise = 0.2\n",
      "RandomForestRegressor() : [0.98437961 0.97895244 0.9853897  0.97938226 0.97914446]\n",
      "Process noise = 0.0, Measurement noise = 0.30000000000000004\n",
      "RandomForestRegressor() : [0.97730183 0.98760162 0.9851526  0.97968133 0.97478993]\n",
      "Process noise = 0.0, Measurement noise = 0.4\n",
      "RandomForestRegressor() : [0.98465503 0.96904301 0.98287202 0.98522461 0.97855136]\n",
      "Process noise = 0.0, Measurement noise = 0.5\n",
      "RandomForestRegressor() : [0.97745171 0.98212604 0.98410366 0.98056282 0.97075498]\n",
      "Process noise = 0.1, Measurement noise = 0.0\n",
      "RandomForestRegressor() : [0.9465719  0.93545255 0.94084507 0.94856171 0.93582353]\n",
      "Process noise = 0.1, Measurement noise = 0.1\n",
      "RandomForestRegressor() : [0.94792952 0.93689924 0.94272559 0.94187971 0.94378054]\n",
      "Process noise = 0.1, Measurement noise = 0.2\n",
      "RandomForestRegressor() : [0.94903213 0.93920087 0.93877633 0.9420336  0.95171718]\n",
      "Process noise = 0.1, Measurement noise = 0.30000000000000004\n",
      "RandomForestRegressor() : [0.92340191 0.94630326 0.94048419 0.94518488 0.95562932]\n",
      "Process noise = 0.1, Measurement noise = 0.4\n",
      "RandomForestRegressor() : [0.95089399 0.94741609 0.94783203 0.93854588 0.95033216]\n",
      "Process noise = 0.1, Measurement noise = 0.5\n",
      "RandomForestRegressor() : [0.9340382  0.93999493 0.94699246 0.9430891  0.95083812]\n",
      "Process noise = 0.2, Measurement noise = 0.0\n",
      "RandomForestRegressor() : [0.90375851 0.89653954 0.91706162 0.88546057 0.89291691]\n",
      "Process noise = 0.2, Measurement noise = 0.1\n",
      "RandomForestRegressor() : [0.90517416 0.88700658 0.90024786 0.90599911 0.89437658]\n",
      "Process noise = 0.2, Measurement noise = 0.2\n",
      "RandomForestRegressor() : [0.91142589 0.89323212 0.90777246 0.90088973 0.88066344]\n",
      "Process noise = 0.2, Measurement noise = 0.30000000000000004\n",
      "RandomForestRegressor() : [0.89407468 0.90067456 0.89663815 0.90508619 0.89160976]\n",
      "Process noise = 0.2, Measurement noise = 0.4\n",
      "RandomForestRegressor() : [0.89858689 0.89954728 0.90788273 0.86252939 0.90282472]\n",
      "Process noise = 0.2, Measurement noise = 0.5\n",
      "RandomForestRegressor() : [0.90119568 0.90138245 0.88191259 0.90243135 0.89394912]\n",
      "Process noise = 0.30000000000000004, Measurement noise = 0.0\n",
      "RandomForestRegressor() : [0.87173469 0.86964427 0.88932388 0.87491699 0.85223834]\n",
      "Process noise = 0.30000000000000004, Measurement noise = 0.1\n",
      "RandomForestRegressor() : [0.86504889 0.87611337 0.85468822 0.86997111 0.86676973]\n",
      "Process noise = 0.30000000000000004, Measurement noise = 0.2\n",
      "RandomForestRegressor() : [0.86985265 0.84812177 0.85075711 0.86373495 0.86971753]\n",
      "Process noise = 0.30000000000000004, Measurement noise = 0.30000000000000004\n",
      "RandomForestRegressor() : [0.87338042 0.84614536 0.85489012 0.86978108 0.84530568]\n",
      "Process noise = 0.30000000000000004, Measurement noise = 0.4\n",
      "RandomForestRegressor() : [0.86467221 0.85609874 0.86854088 0.876079   0.85428695]\n",
      "Process noise = 0.30000000000000004, Measurement noise = 0.5\n",
      "RandomForestRegressor() : [0.86213922 0.84827795 0.86290136 0.88047153 0.86844629]\n",
      "Process noise = 0.4, Measurement noise = 0.0\n",
      "RandomForestRegressor() : [0.85263553 0.84340376 0.81451898 0.85866517 0.83367178]\n",
      "Process noise = 0.4, Measurement noise = 0.1\n",
      "RandomForestRegressor() : [0.84394565 0.83340869 0.83926564 0.80485337 0.84124245]\n",
      "Process noise = 0.4, Measurement noise = 0.2\n",
      "RandomForestRegressor() : [0.80440361 0.83172766 0.81546266 0.8388308  0.84992579]\n",
      "Process noise = 0.4, Measurement noise = 0.30000000000000004\n",
      "RandomForestRegressor() : [0.81567676 0.84384035 0.83649769 0.81574636 0.82829906]\n",
      "Process noise = 0.4, Measurement noise = 0.4\n",
      "RandomForestRegressor() : [0.83500782 0.80803134 0.84139555 0.83780363 0.80852779]\n",
      "Process noise = 0.4, Measurement noise = 0.5\n",
      "RandomForestRegressor() : [0.84094249 0.84059659 0.8381275  0.82530657 0.8429353 ]\n",
      "Process noise = 0.5, Measurement noise = 0.0\n",
      "RandomForestRegressor() : [0.81428078 0.80043236 0.80489028 0.8099569  0.81997812]\n",
      "Process noise = 0.5, Measurement noise = 0.1\n",
      "RandomForestRegressor() : [0.80727366 0.81281315 0.83404736 0.81795046 0.80998832]\n",
      "Process noise = 0.5, Measurement noise = 0.2\n",
      "RandomForestRegressor() : [0.82082765 0.80258419 0.79769536 0.80126312 0.79093814]\n",
      "Process noise = 0.5, Measurement noise = 0.30000000000000004\n",
      "RandomForestRegressor() : [0.81225232 0.80216443 0.8053704  0.81288701 0.79156817]\n",
      "Process noise = 0.5, Measurement noise = 0.4\n",
      "RandomForestRegressor() : [0.80088781 0.82029235 0.8296575  0.81138211 0.82527429]\n",
      "Process noise = 0.5, Measurement noise = 0.5\n",
      "RandomForestRegressor() : [0.77651087 0.82593997 0.81374247 0.77785235 0.80376063]\n"
     ]
    }
   ],
   "source": [
    "### DATA-DRIVEN PROGNOSTICS II ###\n",
    "### FIT AND TEST MODEL ###\n",
    "from sklearn import tree, linear_model, kernel_ridge, svm, neighbors, gaussian_process, ensemble, neural_network\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import pickle\n",
    "\n",
    "for pn in range (6): \n",
    "    for mn in range(6):\n",
    "        print('Process noise = ' + str(0+0.1*pn) + ', Measurement noise = ' + str(0+0.1*mn))\n",
    "        dataset = pd.read_pickle('diagnostics/data_' + 'pn' + str(pn) + '_mn' + str(mn))\n",
    "        X = dataset[['t', 'v', 't_1', 'v_1', 't_2', 'v_2', 't_3', 'v_3']]\n",
    "        y = dataset['health']\n",
    "        learner = [ensemble.RandomForestRegressor()]\n",
    "        for i in learner:\n",
    "            reg = i\n",
    "            print(i, \":\", cross_val_score(reg, X, y, cv=5)) # default scoring R2\n",
    "        # Fit on all data\n",
    "        model = learner[0].fit(X, y)\n",
    "        pickle.dump(model, open('diagnostics/model_' + 'pn' + str(pn) + '_mn' + str(mn), 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DATA-DRIVEN PROGNOSTICS IIIa ###\n",
    "### FIT, TEST, VISUALIZE MODEL USING TRAIN AND TEST SETS ###\n",
    "\n",
    "# Find index of healthy machines\n",
    "index_df = X.index[(X['t_1'] == 0) & (X['v_1'] == 0) & (X['t_2'] == 0) & (X['v_2'] == 0) & (X['t_3'] == 0) & (X['v_3'] == 0)].tolist()\n",
    "index_test = round(len(index_df)*0.8)\n",
    "\n",
    "# Create train and test set without disrupting machine runs to-failure\n",
    "X_train = X.iloc[0:(index_df[index_test])]\n",
    "y_train = y.iloc[0:(index_df[index_test])]\n",
    "X_test = X.iloc[index_df[index_test]:(len(X))]\n",
    "y_test = y.iloc[index_df[index_test]:(len(y))]\n",
    "\n",
    "## Train\n",
    "learner = [ensemble.RandomForestRegressor()]\n",
    "model = learner[0].fit(X_train, y_train)\n",
    "## Predict\n",
    "y_pred = pd.DataFrame(model.predict(X_test), columns=['Pred'])\n",
    "## Analyze\n",
    "#reset index of each DataFrame\n",
    "X_test.reset_index(drop=True, inplace=True)\n",
    "y_test.reset_index(drop=True, inplace=True)\n",
    "# Concat dataframes\n",
    "test_df = pd.concat([X_test, y_test, y_pred], axis=1)\n",
    "# Print for visualization (e.g., in R)\n",
    "test_df.to_excel(\"diagnostics/test_results.xlsx\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=1000, episode_reward=-8859.80 +/- 516.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=-8976.20 +/- 496.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=3000, episode_reward=-4108.60 +/- 48.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4000, episode_reward=-4085.00 +/- 25.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=5000, episode_reward=-2506.80 +/- 57.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=6000, episode_reward=-2478.80 +/- 11.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=7000, episode_reward=-2445.40 +/- 36.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=8000, episode_reward=-2457.40 +/- 53.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=9000, episode_reward=-2459.00 +/- 19.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=10000, episode_reward=-2493.80 +/- 40.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=11000, episode_reward=-2467.00 +/- 49.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=12000, episode_reward=-2478.60 +/- 43.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=13000, episode_reward=-2473.60 +/- 28.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=14000, episode_reward=-2479.20 +/- 26.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=15000, episode_reward=-2461.20 +/- 30.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=16000, episode_reward=-2435.20 +/- 32.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=17000, episode_reward=-1241.60 +/- 49.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=18000, episode_reward=-1272.00 +/- 23.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=19000, episode_reward=-1262.40 +/- 33.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=20000, episode_reward=-1289.60 +/- 38.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=21000, episode_reward=-1288.80 +/- 57.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=22000, episode_reward=-1279.20 +/- 20.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=23000, episode_reward=-1279.20 +/- 39.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=24000, episode_reward=-1285.60 +/- 31.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=25000, episode_reward=-1294.40 +/- 58.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=26000, episode_reward=-1294.40 +/- 50.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=27000, episode_reward=-1268.00 +/- 17.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=28000, episode_reward=-1305.60 +/- 35.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=29000, episode_reward=-1284.80 +/- 50.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=30000, episode_reward=-1288.80 +/- 73.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=31000, episode_reward=-1263.20 +/- 46.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=32000, episode_reward=-1287.80 +/- 48.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=33000, episode_reward=-1640.20 +/- 59.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=34000, episode_reward=-1690.40 +/- 34.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=35000, episode_reward=-1273.00 +/- 18.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=36000, episode_reward=-1271.40 +/- 45.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=37000, episode_reward=-1276.00 +/- 22.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=38000, episode_reward=-1276.00 +/- 40.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=39000, episode_reward=-1292.80 +/- 13.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=40000, episode_reward=-1271.20 +/- 52.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=41000, episode_reward=-1675.40 +/- 51.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=42000, episode_reward=-1710.60 +/- 44.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=43000, episode_reward=-1710.80 +/- 28.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=44000, episode_reward=-1692.20 +/- 12.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=45000, episode_reward=-1687.00 +/- 23.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=46000, episode_reward=-1680.80 +/- 67.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=47000, episode_reward=-1668.40 +/- 32.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=48000, episode_reward=-1119.40 +/- 31.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=49000, episode_reward=-1186.40 +/- 17.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=50000, episode_reward=-1668.40 +/- 42.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=51000, episode_reward=-1674.80 +/- 78.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=52000, episode_reward=-1558.00 +/- 264.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=53000, episode_reward=-1660.40 +/- 27.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=54000, episode_reward=-1240.20 +/- 52.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=55000, episode_reward=-1268.60 +/- 41.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=56000, episode_reward=-1451.40 +/- 407.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=57000, episode_reward=-1175.80 +/- 284.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=58000, episode_reward=-1646.00 +/- 61.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=59000, episode_reward=-1696.60 +/- 43.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=60000, episode_reward=-1738.20 +/- 30.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=61000, episode_reward=-1614.00 +/- 281.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=62000, episode_reward=-1239.60 +/- 75.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=63000, episode_reward=-1206.20 +/- 70.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=64000, episode_reward=-1223.00 +/- 21.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=65000, episode_reward=-1242.40 +/- 69.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=66000, episode_reward=-1294.80 +/- 37.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=67000, episode_reward=-1272.80 +/- 26.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=68000, episode_reward=-1242.80 +/- 42.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=69000, episode_reward=-1251.00 +/- 22.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=70000, episode_reward=-1147.60 +/- 40.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=71000, episode_reward=-1184.60 +/- 43.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=72000, episode_reward=-1197.80 +/- 56.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=73000, episode_reward=-1203.80 +/- 28.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=74000, episode_reward=-1237.00 +/- 55.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=75000, episode_reward=-1206.60 +/- 32.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=76000, episode_reward=-1234.60 +/- 28.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=77000, episode_reward=-1217.80 +/- 29.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=78000, episode_reward=-1133.00 +/- 243.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=79000, episode_reward=-993.20 +/- 44.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=80000, episode_reward=-1657.20 +/- 53.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=81000, episode_reward=-1694.00 +/- 25.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=82000, episode_reward=-1678.00 +/- 20.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=83000, episode_reward=-1535.00 +/- 251.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=84000, episode_reward=-1140.00 +/- 92.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=85000, episode_reward=-1158.60 +/- 62.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=86000, episode_reward=-1150.80 +/- 57.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=87000, episode_reward=-1056.40 +/- 32.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=88000, episode_reward=-1150.80 +/- 308.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=89000, episode_reward=-1428.40 +/- 345.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=90000, episode_reward=-1304.00 +/- 364.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=91000, episode_reward=-1186.20 +/- 43.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=92000, episode_reward=-1153.60 +/- 55.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=93000, episode_reward=-1440.80 +/- 329.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=94000, episode_reward=-1547.80 +/- 287.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=95000, episode_reward=-1655.80 +/- 18.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=96000, episode_reward=-1664.80 +/- 24.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=97000, episode_reward=-1679.80 +/- 29.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=98000, episode_reward=-1645.40 +/- 35.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=99000, episode_reward=-1679.80 +/- 26.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=100000, episode_reward=-1675.00 +/- 62.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=101000, episode_reward=-1691.00 +/- 45.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=102000, episode_reward=-1676.00 +/- 36.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=103000, episode_reward=-1663.80 +/- 58.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=104000, episode_reward=-1684.00 +/- 47.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=105000, episode_reward=-1689.20 +/- 35.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=106000, episode_reward=-1637.60 +/- 20.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=107000, episode_reward=-1698.40 +/- 37.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=108000, episode_reward=-1674.20 +/- 56.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=109000, episode_reward=-1709.40 +/- 29.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=110000, episode_reward=-1659.60 +/- 41.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=111000, episode_reward=-1678.20 +/- 28.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=112000, episode_reward=-1666.20 +/- 33.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=113000, episode_reward=-1698.40 +/- 45.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=114000, episode_reward=-1658.00 +/- 45.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=115000, episode_reward=-1656.60 +/- 40.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=116000, episode_reward=-1676.40 +/- 29.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=117000, episode_reward=-1704.80 +/- 27.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=118000, episode_reward=-1673.60 +/- 27.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=119000, episode_reward=-1664.20 +/- 29.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=120000, episode_reward=-1675.60 +/- 76.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=121000, episode_reward=-1644.20 +/- 15.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=122000, episode_reward=-1685.00 +/- 38.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=123000, episode_reward=-1634.60 +/- 37.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=124000, episode_reward=-1676.60 +/- 24.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=125000, episode_reward=-1673.40 +/- 43.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=126000, episode_reward=-1650.20 +/- 38.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=127000, episode_reward=-1693.20 +/- 27.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=128000, episode_reward=-1691.80 +/- 50.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=129000, episode_reward=-1640.40 +/- 30.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=130000, episode_reward=-1650.80 +/- 26.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=131000, episode_reward=-1709.80 +/- 33.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=132000, episode_reward=-1676.20 +/- 30.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=133000, episode_reward=-1682.60 +/- 35.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=134000, episode_reward=-1650.20 +/- 42.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=135000, episode_reward=-1658.40 +/- 74.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=136000, episode_reward=-1711.20 +/- 31.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=137000, episode_reward=-1714.60 +/- 45.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=138000, episode_reward=-1708.60 +/- 14.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=139000, episode_reward=-1713.60 +/- 46.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=140000, episode_reward=-1669.40 +/- 15.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=141000, episode_reward=-1691.40 +/- 30.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=142000, episode_reward=-1692.00 +/- 34.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=143000, episode_reward=-1691.80 +/- 39.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=144000, episode_reward=-1681.40 +/- 22.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=145000, episode_reward=-1676.20 +/- 35.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=146000, episode_reward=-1674.20 +/- 42.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=147000, episode_reward=-1679.00 +/- 12.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=148000, episode_reward=-1649.40 +/- 35.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=149000, episode_reward=-1674.60 +/- 43.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=150000, episode_reward=-1642.40 +/- 52.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=151000, episode_reward=-1702.40 +/- 18.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=152000, episode_reward=-1664.00 +/- 36.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=153000, episode_reward=-1704.20 +/- 5.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=154000, episode_reward=-1697.60 +/- 32.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=155000, episode_reward=-1690.60 +/- 66.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=156000, episode_reward=-1724.40 +/- 24.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=157000, episode_reward=-1708.20 +/- 30.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=158000, episode_reward=-1683.20 +/- 23.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=159000, episode_reward=-1717.60 +/- 56.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=160000, episode_reward=-1692.40 +/- 20.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=161000, episode_reward=-1697.00 +/- 43.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=162000, episode_reward=-1667.40 +/- 44.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=163000, episode_reward=-1672.20 +/- 26.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=164000, episode_reward=-1708.80 +/- 37.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=165000, episode_reward=-1628.40 +/- 58.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=166000, episode_reward=-1698.80 +/- 26.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=167000, episode_reward=-1683.80 +/- 23.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=168000, episode_reward=-1671.20 +/- 26.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=169000, episode_reward=-1678.00 +/- 50.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=170000, episode_reward=-1679.60 +/- 25.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=171000, episode_reward=-1644.60 +/- 40.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=172000, episode_reward=-1697.20 +/- 40.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=173000, episode_reward=-1686.60 +/- 40.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=174000, episode_reward=-1672.80 +/- 41.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=175000, episode_reward=-1687.80 +/- 56.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=176000, episode_reward=-1680.40 +/- 32.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=177000, episode_reward=-1662.40 +/- 48.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=178000, episode_reward=-1676.80 +/- 6.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=179000, episode_reward=-1689.20 +/- 14.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=180000, episode_reward=-1686.40 +/- 27.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=181000, episode_reward=-1684.80 +/- 43.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=182000, episode_reward=-1683.20 +/- 13.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=183000, episode_reward=-1652.00 +/- 33.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=184000, episode_reward=-1682.40 +/- 38.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=185000, episode_reward=-1722.40 +/- 53.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=186000, episode_reward=-1663.20 +/- 16.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=187000, episode_reward=-1691.00 +/- 52.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=188000, episode_reward=-1705.00 +/- 46.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=189000, episode_reward=-1671.80 +/- 35.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=190000, episode_reward=-1654.60 +/- 48.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=191000, episode_reward=-1680.60 +/- 45.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=192000, episode_reward=-1682.20 +/- 23.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=193000, episode_reward=-1648.80 +/- 86.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=194000, episode_reward=-1683.40 +/- 25.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=195000, episode_reward=-1664.40 +/- 40.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=196000, episode_reward=-1671.40 +/- 37.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=197000, episode_reward=-1716.20 +/- 9.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=198000, episode_reward=-1682.60 +/- 51.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=199000, episode_reward=-1687.20 +/- 57.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=200000, episode_reward=-1677.80 +/- 19.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=201000, episode_reward=-1664.80 +/- 30.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=202000, episode_reward=-1639.80 +/- 30.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=203000, episode_reward=-1672.00 +/- 59.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=204000, episode_reward=-1699.60 +/- 34.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=205000, episode_reward=-1690.40 +/- 29.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=206000, episode_reward=-1674.60 +/- 38.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=207000, episode_reward=-1659.40 +/- 60.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=208000, episode_reward=-1681.80 +/- 40.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=209000, episode_reward=-1665.20 +/- 14.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=210000, episode_reward=-1697.40 +/- 34.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=211000, episode_reward=-1688.80 +/- 30.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=212000, episode_reward=-1643.40 +/- 28.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=213000, episode_reward=-1684.80 +/- 32.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=214000, episode_reward=-1685.00 +/- 25.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=215000, episode_reward=-1660.40 +/- 40.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=216000, episode_reward=-1684.20 +/- 68.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=217000, episode_reward=-1610.40 +/- 53.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=218000, episode_reward=-1653.20 +/- 20.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=219000, episode_reward=-1686.80 +/- 34.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=220000, episode_reward=-1683.40 +/- 49.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=221000, episode_reward=-1691.60 +/- 28.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=222000, episode_reward=-1653.00 +/- 26.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=223000, episode_reward=-1666.00 +/- 21.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=224000, episode_reward=-1687.80 +/- 34.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=225000, episode_reward=-1689.00 +/- 28.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=226000, episode_reward=-1656.20 +/- 61.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=227000, episode_reward=-1656.60 +/- 30.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=228000, episode_reward=-1667.20 +/- 25.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=229000, episode_reward=-1658.80 +/- 34.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=230000, episode_reward=-1655.60 +/- 20.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=231000, episode_reward=-1677.60 +/- 44.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=232000, episode_reward=-1677.80 +/- 7.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=233000, episode_reward=-1682.00 +/- 36.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=234000, episode_reward=-1700.00 +/- 31.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=235000, episode_reward=-1671.00 +/- 15.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=236000, episode_reward=-1674.60 +/- 37.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=237000, episode_reward=-1656.80 +/- 34.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=238000, episode_reward=-1692.20 +/- 51.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=239000, episode_reward=-1675.40 +/- 24.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=240000, episode_reward=-1651.40 +/- 30.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=241000, episode_reward=-1625.20 +/- 46.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=242000, episode_reward=-1696.20 +/- 44.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=243000, episode_reward=-1658.00 +/- 35.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=244000, episode_reward=-1690.80 +/- 34.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=245000, episode_reward=-1680.00 +/- 30.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=246000, episode_reward=-1680.80 +/- 69.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=247000, episode_reward=-1671.20 +/- 84.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=248000, episode_reward=-1695.60 +/- 28.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=249000, episode_reward=-1668.80 +/- 35.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=250000, episode_reward=-1685.60 +/- 32.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=251000, episode_reward=-1684.60 +/- 33.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=252000, episode_reward=-1647.60 +/- 22.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=253000, episode_reward=-1677.00 +/- 40.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=254000, episode_reward=-1681.40 +/- 57.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=255000, episode_reward=-1658.60 +/- 43.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=256000, episode_reward=-1665.40 +/- 37.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=257000, episode_reward=-1682.80 +/- 49.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=258000, episode_reward=-1692.80 +/- 35.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=259000, episode_reward=-1652.40 +/- 39.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=260000, episode_reward=-1667.40 +/- 45.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=261000, episode_reward=-1675.20 +/- 39.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=262000, episode_reward=-1704.00 +/- 44.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=263000, episode_reward=-1693.80 +/- 17.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=264000, episode_reward=-1693.80 +/- 28.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=265000, episode_reward=-1715.40 +/- 29.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=266000, episode_reward=-1683.40 +/- 46.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=267000, episode_reward=-1697.20 +/- 39.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=268000, episode_reward=-1687.80 +/- 29.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=269000, episode_reward=-1610.80 +/- 54.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=270000, episode_reward=-1658.20 +/- 48.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=271000, episode_reward=-1676.20 +/- 53.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=272000, episode_reward=-1682.60 +/- 46.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=273000, episode_reward=-1644.20 +/- 39.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=274000, episode_reward=-1691.20 +/- 54.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=275000, episode_reward=-1719.20 +/- 23.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=276000, episode_reward=-1749.20 +/- 34.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=277000, episode_reward=-1687.40 +/- 47.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=278000, episode_reward=-1670.00 +/- 28.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=279000, episode_reward=-1660.80 +/- 9.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=280000, episode_reward=-1672.60 +/- 51.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=281000, episode_reward=-1659.00 +/- 69.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=282000, episode_reward=-1661.20 +/- 47.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=283000, episode_reward=-1650.20 +/- 61.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=284000, episode_reward=-1675.80 +/- 53.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=285000, episode_reward=-1689.00 +/- 11.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=286000, episode_reward=-1700.80 +/- 37.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=287000, episode_reward=-1692.40 +/- 27.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=288000, episode_reward=-1669.80 +/- 43.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=289000, episode_reward=-1680.20 +/- 42.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=290000, episode_reward=-1658.60 +/- 42.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=291000, episode_reward=-1683.00 +/- 33.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=292000, episode_reward=-1673.80 +/- 38.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=293000, episode_reward=-1711.80 +/- 23.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=294000, episode_reward=-1651.60 +/- 52.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=295000, episode_reward=-1675.40 +/- 44.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=296000, episode_reward=-1673.60 +/- 62.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=297000, episode_reward=-1691.60 +/- 26.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=298000, episode_reward=-1682.00 +/- 46.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=299000, episode_reward=-1691.40 +/- 17.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=300000, episode_reward=-1695.00 +/- 30.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=301000, episode_reward=-1676.60 +/- 45.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=302000, episode_reward=-1668.00 +/- 37.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=303000, episode_reward=-1677.60 +/- 31.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=304000, episode_reward=-1654.40 +/- 55.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=305000, episode_reward=-1704.60 +/- 37.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=306000, episode_reward=-1691.80 +/- 38.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=307000, episode_reward=-1672.20 +/- 31.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=308000, episode_reward=-1691.80 +/- 24.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=309000, episode_reward=-1658.00 +/- 69.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=310000, episode_reward=-1673.40 +/- 43.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=311000, episode_reward=-1674.60 +/- 25.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=312000, episode_reward=-1664.60 +/- 48.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=313000, episode_reward=-1665.60 +/- 48.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=314000, episode_reward=-1744.40 +/- 26.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=315000, episode_reward=-1672.60 +/- 29.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=316000, episode_reward=-1660.60 +/- 28.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=317000, episode_reward=-1686.00 +/- 18.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=318000, episode_reward=-1673.40 +/- 31.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=319000, episode_reward=-1691.60 +/- 49.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=320000, episode_reward=-1707.40 +/- 38.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=321000, episode_reward=-1679.40 +/- 32.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=322000, episode_reward=-1698.00 +/- 54.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=323000, episode_reward=-1694.00 +/- 41.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=324000, episode_reward=-1701.60 +/- 26.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=325000, episode_reward=-1656.20 +/- 39.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=326000, episode_reward=-1703.20 +/- 19.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=327000, episode_reward=-1676.00 +/- 50.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=328000, episode_reward=-1655.00 +/- 26.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=329000, episode_reward=-1682.60 +/- 46.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=330000, episode_reward=-1699.00 +/- 35.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=331000, episode_reward=-1727.80 +/- 43.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=332000, episode_reward=-1666.60 +/- 50.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=333000, episode_reward=-1677.40 +/- 36.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=334000, episode_reward=-1699.60 +/- 36.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=335000, episode_reward=-1650.80 +/- 43.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=336000, episode_reward=-1679.80 +/- 30.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=337000, episode_reward=-1665.20 +/- 30.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=338000, episode_reward=-1686.60 +/- 55.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=339000, episode_reward=-1641.80 +/- 37.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=340000, episode_reward=-1664.20 +/- 39.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=341000, episode_reward=-1709.00 +/- 39.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=342000, episode_reward=-1671.60 +/- 41.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=343000, episode_reward=-1677.40 +/- 46.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=344000, episode_reward=-1643.60 +/- 17.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=345000, episode_reward=-1680.00 +/- 25.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=346000, episode_reward=-1679.20 +/- 30.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=347000, episode_reward=-1663.00 +/- 59.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=348000, episode_reward=-1669.00 +/- 19.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=349000, episode_reward=-1681.00 +/- 25.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=350000, episode_reward=-1666.00 +/- 47.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=351000, episode_reward=-1661.60 +/- 37.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=352000, episode_reward=-1687.20 +/- 17.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=353000, episode_reward=-1732.60 +/- 40.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=354000, episode_reward=-1687.20 +/- 4.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=355000, episode_reward=-1681.00 +/- 63.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=356000, episode_reward=-1658.20 +/- 56.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=357000, episode_reward=-1637.80 +/- 41.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=358000, episode_reward=-1700.00 +/- 42.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=359000, episode_reward=-1687.60 +/- 34.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=360000, episode_reward=-1619.40 +/- 45.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=361000, episode_reward=-1664.00 +/- 38.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=362000, episode_reward=-1666.60 +/- 55.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=363000, episode_reward=-1675.00 +/- 16.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=364000, episode_reward=-1678.40 +/- 29.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=365000, episode_reward=-1687.00 +/- 29.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=366000, episode_reward=-1700.20 +/- 41.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=367000, episode_reward=-1691.60 +/- 32.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=368000, episode_reward=-1706.00 +/- 19.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=369000, episode_reward=-1681.00 +/- 40.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=370000, episode_reward=-1690.80 +/- 39.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=371000, episode_reward=-1692.60 +/- 70.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=372000, episode_reward=-1674.80 +/- 44.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=373000, episode_reward=-1657.20 +/- 30.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=374000, episode_reward=-1661.20 +/- 20.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=375000, episode_reward=-1663.00 +/- 30.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=376000, episode_reward=-1663.20 +/- 49.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=377000, episode_reward=-1654.20 +/- 35.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=378000, episode_reward=-1688.80 +/- 39.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=379000, episode_reward=-1655.00 +/- 10.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=380000, episode_reward=-1657.00 +/- 70.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=381000, episode_reward=-1676.40 +/- 67.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=382000, episode_reward=-1656.20 +/- 33.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=383000, episode_reward=-1681.20 +/- 40.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=384000, episode_reward=-1679.00 +/- 36.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=385000, episode_reward=-1705.20 +/- 55.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=386000, episode_reward=-1659.80 +/- 23.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=387000, episode_reward=-1674.00 +/- 42.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=388000, episode_reward=-1698.20 +/- 23.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=389000, episode_reward=-1667.00 +/- 47.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=390000, episode_reward=-1679.60 +/- 59.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=391000, episode_reward=-1617.40 +/- 54.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=392000, episode_reward=-1681.00 +/- 33.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=393000, episode_reward=-1715.00 +/- 28.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=394000, episode_reward=-1690.60 +/- 40.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=395000, episode_reward=-1648.40 +/- 23.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=396000, episode_reward=-1661.80 +/- 49.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=397000, episode_reward=-1678.80 +/- 30.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=398000, episode_reward=-1681.20 +/- 49.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=399000, episode_reward=-1683.20 +/- 25.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=400000, episode_reward=-1643.40 +/- 58.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=401000, episode_reward=-1713.80 +/- 22.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=402000, episode_reward=-1699.20 +/- 16.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=403000, episode_reward=-1625.00 +/- 29.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=404000, episode_reward=-1674.40 +/- 29.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=405000, episode_reward=-1687.80 +/- 45.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=406000, episode_reward=-1715.80 +/- 26.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=407000, episode_reward=-1644.40 +/- 68.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=408000, episode_reward=-1708.60 +/- 60.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=409000, episode_reward=-1704.80 +/- 21.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=410000, episode_reward=-1708.40 +/- 29.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=411000, episode_reward=-1698.00 +/- 23.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=412000, episode_reward=-1693.20 +/- 16.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=413000, episode_reward=-1697.00 +/- 23.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=414000, episode_reward=-1686.00 +/- 41.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=415000, episode_reward=-1682.60 +/- 48.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=416000, episode_reward=-1655.40 +/- 48.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=417000, episode_reward=-1703.80 +/- 39.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=418000, episode_reward=-1663.80 +/- 24.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=419000, episode_reward=-1652.00 +/- 18.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=420000, episode_reward=-1664.00 +/- 42.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=421000, episode_reward=-1709.80 +/- 33.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=422000, episode_reward=-1683.40 +/- 75.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=423000, episode_reward=-1658.20 +/- 36.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=424000, episode_reward=-1668.60 +/- 36.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=425000, episode_reward=-1688.00 +/- 41.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=426000, episode_reward=-1678.40 +/- 12.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=427000, episode_reward=-1687.80 +/- 32.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=428000, episode_reward=-1687.60 +/- 22.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=429000, episode_reward=-1683.80 +/- 35.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=430000, episode_reward=-1693.80 +/- 21.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=431000, episode_reward=-1663.20 +/- 23.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=432000, episode_reward=-1669.40 +/- 13.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=433000, episode_reward=-1682.80 +/- 47.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=434000, episode_reward=-1675.20 +/- 34.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=435000, episode_reward=-1644.80 +/- 29.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=436000, episode_reward=-1684.00 +/- 44.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=437000, episode_reward=-1679.60 +/- 35.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=438000, episode_reward=-1701.00 +/- 26.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=439000, episode_reward=-1690.20 +/- 25.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=440000, episode_reward=-1679.80 +/- 39.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=441000, episode_reward=-1656.40 +/- 24.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=442000, episode_reward=-1679.60 +/- 29.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=443000, episode_reward=-1693.00 +/- 43.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=444000, episode_reward=-1673.40 +/- 70.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=445000, episode_reward=-1682.20 +/- 28.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=446000, episode_reward=-1672.80 +/- 25.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=447000, episode_reward=-1670.80 +/- 39.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=448000, episode_reward=-1658.80 +/- 42.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=449000, episode_reward=-1684.80 +/- 33.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=450000, episode_reward=-1699.80 +/- 47.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=451000, episode_reward=-1650.20 +/- 54.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=452000, episode_reward=-1704.00 +/- 38.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=453000, episode_reward=-1676.80 +/- 36.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=454000, episode_reward=-1696.80 +/- 53.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=455000, episode_reward=-1651.40 +/- 39.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=456000, episode_reward=-1687.20 +/- 23.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=457000, episode_reward=-1673.60 +/- 48.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=458000, episode_reward=-1654.80 +/- 37.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=459000, episode_reward=-1681.40 +/- 18.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=460000, episode_reward=-1700.00 +/- 41.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=461000, episode_reward=-1692.20 +/- 45.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=462000, episode_reward=-1687.80 +/- 49.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=463000, episode_reward=-1659.80 +/- 42.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=464000, episode_reward=-1702.60 +/- 21.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=465000, episode_reward=-1705.80 +/- 47.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=466000, episode_reward=-1627.60 +/- 26.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=467000, episode_reward=-1701.40 +/- 18.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=468000, episode_reward=-1674.20 +/- 32.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=469000, episode_reward=-1670.00 +/- 38.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=470000, episode_reward=-1692.20 +/- 24.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=471000, episode_reward=-1651.40 +/- 48.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=472000, episode_reward=-1678.00 +/- 3.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=473000, episode_reward=-1706.40 +/- 28.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=474000, episode_reward=-1678.60 +/- 30.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=475000, episode_reward=-1654.80 +/- 39.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=476000, episode_reward=-1714.60 +/- 20.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=477000, episode_reward=-1667.60 +/- 24.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=478000, episode_reward=-1689.00 +/- 23.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=479000, episode_reward=-1667.80 +/- 43.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=480000, episode_reward=-1681.60 +/- 36.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=481000, episode_reward=-1669.20 +/- 12.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=482000, episode_reward=-1688.20 +/- 33.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=483000, episode_reward=-1710.60 +/- 27.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=484000, episode_reward=-1675.60 +/- 47.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=485000, episode_reward=-1674.40 +/- 30.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=486000, episode_reward=-1692.60 +/- 24.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=487000, episode_reward=-1658.60 +/- 42.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=488000, episode_reward=-1708.60 +/- 57.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=489000, episode_reward=-1664.20 +/- 23.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=490000, episode_reward=-1653.00 +/- 42.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=491000, episode_reward=-1643.20 +/- 29.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=492000, episode_reward=-1685.20 +/- 41.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=493000, episode_reward=-1671.40 +/- 35.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=494000, episode_reward=-1671.80 +/- 26.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=495000, episode_reward=-1657.80 +/- 39.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=496000, episode_reward=-1707.80 +/- 19.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=497000, episode_reward=-1672.00 +/- 39.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=498000, episode_reward=-1681.20 +/- 24.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=499000, episode_reward=-1643.40 +/- 35.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=500000, episode_reward=-1679.60 +/- 36.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=501000, episode_reward=-1660.60 +/- 38.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=502000, episode_reward=-1710.20 +/- 19.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=503000, episode_reward=-1687.60 +/- 31.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=504000, episode_reward=-1687.20 +/- 48.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=505000, episode_reward=-1682.80 +/- 28.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=506000, episode_reward=-1711.00 +/- 47.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=507000, episode_reward=-1667.00 +/- 26.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=508000, episode_reward=-1658.20 +/- 37.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=509000, episode_reward=-1662.80 +/- 12.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=510000, episode_reward=-1675.00 +/- 58.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=511000, episode_reward=-1673.80 +/- 53.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=512000, episode_reward=-1702.60 +/- 28.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=513000, episode_reward=-1659.20 +/- 37.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=514000, episode_reward=-1650.20 +/- 49.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=515000, episode_reward=-1665.60 +/- 24.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=516000, episode_reward=-1699.00 +/- 22.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=517000, episode_reward=-1682.80 +/- 45.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=518000, episode_reward=-1691.00 +/- 28.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=519000, episode_reward=-1673.20 +/- 36.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=520000, episode_reward=-1674.60 +/- 46.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=521000, episode_reward=-1668.40 +/- 36.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=522000, episode_reward=-1692.00 +/- 38.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=523000, episode_reward=-1654.80 +/- 50.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=524000, episode_reward=-1693.60 +/- 37.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=525000, episode_reward=-1645.80 +/- 50.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=526000, episode_reward=-1680.00 +/- 45.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=527000, episode_reward=-1660.80 +/- 34.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=528000, episode_reward=-1688.80 +/- 46.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=529000, episode_reward=-1674.40 +/- 49.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=530000, episode_reward=-1653.80 +/- 54.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=531000, episode_reward=-1680.40 +/- 52.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=532000, episode_reward=-1679.60 +/- 19.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=533000, episode_reward=-1684.00 +/- 21.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=534000, episode_reward=-1649.60 +/- 25.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=535000, episode_reward=-1657.20 +/- 17.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=536000, episode_reward=-1662.00 +/- 44.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=537000, episode_reward=-1682.40 +/- 39.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=538000, episode_reward=-1687.40 +/- 33.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=539000, episode_reward=-1662.60 +/- 32.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=540000, episode_reward=-1702.00 +/- 47.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=541000, episode_reward=-1689.20 +/- 22.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=542000, episode_reward=-1653.80 +/- 47.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=543000, episode_reward=-1709.80 +/- 34.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=544000, episode_reward=-1680.60 +/- 37.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=545000, episode_reward=-1651.80 +/- 26.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=546000, episode_reward=-1695.60 +/- 32.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=547000, episode_reward=-1661.00 +/- 59.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=548000, episode_reward=-1695.40 +/- 35.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=549000, episode_reward=-1672.40 +/- 41.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=550000, episode_reward=-1686.00 +/- 42.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=551000, episode_reward=-1645.00 +/- 22.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=552000, episode_reward=-1681.00 +/- 53.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=553000, episode_reward=-1644.00 +/- 49.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=554000, episode_reward=-1696.00 +/- 29.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=555000, episode_reward=-1685.20 +/- 56.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=556000, episode_reward=-1690.00 +/- 28.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=557000, episode_reward=-1662.60 +/- 22.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=558000, episode_reward=-1694.80 +/- 42.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=559000, episode_reward=-1711.00 +/- 17.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=560000, episode_reward=-1712.80 +/- 60.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=561000, episode_reward=-1685.60 +/- 33.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=562000, episode_reward=-1694.40 +/- 32.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=563000, episode_reward=-1690.20 +/- 30.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=564000, episode_reward=-1665.00 +/- 31.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=565000, episode_reward=-1709.20 +/- 28.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=566000, episode_reward=-1688.20 +/- 56.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=567000, episode_reward=-1719.00 +/- 55.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=568000, episode_reward=-1650.60 +/- 30.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=569000, episode_reward=-1691.80 +/- 41.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=570000, episode_reward=-1662.40 +/- 28.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=571000, episode_reward=-1663.80 +/- 49.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=572000, episode_reward=-1661.20 +/- 35.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=573000, episode_reward=-1683.80 +/- 17.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=574000, episode_reward=-1608.60 +/- 55.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=575000, episode_reward=-1648.40 +/- 32.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=576000, episode_reward=-1688.20 +/- 29.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=577000, episode_reward=-1653.00 +/- 41.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=578000, episode_reward=-1675.60 +/- 54.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=579000, episode_reward=-1674.20 +/- 48.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=580000, episode_reward=-1656.40 +/- 50.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=581000, episode_reward=-1677.40 +/- 47.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=582000, episode_reward=-1691.60 +/- 38.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=583000, episode_reward=-1692.00 +/- 25.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=584000, episode_reward=-1695.60 +/- 30.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=585000, episode_reward=-1715.20 +/- 46.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=586000, episode_reward=-1683.80 +/- 15.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=587000, episode_reward=-1642.20 +/- 33.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=588000, episode_reward=-1705.80 +/- 41.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=589000, episode_reward=-1684.20 +/- 44.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=590000, episode_reward=-1649.80 +/- 58.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=591000, episode_reward=-1693.20 +/- 25.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=592000, episode_reward=-1687.40 +/- 28.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=593000, episode_reward=-1701.40 +/- 53.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=594000, episode_reward=-1699.40 +/- 29.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=595000, episode_reward=-1678.60 +/- 25.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=596000, episode_reward=-1690.60 +/- 34.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=597000, episode_reward=-1676.60 +/- 62.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=598000, episode_reward=-1681.20 +/- 26.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=599000, episode_reward=-1699.60 +/- 47.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=600000, episode_reward=-1649.40 +/- 44.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=601000, episode_reward=-1674.20 +/- 52.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=602000, episode_reward=-1694.80 +/- 48.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=603000, episode_reward=-1653.40 +/- 49.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=604000, episode_reward=-1690.20 +/- 38.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=605000, episode_reward=-1668.40 +/- 54.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=606000, episode_reward=-1658.60 +/- 38.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=607000, episode_reward=-1725.40 +/- 35.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=608000, episode_reward=-1689.60 +/- 45.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=609000, episode_reward=-1677.60 +/- 31.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=610000, episode_reward=-1638.00 +/- 34.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=611000, episode_reward=-1654.40 +/- 50.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=612000, episode_reward=-1707.80 +/- 75.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=613000, episode_reward=-1664.20 +/- 6.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=614000, episode_reward=-1652.20 +/- 46.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=615000, episode_reward=-1692.00 +/- 18.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=616000, episode_reward=-1677.00 +/- 46.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=617000, episode_reward=-1664.00 +/- 46.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=618000, episode_reward=-1714.00 +/- 72.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=619000, episode_reward=-1679.20 +/- 14.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=620000, episode_reward=-1653.00 +/- 53.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=621000, episode_reward=-1665.00 +/- 52.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=622000, episode_reward=-1704.40 +/- 21.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=623000, episode_reward=-1667.40 +/- 54.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=624000, episode_reward=-1673.00 +/- 37.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=625000, episode_reward=-1674.20 +/- 29.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=626000, episode_reward=-1687.80 +/- 48.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=627000, episode_reward=-1671.20 +/- 31.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=628000, episode_reward=-1672.20 +/- 26.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=629000, episode_reward=-1717.60 +/- 20.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=630000, episode_reward=-1664.80 +/- 24.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=631000, episode_reward=-1680.60 +/- 47.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=632000, episode_reward=-1699.60 +/- 21.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=633000, episode_reward=-1660.20 +/- 32.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=634000, episode_reward=-1683.00 +/- 16.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=635000, episode_reward=-1644.20 +/- 47.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=636000, episode_reward=-1692.40 +/- 21.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=637000, episode_reward=-1698.20 +/- 40.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=638000, episode_reward=-1694.60 +/- 22.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=639000, episode_reward=-1719.80 +/- 33.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=640000, episode_reward=-1699.20 +/- 10.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=641000, episode_reward=-1685.80 +/- 33.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=642000, episode_reward=-1688.60 +/- 40.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=643000, episode_reward=-1714.20 +/- 24.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=644000, episode_reward=-1721.60 +/- 37.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=645000, episode_reward=-1676.40 +/- 26.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=646000, episode_reward=-1684.00 +/- 23.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=647000, episode_reward=-1665.60 +/- 49.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=648000, episode_reward=-1665.80 +/- 44.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=649000, episode_reward=-1685.40 +/- 30.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=650000, episode_reward=-1673.80 +/- 55.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=651000, episode_reward=-1665.80 +/- 24.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=652000, episode_reward=-1679.20 +/- 54.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=653000, episode_reward=-1678.20 +/- 49.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=654000, episode_reward=-1663.00 +/- 37.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=655000, episode_reward=-1669.20 +/- 20.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=656000, episode_reward=-1692.60 +/- 22.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=657000, episode_reward=-1696.40 +/- 7.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=658000, episode_reward=-1675.60 +/- 29.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=659000, episode_reward=-1696.00 +/- 31.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=660000, episode_reward=-1679.00 +/- 29.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=661000, episode_reward=-1643.60 +/- 34.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=662000, episode_reward=-1658.40 +/- 20.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=663000, episode_reward=-1676.40 +/- 43.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=664000, episode_reward=-1666.20 +/- 32.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=665000, episode_reward=-1711.80 +/- 12.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=666000, episode_reward=-1723.20 +/- 30.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=667000, episode_reward=-1656.20 +/- 45.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=668000, episode_reward=-1686.60 +/- 53.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=669000, episode_reward=-1676.40 +/- 39.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=670000, episode_reward=-1714.00 +/- 48.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=671000, episode_reward=-1675.80 +/- 54.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=672000, episode_reward=-1674.40 +/- 51.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=673000, episode_reward=-1703.40 +/- 27.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=674000, episode_reward=-1666.60 +/- 48.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=675000, episode_reward=-1664.40 +/- 20.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=676000, episode_reward=-1669.60 +/- 33.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=677000, episode_reward=-1706.20 +/- 38.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=678000, episode_reward=-1653.00 +/- 18.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=679000, episode_reward=-1615.80 +/- 52.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=680000, episode_reward=-1691.80 +/- 38.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=681000, episode_reward=-1672.40 +/- 36.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=682000, episode_reward=-1705.80 +/- 28.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=683000, episode_reward=-1663.80 +/- 16.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=684000, episode_reward=-1678.80 +/- 51.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=685000, episode_reward=-1672.20 +/- 36.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=686000, episode_reward=-1712.60 +/- 26.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=687000, episode_reward=-1645.40 +/- 40.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=688000, episode_reward=-1671.80 +/- 57.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=689000, episode_reward=-1709.40 +/- 36.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=690000, episode_reward=-1686.60 +/- 35.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=691000, episode_reward=-1662.20 +/- 55.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=692000, episode_reward=-1684.20 +/- 69.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=693000, episode_reward=-1667.60 +/- 64.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=694000, episode_reward=-1665.00 +/- 37.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=695000, episode_reward=-1654.80 +/- 47.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=696000, episode_reward=-1686.60 +/- 39.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=697000, episode_reward=-1688.60 +/- 40.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=698000, episode_reward=-1704.20 +/- 36.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=699000, episode_reward=-1664.40 +/- 29.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=700000, episode_reward=-1682.60 +/- 53.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=701000, episode_reward=-1661.40 +/- 58.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=702000, episode_reward=-1688.80 +/- 19.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=703000, episode_reward=-1709.60 +/- 38.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=704000, episode_reward=-1699.60 +/- 30.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=705000, episode_reward=-1634.80 +/- 22.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=706000, episode_reward=-1689.00 +/- 31.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=707000, episode_reward=-1677.40 +/- 34.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=708000, episode_reward=-1721.80 +/- 30.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=709000, episode_reward=-1670.80 +/- 70.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=710000, episode_reward=-1683.20 +/- 43.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=711000, episode_reward=-1691.40 +/- 58.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=712000, episode_reward=-1671.80 +/- 55.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=713000, episode_reward=-1683.80 +/- 26.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=714000, episode_reward=-1693.40 +/- 30.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=715000, episode_reward=-1699.80 +/- 40.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=716000, episode_reward=-1618.20 +/- 62.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=717000, episode_reward=-1700.60 +/- 41.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=718000, episode_reward=-1716.40 +/- 49.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=719000, episode_reward=-1678.80 +/- 45.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=720000, episode_reward=-1665.40 +/- 36.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=721000, episode_reward=-1700.40 +/- 40.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=722000, episode_reward=-1656.80 +/- 21.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=723000, episode_reward=-1693.00 +/- 32.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=724000, episode_reward=-1682.60 +/- 18.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=725000, episode_reward=-1677.60 +/- 61.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=726000, episode_reward=-1670.80 +/- 51.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=727000, episode_reward=-1674.60 +/- 16.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=728000, episode_reward=-1663.80 +/- 37.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=729000, episode_reward=-1683.60 +/- 40.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=730000, episode_reward=-1684.00 +/- 33.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=731000, episode_reward=-1705.00 +/- 22.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=732000, episode_reward=-1679.20 +/- 36.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=733000, episode_reward=-1667.00 +/- 23.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=734000, episode_reward=-1664.60 +/- 40.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=735000, episode_reward=-1698.20 +/- 37.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=736000, episode_reward=-1685.40 +/- 32.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=737000, episode_reward=-1651.60 +/- 53.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=738000, episode_reward=-1656.00 +/- 45.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=739000, episode_reward=-1655.60 +/- 53.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=740000, episode_reward=-1699.40 +/- 46.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=741000, episode_reward=-1665.20 +/- 47.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=742000, episode_reward=-1611.80 +/- 11.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=743000, episode_reward=-1642.40 +/- 53.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=744000, episode_reward=-1702.80 +/- 42.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=745000, episode_reward=-1682.60 +/- 20.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=746000, episode_reward=-1679.40 +/- 29.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=747000, episode_reward=-1688.20 +/- 30.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=748000, episode_reward=-1681.80 +/- 9.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=749000, episode_reward=-1673.80 +/- 28.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=750000, episode_reward=-1684.40 +/- 23.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=751000, episode_reward=-1685.40 +/- 35.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=752000, episode_reward=-1695.60 +/- 34.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=753000, episode_reward=-1670.80 +/- 41.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=754000, episode_reward=-1694.20 +/- 43.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=755000, episode_reward=-1671.60 +/- 44.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=756000, episode_reward=-1699.80 +/- 18.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=757000, episode_reward=-1674.80 +/- 40.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=758000, episode_reward=-1543.40 +/- 255.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=759000, episode_reward=-1409.60 +/- 291.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=760000, episode_reward=-1696.00 +/- 28.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=761000, episode_reward=-1684.60 +/- 30.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=762000, episode_reward=-1672.40 +/- 64.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=763000, episode_reward=-1532.40 +/- 264.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=764000, episode_reward=-1643.60 +/- 44.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=765000, episode_reward=-1682.00 +/- 51.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=766000, episode_reward=-1691.80 +/- 44.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=767000, episode_reward=-1655.80 +/- 37.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=768000, episode_reward=-1670.00 +/- 28.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=769000, episode_reward=-1699.20 +/- 43.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=770000, episode_reward=-1708.40 +/- 34.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=771000, episode_reward=-1716.00 +/- 37.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=772000, episode_reward=-1661.60 +/- 34.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=773000, episode_reward=-1674.80 +/- 35.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=774000, episode_reward=-1539.00 +/- 293.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=775000, episode_reward=-1421.60 +/- 320.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=776000, episode_reward=-1400.80 +/- 343.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=777000, episode_reward=-1681.00 +/- 22.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=778000, episode_reward=-1688.60 +/- 31.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=779000, episode_reward=-1685.60 +/- 46.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=780000, episode_reward=-1680.80 +/- 55.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=781000, episode_reward=-1699.20 +/- 27.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=782000, episode_reward=-1681.80 +/- 23.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=783000, episode_reward=-1702.60 +/- 11.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=784000, episode_reward=-1530.80 +/- 297.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=785000, episode_reward=-1684.80 +/- 23.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=786000, episode_reward=-1561.20 +/- 274.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=787000, episode_reward=-1676.80 +/- 66.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=788000, episode_reward=-1562.20 +/- 309.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=789000, episode_reward=-1688.20 +/- 54.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=790000, episode_reward=-1704.20 +/- 22.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=791000, episode_reward=-1690.40 +/- 36.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=792000, episode_reward=-1706.40 +/- 34.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=793000, episode_reward=-1670.60 +/- 38.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=794000, episode_reward=-1637.80 +/- 34.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=795000, episode_reward=-1662.40 +/- 46.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=796000, episode_reward=-1665.00 +/- 20.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=797000, episode_reward=-1645.60 +/- 34.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=798000, episode_reward=-1549.80 +/- 270.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=799000, episode_reward=-1559.00 +/- 294.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=800000, episode_reward=-1539.00 +/- 303.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=801000, episode_reward=-1669.20 +/- 33.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=802000, episode_reward=-1672.40 +/- 22.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=803000, episode_reward=-1731.40 +/- 50.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=804000, episode_reward=-1661.40 +/- 59.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=805000, episode_reward=-1683.60 +/- 35.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=806000, episode_reward=-1675.20 +/- 35.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=807000, episode_reward=-1724.20 +/- 43.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=808000, episode_reward=-1690.60 +/- 29.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=809000, episode_reward=-1704.20 +/- 30.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=810000, episode_reward=-1688.80 +/- 27.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=811000, episode_reward=-1669.80 +/- 34.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=812000, episode_reward=-1675.60 +/- 84.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=813000, episode_reward=-1669.00 +/- 40.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=814000, episode_reward=-1694.60 +/- 40.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=815000, episode_reward=-1699.60 +/- 51.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=816000, episode_reward=-1667.00 +/- 18.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=817000, episode_reward=-1679.60 +/- 60.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=818000, episode_reward=-1730.60 +/- 30.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=819000, episode_reward=-1665.20 +/- 29.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=820000, episode_reward=-1709.80 +/- 70.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=821000, episode_reward=-1710.80 +/- 24.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=822000, episode_reward=-1655.80 +/- 42.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=823000, episode_reward=-1688.80 +/- 29.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=824000, episode_reward=-1665.60 +/- 76.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=825000, episode_reward=-1653.40 +/- 30.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=826000, episode_reward=-1696.60 +/- 26.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=827000, episode_reward=-1714.20 +/- 51.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=828000, episode_reward=-1681.80 +/- 37.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=829000, episode_reward=-1679.40 +/- 30.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=830000, episode_reward=-1677.40 +/- 39.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=831000, episode_reward=-1658.80 +/- 54.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=832000, episode_reward=-1554.60 +/- 284.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=833000, episode_reward=-1414.40 +/- 380.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=834000, episode_reward=-1671.80 +/- 30.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=835000, episode_reward=-1672.20 +/- 55.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=836000, episode_reward=-1664.40 +/- 42.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=837000, episode_reward=-1659.20 +/- 43.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=838000, episode_reward=-1677.60 +/- 44.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=839000, episode_reward=-1685.60 +/- 31.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=840000, episode_reward=-1698.60 +/- 33.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=841000, episode_reward=-1650.20 +/- 55.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=842000, episode_reward=-1658.80 +/- 64.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=843000, episode_reward=-1691.00 +/- 43.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=844000, episode_reward=-1714.60 +/- 67.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=845000, episode_reward=-1701.80 +/- 28.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=846000, episode_reward=-1664.00 +/- 37.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=847000, episode_reward=-1691.60 +/- 39.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=848000, episode_reward=-1663.00 +/- 39.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=849000, episode_reward=-1679.80 +/- 65.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=850000, episode_reward=-1668.60 +/- 29.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=851000, episode_reward=-1676.00 +/- 56.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=852000, episode_reward=-1666.00 +/- 34.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=853000, episode_reward=-1386.60 +/- 333.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=854000, episode_reward=-1674.00 +/- 69.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=855000, episode_reward=-1567.40 +/- 273.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=856000, episode_reward=-1745.80 +/- 24.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=857000, episode_reward=-1707.40 +/- 77.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=858000, episode_reward=-1553.00 +/- 284.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=859000, episode_reward=-1667.00 +/- 32.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=860000, episode_reward=-1527.00 +/- 275.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=861000, episode_reward=-1698.60 +/- 37.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=862000, episode_reward=-1669.80 +/- 51.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=863000, episode_reward=-1635.00 +/- 50.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=864000, episode_reward=-1562.00 +/- 280.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=865000, episode_reward=-1635.80 +/- 70.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=866000, episode_reward=-1561.20 +/- 253.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=867000, episode_reward=-1725.80 +/- 66.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=868000, episode_reward=-1534.60 +/- 255.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=869000, episode_reward=-1690.80 +/- 21.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=870000, episode_reward=-1671.60 +/- 78.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=871000, episode_reward=-1653.40 +/- 33.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=872000, episode_reward=-1673.40 +/- 41.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=873000, episode_reward=-1698.20 +/- 52.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=874000, episode_reward=-1681.60 +/- 40.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=875000, episode_reward=-1723.60 +/- 49.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=876000, episode_reward=-1678.40 +/- 30.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=877000, episode_reward=-1667.80 +/- 46.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=878000, episode_reward=-1561.80 +/- 310.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=879000, episode_reward=-1688.80 +/- 39.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=880000, episode_reward=-1695.40 +/- 36.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=881000, episode_reward=-1685.20 +/- 53.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=882000, episode_reward=-1558.60 +/- 274.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=883000, episode_reward=-1671.20 +/- 64.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=884000, episode_reward=-1713.40 +/- 26.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=885000, episode_reward=-1661.80 +/- 35.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=886000, episode_reward=-1545.00 +/- 287.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=887000, episode_reward=-1378.60 +/- 307.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=888000, episode_reward=-1531.80 +/- 276.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=889000, episode_reward=-1682.40 +/- 19.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=890000, episode_reward=-1704.40 +/- 33.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=891000, episode_reward=-1695.80 +/- 37.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=892000, episode_reward=-1689.00 +/- 38.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=893000, episode_reward=-1681.40 +/- 58.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=894000, episode_reward=-1692.20 +/- 32.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=895000, episode_reward=-1706.80 +/- 67.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=896000, episode_reward=-1671.40 +/- 32.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=897000, episode_reward=-1681.80 +/- 33.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=898000, episode_reward=-1664.00 +/- 15.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=899000, episode_reward=-1649.60 +/- 37.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=900000, episode_reward=-1524.60 +/- 282.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=901000, episode_reward=-1671.20 +/- 34.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=902000, episode_reward=-1695.00 +/- 48.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=903000, episode_reward=-1700.80 +/- 36.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=904000, episode_reward=-1677.60 +/- 66.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=905000, episode_reward=-1696.40 +/- 25.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=906000, episode_reward=-1681.80 +/- 70.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=907000, episode_reward=-1678.80 +/- 20.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=908000, episode_reward=-1676.20 +/- 40.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=909000, episode_reward=-1662.80 +/- 30.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=910000, episode_reward=-1521.80 +/- 253.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=911000, episode_reward=-1698.20 +/- 20.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=912000, episode_reward=-1676.20 +/- 42.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=913000, episode_reward=-1690.20 +/- 30.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=914000, episode_reward=-1689.00 +/- 37.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=915000, episode_reward=-1684.80 +/- 33.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=916000, episode_reward=-1651.20 +/- 51.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=917000, episode_reward=-1689.40 +/- 50.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=918000, episode_reward=-1658.80 +/- 29.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=919000, episode_reward=-1639.60 +/- 42.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=920000, episode_reward=-1654.40 +/- 48.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=921000, episode_reward=-1672.60 +/- 30.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=922000, episode_reward=-1711.20 +/- 35.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=923000, episode_reward=-1635.80 +/- 29.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=924000, episode_reward=-1691.40 +/- 43.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=925000, episode_reward=-1688.00 +/- 34.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=926000, episode_reward=-1675.20 +/- 36.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=927000, episode_reward=-1707.00 +/- 26.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=928000, episode_reward=-1667.40 +/- 45.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=929000, episode_reward=-1658.60 +/- 61.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=930000, episode_reward=-1665.40 +/- 19.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=931000, episode_reward=-1676.80 +/- 52.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=932000, episode_reward=-1694.60 +/- 27.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=933000, episode_reward=-1692.00 +/- 29.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=934000, episode_reward=-1713.00 +/- 29.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=935000, episode_reward=-1705.20 +/- 45.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=936000, episode_reward=-1566.20 +/- 279.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=937000, episode_reward=-1582.40 +/- 290.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=938000, episode_reward=-1705.40 +/- 48.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=939000, episode_reward=-1545.80 +/- 328.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=940000, episode_reward=-1651.60 +/- 61.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=941000, episode_reward=-1736.60 +/- 36.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=942000, episode_reward=-1572.40 +/- 334.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=943000, episode_reward=-1527.60 +/- 278.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=944000, episode_reward=-1491.00 +/- 325.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=945000, episode_reward=-1536.80 +/- 308.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=946000, episode_reward=-1510.20 +/- 272.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=947000, episode_reward=-1560.40 +/- 305.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=948000, episode_reward=-1548.40 +/- 317.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=949000, episode_reward=-1674.40 +/- 67.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=950000, episode_reward=-1697.40 +/- 32.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=951000, episode_reward=-1543.60 +/- 292.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=952000, episode_reward=-1713.80 +/- 57.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=953000, episode_reward=-1682.00 +/- 16.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=954000, episode_reward=-1656.40 +/- 23.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=955000, episode_reward=-1712.00 +/- 53.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=956000, episode_reward=-1698.40 +/- 31.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=957000, episode_reward=-1672.40 +/- 21.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=958000, episode_reward=-1719.20 +/- 25.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=959000, episode_reward=-1689.00 +/- 36.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=960000, episode_reward=-1587.00 +/- 282.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=961000, episode_reward=-1686.80 +/- 49.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=962000, episode_reward=-1685.00 +/- 55.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=963000, episode_reward=-1658.20 +/- 24.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=964000, episode_reward=-1690.00 +/- 33.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=965000, episode_reward=-1520.20 +/- 282.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=966000, episode_reward=-1732.80 +/- 13.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=967000, episode_reward=-1667.60 +/- 45.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=968000, episode_reward=-1686.00 +/- 17.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=969000, episode_reward=-1668.20 +/- 28.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=970000, episode_reward=-1551.80 +/- 262.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=971000, episode_reward=-1510.40 +/- 241.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=972000, episode_reward=-1691.00 +/- 32.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=973000, episode_reward=-1698.00 +/- 41.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=974000, episode_reward=-1677.40 +/- 60.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=975000, episode_reward=-1691.00 +/- 38.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=976000, episode_reward=-1627.60 +/- 20.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=977000, episode_reward=-1705.40 +/- 41.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=978000, episode_reward=-1670.40 +/- 48.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=979000, episode_reward=-1734.00 +/- 27.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=980000, episode_reward=-1675.40 +/- 46.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=981000, episode_reward=-1401.20 +/- 320.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=982000, episode_reward=-1687.80 +/- 15.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=983000, episode_reward=-1675.60 +/- 39.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=984000, episode_reward=-1663.60 +/- 44.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=985000, episode_reward=-1634.00 +/- 46.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=986000, episode_reward=-1700.00 +/- 51.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=987000, episode_reward=-1709.20 +/- 41.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=988000, episode_reward=-1539.60 +/- 281.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=989000, episode_reward=-1661.20 +/- 28.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=990000, episode_reward=-1551.00 +/- 308.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=991000, episode_reward=-1639.80 +/- 21.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=992000, episode_reward=-1527.40 +/- 315.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=993000, episode_reward=-1663.80 +/- 38.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=994000, episode_reward=-1524.80 +/- 288.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=995000, episode_reward=-1667.60 +/- 25.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=996000, episode_reward=-1695.60 +/- 57.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=997000, episode_reward=-1681.20 +/- 55.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=998000, episode_reward=-1701.20 +/- 41.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=999000, episode_reward=-1693.40 +/- 49.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1000000, episode_reward=-1645.80 +/- 45.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1001000, episode_reward=-1676.60 +/- 13.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1002000, episode_reward=-1527.80 +/- 291.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1003000, episode_reward=-1641.60 +/- 52.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1004000, episode_reward=-1697.40 +/- 27.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1005000, episode_reward=-1679.80 +/- 36.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1006000, episode_reward=-1707.60 +/- 39.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1007000, episode_reward=-1700.40 +/- 81.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1008000, episode_reward=-1678.40 +/- 63.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1009000, episode_reward=-1507.80 +/- 296.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1010000, episode_reward=-1535.20 +/- 290.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1011000, episode_reward=-1689.00 +/- 39.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1012000, episode_reward=-1649.20 +/- 29.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1013000, episode_reward=-1561.00 +/- 290.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1014000, episode_reward=-1664.60 +/- 31.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1015000, episode_reward=-1684.00 +/- 52.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1016000, episode_reward=-1662.00 +/- 52.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1017000, episode_reward=-1533.80 +/- 301.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1018000, episode_reward=-1667.60 +/- 33.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1019000, episode_reward=-1681.80 +/- 54.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1020000, episode_reward=-1510.20 +/- 298.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1021000, episode_reward=-1665.80 +/- 60.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1022000, episode_reward=-1695.80 +/- 39.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1023000, episode_reward=-1666.80 +/- 50.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1024000, episode_reward=-1683.40 +/- 33.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1025000, episode_reward=-1677.20 +/- 61.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1026000, episode_reward=-1526.40 +/- 281.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1027000, episode_reward=-1672.00 +/- 38.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1028000, episode_reward=-1707.00 +/- 22.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1029000, episode_reward=-1521.20 +/- 288.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1030000, episode_reward=-1571.20 +/- 303.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1031000, episode_reward=-1696.00 +/- 26.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1032000, episode_reward=-1670.80 +/- 25.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1033000, episode_reward=-1411.20 +/- 324.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1034000, episode_reward=-1699.40 +/- 49.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1035000, episode_reward=-1691.60 +/- 45.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1036000, episode_reward=-1675.80 +/- 44.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1037000, episode_reward=-1534.40 +/- 296.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1038000, episode_reward=-1665.80 +/- 58.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1039000, episode_reward=-1518.20 +/- 287.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1040000, episode_reward=-1507.40 +/- 298.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1041000, episode_reward=-1518.20 +/- 263.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1042000, episode_reward=-1517.00 +/- 297.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1043000, episode_reward=-1694.60 +/- 55.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1044000, episode_reward=-1668.20 +/- 46.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1045000, episode_reward=-1521.00 +/- 311.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1046000, episode_reward=-1694.20 +/- 65.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1047000, episode_reward=-1716.20 +/- 19.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1048000, episode_reward=-1651.60 +/- 62.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1049000, episode_reward=-1435.00 +/- 338.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1050000, episode_reward=-1676.60 +/- 55.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1051000, episode_reward=-1559.80 +/- 274.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1052000, episode_reward=-1683.80 +/- 43.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1053000, episode_reward=-1696.00 +/- 38.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1054000, episode_reward=-1515.80 +/- 253.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1055000, episode_reward=-1670.60 +/- 18.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1056000, episode_reward=-1702.80 +/- 21.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1057000, episode_reward=-1735.20 +/- 34.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1058000, episode_reward=-1670.00 +/- 29.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1059000, episode_reward=-1688.80 +/- 49.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1060000, episode_reward=-1709.80 +/- 40.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1061000, episode_reward=-1560.60 +/- 288.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1062000, episode_reward=-1673.80 +/- 26.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1063000, episode_reward=-1549.80 +/- 287.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1064000, episode_reward=-1673.40 +/- 53.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1065000, episode_reward=-1679.20 +/- 30.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1066000, episode_reward=-1399.20 +/- 346.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1067000, episode_reward=-1707.20 +/- 39.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1068000, episode_reward=-1548.60 +/- 264.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1069000, episode_reward=-1549.20 +/- 297.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1070000, episode_reward=-1565.00 +/- 289.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1071000, episode_reward=-1551.60 +/- 274.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1072000, episode_reward=-1545.60 +/- 298.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1073000, episode_reward=-1654.40 +/- 35.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1074000, episode_reward=-1674.80 +/- 75.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1075000, episode_reward=-1540.00 +/- 305.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1076000, episode_reward=-1392.60 +/- 379.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1077000, episode_reward=-1686.60 +/- 58.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1078000, episode_reward=-1683.80 +/- 29.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1079000, episode_reward=-1665.20 +/- 60.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1080000, episode_reward=-1660.60 +/- 31.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1081000, episode_reward=-1716.40 +/- 30.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1082000, episode_reward=-1713.20 +/- 38.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1083000, episode_reward=-1636.00 +/- 34.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1084000, episode_reward=-1668.00 +/- 71.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1085000, episode_reward=-1671.40 +/- 82.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1086000, episode_reward=-1549.60 +/- 298.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1087000, episode_reward=-1561.60 +/- 269.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1088000, episode_reward=-1662.40 +/- 35.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1089000, episode_reward=-1698.00 +/- 35.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1090000, episode_reward=-1670.40 +/- 53.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1091000, episode_reward=-1684.60 +/- 23.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1092000, episode_reward=-1718.60 +/- 28.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1093000, episode_reward=-1415.80 +/- 339.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1094000, episode_reward=-1371.40 +/- 363.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1095000, episode_reward=-1551.00 +/- 296.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1096000, episode_reward=-1496.00 +/- 285.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1097000, episode_reward=-1549.40 +/- 252.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1098000, episode_reward=-1674.40 +/- 54.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1099000, episode_reward=-1579.60 +/- 292.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1100000, episode_reward=-1681.60 +/- 80.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1101000, episode_reward=-1532.40 +/- 274.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1102000, episode_reward=-1684.80 +/- 44.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1103000, episode_reward=-1604.60 +/- 311.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1104000, episode_reward=-1427.40 +/- 351.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1105000, episode_reward=-1556.60 +/- 314.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1106000, episode_reward=-1701.20 +/- 25.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1107000, episode_reward=-1716.20 +/- 27.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1108000, episode_reward=-1667.20 +/- 33.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1109000, episode_reward=-1686.20 +/- 46.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1110000, episode_reward=-1678.40 +/- 70.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1111000, episode_reward=-1714.20 +/- 39.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1112000, episode_reward=-1681.80 +/- 24.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1113000, episode_reward=-1696.40 +/- 23.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1114000, episode_reward=-1655.80 +/- 32.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1115000, episode_reward=-1664.20 +/- 53.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1116000, episode_reward=-1666.80 +/- 54.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1117000, episode_reward=-1580.00 +/- 274.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1118000, episode_reward=-1419.60 +/- 378.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1119000, episode_reward=-1532.40 +/- 280.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1120000, episode_reward=-1679.20 +/- 32.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1121000, episode_reward=-1565.20 +/- 300.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1122000, episode_reward=-1694.80 +/- 49.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1123000, episode_reward=-1541.40 +/- 298.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1124000, episode_reward=-1663.00 +/- 35.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1125000, episode_reward=-1729.00 +/- 34.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1126000, episode_reward=-1521.80 +/- 297.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1127000, episode_reward=-1686.80 +/- 36.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1128000, episode_reward=-1555.00 +/- 291.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1129000, episode_reward=-1516.20 +/- 282.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1130000, episode_reward=-1719.80 +/- 26.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1131000, episode_reward=-1674.60 +/- 40.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1132000, episode_reward=-1643.60 +/- 48.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1133000, episode_reward=-1743.20 +/- 68.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1134000, episode_reward=-1645.80 +/- 82.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1135000, episode_reward=-1695.40 +/- 34.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1136000, episode_reward=-1414.40 +/- 295.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1137000, episode_reward=-1687.20 +/- 24.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1138000, episode_reward=-1655.00 +/- 47.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1139000, episode_reward=-1660.20 +/- 30.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1140000, episode_reward=-1427.20 +/- 327.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1141000, episode_reward=-1685.00 +/- 36.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1142000, episode_reward=-1534.20 +/- 266.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1143000, episode_reward=-1670.20 +/- 45.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1144000, episode_reward=-1735.60 +/- 59.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1145000, episode_reward=-1682.40 +/- 48.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1146000, episode_reward=-1689.40 +/- 28.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1147000, episode_reward=-1696.00 +/- 48.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1148000, episode_reward=-1571.80 +/- 230.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1149000, episode_reward=-1574.20 +/- 287.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1150000, episode_reward=-1483.40 +/- 290.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1151000, episode_reward=-1728.00 +/- 29.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1152000, episode_reward=-1694.00 +/- 53.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1153000, episode_reward=-1633.40 +/- 39.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1154000, episode_reward=-1560.00 +/- 281.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1155000, episode_reward=-1687.20 +/- 58.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1156000, episode_reward=-1697.80 +/- 39.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1157000, episode_reward=-1527.60 +/- 255.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1158000, episode_reward=-1665.60 +/- 32.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1159000, episode_reward=-1550.80 +/- 310.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1160000, episode_reward=-1685.00 +/- 54.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1161000, episode_reward=-1418.00 +/- 352.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1162000, episode_reward=-1636.00 +/- 41.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1163000, episode_reward=-1641.00 +/- 47.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1164000, episode_reward=-1642.40 +/- 56.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1165000, episode_reward=-1560.40 +/- 286.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1166000, episode_reward=-1710.60 +/- 23.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1167000, episode_reward=-1680.00 +/- 33.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1168000, episode_reward=-1537.00 +/- 293.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1169000, episode_reward=-1680.80 +/- 65.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1170000, episode_reward=-1543.80 +/- 303.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1171000, episode_reward=-1700.40 +/- 26.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1172000, episode_reward=-1681.60 +/- 43.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1173000, episode_reward=-1700.00 +/- 22.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1174000, episode_reward=-1691.20 +/- 48.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1175000, episode_reward=-1659.60 +/- 52.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1176000, episode_reward=-1645.60 +/- 34.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1177000, episode_reward=-1689.20 +/- 26.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1178000, episode_reward=-1695.80 +/- 12.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1179000, episode_reward=-1662.00 +/- 47.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1180000, episode_reward=-1692.40 +/- 46.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1181000, episode_reward=-1576.60 +/- 331.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1182000, episode_reward=-1708.20 +/- 55.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1183000, episode_reward=-1673.60 +/- 63.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1184000, episode_reward=-1674.00 +/- 35.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1185000, episode_reward=-1538.20 +/- 257.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1186000, episode_reward=-1676.40 +/- 45.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1187000, episode_reward=-1702.40 +/- 22.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1188000, episode_reward=-1536.60 +/- 276.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1189000, episode_reward=-1697.60 +/- 32.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1190000, episode_reward=-1547.80 +/- 282.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1191000, episode_reward=-1647.80 +/- 22.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1192000, episode_reward=-1668.40 +/- 34.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1193000, episode_reward=-1673.20 +/- 39.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1194000, episode_reward=-1672.80 +/- 53.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1195000, episode_reward=-1553.20 +/- 302.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1196000, episode_reward=-1666.20 +/- 44.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1197000, episode_reward=-1656.00 +/- 14.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1198000, episode_reward=-1700.20 +/- 33.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1199000, episode_reward=-1711.80 +/- 38.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1200000, episode_reward=-1671.60 +/- 19.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1201000, episode_reward=-1589.00 +/- 294.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1202000, episode_reward=-1700.00 +/- 45.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1203000, episode_reward=-1734.20 +/- 42.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1204000, episode_reward=-1663.40 +/- 40.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1205000, episode_reward=-1673.60 +/- 40.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1206000, episode_reward=-1530.00 +/- 328.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1207000, episode_reward=-1524.40 +/- 273.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1208000, episode_reward=-1679.20 +/- 41.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1209000, episode_reward=-1686.20 +/- 52.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1210000, episode_reward=-1377.40 +/- 359.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1211000, episode_reward=-1512.00 +/- 305.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1212000, episode_reward=-1599.20 +/- 308.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1213000, episode_reward=-1536.40 +/- 301.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1214000, episode_reward=-1575.20 +/- 319.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1215000, episode_reward=-1417.40 +/- 394.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1216000, episode_reward=-1754.60 +/- 23.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1217000, episode_reward=-1532.60 +/- 310.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1218000, episode_reward=-1533.20 +/- 269.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1219000, episode_reward=-1705.80 +/- 24.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1220000, episode_reward=-1693.00 +/- 55.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1221000, episode_reward=-1557.40 +/- 270.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1222000, episode_reward=-1590.00 +/- 318.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1223000, episode_reward=-1397.40 +/- 370.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1224000, episode_reward=-1161.40 +/- 290.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1225000, episode_reward=-977.00 +/- 24.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1226000, episode_reward=-1002.80 +/- 45.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1227000, episode_reward=-1268.60 +/- 366.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1228000, episode_reward=-1534.80 +/- 319.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1229000, episode_reward=-1705.40 +/- 62.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1230000, episode_reward=-1720.00 +/- 35.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1231000, episode_reward=-1532.20 +/- 242.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1232000, episode_reward=-1411.40 +/- 344.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1233000, episode_reward=-1434.00 +/- 368.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1234000, episode_reward=-1553.20 +/- 290.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1235000, episode_reward=-1412.60 +/- 339.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1236000, episode_reward=-1582.20 +/- 292.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1237000, episode_reward=-1270.80 +/- 385.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1238000, episode_reward=-1426.40 +/- 377.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1239000, episode_reward=-1529.60 +/- 315.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1240000, episode_reward=-1528.00 +/- 289.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1241000, episode_reward=-1679.00 +/- 51.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1242000, episode_reward=-1684.80 +/- 42.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1243000, episode_reward=-1564.40 +/- 275.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1244000, episode_reward=-1653.00 +/- 29.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1245000, episode_reward=-1534.80 +/- 288.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1246000, episode_reward=-1555.00 +/- 268.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1247000, episode_reward=-1523.40 +/- 304.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1248000, episode_reward=-1545.60 +/- 288.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1249000, episode_reward=-1410.60 +/- 359.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1250000, episode_reward=-1537.20 +/- 306.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1251000, episode_reward=-1536.00 +/- 274.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1252000, episode_reward=-1500.40 +/- 303.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1253000, episode_reward=-1526.20 +/- 283.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1254000, episode_reward=-1689.20 +/- 43.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1255000, episode_reward=-1558.80 +/- 291.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1256000, episode_reward=-1703.60 +/- 49.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1257000, episode_reward=-1687.60 +/- 41.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1258000, episode_reward=-1682.20 +/- 47.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1259000, episode_reward=-1561.20 +/- 266.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1260000, episode_reward=-1716.20 +/- 69.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1261000, episode_reward=-1681.40 +/- 32.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1262000, episode_reward=-1704.00 +/- 46.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1263000, episode_reward=-1629.60 +/- 22.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1264000, episode_reward=-1554.20 +/- 293.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1265000, episode_reward=-1686.80 +/- 48.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1266000, episode_reward=-1661.00 +/- 62.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1267000, episode_reward=-1679.00 +/- 24.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1268000, episode_reward=-1665.00 +/- 44.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1269000, episode_reward=-1559.80 +/- 323.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1270000, episode_reward=-1705.00 +/- 71.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1271000, episode_reward=-1676.80 +/- 33.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1272000, episode_reward=-1679.80 +/- 43.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1273000, episode_reward=-1731.20 +/- 21.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1274000, episode_reward=-1708.20 +/- 25.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1275000, episode_reward=-1557.20 +/- 285.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1276000, episode_reward=-1699.40 +/- 45.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1277000, episode_reward=-1691.20 +/- 37.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1278000, episode_reward=-1695.20 +/- 32.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1279000, episode_reward=-1550.20 +/- 284.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1280000, episode_reward=-1669.60 +/- 26.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1281000, episode_reward=-1690.00 +/- 22.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1282000, episode_reward=-1654.00 +/- 41.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1283000, episode_reward=-1673.40 +/- 51.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1284000, episode_reward=-1242.80 +/- 358.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1285000, episode_reward=-1684.20 +/- 45.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1286000, episode_reward=-1733.20 +/- 32.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1287000, episode_reward=-1381.80 +/- 342.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1288000, episode_reward=-1674.60 +/- 26.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1289000, episode_reward=-1569.60 +/- 268.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1290000, episode_reward=-1710.00 +/- 63.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1291000, episode_reward=-1683.40 +/- 24.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1292000, episode_reward=-1694.80 +/- 51.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1293000, episode_reward=-1526.20 +/- 276.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1294000, episode_reward=-1544.60 +/- 276.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1295000, episode_reward=-1525.60 +/- 281.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1296000, episode_reward=-1472.40 +/- 291.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1297000, episode_reward=-1691.40 +/- 40.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1298000, episode_reward=-1660.40 +/- 35.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1299000, episode_reward=-1674.80 +/- 50.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1300000, episode_reward=-1661.60 +/- 36.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1301000, episode_reward=-1691.20 +/- 37.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1302000, episode_reward=-1654.00 +/- 32.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1303000, episode_reward=-1550.00 +/- 288.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1304000, episode_reward=-1427.40 +/- 328.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1305000, episode_reward=-1524.00 +/- 314.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1306000, episode_reward=-1675.00 +/- 30.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1307000, episode_reward=-1717.00 +/- 54.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1308000, episode_reward=-1527.40 +/- 268.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1309000, episode_reward=-1684.80 +/- 69.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1310000, episode_reward=-1677.20 +/- 32.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1311000, episode_reward=-1698.80 +/- 19.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1312000, episode_reward=-1678.40 +/- 34.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1313000, episode_reward=-1670.60 +/- 34.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1314000, episode_reward=-1533.20 +/- 282.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1315000, episode_reward=-1691.00 +/- 11.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1316000, episode_reward=-1401.80 +/- 336.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1317000, episode_reward=-1402.20 +/- 335.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1318000, episode_reward=-1691.40 +/- 29.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1319000, episode_reward=-1523.20 +/- 268.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1320000, episode_reward=-1683.20 +/- 59.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1321000, episode_reward=-1693.60 +/- 41.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1322000, episode_reward=-1674.60 +/- 42.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1323000, episode_reward=-1637.00 +/- 35.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1324000, episode_reward=-1537.20 +/- 283.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1325000, episode_reward=-1539.80 +/- 284.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1326000, episode_reward=-1555.60 +/- 306.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1327000, episode_reward=-1376.80 +/- 385.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1328000, episode_reward=-1571.00 +/- 295.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1329000, episode_reward=-1554.60 +/- 296.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1330000, episode_reward=-1704.80 +/- 19.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1331000, episode_reward=-1671.40 +/- 69.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1332000, episode_reward=-1546.60 +/- 285.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1333000, episode_reward=-1701.80 +/- 33.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1334000, episode_reward=-1665.00 +/- 29.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1335000, episode_reward=-1414.00 +/- 352.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1336000, episode_reward=-1549.00 +/- 312.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1337000, episode_reward=-1697.60 +/- 39.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1338000, episode_reward=-1706.00 +/- 22.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1339000, episode_reward=-1372.40 +/- 350.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1340000, episode_reward=-1412.00 +/- 341.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1341000, episode_reward=-1647.80 +/- 33.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1342000, episode_reward=-1652.40 +/- 29.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1343000, episode_reward=-1532.60 +/- 272.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1344000, episode_reward=-1535.80 +/- 305.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1345000, episode_reward=-1514.60 +/- 270.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1346000, episode_reward=-1538.20 +/- 317.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1347000, episode_reward=-1706.80 +/- 22.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1348000, episode_reward=-1652.20 +/- 30.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1349000, episode_reward=-1701.20 +/- 48.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1350000, episode_reward=-1495.00 +/- 280.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1351000, episode_reward=-1717.00 +/- 65.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1352000, episode_reward=-1520.60 +/- 299.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1353000, episode_reward=-1561.60 +/- 301.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1354000, episode_reward=-1416.40 +/- 331.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1355000, episode_reward=-1539.60 +/- 311.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1356000, episode_reward=-1697.40 +/- 77.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1357000, episode_reward=-1431.00 +/- 336.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1358000, episode_reward=-1547.60 +/- 281.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1359000, episode_reward=-1724.00 +/- 48.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1360000, episode_reward=-1687.80 +/- 29.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1361000, episode_reward=-1691.40 +/- 44.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1362000, episode_reward=-1655.80 +/- 32.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1363000, episode_reward=-1557.80 +/- 306.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1364000, episode_reward=-1678.00 +/- 43.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1365000, episode_reward=-1286.20 +/- 346.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1366000, episode_reward=-1611.00 +/- 255.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1367000, episode_reward=-1693.00 +/- 22.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1368000, episode_reward=-1416.00 +/- 361.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1369000, episode_reward=-1694.60 +/- 64.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1370000, episode_reward=-1563.00 +/- 299.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1371000, episode_reward=-1383.00 +/- 343.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1372000, episode_reward=-1457.80 +/- 351.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1373000, episode_reward=-1705.00 +/- 32.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1374000, episode_reward=-1439.80 +/- 378.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1375000, episode_reward=-1503.60 +/- 287.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1376000, episode_reward=-1652.40 +/- 23.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1377000, episode_reward=-1696.40 +/- 72.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1378000, episode_reward=-1451.20 +/- 351.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1379000, episode_reward=-1225.20 +/- 371.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1380000, episode_reward=-1581.60 +/- 292.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1381000, episode_reward=-1565.40 +/- 274.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1382000, episode_reward=-1676.80 +/- 38.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1383000, episode_reward=-1534.40 +/- 257.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1384000, episode_reward=-1561.00 +/- 303.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1385000, episode_reward=-1465.40 +/- 363.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1386000, episode_reward=-1537.20 +/- 305.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1387000, episode_reward=-1685.80 +/- 37.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1388000, episode_reward=-1579.80 +/- 271.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1389000, episode_reward=-1603.20 +/- 294.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1390000, episode_reward=-1399.00 +/- 352.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1391000, episode_reward=-1408.60 +/- 344.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1392000, episode_reward=-1405.40 +/- 371.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1393000, episode_reward=-1684.40 +/- 39.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1394000, episode_reward=-1701.00 +/- 35.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1395000, episode_reward=-1679.60 +/- 46.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1396000, episode_reward=-1256.20 +/- 345.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1397000, episode_reward=-1373.00 +/- 350.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1398000, episode_reward=-1277.80 +/- 352.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1399000, episode_reward=-1408.60 +/- 347.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1400000, episode_reward=-1574.20 +/- 282.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1401000, episode_reward=-1686.80 +/- 33.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1402000, episode_reward=-1662.20 +/- 78.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1403000, episode_reward=-1422.40 +/- 353.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1404000, episode_reward=-1530.20 +/- 266.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1405000, episode_reward=-1651.00 +/- 43.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1406000, episode_reward=-1706.40 +/- 23.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1407000, episode_reward=-1585.60 +/- 306.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1408000, episode_reward=-1573.60 +/- 334.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1409000, episode_reward=-1572.40 +/- 261.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1410000, episode_reward=-1440.80 +/- 336.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1411000, episode_reward=-1711.00 +/- 35.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1412000, episode_reward=-1250.20 +/- 346.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1413000, episode_reward=-1565.00 +/- 295.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1414000, episode_reward=-1425.80 +/- 396.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1415000, episode_reward=-1454.00 +/- 385.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1416000, episode_reward=-1432.20 +/- 385.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1417000, episode_reward=-1591.40 +/- 285.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1418000, episode_reward=-1287.00 +/- 369.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1419000, episode_reward=-1591.80 +/- 307.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1420000, episode_reward=-1557.40 +/- 297.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1421000, episode_reward=-1370.40 +/- 372.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1422000, episode_reward=-1438.80 +/- 359.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1423000, episode_reward=-1639.20 +/- 37.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1424000, episode_reward=-1728.60 +/- 55.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1425000, episode_reward=-1687.40 +/- 55.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1426000, episode_reward=-1261.40 +/- 323.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1427000, episode_reward=-1552.40 +/- 261.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1428000, episode_reward=-1548.60 +/- 283.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1429000, episode_reward=-1547.60 +/- 320.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1430000, episode_reward=-1698.80 +/- 38.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1431000, episode_reward=-1703.00 +/- 51.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1432000, episode_reward=-1727.00 +/- 34.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1433000, episode_reward=-1533.80 +/- 289.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1434000, episode_reward=-1708.00 +/- 39.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1435000, episode_reward=-1696.20 +/- 40.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1436000, episode_reward=-1579.20 +/- 339.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1437000, episode_reward=-1682.20 +/- 62.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1438000, episode_reward=-1725.80 +/- 30.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1439000, episode_reward=-1293.40 +/- 362.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1440000, episode_reward=-1298.20 +/- 348.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1441000, episode_reward=-1709.00 +/- 67.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1442000, episode_reward=-1393.80 +/- 367.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1443000, episode_reward=-1409.00 +/- 315.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1444000, episode_reward=-1542.40 +/- 303.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1445000, episode_reward=-1577.60 +/- 259.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1446000, episode_reward=-1728.20 +/- 46.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1447000, episode_reward=-1547.40 +/- 315.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1448000, episode_reward=-1725.20 +/- 31.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1449000, episode_reward=-1682.80 +/- 67.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1450000, episode_reward=-1538.40 +/- 305.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1451000, episode_reward=-1700.20 +/- 38.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1452000, episode_reward=-1622.60 +/- 45.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1453000, episode_reward=-1525.80 +/- 286.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1454000, episode_reward=-1437.40 +/- 345.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1455000, episode_reward=-1720.20 +/- 49.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1456000, episode_reward=-1658.20 +/- 59.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1457000, episode_reward=-1518.40 +/- 281.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1458000, episode_reward=-1668.60 +/- 36.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1459000, episode_reward=-1545.20 +/- 273.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1460000, episode_reward=-1656.20 +/- 22.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1461000, episode_reward=-1681.40 +/- 47.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1462000, episode_reward=-1710.20 +/- 26.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1463000, episode_reward=-1420.20 +/- 360.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1464000, episode_reward=-1667.60 +/- 85.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1465000, episode_reward=-1693.40 +/- 63.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1466000, episode_reward=-1568.00 +/- 281.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1467000, episode_reward=-1605.60 +/- 264.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1468000, episode_reward=-1705.60 +/- 45.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1469000, episode_reward=-1543.80 +/- 291.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1470000, episode_reward=-1549.40 +/- 286.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1471000, episode_reward=-1672.60 +/- 46.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1472000, episode_reward=-1702.20 +/- 43.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1473000, episode_reward=-1546.20 +/- 256.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1474000, episode_reward=-1724.60 +/- 56.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1475000, episode_reward=-1695.60 +/- 53.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1476000, episode_reward=-1540.80 +/- 294.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1477000, episode_reward=-1535.00 +/- 304.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1478000, episode_reward=-1669.00 +/- 35.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1479000, episode_reward=-1733.60 +/- 37.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1480000, episode_reward=-1668.00 +/- 34.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1481000, episode_reward=-1723.20 +/- 39.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1482000, episode_reward=-1541.80 +/- 353.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1483000, episode_reward=-1685.80 +/- 20.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1484000, episode_reward=-1557.60 +/- 274.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1485000, episode_reward=-1691.40 +/- 32.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1486000, episode_reward=-1562.60 +/- 278.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1487000, episode_reward=-1416.60 +/- 346.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1488000, episode_reward=-1565.40 +/- 245.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1489000, episode_reward=-1692.20 +/- 58.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1490000, episode_reward=-1561.00 +/- 321.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1491000, episode_reward=-1667.20 +/- 29.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1492000, episode_reward=-1550.40 +/- 322.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1493000, episode_reward=-1688.20 +/- 27.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1494000, episode_reward=-1383.20 +/- 348.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1495000, episode_reward=-1534.60 +/- 290.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1496000, episode_reward=-1647.60 +/- 86.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1497000, episode_reward=-1541.40 +/- 276.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1498000, episode_reward=-1687.40 +/- 14.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1499000, episode_reward=-1572.80 +/- 307.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1500000, episode_reward=-1675.20 +/- 37.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1501000, episode_reward=-1714.40 +/- 33.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1502000, episode_reward=-1675.80 +/- 20.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1503000, episode_reward=-1714.60 +/- 29.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1504000, episode_reward=-1546.60 +/- 291.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1505000, episode_reward=-1703.40 +/- 34.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1506000, episode_reward=-1563.40 +/- 263.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1507000, episode_reward=-1555.40 +/- 280.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1508000, episode_reward=-1722.40 +/- 53.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1509000, episode_reward=-1405.20 +/- 360.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1510000, episode_reward=-1690.20 +/- 49.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1511000, episode_reward=-1683.80 +/- 49.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1512000, episode_reward=-1533.00 +/- 302.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1513000, episode_reward=-1557.80 +/- 311.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1514000, episode_reward=-1536.20 +/- 304.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1515000, episode_reward=-1724.60 +/- 63.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1516000, episode_reward=-1407.80 +/- 320.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1517000, episode_reward=-1529.00 +/- 310.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1518000, episode_reward=-1548.20 +/- 318.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1519000, episode_reward=-1546.00 +/- 250.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1520000, episode_reward=-1542.60 +/- 310.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1521000, episode_reward=-1662.40 +/- 69.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1522000, episode_reward=-1391.00 +/- 411.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1523000, episode_reward=-1572.00 +/- 298.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1524000, episode_reward=-1405.00 +/- 373.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1525000, episode_reward=-1400.00 +/- 351.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1526000, episode_reward=-1703.40 +/- 46.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1527000, episode_reward=-1531.80 +/- 294.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1528000, episode_reward=-1385.60 +/- 330.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1529000, episode_reward=-1366.40 +/- 342.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1530000, episode_reward=-1699.20 +/- 27.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1531000, episode_reward=-1520.00 +/- 301.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1532000, episode_reward=-1677.20 +/- 62.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1533000, episode_reward=-1557.80 +/- 267.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1534000, episode_reward=-1116.40 +/- 297.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1535000, episode_reward=-1672.40 +/- 24.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1536000, episode_reward=-1680.60 +/- 37.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1537000, episode_reward=-1658.80 +/- 46.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1538000, episode_reward=-1702.40 +/- 18.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1539000, episode_reward=-1688.60 +/- 48.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1540000, episode_reward=-1706.80 +/- 27.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1541000, episode_reward=-1675.60 +/- 28.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1542000, episode_reward=-1692.80 +/- 43.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1543000, episode_reward=-1680.00 +/- 66.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1544000, episode_reward=-1726.80 +/- 54.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1545000, episode_reward=-1538.40 +/- 294.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1546000, episode_reward=-1688.60 +/- 35.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1547000, episode_reward=-1503.20 +/- 305.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1548000, episode_reward=-1711.40 +/- 46.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1549000, episode_reward=-1667.80 +/- 54.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1550000, episode_reward=-1733.40 +/- 55.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1551000, episode_reward=-1570.40 +/- 279.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1552000, episode_reward=-1676.00 +/- 56.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1553000, episode_reward=-1686.60 +/- 56.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1554000, episode_reward=-1586.00 +/- 309.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1555000, episode_reward=-1675.40 +/- 12.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1556000, episode_reward=-1562.20 +/- 275.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1557000, episode_reward=-1672.40 +/- 31.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1558000, episode_reward=-1674.60 +/- 35.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1559000, episode_reward=-1657.20 +/- 37.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1560000, episode_reward=-1526.60 +/- 310.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1561000, episode_reward=-1627.60 +/- 20.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1562000, episode_reward=-1502.40 +/- 273.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1563000, episode_reward=-1672.60 +/- 37.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1564000, episode_reward=-1538.60 +/- 267.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1565000, episode_reward=-1390.60 +/- 345.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1566000, episode_reward=-1710.40 +/- 23.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1567000, episode_reward=-1745.00 +/- 63.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1568000, episode_reward=-1544.40 +/- 254.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1569000, episode_reward=-1578.80 +/- 287.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1570000, episode_reward=-1697.40 +/- 32.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1571000, episode_reward=-1543.40 +/- 247.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1572000, episode_reward=-1696.80 +/- 17.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1573000, episode_reward=-1404.80 +/- 334.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1574000, episode_reward=-1518.80 +/- 248.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1575000, episode_reward=-1528.60 +/- 287.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1576000, episode_reward=-1732.60 +/- 46.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1577000, episode_reward=-1506.00 +/- 281.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1578000, episode_reward=-1714.60 +/- 56.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1579000, episode_reward=-1733.00 +/- 39.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1580000, episode_reward=-1535.00 +/- 259.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1581000, episode_reward=-1404.20 +/- 325.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1582000, episode_reward=-1557.40 +/- 317.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1583000, episode_reward=-1726.00 +/- 33.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1584000, episode_reward=-1557.00 +/- 301.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1585000, episode_reward=-1718.80 +/- 49.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1586000, episode_reward=-1581.20 +/- 285.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1587000, episode_reward=-1645.60 +/- 40.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1588000, episode_reward=-1570.80 +/- 299.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1589000, episode_reward=-1585.00 +/- 283.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1590000, episode_reward=-1645.60 +/- 34.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1591000, episode_reward=-1573.00 +/- 278.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1592000, episode_reward=-1526.20 +/- 306.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1593000, episode_reward=-1687.00 +/- 44.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1594000, episode_reward=-1273.20 +/- 365.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1595000, episode_reward=-1657.60 +/- 39.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1596000, episode_reward=-1547.20 +/- 269.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1597000, episode_reward=-1402.80 +/- 379.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1598000, episode_reward=-1722.00 +/- 23.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1599000, episode_reward=-1695.20 +/- 27.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1600000, episode_reward=-1665.60 +/- 34.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1601000, episode_reward=-1663.20 +/- 35.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1602000, episode_reward=-1708.20 +/- 58.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1603000, episode_reward=-1556.60 +/- 259.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1604000, episode_reward=-1695.00 +/- 57.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1605000, episode_reward=-1682.80 +/- 67.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1606000, episode_reward=-1703.20 +/- 63.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1607000, episode_reward=-1556.40 +/- 276.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1608000, episode_reward=-1720.00 +/- 42.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1609000, episode_reward=-1688.20 +/- 55.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1610000, episode_reward=-1708.80 +/- 40.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1611000, episode_reward=-1697.80 +/- 43.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1612000, episode_reward=-1688.20 +/- 63.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1613000, episode_reward=-1665.80 +/- 42.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1614000, episode_reward=-1560.20 +/- 311.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1615000, episode_reward=-1689.20 +/- 46.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1616000, episode_reward=-1391.20 +/- 319.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1617000, episode_reward=-1535.60 +/- 308.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1618000, episode_reward=-1641.80 +/- 60.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1619000, episode_reward=-1247.20 +/- 337.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1620000, episode_reward=-1731.60 +/- 17.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1621000, episode_reward=-1681.40 +/- 38.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1622000, episode_reward=-1400.40 +/- 338.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1623000, episode_reward=-1554.00 +/- 280.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1624000, episode_reward=-1702.00 +/- 20.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1625000, episode_reward=-1528.80 +/- 276.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1626000, episode_reward=-1663.60 +/- 43.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1627000, episode_reward=-1660.40 +/- 33.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1628000, episode_reward=-1698.00 +/- 31.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1629000, episode_reward=-1724.00 +/- 44.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1630000, episode_reward=-1531.00 +/- 298.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1631000, episode_reward=-1667.60 +/- 41.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1632000, episode_reward=-1575.00 +/- 261.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1633000, episode_reward=-1525.00 +/- 265.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1634000, episode_reward=-1361.20 +/- 365.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1635000, episode_reward=-1660.80 +/- 47.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1636000, episode_reward=-1562.80 +/- 294.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1637000, episode_reward=-1691.60 +/- 48.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1638000, episode_reward=-1228.80 +/- 361.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1639000, episode_reward=-1563.00 +/- 296.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1640000, episode_reward=-1405.80 +/- 324.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1641000, episode_reward=-1565.00 +/- 262.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1642000, episode_reward=-1677.60 +/- 73.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1643000, episode_reward=-1519.20 +/- 260.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1644000, episode_reward=-1391.80 +/- 317.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1645000, episode_reward=-1436.60 +/- 335.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1646000, episode_reward=-1674.00 +/- 23.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1647000, episode_reward=-1541.00 +/- 283.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1648000, episode_reward=-1388.20 +/- 342.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1649000, episode_reward=-1693.20 +/- 19.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1650000, episode_reward=-1272.80 +/- 371.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1651000, episode_reward=-1529.00 +/- 253.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1652000, episode_reward=-1670.00 +/- 35.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1653000, episode_reward=-1540.00 +/- 281.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1654000, episode_reward=-1564.40 +/- 284.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1655000, episode_reward=-1673.60 +/- 35.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1656000, episode_reward=-1403.40 +/- 375.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1657000, episode_reward=-1577.20 +/- 294.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1658000, episode_reward=-1653.00 +/- 39.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1659000, episode_reward=-1690.20 +/- 28.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1660000, episode_reward=-1702.80 +/- 27.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1661000, episode_reward=-1698.00 +/- 23.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1662000, episode_reward=-1680.20 +/- 33.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1663000, episode_reward=-1566.60 +/- 281.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1664000, episode_reward=-1405.00 +/- 359.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1665000, episode_reward=-1402.80 +/- 344.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1666000, episode_reward=-1559.80 +/- 296.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1667000, episode_reward=-1394.00 +/- 371.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1668000, episode_reward=-1683.80 +/- 38.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1669000, episode_reward=-1724.80 +/- 64.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1670000, episode_reward=-1679.00 +/- 50.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1671000, episode_reward=-1660.20 +/- 50.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1672000, episode_reward=-1430.20 +/- 383.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1673000, episode_reward=-1544.80 +/- 307.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1674000, episode_reward=-1762.00 +/- 43.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1675000, episode_reward=-1681.60 +/- 27.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1676000, episode_reward=-1698.20 +/- 52.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1677000, episode_reward=-1669.40 +/- 35.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1678000, episode_reward=-1679.40 +/- 55.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1679000, episode_reward=-1630.60 +/- 107.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1680000, episode_reward=-1371.20 +/- 357.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1681000, episode_reward=-1604.80 +/- 221.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1682000, episode_reward=-1543.60 +/- 304.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1683000, episode_reward=-1520.20 +/- 285.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1684000, episode_reward=-1418.80 +/- 357.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1685000, episode_reward=-1570.60 +/- 269.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1686000, episode_reward=-1321.20 +/- 274.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1687000, episode_reward=-1707.20 +/- 20.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1688000, episode_reward=-1237.60 +/- 204.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1689000, episode_reward=-1416.40 +/- 225.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1690000, episode_reward=-1488.80 +/- 271.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1691000, episode_reward=-1296.60 +/- 353.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1692000, episode_reward=-1471.00 +/- 254.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1693000, episode_reward=-1345.00 +/- 280.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1694000, episode_reward=-1255.60 +/- 247.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1695000, episode_reward=-1228.80 +/- 220.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1696000, episode_reward=-1363.60 +/- 123.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1697000, episode_reward=-1489.40 +/- 262.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1698000, episode_reward=-1267.40 +/- 278.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1699000, episode_reward=-1299.00 +/- 324.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1700000, episode_reward=-1249.80 +/- 252.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1701000, episode_reward=-1403.80 +/- 313.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1702000, episode_reward=-1360.40 +/- 291.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1703000, episode_reward=-1456.60 +/- 301.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1704000, episode_reward=-1339.40 +/- 193.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1705000, episode_reward=-1415.40 +/- 204.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1706000, episode_reward=-1431.20 +/- 347.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1707000, episode_reward=-1658.60 +/- 58.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1708000, episode_reward=-1498.00 +/- 250.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1709000, episode_reward=-1556.80 +/- 262.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1710000, episode_reward=-1697.80 +/- 64.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1711000, episode_reward=-1715.80 +/- 44.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1712000, episode_reward=-1713.00 +/- 58.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1713000, episode_reward=-1689.80 +/- 15.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1714000, episode_reward=-1499.40 +/- 319.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1715000, episode_reward=-1385.00 +/- 323.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1716000, episode_reward=-1554.00 +/- 312.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1717000, episode_reward=-1243.40 +/- 359.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1718000, episode_reward=-1538.00 +/- 285.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1719000, episode_reward=-1427.00 +/- 366.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1720000, episode_reward=-1419.20 +/- 325.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1721000, episode_reward=-1542.80 +/- 311.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1722000, episode_reward=-1530.40 +/- 319.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1723000, episode_reward=-1697.00 +/- 51.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1724000, episode_reward=-1673.20 +/- 65.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1725000, episode_reward=-1239.60 +/- 321.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1726000, episode_reward=-1546.20 +/- 281.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1727000, episode_reward=-1479.60 +/- 301.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1728000, episode_reward=-1725.40 +/- 41.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1729000, episode_reward=-1684.00 +/- 39.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1730000, episode_reward=-1694.20 +/- 16.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1731000, episode_reward=-1638.00 +/- 33.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1732000, episode_reward=-1651.20 +/- 35.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1733000, episode_reward=-1644.60 +/- 43.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1734000, episode_reward=-1676.20 +/- 56.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1735000, episode_reward=-1714.20 +/- 18.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1736000, episode_reward=-1647.20 +/- 47.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1737000, episode_reward=-1416.60 +/- 350.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1738000, episode_reward=-1551.40 +/- 305.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1739000, episode_reward=-1696.60 +/- 43.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1740000, episode_reward=-1537.60 +/- 266.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1741000, episode_reward=-1388.40 +/- 363.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1742000, episode_reward=-1685.00 +/- 44.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1743000, episode_reward=-1563.00 +/- 313.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1744000, episode_reward=-1389.00 +/- 373.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1745000, episode_reward=-1527.80 +/- 310.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1746000, episode_reward=-1647.80 +/- 24.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1747000, episode_reward=-1528.80 +/- 230.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1748000, episode_reward=-1711.20 +/- 23.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1749000, episode_reward=-1699.20 +/- 36.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1750000, episode_reward=-1669.80 +/- 44.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1751000, episode_reward=-1727.20 +/- 12.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1752000, episode_reward=-1670.20 +/- 22.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1753000, episode_reward=-1545.60 +/- 278.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1754000, episode_reward=-1667.60 +/- 44.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1755000, episode_reward=-1696.40 +/- 55.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1756000, episode_reward=-1679.40 +/- 46.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1757000, episode_reward=-1561.60 +/- 333.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1758000, episode_reward=-1679.00 +/- 49.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1759000, episode_reward=-1689.00 +/- 42.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1760000, episode_reward=-1717.80 +/- 58.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1761000, episode_reward=-1603.80 +/- 309.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1762000, episode_reward=-1683.40 +/- 45.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1763000, episode_reward=-1676.80 +/- 45.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1764000, episode_reward=-1678.60 +/- 52.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1765000, episode_reward=-1686.20 +/- 34.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1766000, episode_reward=-1688.80 +/- 10.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1767000, episode_reward=-1432.60 +/- 355.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1768000, episode_reward=-1416.20 +/- 368.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1769000, episode_reward=-1683.00 +/- 71.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1770000, episode_reward=-1554.40 +/- 257.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1771000, episode_reward=-1685.40 +/- 64.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1772000, episode_reward=-1685.20 +/- 46.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1773000, episode_reward=-1680.80 +/- 42.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1774000, episode_reward=-1396.40 +/- 353.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1775000, episode_reward=-1391.20 +/- 369.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1776000, episode_reward=-1695.20 +/- 56.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1777000, episode_reward=-1542.20 +/- 299.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1778000, episode_reward=-1407.00 +/- 339.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1779000, episode_reward=-1412.00 +/- 345.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1780000, episode_reward=-1380.20 +/- 301.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1781000, episode_reward=-1538.20 +/- 285.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1782000, episode_reward=-1563.40 +/- 327.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1783000, episode_reward=-1689.40 +/- 18.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1784000, episode_reward=-1620.60 +/- 279.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1785000, episode_reward=-1570.00 +/- 300.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1786000, episode_reward=-1718.20 +/- 48.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1787000, episode_reward=-1342.60 +/- 365.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1788000, episode_reward=-1409.20 +/- 316.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1789000, episode_reward=-1436.40 +/- 318.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1790000, episode_reward=-1247.80 +/- 229.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1791000, episode_reward=-1229.80 +/- 346.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1792000, episode_reward=-1265.40 +/- 327.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1793000, episode_reward=-1281.80 +/- 300.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1794000, episode_reward=-1662.40 +/- 24.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1795000, episode_reward=-1710.80 +/- 70.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1796000, episode_reward=-1437.60 +/- 362.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1797000, episode_reward=-1518.40 +/- 259.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1798000, episode_reward=-1677.40 +/- 33.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1799000, episode_reward=-1716.40 +/- 55.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1800000, episode_reward=-1705.20 +/- 47.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1801000, episode_reward=-1551.00 +/- 320.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1802000, episode_reward=-1667.80 +/- 36.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1803000, episode_reward=-1579.20 +/- 291.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1804000, episode_reward=-1549.40 +/- 268.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1805000, episode_reward=-1595.40 +/- 313.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1806000, episode_reward=-1706.40 +/- 34.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1807000, episode_reward=-1703.60 +/- 45.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1808000, episode_reward=-1679.60 +/- 51.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1809000, episode_reward=-1681.20 +/- 35.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1810000, episode_reward=-1535.20 +/- 295.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1811000, episode_reward=-1668.00 +/- 41.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1812000, episode_reward=-1243.80 +/- 364.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1813000, episode_reward=-1359.40 +/- 330.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1814000, episode_reward=-1031.40 +/- 39.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1815000, episode_reward=-1605.20 +/- 126.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1816000, episode_reward=-1429.60 +/- 233.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1817000, episode_reward=-1406.20 +/- 314.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1818000, episode_reward=-1556.00 +/- 285.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1819000, episode_reward=-1394.40 +/- 330.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1820000, episode_reward=-1300.40 +/- 357.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1821000, episode_reward=-1686.40 +/- 37.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1822000, episode_reward=-1530.40 +/- 272.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1823000, episode_reward=-1410.40 +/- 342.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1824000, episode_reward=-1549.20 +/- 292.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1825000, episode_reward=-1366.60 +/- 365.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1826000, episode_reward=-1741.00 +/- 6.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1827000, episode_reward=-1540.00 +/- 260.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1828000, episode_reward=-1433.20 +/- 365.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1829000, episode_reward=-1543.00 +/- 304.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1830000, episode_reward=-1374.40 +/- 342.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1831000, episode_reward=-1553.40 +/- 277.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1832000, episode_reward=-1674.00 +/- 40.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1833000, episode_reward=-1680.40 +/- 38.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1834000, episode_reward=-1459.20 +/- 337.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1835000, episode_reward=-1390.80 +/- 367.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1836000, episode_reward=-1725.60 +/- 48.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1837000, episode_reward=-1543.60 +/- 293.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1838000, episode_reward=-1538.20 +/- 286.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1839000, episode_reward=-1650.80 +/- 34.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1840000, episode_reward=-1570.80 +/- 305.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1841000, episode_reward=-1572.60 +/- 250.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1842000, episode_reward=-1387.60 +/- 318.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1843000, episode_reward=-1730.80 +/- 82.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1844000, episode_reward=-1274.20 +/- 366.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1845000, episode_reward=-1691.80 +/- 53.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1846000, episode_reward=-1564.80 +/- 289.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1847000, episode_reward=-1493.00 +/- 294.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1848000, episode_reward=-1379.00 +/- 341.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1849000, episode_reward=-1683.40 +/- 71.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1850000, episode_reward=-1571.80 +/- 324.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1851000, episode_reward=-1688.80 +/- 29.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1852000, episode_reward=-1301.80 +/- 315.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1853000, episode_reward=-1428.20 +/- 298.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1854000, episode_reward=-1492.20 +/- 243.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1855000, episode_reward=-1229.20 +/- 308.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1856000, episode_reward=-1383.60 +/- 308.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1857000, episode_reward=-1311.00 +/- 246.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1858000, episode_reward=-1334.80 +/- 202.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1859000, episode_reward=-1050.40 +/- 60.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1860000, episode_reward=-1161.60 +/- 273.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1861000, episode_reward=-1289.80 +/- 263.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1862000, episode_reward=-1059.80 +/- 82.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1863000, episode_reward=-1311.40 +/- 178.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1864000, episode_reward=-1351.60 +/- 273.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1865000, episode_reward=-1465.80 +/- 189.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1866000, episode_reward=-1470.40 +/- 186.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1867000, episode_reward=-1385.20 +/- 276.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1868000, episode_reward=-1310.20 +/- 300.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1869000, episode_reward=-1581.60 +/- 262.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1870000, episode_reward=-1413.20 +/- 312.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1871000, episode_reward=-1295.00 +/- 327.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1872000, episode_reward=-1183.60 +/- 256.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1873000, episode_reward=-1321.00 +/- 337.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1874000, episode_reward=-1545.60 +/- 323.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1875000, episode_reward=-1286.00 +/- 324.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1876000, episode_reward=-1520.80 +/- 279.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1877000, episode_reward=-1665.00 +/- 66.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1878000, episode_reward=-1520.20 +/- 283.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1879000, episode_reward=-1416.80 +/- 311.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1880000, episode_reward=-1529.80 +/- 298.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1881000, episode_reward=-1415.40 +/- 344.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1882000, episode_reward=-1560.60 +/- 307.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1883000, episode_reward=-1655.20 +/- 24.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1884000, episode_reward=-1672.40 +/- 52.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1885000, episode_reward=-1584.80 +/- 318.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1886000, episode_reward=-1576.00 +/- 268.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1887000, episode_reward=-1575.80 +/- 283.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1888000, episode_reward=-1683.80 +/- 33.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1889000, episode_reward=-1689.20 +/- 56.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1890000, episode_reward=-1693.00 +/- 41.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1891000, episode_reward=-1548.80 +/- 251.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1892000, episode_reward=-1676.80 +/- 38.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1893000, episode_reward=-1589.40 +/- 252.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1894000, episode_reward=-1690.00 +/- 44.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1895000, episode_reward=-1145.40 +/- 276.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1896000, episode_reward=-1326.20 +/- 295.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1897000, episode_reward=-1614.60 +/- 221.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1898000, episode_reward=-1442.60 +/- 325.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1899000, episode_reward=-1686.20 +/- 57.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1900000, episode_reward=-1694.60 +/- 24.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1901000, episode_reward=-1376.20 +/- 337.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1902000, episode_reward=-1696.80 +/- 37.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1903000, episode_reward=-1608.80 +/- 149.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1904000, episode_reward=-1414.20 +/- 315.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1905000, episode_reward=-1730.60 +/- 49.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1906000, episode_reward=-1667.20 +/- 23.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1907000, episode_reward=-1531.60 +/- 309.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1908000, episode_reward=-1726.40 +/- 56.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1909000, episode_reward=-1703.00 +/- 48.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1910000, episode_reward=-1418.60 +/- 371.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1911000, episode_reward=-1552.40 +/- 271.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1912000, episode_reward=-1537.80 +/- 284.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1913000, episode_reward=-1407.00 +/- 371.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1914000, episode_reward=-1683.60 +/- 48.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1915000, episode_reward=-1712.20 +/- 79.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1916000, episode_reward=-1749.60 +/- 49.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1917000, episode_reward=-1425.40 +/- 305.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1918000, episode_reward=-1373.00 +/- 262.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1919000, episode_reward=-1575.40 +/- 286.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1920000, episode_reward=-1708.40 +/- 29.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1921000, episode_reward=-1441.80 +/- 343.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1922000, episode_reward=-1711.60 +/- 41.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1923000, episode_reward=-1266.60 +/- 348.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1924000, episode_reward=-1722.40 +/- 27.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1925000, episode_reward=-1383.80 +/- 358.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1926000, episode_reward=-1564.20 +/- 278.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1927000, episode_reward=-1672.00 +/- 50.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1928000, episode_reward=-1674.20 +/- 69.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1929000, episode_reward=-1592.00 +/- 309.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1930000, episode_reward=-1529.60 +/- 285.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1931000, episode_reward=-1258.80 +/- 336.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1932000, episode_reward=-1260.00 +/- 295.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1933000, episode_reward=-1709.40 +/- 68.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1934000, episode_reward=-1511.20 +/- 325.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1935000, episode_reward=-1676.20 +/- 46.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1936000, episode_reward=-1537.20 +/- 295.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1937000, episode_reward=-1239.60 +/- 360.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1938000, episode_reward=-1532.20 +/- 324.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1939000, episode_reward=-1447.00 +/- 364.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1940000, episode_reward=-1663.00 +/- 33.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1941000, episode_reward=-1401.80 +/- 365.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1942000, episode_reward=-1545.00 +/- 318.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1943000, episode_reward=-1553.80 +/- 292.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1944000, episode_reward=-1716.20 +/- 46.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1945000, episode_reward=-1550.60 +/- 267.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1946000, episode_reward=-1541.20 +/- 300.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1947000, episode_reward=-1279.40 +/- 394.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1948000, episode_reward=-1732.20 +/- 21.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1949000, episode_reward=-1557.60 +/- 310.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1950000, episode_reward=-1714.60 +/- 19.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1951000, episode_reward=-1525.20 +/- 328.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1952000, episode_reward=-1584.60 +/- 276.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1953000, episode_reward=-1550.60 +/- 281.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1954000, episode_reward=-1694.40 +/- 40.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1955000, episode_reward=-1665.60 +/- 36.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1956000, episode_reward=-1729.20 +/- 42.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1957000, episode_reward=-1565.80 +/- 285.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1958000, episode_reward=-1565.80 +/- 299.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1959000, episode_reward=-1534.80 +/- 276.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1960000, episode_reward=-1697.80 +/- 41.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1961000, episode_reward=-1667.20 +/- 44.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1962000, episode_reward=-1725.60 +/- 35.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1963000, episode_reward=-1693.80 +/- 42.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1964000, episode_reward=-1673.20 +/- 54.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1965000, episode_reward=-1674.80 +/- 38.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1966000, episode_reward=-1271.20 +/- 360.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1967000, episode_reward=-1560.00 +/- 258.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1968000, episode_reward=-1581.00 +/- 270.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1969000, episode_reward=-1584.60 +/- 243.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1970000, episode_reward=-1527.00 +/- 270.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1971000, episode_reward=-1417.60 +/- 361.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1972000, episode_reward=-1725.80 +/- 50.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1973000, episode_reward=-1539.00 +/- 283.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1974000, episode_reward=-1678.80 +/- 52.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1975000, episode_reward=-1678.60 +/- 49.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1976000, episode_reward=-1542.00 +/- 304.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1977000, episode_reward=-1550.80 +/- 271.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1978000, episode_reward=-1561.60 +/- 290.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1979000, episode_reward=-1684.00 +/- 60.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1980000, episode_reward=-1725.60 +/- 58.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1981000, episode_reward=-1420.20 +/- 358.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1982000, episode_reward=-1555.80 +/- 232.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1983000, episode_reward=-1392.20 +/- 362.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1984000, episode_reward=-1550.80 +/- 304.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1985000, episode_reward=-1433.00 +/- 370.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1986000, episode_reward=-1374.80 +/- 398.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1987000, episode_reward=-1407.80 +/- 325.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1988000, episode_reward=-1395.20 +/- 354.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1989000, episode_reward=-1421.60 +/- 349.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1990000, episode_reward=-1712.60 +/- 35.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1991000, episode_reward=-1661.20 +/- 53.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1992000, episode_reward=-1411.20 +/- 350.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1993000, episode_reward=-1699.80 +/- 33.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1994000, episode_reward=-1540.20 +/- 303.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1995000, episode_reward=-1563.60 +/- 289.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1996000, episode_reward=-1389.20 +/- 313.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1997000, episode_reward=-1672.80 +/- 65.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1998000, episode_reward=-1542.40 +/- 309.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1999000, episode_reward=-1434.80 +/- 342.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2000000, episode_reward=-1703.60 +/- 67.64\n",
      "Episode length: 100.00 +/- 0.00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(-1258.2, 362.2782908207446)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### REINFORCEMENT LEARNING I ###\n",
    "### TRAIN, SAVE, EVALUATE MODEL ###\n",
    "\n",
    "import gym\n",
    "import stable_baselines3 as sb\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "import pickle\n",
    "\n",
    "for pn in range (6): \n",
    "    for mn in range(6):\n",
    "        # Load diagnostics model from disk\n",
    "        diag_model = pickle.load(open('diagnostics/model_' + 'pn' + str(pn) + '_mn' + str(mn), 'rb'))\n",
    "        # Initiate environment\n",
    "        env = gym.make('Production-v0', diag_model = diag_model)\n",
    "        # Callback for best model\n",
    "        best_callback = EvalCallback(env, best_model_save_path='./callback/' + 'pn' + str(pn) + '_mn' + str(mn),\n",
    "                                    log_path='./callback/' + 'pn' + str(pn) + '_mn' + str(mn), eval_freq=1000,\n",
    "                                    deterministic=True, render=False)\n",
    "\n",
    "\n",
    "        model = sb.PPO('MlpPolicy', env, tensorboard_log=\"./tensorboard/\")\n",
    "        model.learn(total_timesteps=2e6, tb_log_name=\"PPO\" + 'pn' + str(pn) + '_mn' + str(mn), callback = best_callback)\n",
    "        model.save(\"PPO\" + 'pn' + str(pn) + '_mn' + str(mn))\n",
    "\n",
    "\n",
    "        # Evaluate the agent\n",
    "        evaluate_policy(model, model.get_env(), n_eval_episodes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-1282.2, 350.7029512279587)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### REINFORCEMENT LEARNING II ###\n",
    "### LOAD MODEL ###\n",
    "import gym\n",
    "import stable_baselines3 as sb\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3 import A2C\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "import pickle\n",
    "\n",
    "diag_model = pickle.load(open('diagnostics/model', 'rb'))\n",
    "env = gym.make('Production-v0', diag_model = diag_model)\n",
    "# Best Model\n",
    "model = PPO.load('./callback/15', env = env)\n",
    "# Last Model\n",
    "#model = DQN.load('DQN_1_model', env = env)\n",
    "\n",
    "# Evaluate the agent\n",
    "evaluate_policy(model, model.get_env(), n_eval_episodes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average number of reactive maintenance interventions per episode is:  0.6\n",
      "The average number of preventive maintenance interventions per episode is:  0.0\n",
      "The average sum of inventory per episode is:  1.7\n",
      "The average sum of spare parts inventory per episode is:  0.0\n",
      "The average reward per episode is:  -1427.5\n",
      "The average upper bound per episode is:  643.6\n"
     ]
    }
   ],
   "source": [
    "### REINFORCEMENT LEARNING III ###\n",
    "### TRY AND EVALUATE MY MODEL ###\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Initilaize Reward\n",
    "result_df = pd.DataFrame([[0, 0, 0, 0, 0, 0]], columns=['RM', 'PM', 'Inventory', 'Spare Parts Inventory', 'Reward', 'Upper'])\n",
    "# Set iterations\n",
    "iterations = 10\n",
    "for i in range(iterations):\n",
    "    # Initialize episode\n",
    "    store = []\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    store.append([0, obs[0], env.breakdown, obs[2], obs[3], 0, done, obs[1]])\n",
    "    # Compute one episode\n",
    "    while not done:\n",
    "        # Get best action for state\n",
    "        action, _state = model.predict(obs, deterministic=True)\n",
    "        # Compute next state\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        # Store results of this episode\n",
    "        store.append([action, obs[0], env.breakdown, obs[2], obs[3], reward, done, obs[1]])\n",
    "    eps_df = pd.DataFrame(store, columns=['action', 'health', 'breakdown', 'inventory', 'sp_inventory', 'reward', 'done', 'next_order'])\n",
    "    #eps_df.to_excel('visuals/eps_df/eps_df_' + str(i) + '.xlsx')\n",
    "    # Calculate nr. of reactive maintenance interventions by counting health 'resets' and substracting PM actions\n",
    "    result_df.iloc[0]['RM'] = result_df.iloc[0]['RM'] + sum(eps_df['breakdown']==True)\n",
    "    # Calculate nr. of preventive maintenance interventions\n",
    "    result_df.iloc[0]['PM'] = result_df.iloc[0]['PM'] + sum(eps_df['action']==10)\n",
    "    # Calculate inventory\n",
    "    result_df.iloc[0]['Inventory'] = result_df.iloc[0]['Inventory'] + sum(eps_df['inventory'])\n",
    "    # Calculate spare parts inventory per period\n",
    "    result_df.iloc[0]['Spare Parts Inventory'] = result_df.iloc[0]['Spare Parts Inventory'] + sum(eps_df['sp_inventory'])\n",
    "    # Calculate reward\n",
    "    result_df.iloc[0]['Reward'] = result_df.iloc[0]['Reward'] + sum(eps_df['reward'])\n",
    "    # Calculate reward with no costs and fulfillment of all orders\n",
    "    result_df.iloc[0]['Upper'] = result_df.iloc[0]['Upper'] + sum(eps_df.iloc[:-1]['next_order']) * env.order_r\n",
    "\n",
    "print(\"The average number of reactive maintenance interventions per episode is: \", result_df.iloc[0]['RM']/iterations)\n",
    "print(\"The average number of preventive maintenance interventions per episode is: \", result_df.iloc[0]['PM']/iterations) \n",
    "print(\"The average sum of inventory per episode is: \", result_df.iloc[0]['Inventory']/iterations)\n",
    "print(\"The average sum of spare parts inventory per episode is: \", result_df.iloc[0]['Spare Parts Inventory']/iterations)\n",
    "print(\"The average reward per episode is: \", result_df.iloc[0]['Reward']/iterations)\n",
    "print(\"The average upper bound per episode is: \", result_df.iloc[0]['Upper']/iterations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'visuals/eps_df/eps_df_0'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'visuals/eps_df/eps_df_' + str(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=1000, episode_reward=-1704.60 +/- 22.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=-1685.00 +/- 36.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3000, episode_reward=-1688.60 +/- 45.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=4000, episode_reward=-1675.20 +/- 10.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=5000, episode_reward=-1644.00 +/- 30.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=6000, episode_reward=-1702.80 +/- 24.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=7000, episode_reward=-1671.00 +/- 51.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=8000, episode_reward=-1690.40 +/- 35.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=9000, episode_reward=-1713.20 +/- 44.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=10000, episode_reward=-1719.60 +/- 31.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=11000, episode_reward=-1686.60 +/- 50.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=12000, episode_reward=-1686.60 +/- 57.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=13000, episode_reward=-1698.40 +/- 32.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=14000, episode_reward=-1701.00 +/- 49.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=15000, episode_reward=-1685.00 +/- 49.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=16000, episode_reward=-1724.00 +/- 36.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=17000, episode_reward=-1657.40 +/- 39.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=18000, episode_reward=-1697.80 +/- 31.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=19000, episode_reward=-1695.40 +/- 38.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=20000, episode_reward=-1678.60 +/- 22.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=21000, episode_reward=-1652.00 +/- 85.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=22000, episode_reward=-1716.40 +/- 14.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=23000, episode_reward=-1607.40 +/- 113.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=24000, episode_reward=-1678.80 +/- 22.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=25000, episode_reward=-1671.80 +/- 37.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=26000, episode_reward=-1649.60 +/- 24.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=27000, episode_reward=-1668.60 +/- 41.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=28000, episode_reward=-1681.80 +/- 40.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=29000, episode_reward=-1657.40 +/- 32.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=30000, episode_reward=-1683.00 +/- 43.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=31000, episode_reward=-1654.60 +/- 46.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=32000, episode_reward=-1693.00 +/- 24.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=33000, episode_reward=-1677.40 +/- 59.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=34000, episode_reward=-1670.00 +/- 28.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=35000, episode_reward=-1685.80 +/- 31.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=36000, episode_reward=-1689.40 +/- 43.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=37000, episode_reward=-1658.40 +/- 55.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=38000, episode_reward=-1696.40 +/- 14.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=39000, episode_reward=-1658.00 +/- 57.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=40000, episode_reward=-1687.60 +/- 16.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=41000, episode_reward=-1669.40 +/- 52.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=42000, episode_reward=-1677.60 +/- 48.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=43000, episode_reward=-1655.40 +/- 28.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=44000, episode_reward=-1703.40 +/- 35.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=45000, episode_reward=-1602.00 +/- 124.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=46000, episode_reward=-1631.80 +/- 39.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=47000, episode_reward=-1679.00 +/- 33.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=48000, episode_reward=-1683.80 +/- 20.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=49000, episode_reward=-1677.20 +/- 63.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=50000, episode_reward=-1718.60 +/- 41.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=51000, episode_reward=-2740.20 +/- 13.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=52000, episode_reward=-2729.20 +/- 10.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=53000, episode_reward=-2725.40 +/- 26.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=54000, episode_reward=-2716.60 +/- 24.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=55000, episode_reward=-2681.60 +/- 24.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=56000, episode_reward=-2574.00 +/- 305.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=57000, episode_reward=-2758.40 +/- 13.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=58000, episode_reward=-2689.60 +/- 31.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=59000, episode_reward=-2657.40 +/- 25.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=60000, episode_reward=-2709.40 +/- 27.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=61000, episode_reward=-2666.80 +/- 18.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=62000, episode_reward=-2520.00 +/- 309.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=63000, episode_reward=-2666.00 +/- 10.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=64000, episode_reward=-2676.40 +/- 12.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=65000, episode_reward=-2668.60 +/- 10.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=66000, episode_reward=-2523.20 +/- 303.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=67000, episode_reward=-2673.00 +/- 19.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=68000, episode_reward=-2504.40 +/- 306.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=69000, episode_reward=-2655.80 +/- 28.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=70000, episode_reward=-2650.40 +/- 22.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=71000, episode_reward=-2677.80 +/- 39.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=72000, episode_reward=-2645.80 +/- 16.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=73000, episode_reward=-2717.40 +/- 18.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=74000, episode_reward=-2666.40 +/- 14.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=75000, episode_reward=-2710.40 +/- 28.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=76000, episode_reward=-2708.80 +/- 23.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=77000, episode_reward=-2679.40 +/- 27.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=78000, episode_reward=-2486.40 +/- 307.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=79000, episode_reward=-2788.80 +/- 14.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=80000, episode_reward=-2664.40 +/- 21.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=81000, episode_reward=-2691.80 +/- 27.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=82000, episode_reward=-2646.00 +/- 17.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=83000, episode_reward=-2653.60 +/- 19.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=84000, episode_reward=-2639.20 +/- 18.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=85000, episode_reward=-2654.40 +/- 24.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=86000, episode_reward=-2661.60 +/- 15.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=87000, episode_reward=-2656.80 +/- 11.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=88000, episode_reward=-2174.80 +/- 377.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=89000, episode_reward=-2502.00 +/- 309.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=90000, episode_reward=-2636.40 +/- 13.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=91000, episode_reward=-2657.00 +/- 17.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=92000, episode_reward=-2655.60 +/- 19.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=93000, episode_reward=-2654.80 +/- 14.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=94000, episode_reward=-2645.60 +/- 17.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=95000, episode_reward=-2648.00 +/- 15.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=96000, episode_reward=-2646.40 +/- 13.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=97000, episode_reward=-2630.00 +/- 14.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=98000, episode_reward=-2658.80 +/- 30.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=99000, episode_reward=-2643.60 +/- 10.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=100000, episode_reward=-2642.40 +/- 14.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=101000, episode_reward=-2648.80 +/- 16.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=102000, episode_reward=-2516.40 +/- 305.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=103000, episode_reward=-2650.40 +/- 13.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=104000, episode_reward=-2664.60 +/- 9.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=105000, episode_reward=-2656.80 +/- 15.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=106000, episode_reward=-2655.20 +/- 15.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=107000, episode_reward=-2473.20 +/- 301.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=108000, episode_reward=-2648.40 +/- 20.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=109000, episode_reward=-2498.80 +/- 316.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=110000, episode_reward=-2644.20 +/- 12.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=111000, episode_reward=-2640.00 +/- 14.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=112000, episode_reward=-2654.40 +/- 13.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=113000, episode_reward=-2483.60 +/- 307.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=114000, episode_reward=-2637.20 +/- 15.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=115000, episode_reward=-2671.00 +/- 17.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=116000, episode_reward=-2660.60 +/- 11.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=117000, episode_reward=-2643.60 +/- 25.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=118000, episode_reward=-2663.60 +/- 19.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=119000, episode_reward=-2653.20 +/- 21.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=120000, episode_reward=-2663.60 +/- 13.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=121000, episode_reward=-2656.80 +/- 17.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=122000, episode_reward=-2522.00 +/- 304.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=123000, episode_reward=-2640.00 +/- 18.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=124000, episode_reward=-2634.40 +/- 10.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=125000, episode_reward=-2644.80 +/- 6.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=126000, episode_reward=-2683.20 +/- 24.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=127000, episode_reward=-2639.20 +/- 10.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=128000, episode_reward=-2685.80 +/- 3.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=129000, episode_reward=-2689.60 +/- 7.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=130000, episode_reward=-2631.00 +/- 19.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=131000, episode_reward=-2337.20 +/- 372.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=132000, episode_reward=-2676.40 +/- 16.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=133000, episode_reward=-2483.60 +/- 314.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=134000, episode_reward=-2500.80 +/- 315.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=135000, episode_reward=-2650.00 +/- 16.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=136000, episode_reward=-2644.80 +/- 14.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=137000, episode_reward=-2671.00 +/- 15.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=138000, episode_reward=-2496.80 +/- 314.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=139000, episode_reward=-2647.00 +/- 16.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=140000, episode_reward=-2678.00 +/- 15.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=141000, episode_reward=-2646.00 +/- 11.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=142000, episode_reward=-2663.40 +/- 8.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=143000, episode_reward=-2672.40 +/- 23.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=144000, episode_reward=-2474.80 +/- 308.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=145000, episode_reward=-2494.00 +/- 312.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=146000, episode_reward=-2623.60 +/- 26.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=147000, episode_reward=-2646.40 +/- 20.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=148000, episode_reward=-2644.80 +/- 30.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=149000, episode_reward=-2482.80 +/- 305.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=150000, episode_reward=-2633.60 +/- 16.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=151000, episode_reward=-2650.60 +/- 9.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=152000, episode_reward=-2656.00 +/- 16.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=153000, episode_reward=-2644.00 +/- 17.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=154000, episode_reward=-2642.00 +/- 25.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=155000, episode_reward=-2648.80 +/- 12.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=156000, episode_reward=-2651.00 +/- 20.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=157000, episode_reward=-2643.60 +/- 26.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=158000, episode_reward=-2640.80 +/- 21.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=159000, episode_reward=-2652.40 +/- 13.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=160000, episode_reward=-2699.80 +/- 22.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=161000, episode_reward=-2643.60 +/- 13.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=162000, episode_reward=-2647.60 +/- 21.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=163000, episode_reward=-2641.60 +/- 13.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=164000, episode_reward=-2650.80 +/- 19.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=165000, episode_reward=-2336.40 +/- 386.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=166000, episode_reward=-2660.40 +/- 24.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=167000, episode_reward=-2650.20 +/- 7.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=168000, episode_reward=-2644.40 +/- 13.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=169000, episode_reward=-2661.00 +/- 22.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=170000, episode_reward=-2640.00 +/- 11.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=171000, episode_reward=-2479.20 +/- 313.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=172000, episode_reward=-2664.80 +/- 35.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=173000, episode_reward=-2651.60 +/- 16.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=174000, episode_reward=-2646.40 +/- 12.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=175000, episode_reward=-2688.80 +/- 22.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=176000, episode_reward=-2652.20 +/- 27.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=177000, episode_reward=-2484.00 +/- 306.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=178000, episode_reward=-2495.20 +/- 317.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=179000, episode_reward=-2685.40 +/- 10.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=180000, episode_reward=-2660.40 +/- 7.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=181000, episode_reward=-2702.60 +/- 22.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=182000, episode_reward=-2648.40 +/- 19.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=183000, episode_reward=-2682.40 +/- 22.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=184000, episode_reward=-2670.00 +/- 24.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=185000, episode_reward=-2646.20 +/- 21.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=186000, episode_reward=-2658.80 +/- 6.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=187000, episode_reward=-2656.00 +/- 13.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=188000, episode_reward=-2657.60 +/- 16.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=189000, episode_reward=-2721.80 +/- 32.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=190000, episode_reward=-2639.00 +/- 18.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=191000, episode_reward=-2517.40 +/- 300.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=192000, episode_reward=-2641.60 +/- 18.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=193000, episode_reward=-2665.80 +/- 29.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=194000, episode_reward=-2665.40 +/- 28.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=195000, episode_reward=-2496.80 +/- 314.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=196000, episode_reward=-2694.60 +/- 30.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=197000, episode_reward=-2516.20 +/- 302.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=198000, episode_reward=-2499.00 +/- 315.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=199000, episode_reward=-2521.00 +/- 313.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=200000, episode_reward=-2681.20 +/- 19.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=201000, episode_reward=-2501.00 +/- 304.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=202000, episode_reward=-2709.40 +/- 19.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=203000, episode_reward=-2674.00 +/- 13.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=204000, episode_reward=-2628.80 +/- 23.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=205000, episode_reward=-2652.60 +/- 20.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=206000, episode_reward=-2742.20 +/- 63.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=207000, episode_reward=-2660.60 +/- 26.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=208000, episode_reward=-2639.80 +/- 6.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=209000, episode_reward=-2660.60 +/- 14.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=210000, episode_reward=-2664.00 +/- 28.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=211000, episode_reward=-2655.20 +/- 12.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=212000, episode_reward=-2490.20 +/- 318.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=213000, episode_reward=-2637.60 +/- 17.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=214000, episode_reward=-2655.60 +/- 11.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=215000, episode_reward=-2488.40 +/- 305.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=216000, episode_reward=-2675.80 +/- 15.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=217000, episode_reward=-2698.60 +/- 17.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=218000, episode_reward=-2642.40 +/- 16.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=219000, episode_reward=-2657.20 +/- 15.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=220000, episode_reward=-2635.00 +/- 11.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=221000, episode_reward=-2652.60 +/- 21.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=222000, episode_reward=-2651.20 +/- 16.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=223000, episode_reward=-2490.60 +/- 302.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=224000, episode_reward=-2488.40 +/- 310.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=225000, episode_reward=-2637.20 +/- 16.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=226000, episode_reward=-2674.80 +/- 18.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=227000, episode_reward=-2496.00 +/- 318.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=228000, episode_reward=-2659.60 +/- 8.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=229000, episode_reward=-2634.00 +/- 15.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=230000, episode_reward=-2644.40 +/- 19.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=231000, episode_reward=-2498.00 +/- 307.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=232000, episode_reward=-2336.40 +/- 377.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=233000, episode_reward=-2645.60 +/- 20.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=234000, episode_reward=-2641.80 +/- 16.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=235000, episode_reward=-2649.60 +/- 19.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=236000, episode_reward=-2631.60 +/- 15.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=237000, episode_reward=-2659.80 +/- 19.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=238000, episode_reward=-2654.00 +/- 16.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=239000, episode_reward=-2496.80 +/- 302.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=240000, episode_reward=-2662.80 +/- 12.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=241000, episode_reward=-2643.40 +/- 12.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=242000, episode_reward=-2646.00 +/- 9.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=243000, episode_reward=-2642.20 +/- 18.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=244000, episode_reward=-2653.60 +/- 26.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=245000, episode_reward=-2752.80 +/- 53.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=246000, episode_reward=-2640.00 +/- 20.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=247000, episode_reward=-2666.40 +/- 16.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=248000, episode_reward=-2685.60 +/- 14.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=249000, episode_reward=-2484.40 +/- 303.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=250000, episode_reward=-2636.40 +/- 20.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=251000, episode_reward=-2645.60 +/- 18.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=252000, episode_reward=-2659.60 +/- 19.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=253000, episode_reward=-2652.80 +/- 11.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=254000, episode_reward=-2657.20 +/- 9.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=255000, episode_reward=-2662.40 +/- 23.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=256000, episode_reward=-2643.20 +/- 8.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=257000, episode_reward=-2496.80 +/- 309.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=258000, episode_reward=-2631.20 +/- 9.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=259000, episode_reward=-2652.00 +/- 11.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=260000, episode_reward=-2644.00 +/- 4.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=261000, episode_reward=-2492.40 +/- 320.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=262000, episode_reward=-2648.20 +/- 20.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=263000, episode_reward=-2662.80 +/- 7.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=264000, episode_reward=-2650.00 +/- 25.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=265000, episode_reward=-2645.20 +/- 17.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=266000, episode_reward=-2648.80 +/- 16.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=267000, episode_reward=-2675.00 +/- 22.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=268000, episode_reward=-2634.40 +/- 22.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=269000, episode_reward=-2508.60 +/- 314.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=270000, episode_reward=-2633.60 +/- 12.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=271000, episode_reward=-2674.40 +/- 13.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=272000, episode_reward=-2503.60 +/- 307.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=273000, episode_reward=-2507.60 +/- 306.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=274000, episode_reward=-2521.60 +/- 315.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=275000, episode_reward=-2631.00 +/- 27.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=276000, episode_reward=-2648.80 +/- 16.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=277000, episode_reward=-2667.60 +/- 23.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=278000, episode_reward=-2640.20 +/- 26.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=279000, episode_reward=-2645.60 +/- 8.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=280000, episode_reward=-2660.40 +/- 19.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=281000, episode_reward=-2651.80 +/- 19.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=282000, episode_reward=-2505.00 +/- 307.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=283000, episode_reward=-2499.20 +/- 314.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=284000, episode_reward=-2639.80 +/- 20.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=285000, episode_reward=-2725.80 +/- 29.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=286000, episode_reward=-2644.60 +/- 20.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=287000, episode_reward=-2653.00 +/- 13.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=288000, episode_reward=-2654.00 +/- 11.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=289000, episode_reward=-2676.40 +/- 16.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=290000, episode_reward=-2638.00 +/- 14.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=291000, episode_reward=-2658.60 +/- 24.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=292000, episode_reward=-2658.00 +/- 19.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=293000, episode_reward=-2664.00 +/- 4.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=294000, episode_reward=-2668.60 +/- 21.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=295000, episode_reward=-2653.20 +/- 10.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=296000, episode_reward=-2679.60 +/- 13.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=297000, episode_reward=-2640.80 +/- 14.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=298000, episode_reward=-2684.80 +/- 16.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=299000, episode_reward=-2653.80 +/- 32.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=300000, episode_reward=-2484.40 +/- 315.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=301000, episode_reward=-2646.40 +/- 36.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=302000, episode_reward=-2654.00 +/- 10.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=303000, episode_reward=-2640.80 +/- 14.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=304000, episode_reward=-2650.80 +/- 14.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=305000, episode_reward=-2500.40 +/- 313.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=306000, episode_reward=-2635.20 +/- 9.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=307000, episode_reward=-2651.60 +/- 20.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=308000, episode_reward=-2665.20 +/- 12.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=309000, episode_reward=-2637.20 +/- 7.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=310000, episode_reward=-2334.40 +/- 380.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=311000, episode_reward=-2651.80 +/- 23.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=312000, episode_reward=-2677.80 +/- 31.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=313000, episode_reward=-2502.60 +/- 313.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=314000, episode_reward=-2647.60 +/- 11.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=315000, episode_reward=-2659.00 +/- 11.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=316000, episode_reward=-2670.80 +/- 22.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=317000, episode_reward=-2629.60 +/- 20.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=318000, episode_reward=-2654.00 +/- 13.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=319000, episode_reward=-2638.40 +/- 10.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=320000, episode_reward=-2652.80 +/- 10.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=321000, episode_reward=-2667.20 +/- 17.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=322000, episode_reward=-2635.20 +/- 16.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=323000, episode_reward=-2686.80 +/- 23.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=324000, episode_reward=-2750.00 +/- 19.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=325000, episode_reward=-2655.60 +/- 22.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=326000, episode_reward=-2694.00 +/- 28.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=327000, episode_reward=-2680.20 +/- 19.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=328000, episode_reward=-2694.80 +/- 13.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=329000, episode_reward=-2700.20 +/- 17.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=330000, episode_reward=-2715.40 +/- 14.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=331000, episode_reward=-2660.00 +/- 11.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=332000, episode_reward=-2656.40 +/- 11.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=333000, episode_reward=-2660.40 +/- 17.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=334000, episode_reward=-2503.60 +/- 320.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=335000, episode_reward=-2639.20 +/- 20.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=336000, episode_reward=-2638.00 +/- 19.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=337000, episode_reward=-2638.00 +/- 20.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=338000, episode_reward=-2683.20 +/- 20.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=339000, episode_reward=-2664.20 +/- 6.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=340000, episode_reward=-2652.00 +/- 14.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=341000, episode_reward=-2639.20 +/- 27.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=342000, episode_reward=-2675.00 +/- 19.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=343000, episode_reward=-2664.20 +/- 10.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=344000, episode_reward=-2650.40 +/- 14.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=345000, episode_reward=-2646.00 +/- 16.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=346000, episode_reward=-2638.40 +/- 22.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=347000, episode_reward=-2654.80 +/- 16.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=348000, episode_reward=-2499.40 +/- 298.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=349000, episode_reward=-2658.00 +/- 5.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=350000, episode_reward=-2656.80 +/- 21.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=351000, episode_reward=-2663.20 +/- 13.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=352000, episode_reward=-2649.20 +/- 14.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=353000, episode_reward=-2652.80 +/- 13.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=354000, episode_reward=-2497.20 +/- 324.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=355000, episode_reward=-2635.20 +/- 19.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=356000, episode_reward=-2492.80 +/- 312.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=357000, episode_reward=-2658.40 +/- 17.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=358000, episode_reward=-2642.80 +/- 17.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=359000, episode_reward=-2647.20 +/- 18.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=360000, episode_reward=-2640.40 +/- 25.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=361000, episode_reward=-2638.00 +/- 19.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=362000, episode_reward=-2646.80 +/- 22.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=363000, episode_reward=-2650.80 +/- 14.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=364000, episode_reward=-2671.60 +/- 9.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=365000, episode_reward=-2478.20 +/- 306.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=366000, episode_reward=-2654.40 +/- 8.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=367000, episode_reward=-2672.60 +/- 16.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=368000, episode_reward=-2670.80 +/- 14.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=369000, episode_reward=-2636.00 +/- 9.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=370000, episode_reward=-2656.00 +/- 24.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=371000, episode_reward=-2641.60 +/- 11.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=372000, episode_reward=-2652.40 +/- 33.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=373000, episode_reward=-2662.60 +/- 21.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=374000, episode_reward=-2490.40 +/- 310.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=375000, episode_reward=-2654.00 +/- 19.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=376000, episode_reward=-2698.40 +/- 11.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=377000, episode_reward=-2654.20 +/- 15.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=378000, episode_reward=-2636.80 +/- 9.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=379000, episode_reward=-2662.80 +/- 15.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=380000, episode_reward=-2685.60 +/- 38.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=381000, episode_reward=-3370.40 +/- 525.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=382000, episode_reward=-2641.40 +/- 12.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=383000, episode_reward=-2637.60 +/- 18.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=384000, episode_reward=-4102.40 +/- 50.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=385000, episode_reward=-2630.80 +/- 25.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=386000, episode_reward=-2652.20 +/- 14.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=387000, episode_reward=-2496.00 +/- 303.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=388000, episode_reward=-2646.80 +/- 10.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=389000, episode_reward=-2640.40 +/- 16.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=390000, episode_reward=-2645.00 +/- 21.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=391000, episode_reward=-2499.20 +/- 307.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=392000, episode_reward=-2650.80 +/- 12.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=393000, episode_reward=-2477.20 +/- 304.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=394000, episode_reward=-2502.00 +/- 318.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=395000, episode_reward=-2647.60 +/- 7.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=396000, episode_reward=-2647.60 +/- 18.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=397000, episode_reward=-2492.00 +/- 312.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=398000, episode_reward=-2479.60 +/- 306.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=399000, episode_reward=-2637.20 +/- 23.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=400000, episode_reward=-2646.40 +/- 21.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=401000, episode_reward=-2484.80 +/- 303.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=402000, episode_reward=-2641.60 +/- 19.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=403000, episode_reward=-2665.60 +/- 18.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=404000, episode_reward=-2661.00 +/- 31.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=405000, episode_reward=-2633.20 +/- 14.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=406000, episode_reward=-2650.00 +/- 21.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=407000, episode_reward=-2475.60 +/- 304.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=408000, episode_reward=-2661.20 +/- 18.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=409000, episode_reward=-2650.40 +/- 26.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=410000, episode_reward=-2643.60 +/- 21.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=411000, episode_reward=-2655.60 +/- 13.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=412000, episode_reward=-2650.00 +/- 23.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=413000, episode_reward=-2638.80 +/- 23.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=414000, episode_reward=-2650.40 +/- 21.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=415000, episode_reward=-2642.40 +/- 15.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=416000, episode_reward=-2483.60 +/- 316.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=417000, episode_reward=-2650.00 +/- 15.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=418000, episode_reward=-2699.80 +/- 19.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=419000, episode_reward=-2657.20 +/- 18.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=420000, episode_reward=-2643.60 +/- 17.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=421000, episode_reward=-2661.80 +/- 15.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=422000, episode_reward=-2667.80 +/- 24.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=423000, episode_reward=-2665.60 +/- 30.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=424000, episode_reward=-2724.80 +/- 25.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=425000, episode_reward=-2542.80 +/- 306.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=426000, episode_reward=-2662.00 +/- 15.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=427000, episode_reward=-2493.20 +/- 307.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=428000, episode_reward=-2688.80 +/- 29.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=429000, episode_reward=-2504.00 +/- 319.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=430000, episode_reward=-2705.40 +/- 15.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=431000, episode_reward=-2659.00 +/- 27.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=432000, episode_reward=-2496.80 +/- 307.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=433000, episode_reward=-2483.20 +/- 313.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=434000, episode_reward=-2670.00 +/- 27.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=435000, episode_reward=-2678.40 +/- 26.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=436000, episode_reward=-2677.40 +/- 9.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=437000, episode_reward=-2678.00 +/- 20.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=438000, episode_reward=-2647.00 +/- 21.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=439000, episode_reward=-2649.60 +/- 14.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=440000, episode_reward=-2711.60 +/- 32.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=441000, episode_reward=-2496.00 +/- 316.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=442000, episode_reward=-2642.40 +/- 20.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=443000, episode_reward=-2649.80 +/- 27.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=444000, episode_reward=-2336.80 +/- 385.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=445000, episode_reward=-2642.00 +/- 15.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=446000, episode_reward=-2495.20 +/- 310.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=447000, episode_reward=-2633.60 +/- 9.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=448000, episode_reward=-2657.60 +/- 12.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=449000, episode_reward=-2644.40 +/- 27.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=450000, episode_reward=-2644.80 +/- 10.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=451000, episode_reward=-2693.60 +/- 19.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=452000, episode_reward=-2703.60 +/- 10.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=453000, episode_reward=-2526.60 +/- 310.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=454000, episode_reward=-2520.40 +/- 307.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=455000, episode_reward=-2657.00 +/- 18.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=456000, episode_reward=-2673.00 +/- 22.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=457000, episode_reward=-2684.20 +/- 29.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=458000, episode_reward=-2689.80 +/- 15.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=459000, episode_reward=-2773.80 +/- 25.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=460000, episode_reward=-2540.60 +/- 301.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=461000, episode_reward=-2673.00 +/- 26.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=462000, episode_reward=-2653.60 +/- 15.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=463000, episode_reward=-2664.80 +/- 23.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=464000, episode_reward=-2634.80 +/- 7.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=465000, episode_reward=-2650.80 +/- 17.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=466000, episode_reward=-2665.60 +/- 16.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=467000, episode_reward=-2635.20 +/- 15.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=468000, episode_reward=-2645.60 +/- 22.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=469000, episode_reward=-2634.00 +/- 20.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=470000, episode_reward=-2642.40 +/- 20.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=471000, episode_reward=-2645.20 +/- 23.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=472000, episode_reward=-2650.40 +/- 21.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=473000, episode_reward=-2647.00 +/- 23.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=474000, episode_reward=-2483.20 +/- 308.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=475000, episode_reward=-2668.00 +/- 24.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=476000, episode_reward=-2640.80 +/- 29.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=477000, episode_reward=-2633.20 +/- 22.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=478000, episode_reward=-2492.80 +/- 313.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=479000, episode_reward=-2651.20 +/- 13.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=480000, episode_reward=-2648.00 +/- 21.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=481000, episode_reward=-2646.00 +/- 28.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=482000, episode_reward=-2653.20 +/- 9.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=483000, episode_reward=-2645.20 +/- 13.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=484000, episode_reward=-2730.00 +/- 24.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=485000, episode_reward=-2490.00 +/- 304.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=486000, episode_reward=-2637.20 +/- 18.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=487000, episode_reward=-2687.20 +/- 23.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=488000, episode_reward=-2649.20 +/- 13.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=489000, episode_reward=-2659.60 +/- 16.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=490000, episode_reward=-2528.40 +/- 312.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=491000, episode_reward=-2644.40 +/- 10.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=492000, episode_reward=-2648.00 +/- 22.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=493000, episode_reward=-2626.40 +/- 25.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=494000, episode_reward=-2643.60 +/- 19.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=495000, episode_reward=-2640.00 +/- 23.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=496000, episode_reward=-2643.20 +/- 13.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=497000, episode_reward=-2648.80 +/- 11.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=498000, episode_reward=-2645.40 +/- 18.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=499000, episode_reward=-2637.60 +/- 10.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=500000, episode_reward=-2640.40 +/- 8.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=501000, episode_reward=-2646.80 +/- 14.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=502000, episode_reward=-2666.00 +/- 14.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=503000, episode_reward=-2641.60 +/- 22.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=504000, episode_reward=-2640.80 +/- 11.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=505000, episode_reward=-2642.80 +/- 10.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=506000, episode_reward=-2634.40 +/- 20.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=507000, episode_reward=-2651.20 +/- 22.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=508000, episode_reward=-2644.40 +/- 15.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=509000, episode_reward=-2657.20 +/- 15.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=510000, episode_reward=-2633.20 +/- 14.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=511000, episode_reward=-2486.00 +/- 306.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=512000, episode_reward=-2601.80 +/- 312.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=513000, episode_reward=-2653.60 +/- 36.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=514000, episode_reward=-2646.80 +/- 16.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=515000, episode_reward=-2477.20 +/- 310.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=516000, episode_reward=-2654.60 +/- 12.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=517000, episode_reward=-2636.00 +/- 10.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=518000, episode_reward=-2649.20 +/- 23.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=519000, episode_reward=-2478.40 +/- 307.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=520000, episode_reward=-2636.80 +/- 7.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=521000, episode_reward=-2494.00 +/- 304.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=522000, episode_reward=-2642.40 +/- 17.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=523000, episode_reward=-2655.40 +/- 23.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=524000, episode_reward=-2644.80 +/- 12.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=525000, episode_reward=-2651.00 +/- 15.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=526000, episode_reward=-2661.20 +/- 5.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=527000, episode_reward=-2647.00 +/- 12.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=528000, episode_reward=-2671.00 +/- 11.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=529000, episode_reward=-2637.80 +/- 15.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=530000, episode_reward=-2646.80 +/- 23.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=531000, episode_reward=-2651.60 +/- 15.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=532000, episode_reward=-2628.40 +/- 12.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=533000, episode_reward=-2657.20 +/- 8.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=534000, episode_reward=-2645.40 +/- 18.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=535000, episode_reward=-2366.60 +/- 353.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=536000, episode_reward=-2656.80 +/- 16.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=537000, episode_reward=-2491.20 +/- 310.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=538000, episode_reward=-2646.00 +/- 8.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=539000, episode_reward=-2642.00 +/- 10.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=540000, episode_reward=-2645.60 +/- 33.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=541000, episode_reward=-2684.40 +/- 17.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=542000, episode_reward=-2647.80 +/- 10.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=543000, episode_reward=-2779.00 +/- 25.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=544000, episode_reward=-2763.00 +/- 17.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=545000, episode_reward=-2690.40 +/- 29.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=546000, episode_reward=-2680.80 +/- 20.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=547000, episode_reward=-2818.60 +/- 40.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=548000, episode_reward=-2682.80 +/- 19.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=549000, episode_reward=-2661.20 +/- 18.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=550000, episode_reward=-2501.40 +/- 311.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=551000, episode_reward=-2327.60 +/- 375.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=552000, episode_reward=-2652.80 +/- 14.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=553000, episode_reward=-2633.20 +/- 20.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=554000, episode_reward=-2489.60 +/- 304.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=555000, episode_reward=-2637.60 +/- 8.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=556000, episode_reward=-2662.20 +/- 21.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=557000, episode_reward=-2652.60 +/- 19.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=558000, episode_reward=-2342.80 +/- 391.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=559000, episode_reward=-2490.40 +/- 304.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=560000, episode_reward=-2658.40 +/- 18.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=561000, episode_reward=-2647.20 +/- 23.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=562000, episode_reward=-2661.60 +/- 18.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=563000, episode_reward=-2649.60 +/- 25.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=564000, episode_reward=-2660.20 +/- 17.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=565000, episode_reward=-2665.60 +/- 19.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=566000, episode_reward=-2645.60 +/- 16.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=567000, episode_reward=-2657.60 +/- 11.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=568000, episode_reward=-2676.60 +/- 12.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=569000, episode_reward=-2650.40 +/- 20.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=570000, episode_reward=-2655.00 +/- 16.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=571000, episode_reward=-2637.20 +/- 16.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=572000, episode_reward=-2645.20 +/- 26.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=573000, episode_reward=-2487.60 +/- 307.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=574000, episode_reward=-2485.60 +/- 310.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=575000, episode_reward=-2644.80 +/- 23.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=576000, episode_reward=-2634.80 +/- 15.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=577000, episode_reward=-2386.00 +/- 360.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=578000, episode_reward=-2646.40 +/- 14.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=579000, episode_reward=-2486.80 +/- 318.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=580000, episode_reward=-2656.00 +/- 20.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=581000, episode_reward=-2642.80 +/- 28.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=582000, episode_reward=-2332.80 +/- 367.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=583000, episode_reward=-2640.00 +/- 16.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=584000, episode_reward=-2638.00 +/- 25.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=585000, episode_reward=-2639.60 +/- 29.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=586000, episode_reward=-2636.00 +/- 15.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=587000, episode_reward=-2642.00 +/- 21.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=588000, episode_reward=-2645.20 +/- 11.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=589000, episode_reward=-2656.00 +/- 15.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=590000, episode_reward=-2686.40 +/- 13.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=591000, episode_reward=-2513.20 +/- 315.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=592000, episode_reward=-2642.80 +/- 20.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=593000, episode_reward=-2711.80 +/- 21.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=594000, episode_reward=-2652.40 +/- 30.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=595000, episode_reward=-2659.40 +/- 11.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=596000, episode_reward=-2655.20 +/- 26.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=597000, episode_reward=-2642.00 +/- 13.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=598000, episode_reward=-2642.00 +/- 12.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=599000, episode_reward=-2650.40 +/- 13.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=600000, episode_reward=-2683.40 +/- 18.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=601000, episode_reward=-2652.00 +/- 14.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=602000, episode_reward=-2650.40 +/- 13.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=603000, episode_reward=-2629.20 +/- 25.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=604000, episode_reward=-2640.40 +/- 23.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=605000, episode_reward=-2668.20 +/- 23.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=606000, episode_reward=-2641.20 +/- 11.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=607000, episode_reward=-2646.40 +/- 25.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=608000, episode_reward=-2668.40 +/- 20.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=609000, episode_reward=-2643.60 +/- 19.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=610000, episode_reward=-2656.40 +/- 15.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=611000, episode_reward=-2670.00 +/- 15.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=612000, episode_reward=-2649.20 +/- 3.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=613000, episode_reward=-2680.80 +/- 14.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=614000, episode_reward=-2695.00 +/- 22.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=615000, episode_reward=-2640.00 +/- 11.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=616000, episode_reward=-2656.80 +/- 14.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=617000, episode_reward=-2490.80 +/- 304.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=618000, episode_reward=-2639.80 +/- 24.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=619000, episode_reward=-2654.00 +/- 15.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=620000, episode_reward=-2660.00 +/- 15.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=621000, episode_reward=-2646.00 +/- 14.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=622000, episode_reward=-2628.40 +/- 18.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=623000, episode_reward=-2485.20 +/- 307.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=624000, episode_reward=-2478.40 +/- 315.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=625000, episode_reward=-2491.60 +/- 313.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=626000, episode_reward=-2636.80 +/- 17.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=627000, episode_reward=-2653.60 +/- 13.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=628000, episode_reward=-2642.40 +/- 18.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=629000, episode_reward=-2655.20 +/- 21.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=630000, episode_reward=-2642.40 +/- 11.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=631000, episode_reward=-2634.80 +/- 12.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=632000, episode_reward=-2664.20 +/- 11.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=633000, episode_reward=-2637.20 +/- 22.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=634000, episode_reward=-2664.00 +/- 14.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=635000, episode_reward=-2634.40 +/- 18.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=636000, episode_reward=-2654.00 +/- 16.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=637000, episode_reward=-2650.00 +/- 18.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=638000, episode_reward=-2645.20 +/- 11.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=639000, episode_reward=-2642.80 +/- 20.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=640000, episode_reward=-2654.80 +/- 24.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=641000, episode_reward=-2648.40 +/- 17.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=642000, episode_reward=-2650.80 +/- 5.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=643000, episode_reward=-2632.80 +/- 16.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=644000, episode_reward=-2799.60 +/- 22.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=645000, episode_reward=-2516.40 +/- 305.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=646000, episode_reward=-2640.40 +/- 21.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=647000, episode_reward=-2652.40 +/- 18.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=648000, episode_reward=-2681.80 +/- 24.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=649000, episode_reward=-2529.20 +/- 289.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=650000, episode_reward=-2663.00 +/- 25.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=651000, episode_reward=-2661.00 +/- 23.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=652000, episode_reward=-2649.60 +/- 17.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=653000, episode_reward=-2516.20 +/- 320.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=654000, episode_reward=-2501.20 +/- 313.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=655000, episode_reward=-2643.60 +/- 17.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=656000, episode_reward=-2459.20 +/- 299.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=657000, episode_reward=-2652.00 +/- 13.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=658000, episode_reward=-2664.60 +/- 12.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=659000, episode_reward=-2629.60 +/- 13.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=660000, episode_reward=-2495.20 +/- 314.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=661000, episode_reward=-2513.20 +/- 301.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=662000, episode_reward=-2656.40 +/- 19.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=663000, episode_reward=-2655.00 +/- 15.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=664000, episode_reward=-2654.00 +/- 15.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=665000, episode_reward=-2645.40 +/- 7.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=666000, episode_reward=-2640.40 +/- 22.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=667000, episode_reward=-2647.80 +/- 33.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=668000, episode_reward=-2498.00 +/- 320.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=669000, episode_reward=-2696.60 +/- 18.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=670000, episode_reward=-2645.60 +/- 11.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=671000, episode_reward=-2664.00 +/- 19.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=672000, episode_reward=-2666.60 +/- 29.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=673000, episode_reward=-2646.80 +/- 15.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=674000, episode_reward=-2649.20 +/- 23.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=675000, episode_reward=-2630.80 +/- 27.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=676000, episode_reward=-2638.00 +/- 18.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=677000, episode_reward=-2637.60 +/- 19.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=678000, episode_reward=-2655.20 +/- 22.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=679000, episode_reward=-2646.40 +/- 22.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=680000, episode_reward=-2631.60 +/- 17.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=681000, episode_reward=-2638.00 +/- 15.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=682000, episode_reward=-2668.80 +/- 32.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=683000, episode_reward=-2657.20 +/- 10.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=684000, episode_reward=-2672.80 +/- 19.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=685000, episode_reward=-2650.60 +/- 16.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=686000, episode_reward=-2660.80 +/- 9.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=687000, episode_reward=-2639.60 +/- 8.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=688000, episode_reward=-2679.00 +/- 8.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=689000, episode_reward=-2662.80 +/- 24.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=690000, episode_reward=-2648.80 +/- 25.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=691000, episode_reward=-2631.40 +/- 19.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=692000, episode_reward=-2654.80 +/- 21.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=693000, episode_reward=-2636.40 +/- 16.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=694000, episode_reward=-2649.20 +/- 10.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=695000, episode_reward=-2677.40 +/- 13.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=696000, episode_reward=-2632.40 +/- 13.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=697000, episode_reward=-2639.20 +/- 11.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=698000, episode_reward=-2677.60 +/- 25.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=699000, episode_reward=-2649.20 +/- 18.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=700000, episode_reward=-2679.40 +/- 20.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=701000, episode_reward=-2477.60 +/- 300.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=702000, episode_reward=-2655.60 +/- 23.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=703000, episode_reward=-2640.00 +/- 16.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=704000, episode_reward=-2645.80 +/- 19.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=705000, episode_reward=-2651.80 +/- 23.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=706000, episode_reward=-2657.60 +/- 18.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=707000, episode_reward=-2661.60 +/- 22.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=708000, episode_reward=-2496.20 +/- 319.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=709000, episode_reward=-2645.60 +/- 7.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=710000, episode_reward=-2655.20 +/- 15.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=711000, episode_reward=-2642.80 +/- 9.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=712000, episode_reward=-2635.60 +/- 26.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=713000, episode_reward=-2474.00 +/- 312.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=714000, episode_reward=-2683.60 +/- 19.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=715000, episode_reward=-2662.60 +/- 9.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=716000, episode_reward=-2633.60 +/- 10.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=717000, episode_reward=-2649.80 +/- 14.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=718000, episode_reward=-2652.40 +/- 17.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=719000, episode_reward=-2648.80 +/- 8.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=720000, episode_reward=-2645.40 +/- 11.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=721000, episode_reward=-2642.00 +/- 28.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=722000, episode_reward=-2688.80 +/- 12.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=723000, episode_reward=-2639.60 +/- 8.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=724000, episode_reward=-2635.20 +/- 16.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=725000, episode_reward=-2652.80 +/- 20.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=726000, episode_reward=-2653.00 +/- 28.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=727000, episode_reward=-2646.00 +/- 18.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=728000, episode_reward=-2650.20 +/- 21.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=729000, episode_reward=-2701.20 +/- 14.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=730000, episode_reward=-2643.40 +/- 10.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=731000, episode_reward=-2649.60 +/- 23.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=732000, episode_reward=-2482.00 +/- 304.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=733000, episode_reward=-2486.80 +/- 312.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=734000, episode_reward=-2492.40 +/- 308.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=735000, episode_reward=-2652.80 +/- 22.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=736000, episode_reward=-2645.80 +/- 10.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=737000, episode_reward=-2636.20 +/- 19.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=738000, episode_reward=-2642.00 +/- 12.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=739000, episode_reward=-2508.20 +/- 304.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=740000, episode_reward=-2672.40 +/- 26.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=741000, episode_reward=-2634.00 +/- 16.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=742000, episode_reward=-2644.40 +/- 6.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=743000, episode_reward=-2482.00 +/- 318.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=744000, episode_reward=-2497.00 +/- 313.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=745000, episode_reward=-2642.00 +/- 7.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=746000, episode_reward=-2673.60 +/- 15.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=747000, episode_reward=-2664.00 +/- 12.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=748000, episode_reward=-2658.60 +/- 20.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=749000, episode_reward=-2646.80 +/- 13.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=750000, episode_reward=-2488.40 +/- 308.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=751000, episode_reward=-2653.80 +/- 30.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=752000, episode_reward=-2650.40 +/- 17.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=753000, episode_reward=-2497.60 +/- 305.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=754000, episode_reward=-2650.40 +/- 10.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=755000, episode_reward=-2655.40 +/- 15.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=756000, episode_reward=-2657.40 +/- 24.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=757000, episode_reward=-2640.00 +/- 8.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=758000, episode_reward=-2653.60 +/- 18.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=759000, episode_reward=-2495.80 +/- 311.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=760000, episode_reward=-2643.60 +/- 14.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=761000, episode_reward=-2640.60 +/- 24.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=762000, episode_reward=-2494.00 +/- 316.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=763000, episode_reward=-2652.40 +/- 18.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=764000, episode_reward=-2671.60 +/- 24.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=765000, episode_reward=-2645.60 +/- 9.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=766000, episode_reward=-2660.00 +/- 19.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=767000, episode_reward=-2639.60 +/- 27.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=768000, episode_reward=-2675.40 +/- 12.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=769000, episode_reward=-2501.60 +/- 309.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=770000, episode_reward=-2647.20 +/- 27.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=771000, episode_reward=-2481.80 +/- 300.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=772000, episode_reward=-2653.00 +/- 12.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=773000, episode_reward=-2478.00 +/- 308.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=774000, episode_reward=-2506.40 +/- 319.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=775000, episode_reward=-2553.80 +/- 297.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=776000, episode_reward=-2496.40 +/- 321.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=777000, episode_reward=-2654.20 +/- 16.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=778000, episode_reward=-2639.60 +/- 14.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=779000, episode_reward=-2628.40 +/- 21.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=780000, episode_reward=-2642.00 +/- 22.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=781000, episode_reward=-2648.40 +/- 21.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=782000, episode_reward=-2642.00 +/- 18.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=783000, episode_reward=-2648.20 +/- 12.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=784000, episode_reward=-2658.40 +/- 7.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=785000, episode_reward=-2644.00 +/- 21.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=786000, episode_reward=-2655.80 +/- 18.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=787000, episode_reward=-2480.00 +/- 309.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=788000, episode_reward=-2674.40 +/- 22.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=789000, episode_reward=-2476.00 +/- 304.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=790000, episode_reward=-2655.60 +/- 20.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=791000, episode_reward=-2640.40 +/- 12.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=792000, episode_reward=-2341.20 +/- 391.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=793000, episode_reward=-2640.20 +/- 23.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=794000, episode_reward=-2643.20 +/- 29.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=795000, episode_reward=-2650.00 +/- 20.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=796000, episode_reward=-2496.60 +/- 306.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=797000, episode_reward=-2658.00 +/- 11.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=798000, episode_reward=-2664.80 +/- 16.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=799000, episode_reward=-2664.00 +/- 16.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=800000, episode_reward=-2653.60 +/- 14.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=801000, episode_reward=-2636.80 +/- 18.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=802000, episode_reward=-2505.00 +/- 312.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=803000, episode_reward=-2661.00 +/- 9.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=804000, episode_reward=-2649.40 +/- 18.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=805000, episode_reward=-2462.60 +/- 294.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=806000, episode_reward=-2644.00 +/- 23.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=807000, episode_reward=-2668.20 +/- 26.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=808000, episode_reward=-2648.00 +/- 14.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=809000, episode_reward=-2640.60 +/- 22.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=810000, episode_reward=-2663.20 +/- 13.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=811000, episode_reward=-2649.20 +/- 12.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=812000, episode_reward=-2565.60 +/- 295.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=813000, episode_reward=-2643.60 +/- 13.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=814000, episode_reward=-2518.40 +/- 305.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=815000, episode_reward=-2653.40 +/- 26.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=816000, episode_reward=-2665.00 +/- 24.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=817000, episode_reward=-2486.40 +/- 312.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=818000, episode_reward=-2650.40 +/- 26.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=819000, episode_reward=-2650.80 +/- 19.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=820000, episode_reward=-2312.00 +/- 369.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=821000, episode_reward=-2648.40 +/- 19.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=822000, episode_reward=-2476.40 +/- 307.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=823000, episode_reward=-2479.20 +/- 301.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=824000, episode_reward=-2648.00 +/- 22.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=825000, episode_reward=-2652.00 +/- 15.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=826000, episode_reward=-2646.40 +/- 17.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=827000, episode_reward=-2650.00 +/- 14.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=828000, episode_reward=-2645.20 +/- 3.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=829000, episode_reward=-2654.00 +/- 24.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=830000, episode_reward=-2658.80 +/- 30.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=831000, episode_reward=-2492.00 +/- 305.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=832000, episode_reward=-2491.20 +/- 318.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=833000, episode_reward=-2650.00 +/- 17.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=834000, episode_reward=-2653.60 +/- 15.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=835000, episode_reward=-2660.80 +/- 13.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=836000, episode_reward=-2649.60 +/- 13.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=837000, episode_reward=-2645.80 +/- 10.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=838000, episode_reward=-2506.80 +/- 306.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=839000, episode_reward=-2648.40 +/- 23.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=840000, episode_reward=-2506.40 +/- 316.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=841000, episode_reward=-2361.00 +/- 396.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=842000, episode_reward=-2654.80 +/- 13.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=843000, episode_reward=-2336.00 +/- 380.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=844000, episode_reward=-2660.40 +/- 19.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=845000, episode_reward=-2668.60 +/- 9.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=846000, episode_reward=-2637.40 +/- 15.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=847000, episode_reward=-2653.60 +/- 17.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=848000, episode_reward=-2662.80 +/- 11.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=849000, episode_reward=-2640.60 +/- 24.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=850000, episode_reward=-2635.20 +/- 15.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=851000, episode_reward=-2505.40 +/- 309.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=852000, episode_reward=-2641.20 +/- 11.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=853000, episode_reward=-2482.40 +/- 304.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=854000, episode_reward=-2664.40 +/- 19.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=855000, episode_reward=-2678.20 +/- 19.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=856000, episode_reward=-2650.40 +/- 16.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=857000, episode_reward=-2502.60 +/- 308.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=858000, episode_reward=-2641.20 +/- 22.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=859000, episode_reward=-2645.40 +/- 11.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=860000, episode_reward=-2645.00 +/- 8.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=861000, episode_reward=-2647.80 +/- 19.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=862000, episode_reward=-2635.60 +/- 23.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=863000, episode_reward=-2639.40 +/- 17.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=864000, episode_reward=-2484.80 +/- 311.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=865000, episode_reward=-2492.40 +/- 314.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=866000, episode_reward=-2626.40 +/- 8.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=867000, episode_reward=-2643.40 +/- 20.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=868000, episode_reward=-2681.20 +/- 21.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=869000, episode_reward=-2670.60 +/- 16.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=870000, episode_reward=-2493.20 +/- 305.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=871000, episode_reward=-2485.20 +/- 308.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=872000, episode_reward=-2649.20 +/- 6.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=873000, episode_reward=-2654.40 +/- 18.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=874000, episode_reward=-2659.20 +/- 22.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=875000, episode_reward=-2645.60 +/- 16.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=876000, episode_reward=-2493.60 +/- 315.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=877000, episode_reward=-2650.80 +/- 17.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=878000, episode_reward=-2652.40 +/- 11.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=879000, episode_reward=-2655.20 +/- 8.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=880000, episode_reward=-2675.20 +/- 17.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=881000, episode_reward=-2649.40 +/- 30.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=882000, episode_reward=-2644.80 +/- 19.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=883000, episode_reward=-2481.60 +/- 305.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=884000, episode_reward=-2643.60 +/- 16.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=885000, episode_reward=-2664.20 +/- 23.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=886000, episode_reward=-2654.40 +/- 24.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=887000, episode_reward=-2646.80 +/- 19.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=888000, episode_reward=-2656.40 +/- 14.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=889000, episode_reward=-2645.60 +/- 15.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=890000, episode_reward=-2497.20 +/- 315.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=891000, episode_reward=-2649.60 +/- 13.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=892000, episode_reward=-2493.20 +/- 304.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=893000, episode_reward=-2502.80 +/- 301.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=894000, episode_reward=-2647.20 +/- 20.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=895000, episode_reward=-2496.40 +/- 298.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=896000, episode_reward=-2650.40 +/- 18.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=897000, episode_reward=-2643.20 +/- 4.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=898000, episode_reward=-2674.80 +/- 10.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=899000, episode_reward=-2654.40 +/- 23.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=900000, episode_reward=-2647.60 +/- 17.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=901000, episode_reward=-2685.80 +/- 18.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=902000, episode_reward=-2662.80 +/- 20.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=903000, episode_reward=-2675.40 +/- 19.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=904000, episode_reward=-2710.20 +/- 31.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=905000, episode_reward=-2635.60 +/- 18.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=906000, episode_reward=-2641.80 +/- 28.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=907000, episode_reward=-2692.80 +/- 14.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=908000, episode_reward=-2647.60 +/- 8.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=909000, episode_reward=-2665.40 +/- 16.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=910000, episode_reward=-2500.20 +/- 306.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=911000, episode_reward=-2639.00 +/- 14.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=912000, episode_reward=-2650.80 +/- 12.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=913000, episode_reward=-2518.80 +/- 305.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=914000, episode_reward=-2651.60 +/- 18.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=915000, episode_reward=-2653.00 +/- 20.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=916000, episode_reward=-2646.00 +/- 3.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=917000, episode_reward=-2640.60 +/- 12.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=918000, episode_reward=-2628.40 +/- 13.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=919000, episode_reward=-2649.20 +/- 15.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=920000, episode_reward=-2641.20 +/- 22.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=921000, episode_reward=-2664.00 +/- 10.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=922000, episode_reward=-2634.40 +/- 14.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=923000, episode_reward=-2656.00 +/- 22.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=924000, episode_reward=-2664.40 +/- 17.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=925000, episode_reward=-2648.40 +/- 20.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=926000, episode_reward=-2497.40 +/- 303.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=927000, episode_reward=-2506.20 +/- 309.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=928000, episode_reward=-2652.60 +/- 9.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=929000, episode_reward=-2502.40 +/- 321.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=930000, episode_reward=-2624.40 +/- 24.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=931000, episode_reward=-2639.60 +/- 18.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=932000, episode_reward=-2647.60 +/- 14.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=933000, episode_reward=-2636.00 +/- 15.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=934000, episode_reward=-2641.20 +/- 12.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=935000, episode_reward=-2671.40 +/- 27.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=936000, episode_reward=-2638.20 +/- 19.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=937000, episode_reward=-2650.80 +/- 15.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=938000, episode_reward=-2628.20 +/- 29.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=939000, episode_reward=-2643.60 +/- 24.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=940000, episode_reward=-2509.60 +/- 308.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=941000, episode_reward=-2519.40 +/- 306.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=942000, episode_reward=-2685.80 +/- 20.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=943000, episode_reward=-2642.00 +/- 18.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=944000, episode_reward=-2669.20 +/- 8.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=945000, episode_reward=-2643.60 +/- 20.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=946000, episode_reward=-2655.60 +/- 17.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=947000, episode_reward=-2670.60 +/- 21.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=948000, episode_reward=-2635.20 +/- 20.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=949000, episode_reward=-2501.60 +/- 311.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=950000, episode_reward=-2640.80 +/- 9.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=951000, episode_reward=-2519.00 +/- 310.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=952000, episode_reward=-2660.20 +/- 12.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=953000, episode_reward=-2639.60 +/- 17.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=954000, episode_reward=-2639.60 +/- 17.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=955000, episode_reward=-2698.20 +/- 26.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=956000, episode_reward=-2764.20 +/- 18.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=957000, episode_reward=-2625.80 +/- 25.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=958000, episode_reward=-2645.60 +/- 22.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=959000, episode_reward=-2671.60 +/- 17.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=960000, episode_reward=-2645.60 +/- 27.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=961000, episode_reward=-2664.80 +/- 18.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=962000, episode_reward=-2675.80 +/- 15.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=963000, episode_reward=-2654.40 +/- 15.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=964000, episode_reward=-2508.20 +/- 309.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=965000, episode_reward=-2693.20 +/- 15.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=966000, episode_reward=-2497.60 +/- 312.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=967000, episode_reward=-2671.00 +/- 8.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=968000, episode_reward=-2483.80 +/- 308.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=969000, episode_reward=-2649.00 +/- 19.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=970000, episode_reward=-2665.00 +/- 33.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=971000, episode_reward=-2647.20 +/- 14.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=972000, episode_reward=-2649.40 +/- 25.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=973000, episode_reward=-2650.20 +/- 16.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=974000, episode_reward=-2643.60 +/- 15.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=975000, episode_reward=-2652.20 +/- 15.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=976000, episode_reward=-2648.40 +/- 13.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=977000, episode_reward=-2640.80 +/- 22.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=978000, episode_reward=-2636.80 +/- 37.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=979000, episode_reward=-2479.20 +/- 304.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=980000, episode_reward=-2640.00 +/- 21.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=981000, episode_reward=-2638.00 +/- 12.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=982000, episode_reward=-2621.20 +/- 13.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=983000, episode_reward=-2505.00 +/- 296.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=984000, episode_reward=-2767.60 +/- 51.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=985000, episode_reward=-2665.60 +/- 18.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=986000, episode_reward=-2672.00 +/- 16.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=987000, episode_reward=-2637.40 +/- 307.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=988000, episode_reward=-2752.20 +/- 15.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=989000, episode_reward=-2659.40 +/- 23.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=990000, episode_reward=-2673.40 +/- 12.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=991000, episode_reward=-2705.20 +/- 21.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=992000, episode_reward=-2693.00 +/- 33.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=993000, episode_reward=-2635.20 +/- 14.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=994000, episode_reward=-2661.00 +/- 28.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=995000, episode_reward=-2658.00 +/- 17.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=996000, episode_reward=-2651.20 +/- 11.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=997000, episode_reward=-2527.20 +/- 307.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=998000, episode_reward=-2638.40 +/- 15.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=999000, episode_reward=-2653.00 +/- 7.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1000000, episode_reward=-2656.00 +/- 16.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1001000, episode_reward=-2689.40 +/- 16.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1002000, episode_reward=-2657.60 +/- 6.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1003000, episode_reward=-2651.00 +/- 25.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1004000, episode_reward=-2641.60 +/- 19.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1005000, episode_reward=-2641.20 +/- 25.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1006000, episode_reward=-2652.80 +/- 10.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1007000, episode_reward=-2530.60 +/- 286.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1008000, episode_reward=-2631.60 +/- 22.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1009000, episode_reward=-2657.20 +/- 11.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1010000, episode_reward=-2661.60 +/- 15.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1011000, episode_reward=-2648.40 +/- 12.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1012000, episode_reward=-2493.20 +/- 320.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1013000, episode_reward=-2648.40 +/- 23.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1014000, episode_reward=-2657.20 +/- 16.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1015000, episode_reward=-2651.60 +/- 12.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1016000, episode_reward=-2643.60 +/- 18.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1017000, episode_reward=-2346.40 +/- 387.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1018000, episode_reward=-2660.80 +/- 15.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1019000, episode_reward=-2636.00 +/- 14.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1020000, episode_reward=-2646.80 +/- 16.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1021000, episode_reward=-2636.00 +/- 14.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1022000, episode_reward=-2644.00 +/- 14.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1023000, episode_reward=-2656.80 +/- 29.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1024000, episode_reward=-2665.80 +/- 21.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1025000, episode_reward=-2660.60 +/- 16.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1026000, episode_reward=-2706.40 +/- 7.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1027000, episode_reward=-2504.40 +/- 292.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1028000, episode_reward=-2650.40 +/- 15.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1029000, episode_reward=-2360.60 +/- 369.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1030000, episode_reward=-2675.20 +/- 16.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1031000, episode_reward=-2696.20 +/- 38.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1032000, episode_reward=-2702.80 +/- 53.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1033000, episode_reward=-2704.60 +/- 19.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1034000, episode_reward=-2652.00 +/- 13.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1035000, episode_reward=-2654.80 +/- 9.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1036000, episode_reward=-2646.40 +/- 14.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1037000, episode_reward=-3167.20 +/- 75.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1038000, episode_reward=-2500.80 +/- 319.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1039000, episode_reward=-2660.40 +/- 13.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1040000, episode_reward=-2642.80 +/- 12.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1041000, episode_reward=-2636.00 +/- 14.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1042000, episode_reward=-2637.20 +/- 10.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1043000, episode_reward=-2644.40 +/- 16.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1044000, episode_reward=-2485.20 +/- 298.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1045000, episode_reward=-2646.40 +/- 20.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1046000, episode_reward=-2656.00 +/- 12.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1047000, episode_reward=-2657.80 +/- 15.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1048000, episode_reward=-2497.40 +/- 315.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1049000, episode_reward=-2644.00 +/- 19.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1050000, episode_reward=-2676.80 +/- 18.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1051000, episode_reward=-2653.00 +/- 20.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1052000, episode_reward=-2649.40 +/- 13.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1053000, episode_reward=-2642.00 +/- 14.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1054000, episode_reward=-2677.20 +/- 13.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1055000, episode_reward=-2650.00 +/- 24.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1056000, episode_reward=-2488.00 +/- 307.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1057000, episode_reward=-2645.20 +/- 16.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1058000, episode_reward=-2661.40 +/- 21.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1059000, episode_reward=-2486.00 +/- 304.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1060000, episode_reward=-2655.20 +/- 23.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1061000, episode_reward=-2657.20 +/- 23.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1062000, episode_reward=-2670.40 +/- 18.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1063000, episode_reward=-2633.60 +/- 15.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1064000, episode_reward=-2506.60 +/- 321.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1065000, episode_reward=-2654.00 +/- 25.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1066000, episode_reward=-2641.60 +/- 12.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1067000, episode_reward=-2644.40 +/- 15.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1068000, episode_reward=-2642.00 +/- 14.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1069000, episode_reward=-2680.00 +/- 23.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1070000, episode_reward=-2500.40 +/- 319.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1071000, episode_reward=-2488.00 +/- 308.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1072000, episode_reward=-2658.00 +/- 7.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1073000, episode_reward=-2643.20 +/- 15.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1074000, episode_reward=-2641.00 +/- 12.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1075000, episode_reward=-2673.20 +/- 23.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1076000, episode_reward=-2676.40 +/- 14.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1077000, episode_reward=-2647.20 +/- 13.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1078000, episode_reward=-2641.20 +/- 16.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1079000, episode_reward=-2740.80 +/- 29.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1080000, episode_reward=-2657.20 +/- 18.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1081000, episode_reward=-2636.40 +/- 15.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1082000, episode_reward=-2648.00 +/- 16.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1083000, episode_reward=-2667.20 +/- 11.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1084000, episode_reward=-2636.60 +/- 16.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1085000, episode_reward=-2678.20 +/- 8.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1086000, episode_reward=-2663.80 +/- 16.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1087000, episode_reward=-2667.00 +/- 15.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1088000, episode_reward=-2684.60 +/- 11.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1089000, episode_reward=-2646.20 +/- 18.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1090000, episode_reward=-2644.20 +/- 25.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1091000, episode_reward=-2662.00 +/- 24.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1092000, episode_reward=-2659.20 +/- 10.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1093000, episode_reward=-2657.00 +/- 31.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1094000, episode_reward=-2658.60 +/- 15.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1095000, episode_reward=-2498.80 +/- 317.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1096000, episode_reward=-2649.20 +/- 11.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1097000, episode_reward=-2648.00 +/- 17.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1098000, episode_reward=-2641.20 +/- 16.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1099000, episode_reward=-2643.20 +/- 6.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1100000, episode_reward=-2648.00 +/- 12.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1101000, episode_reward=-2519.40 +/- 321.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1102000, episode_reward=-2487.20 +/- 315.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1103000, episode_reward=-2585.60 +/- 264.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1104000, episode_reward=-2638.80 +/- 15.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1105000, episode_reward=-2625.60 +/- 29.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1106000, episode_reward=-2488.80 +/- 308.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1107000, episode_reward=-2651.60 +/- 18.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1108000, episode_reward=-2658.80 +/- 24.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1109000, episode_reward=-2638.00 +/- 14.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1110000, episode_reward=-2662.60 +/- 9.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1111000, episode_reward=-2655.20 +/- 17.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1112000, episode_reward=-2508.20 +/- 318.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1113000, episode_reward=-2657.60 +/- 13.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1114000, episode_reward=-2650.20 +/- 30.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1115000, episode_reward=-2330.80 +/- 371.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1116000, episode_reward=-2646.20 +/- 14.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1117000, episode_reward=-2487.60 +/- 305.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1118000, episode_reward=-2638.80 +/- 11.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1119000, episode_reward=-2505.80 +/- 322.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1120000, episode_reward=-2684.40 +/- 17.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1121000, episode_reward=-2646.00 +/- 10.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1122000, episode_reward=-2664.20 +/- 23.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1123000, episode_reward=-2630.00 +/- 9.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1124000, episode_reward=-2668.20 +/- 12.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1125000, episode_reward=-2646.00 +/- 18.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1126000, episode_reward=-2644.00 +/- 16.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1127000, episode_reward=-2642.40 +/- 14.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1128000, episode_reward=-2654.40 +/- 18.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1129000, episode_reward=-2490.80 +/- 302.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1130000, episode_reward=-2486.40 +/- 314.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1131000, episode_reward=-2643.20 +/- 21.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1132000, episode_reward=-2661.20 +/- 10.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1133000, episode_reward=-2657.20 +/- 13.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1134000, episode_reward=-2667.20 +/- 10.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1135000, episode_reward=-2477.20 +/- 299.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1136000, episode_reward=-2652.40 +/- 13.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1137000, episode_reward=-2649.20 +/- 7.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1138000, episode_reward=-2644.00 +/- 11.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1139000, episode_reward=-2635.20 +/- 14.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1140000, episode_reward=-2645.20 +/- 7.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1141000, episode_reward=-2691.40 +/- 10.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1142000, episode_reward=-2666.00 +/- 25.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1143000, episode_reward=-2667.40 +/- 20.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1144000, episode_reward=-2663.60 +/- 21.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1145000, episode_reward=-2645.60 +/- 33.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1146000, episode_reward=-2676.40 +/- 17.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1147000, episode_reward=-2662.00 +/- 20.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1148000, episode_reward=-2650.80 +/- 18.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1149000, episode_reward=-2661.40 +/- 21.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1150000, episode_reward=-2484.20 +/- 302.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1151000, episode_reward=-2692.00 +/- 41.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1152000, episode_reward=-2652.00 +/- 24.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1153000, episode_reward=-2503.20 +/- 316.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1154000, episode_reward=-2517.60 +/- 307.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1155000, episode_reward=-2641.00 +/- 15.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1156000, episode_reward=-2658.80 +/- 14.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1157000, episode_reward=-2240.40 +/- 305.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1158000, episode_reward=-2652.60 +/- 27.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1159000, episode_reward=-2657.40 +/- 17.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1160000, episode_reward=-2683.00 +/- 15.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1161000, episode_reward=-2510.40 +/- 313.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1162000, episode_reward=-2497.20 +/- 313.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1163000, episode_reward=-2660.80 +/- 17.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1164000, episode_reward=-2489.20 +/- 303.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1165000, episode_reward=-2661.60 +/- 17.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1166000, episode_reward=-2487.60 +/- 310.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1167000, episode_reward=-2337.60 +/- 385.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1168000, episode_reward=-2647.20 +/- 25.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1169000, episode_reward=-2476.60 +/- 294.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1170000, episode_reward=-2627.20 +/- 13.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1171000, episode_reward=-2699.20 +/- 23.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1172000, episode_reward=-2489.20 +/- 307.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1173000, episode_reward=-2638.80 +/- 13.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1174000, episode_reward=-2653.20 +/- 10.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1175000, episode_reward=-2524.40 +/- 312.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1176000, episode_reward=-2642.80 +/- 30.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1177000, episode_reward=-2661.20 +/- 28.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1178000, episode_reward=-2635.60 +/- 23.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1179000, episode_reward=-2643.60 +/- 26.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1180000, episode_reward=-3113.20 +/- 212.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1181000, episode_reward=-2655.20 +/- 17.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1182000, episode_reward=-2676.20 +/- 29.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1183000, episode_reward=-2649.60 +/- 14.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1184000, episode_reward=-2649.20 +/- 6.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1185000, episode_reward=-2658.80 +/- 22.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1186000, episode_reward=-2664.60 +/- 18.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1187000, episode_reward=-2632.00 +/- 19.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1188000, episode_reward=-2655.60 +/- 18.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1189000, episode_reward=-2643.60 +/- 7.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1190000, episode_reward=-2654.00 +/- 17.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1191000, episode_reward=-2693.00 +/- 46.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1192000, episode_reward=-2660.40 +/- 10.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1193000, episode_reward=-2669.00 +/- 12.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1194000, episode_reward=-2660.80 +/- 14.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1195000, episode_reward=-2671.60 +/- 18.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1196000, episode_reward=-2656.40 +/- 9.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1197000, episode_reward=-2658.40 +/- 15.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1198000, episode_reward=-2502.80 +/- 314.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1199000, episode_reward=-2663.20 +/- 18.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1200000, episode_reward=-2653.40 +/- 14.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1201000, episode_reward=-2629.20 +/- 24.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1202000, episode_reward=-2486.40 +/- 300.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1203000, episode_reward=-2644.40 +/- 16.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1204000, episode_reward=-2501.60 +/- 309.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1205000, episode_reward=-2653.60 +/- 26.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1206000, episode_reward=-2637.60 +/- 15.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1207000, episode_reward=-2645.20 +/- 26.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1208000, episode_reward=-2653.60 +/- 13.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1209000, episode_reward=-2632.80 +/- 27.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1210000, episode_reward=-2663.80 +/- 25.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1211000, episode_reward=-2654.00 +/- 14.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1212000, episode_reward=-2675.00 +/- 20.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1213000, episode_reward=-2347.60 +/- 368.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1214000, episode_reward=-2653.00 +/- 18.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1215000, episode_reward=-2656.00 +/- 16.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1216000, episode_reward=-2713.00 +/- 25.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1217000, episode_reward=-2681.20 +/- 30.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1218000, episode_reward=-2482.00 +/- 311.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1219000, episode_reward=-2680.80 +/- 25.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1220000, episode_reward=-2634.40 +/- 13.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1221000, episode_reward=-2649.20 +/- 16.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1222000, episode_reward=-2652.40 +/- 17.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1223000, episode_reward=-2562.00 +/- 302.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1224000, episode_reward=-2643.20 +/- 11.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1225000, episode_reward=-2350.00 +/- 380.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1226000, episode_reward=-2502.00 +/- 310.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1227000, episode_reward=-2675.60 +/- 23.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1228000, episode_reward=-2638.80 +/- 12.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1229000, episode_reward=-2654.40 +/- 23.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1230000, episode_reward=-2656.80 +/- 8.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1231000, episode_reward=-2641.40 +/- 30.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1232000, episode_reward=-2663.40 +/- 37.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1233000, episode_reward=-2701.80 +/- 23.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1234000, episode_reward=-2643.60 +/- 8.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1235000, episode_reward=-2758.00 +/- 26.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1236000, episode_reward=-2715.40 +/- 30.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1237000, episode_reward=-2442.80 +/- 388.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1238000, episode_reward=-2599.40 +/- 307.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1239000, episode_reward=-2422.80 +/- 366.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1240000, episode_reward=-2641.20 +/- 15.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1241000, episode_reward=-2714.00 +/- 26.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1242000, episode_reward=-2750.80 +/- 12.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1243000, episode_reward=-2660.80 +/- 11.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1244000, episode_reward=-2653.40 +/- 22.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1245000, episode_reward=-2501.20 +/- 297.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1246000, episode_reward=-2647.60 +/- 21.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1247000, episode_reward=-2661.00 +/- 10.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1248000, episode_reward=-2501.40 +/- 310.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1249000, episode_reward=-2656.80 +/- 17.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1250000, episode_reward=-2687.00 +/- 22.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1251000, episode_reward=-2681.20 +/- 13.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1252000, episode_reward=-2651.20 +/- 20.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1253000, episode_reward=-2771.40 +/- 20.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1254000, episode_reward=-2641.60 +/- 9.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1255000, episode_reward=-2672.80 +/- 17.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1256000, episode_reward=-2670.40 +/- 18.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1257000, episode_reward=-2649.60 +/- 9.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1258000, episode_reward=-2673.60 +/- 21.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1259000, episode_reward=-2506.20 +/- 313.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1260000, episode_reward=-2676.20 +/- 19.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1261000, episode_reward=-2680.40 +/- 16.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1262000, episode_reward=-2490.80 +/- 311.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1263000, episode_reward=-2713.00 +/- 31.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1264000, episode_reward=-2687.60 +/- 14.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1265000, episode_reward=-2570.40 +/- 322.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1266000, episode_reward=-2485.60 +/- 305.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1267000, episode_reward=-2759.40 +/- 33.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1268000, episode_reward=-2666.80 +/- 28.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1269000, episode_reward=-2639.60 +/- 15.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1270000, episode_reward=-2688.40 +/- 18.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1271000, episode_reward=-2535.40 +/- 304.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1272000, episode_reward=-2648.40 +/- 20.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1273000, episode_reward=-2665.20 +/- 23.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1274000, episode_reward=-2679.00 +/- 15.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1275000, episode_reward=-2661.60 +/- 11.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1276000, episode_reward=-2639.20 +/- 25.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1277000, episode_reward=-2666.40 +/- 12.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1278000, episode_reward=-2485.20 +/- 307.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1279000, episode_reward=-2645.20 +/- 4.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1280000, episode_reward=-2646.80 +/- 21.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1281000, episode_reward=-2701.60 +/- 27.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1282000, episode_reward=-2644.80 +/- 5.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1283000, episode_reward=-2657.60 +/- 14.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1284000, episode_reward=-2651.60 +/- 8.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1285000, episode_reward=-2643.20 +/- 12.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1286000, episode_reward=-2679.20 +/- 24.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1287000, episode_reward=-2641.20 +/- 21.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1288000, episode_reward=-2648.00 +/- 21.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1289000, episode_reward=-2462.80 +/- 302.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1290000, episode_reward=-2644.00 +/- 20.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1291000, episode_reward=-2640.80 +/- 20.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1292000, episode_reward=-2485.20 +/- 299.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1293000, episode_reward=-2644.80 +/- 27.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1294000, episode_reward=-2643.20 +/- 19.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1295000, episode_reward=-2670.60 +/- 28.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1296000, episode_reward=-2685.40 +/- 11.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1297000, episode_reward=-2645.20 +/- 19.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1298000, episode_reward=-2502.40 +/- 300.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1299000, episode_reward=-2640.40 +/- 14.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1300000, episode_reward=-2479.80 +/- 310.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1301000, episode_reward=-2686.00 +/- 26.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1302000, episode_reward=-2721.60 +/- 20.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1303000, episode_reward=-2626.00 +/- 17.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1304000, episode_reward=-2685.00 +/- 16.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1305000, episode_reward=-2664.40 +/- 8.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1306000, episode_reward=-2669.60 +/- 32.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1307000, episode_reward=-2674.00 +/- 19.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1308000, episode_reward=-2688.60 +/- 15.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1309000, episode_reward=-2490.40 +/- 309.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1310000, episode_reward=-2671.20 +/- 18.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1311000, episode_reward=-2675.40 +/- 18.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1312000, episode_reward=-2767.40 +/- 37.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1313000, episode_reward=-2761.60 +/- 33.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1314000, episode_reward=-2758.80 +/- 24.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1315000, episode_reward=-2689.40 +/- 25.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1316000, episode_reward=-2816.20 +/- 32.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1317000, episode_reward=-2699.00 +/- 22.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1318000, episode_reward=-2749.60 +/- 17.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1319000, episode_reward=-2731.00 +/- 26.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1320000, episode_reward=-2686.60 +/- 38.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1321000, episode_reward=-2562.60 +/- 310.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1322000, episode_reward=-2665.00 +/- 323.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1323000, episode_reward=-2650.00 +/- 24.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1324000, episode_reward=-2705.80 +/- 24.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1325000, episode_reward=-2660.40 +/- 18.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1326000, episode_reward=-2822.00 +/- 81.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1327000, episode_reward=-2671.40 +/- 26.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1328000, episode_reward=-2636.00 +/- 17.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1329000, episode_reward=-2673.80 +/- 22.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1330000, episode_reward=-2523.00 +/- 318.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1331000, episode_reward=-2646.00 +/- 20.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1332000, episode_reward=-2632.80 +/- 24.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1333000, episode_reward=-2630.40 +/- 12.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1334000, episode_reward=-2650.80 +/- 13.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1335000, episode_reward=-2650.80 +/- 21.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1336000, episode_reward=-2643.20 +/- 13.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1337000, episode_reward=-2640.60 +/- 11.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1338000, episode_reward=-2653.80 +/- 21.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1339000, episode_reward=-2651.20 +/- 20.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1340000, episode_reward=-2666.60 +/- 36.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1341000, episode_reward=-2682.20 +/- 34.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1342000, episode_reward=-2650.00 +/- 15.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1343000, episode_reward=-2654.40 +/- 15.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1344000, episode_reward=-2502.00 +/- 306.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1345000, episode_reward=-2639.20 +/- 28.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1346000, episode_reward=-2654.60 +/- 23.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1347000, episode_reward=-2666.20 +/- 28.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1348000, episode_reward=-2665.60 +/- 6.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1349000, episode_reward=-2650.00 +/- 8.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1350000, episode_reward=-2516.80 +/- 299.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1351000, episode_reward=-2484.80 +/- 313.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1352000, episode_reward=-2647.20 +/- 11.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1353000, episode_reward=-2655.60 +/- 11.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1354000, episode_reward=-2492.00 +/- 313.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1355000, episode_reward=-2656.00 +/- 20.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1356000, episode_reward=-2646.00 +/- 16.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1357000, episode_reward=-2640.00 +/- 15.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1358000, episode_reward=-2657.60 +/- 13.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1359000, episode_reward=-2491.20 +/- 311.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1360000, episode_reward=-2650.40 +/- 8.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1361000, episode_reward=-2685.80 +/- 21.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1362000, episode_reward=-2746.80 +/- 14.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1363000, episode_reward=-2696.00 +/- 24.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1364000, episode_reward=-2696.00 +/- 30.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1365000, episode_reward=-2699.00 +/- 8.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1366000, episode_reward=-2678.20 +/- 18.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1367000, episode_reward=-2344.40 +/- 388.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1368000, episode_reward=-2642.00 +/- 11.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1369000, episode_reward=-2654.40 +/- 6.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1370000, episode_reward=-2635.60 +/- 18.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1371000, episode_reward=-2710.60 +/- 18.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1372000, episode_reward=-2642.00 +/- 19.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1373000, episode_reward=-2738.80 +/- 39.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1374000, episode_reward=-2502.60 +/- 385.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1375000, episode_reward=-2721.60 +/- 37.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1376000, episode_reward=-2701.40 +/- 10.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1377000, episode_reward=-2632.80 +/- 26.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1378000, episode_reward=-2501.60 +/- 303.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1379000, episode_reward=-2750.20 +/- 34.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1380000, episode_reward=-2658.20 +/- 14.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1381000, episode_reward=-2700.60 +/- 26.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1382000, episode_reward=-2505.60 +/- 306.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1383000, episode_reward=-2686.00 +/- 18.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1384000, episode_reward=-2643.60 +/- 29.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1385000, episode_reward=-2683.80 +/- 24.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1386000, episode_reward=-2700.00 +/- 21.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1387000, episode_reward=-2639.20 +/- 38.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1388000, episode_reward=-2660.20 +/- 20.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1389000, episode_reward=-2715.20 +/- 19.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1390000, episode_reward=-2506.00 +/- 308.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1391000, episode_reward=-2647.20 +/- 6.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1392000, episode_reward=-2642.00 +/- 8.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1393000, episode_reward=-2678.60 +/- 24.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1394000, episode_reward=-2500.80 +/- 311.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1395000, episode_reward=-2652.00 +/- 18.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1396000, episode_reward=-2661.00 +/- 26.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1397000, episode_reward=-2643.00 +/- 11.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1398000, episode_reward=-2688.80 +/- 35.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1399000, episode_reward=-2648.80 +/- 17.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1400000, episode_reward=-2652.40 +/- 13.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1401000, episode_reward=-2646.60 +/- 5.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1402000, episode_reward=-2708.00 +/- 34.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1403000, episode_reward=-2641.60 +/- 13.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1404000, episode_reward=-2642.80 +/- 15.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1405000, episode_reward=-2641.20 +/- 22.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1406000, episode_reward=-2647.60 +/- 22.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1407000, episode_reward=-2650.40 +/- 12.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1408000, episode_reward=-2654.00 +/- 10.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1409000, episode_reward=-2506.40 +/- 320.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1410000, episode_reward=-2656.80 +/- 14.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1411000, episode_reward=-2647.60 +/- 20.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1412000, episode_reward=-2638.00 +/- 13.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1413000, episode_reward=-2479.60 +/- 304.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1414000, episode_reward=-2650.80 +/- 10.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1415000, episode_reward=-2646.40 +/- 11.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1416000, episode_reward=-2504.20 +/- 314.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1417000, episode_reward=-2638.00 +/- 15.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1418000, episode_reward=-2492.40 +/- 317.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1419000, episode_reward=-2666.80 +/- 22.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1420000, episode_reward=-2651.20 +/- 15.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1421000, episode_reward=-2640.00 +/- 26.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1422000, episode_reward=-2657.60 +/- 43.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1423000, episode_reward=-2639.60 +/- 13.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1424000, episode_reward=-2645.20 +/- 19.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1425000, episode_reward=-2746.00 +/- 35.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1426000, episode_reward=-2635.20 +/- 16.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1427000, episode_reward=-2700.40 +/- 26.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1428000, episode_reward=-2674.00 +/- 8.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1429000, episode_reward=-2632.00 +/- 9.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1430000, episode_reward=-2645.60 +/- 8.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1431000, episode_reward=-2501.60 +/- 319.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1432000, episode_reward=-2490.80 +/- 303.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1433000, episode_reward=-2658.00 +/- 12.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1434000, episode_reward=-2507.40 +/- 309.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1435000, episode_reward=-2315.60 +/- 372.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1436000, episode_reward=-2477.60 +/- 300.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1437000, episode_reward=-2652.00 +/- 7.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1438000, episode_reward=-2653.80 +/- 25.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1439000, episode_reward=-2652.80 +/- 15.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1440000, episode_reward=-2632.00 +/- 16.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1441000, episode_reward=-2662.60 +/- 20.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1442000, episode_reward=-2650.80 +/- 15.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1443000, episode_reward=-2680.00 +/- 30.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1444000, episode_reward=-2504.60 +/- 309.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1445000, episode_reward=-2636.00 +/- 17.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1446000, episode_reward=-2662.60 +/- 20.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1447000, episode_reward=-2396.00 +/- 385.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1448000, episode_reward=-2723.80 +/- 6.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1449000, episode_reward=-2490.00 +/- 308.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1450000, episode_reward=-2772.20 +/- 309.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1451000, episode_reward=-2670.00 +/- 27.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1452000, episode_reward=-2649.80 +/- 9.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1453000, episode_reward=-2649.20 +/- 23.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1454000, episode_reward=-2663.40 +/- 23.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1455000, episode_reward=-2490.40 +/- 306.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1456000, episode_reward=-2650.80 +/- 15.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1457000, episode_reward=-2663.20 +/- 15.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1458000, episode_reward=-2662.60 +/- 21.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1459000, episode_reward=-2644.80 +/- 22.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1460000, episode_reward=-2661.20 +/- 31.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1461000, episode_reward=-2650.00 +/- 20.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1462000, episode_reward=-2642.00 +/- 24.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1463000, episode_reward=-2684.80 +/- 6.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1464000, episode_reward=-2632.80 +/- 14.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1465000, episode_reward=-2635.60 +/- 15.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1466000, episode_reward=-2697.20 +/- 40.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1467000, episode_reward=-2682.60 +/- 25.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1468000, episode_reward=-2656.40 +/- 12.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1469000, episode_reward=-2629.20 +/- 8.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1470000, episode_reward=-2653.60 +/- 16.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1471000, episode_reward=-2738.60 +/- 22.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1472000, episode_reward=-2684.60 +/- 44.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1473000, episode_reward=-2777.00 +/- 77.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1474000, episode_reward=-2851.20 +/- 61.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1475000, episode_reward=-2854.20 +/- 50.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1476000, episode_reward=-2647.60 +/- 15.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1477000, episode_reward=-2813.00 +/- 26.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1478000, episode_reward=-2944.80 +/- 48.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1479000, episode_reward=-2732.20 +/- 19.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1480000, episode_reward=-2774.00 +/- 67.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1481000, episode_reward=-2663.60 +/- 37.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1482000, episode_reward=-2760.00 +/- 27.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1483000, episode_reward=-2679.40 +/- 15.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1484000, episode_reward=-2657.60 +/- 14.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1485000, episode_reward=-2661.00 +/- 18.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1486000, episode_reward=-2554.40 +/- 358.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1487000, episode_reward=-2966.40 +/- 93.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1488000, episode_reward=-2648.20 +/- 26.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1489000, episode_reward=-2878.60 +/- 184.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1490000, episode_reward=-2655.40 +/- 19.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1491000, episode_reward=-2696.00 +/- 18.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1492000, episode_reward=-2645.20 +/- 15.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1493000, episode_reward=-2672.40 +/- 13.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1494000, episode_reward=-2665.40 +/- 18.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1495000, episode_reward=-2644.00 +/- 16.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1496000, episode_reward=-2652.60 +/- 25.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1497000, episode_reward=-2648.60 +/- 19.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1498000, episode_reward=-2659.20 +/- 20.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1499000, episode_reward=-2718.20 +/- 21.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1500000, episode_reward=-2491.80 +/- 377.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1501000, episode_reward=-2505.60 +/- 301.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1502000, episode_reward=-2673.40 +/- 12.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1503000, episode_reward=-2655.20 +/- 14.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1504000, episode_reward=-2628.00 +/- 14.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1505000, episode_reward=-2657.60 +/- 12.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1506000, episode_reward=-2682.60 +/- 29.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1507000, episode_reward=-2683.40 +/- 8.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1508000, episode_reward=-2376.80 +/- 382.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1509000, episode_reward=-2615.80 +/- 306.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1510000, episode_reward=-2494.00 +/- 320.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1511000, episode_reward=-2484.40 +/- 311.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1512000, episode_reward=-2644.40 +/- 11.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1513000, episode_reward=-2652.40 +/- 23.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1514000, episode_reward=-2647.20 +/- 25.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1515000, episode_reward=-2648.80 +/- 22.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1516000, episode_reward=-2646.40 +/- 16.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1517000, episode_reward=-2640.80 +/- 20.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1518000, episode_reward=-2651.20 +/- 22.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1519000, episode_reward=-2643.60 +/- 17.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1520000, episode_reward=-2648.00 +/- 19.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1521000, episode_reward=-2658.80 +/- 21.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1522000, episode_reward=-2649.60 +/- 19.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1523000, episode_reward=-2643.60 +/- 16.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1524000, episode_reward=-2705.60 +/- 19.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1525000, episode_reward=-2639.60 +/- 10.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1526000, episode_reward=-2635.20 +/- 27.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1527000, episode_reward=-2636.40 +/- 22.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1528000, episode_reward=-2647.20 +/- 10.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1529000, episode_reward=-2662.00 +/- 9.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1530000, episode_reward=-2654.00 +/- 25.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1531000, episode_reward=-2514.00 +/- 321.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1532000, episode_reward=-2644.40 +/- 23.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1533000, episode_reward=-2640.00 +/- 12.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1534000, episode_reward=-2642.00 +/- 28.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1535000, episode_reward=-2655.20 +/- 37.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1536000, episode_reward=-2638.40 +/- 11.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1537000, episode_reward=-2651.20 +/- 30.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1538000, episode_reward=-2658.80 +/- 4.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1539000, episode_reward=-2645.60 +/- 20.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1540000, episode_reward=-2642.40 +/- 13.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1541000, episode_reward=-2650.80 +/- 28.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1542000, episode_reward=-2657.60 +/- 20.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1543000, episode_reward=-2656.20 +/- 34.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1544000, episode_reward=-2492.40 +/- 315.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1545000, episode_reward=-2636.80 +/- 14.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1546000, episode_reward=-2652.00 +/- 26.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1547000, episode_reward=-2639.60 +/- 11.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1548000, episode_reward=-2637.20 +/- 10.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1549000, episode_reward=-2645.20 +/- 13.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1550000, episode_reward=-2647.20 +/- 20.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1551000, episode_reward=-2706.00 +/- 17.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1552000, episode_reward=-2501.20 +/- 308.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1553000, episode_reward=-2706.40 +/- 42.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1554000, episode_reward=-2655.20 +/- 20.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1555000, episode_reward=-2497.60 +/- 311.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1556000, episode_reward=-2653.60 +/- 14.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1557000, episode_reward=-2662.80 +/- 13.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1558000, episode_reward=-2660.20 +/- 20.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1559000, episode_reward=-2684.20 +/- 18.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1560000, episode_reward=-2531.60 +/- 317.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1561000, episode_reward=-2646.80 +/- 12.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1562000, episode_reward=-2652.80 +/- 23.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1563000, episode_reward=-2501.20 +/- 307.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1564000, episode_reward=-2692.40 +/- 6.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1565000, episode_reward=-2687.60 +/- 29.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1566000, episode_reward=-2764.00 +/- 66.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1567000, episode_reward=-2643.20 +/- 16.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1568000, episode_reward=-2720.00 +/- 10.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1569000, episode_reward=-2634.80 +/- 30.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1570000, episode_reward=-2634.40 +/- 20.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1571000, episode_reward=-2640.80 +/- 23.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1572000, episode_reward=-2661.60 +/- 13.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1573000, episode_reward=-2627.60 +/- 13.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1574000, episode_reward=-2634.20 +/- 17.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1575000, episode_reward=-2649.20 +/- 12.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1576000, episode_reward=-2635.20 +/- 30.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1577000, episode_reward=-2645.60 +/- 26.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1578000, episode_reward=-2484.40 +/- 317.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1579000, episode_reward=-2649.20 +/- 15.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1580000, episode_reward=-2639.20 +/- 16.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1581000, episode_reward=-2767.60 +/- 92.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1582000, episode_reward=-2636.40 +/- 21.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1583000, episode_reward=-2649.20 +/- 12.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1584000, episode_reward=-2624.40 +/- 19.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1585000, episode_reward=-2657.20 +/- 14.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1586000, episode_reward=-2653.60 +/- 11.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1587000, episode_reward=-2636.80 +/- 7.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1588000, episode_reward=-2648.00 +/- 27.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1589000, episode_reward=-2639.60 +/- 27.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1590000, episode_reward=-2502.80 +/- 314.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1591000, episode_reward=-2672.40 +/- 23.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1592000, episode_reward=-2701.80 +/- 24.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1593000, episode_reward=-2678.80 +/- 29.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1594000, episode_reward=-2638.40 +/- 21.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1595000, episode_reward=-2530.00 +/- 383.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1596000, episode_reward=-2650.20 +/- 5.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1597000, episode_reward=-2646.80 +/- 13.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1598000, episode_reward=-2656.40 +/- 19.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1599000, episode_reward=-2506.20 +/- 308.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1600000, episode_reward=-2359.40 +/- 368.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1601000, episode_reward=-2649.20 +/- 20.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1602000, episode_reward=-2507.20 +/- 312.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1603000, episode_reward=-2674.60 +/- 24.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1604000, episode_reward=-2490.00 +/- 304.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1605000, episode_reward=-2640.80 +/- 12.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1606000, episode_reward=-2650.20 +/- 22.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1607000, episode_reward=-2652.80 +/- 9.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1608000, episode_reward=-2633.60 +/- 21.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1609000, episode_reward=-2644.00 +/- 19.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1610000, episode_reward=-2657.60 +/- 19.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1611000, episode_reward=-2635.20 +/- 24.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1612000, episode_reward=-2639.60 +/- 20.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1613000, episode_reward=-2643.60 +/- 18.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1614000, episode_reward=-2646.00 +/- 5.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1615000, episode_reward=-2655.20 +/- 13.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1616000, episode_reward=-2498.80 +/- 316.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1617000, episode_reward=-2656.40 +/- 16.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1618000, episode_reward=-2630.80 +/- 25.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1619000, episode_reward=-2640.80 +/- 19.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1620000, episode_reward=-2642.40 +/- 20.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1621000, episode_reward=-2335.20 +/- 374.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1622000, episode_reward=-2498.00 +/- 315.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1623000, episode_reward=-2671.20 +/- 5.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1624000, episode_reward=-2636.80 +/- 13.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1625000, episode_reward=-2660.40 +/- 7.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1626000, episode_reward=-2636.00 +/- 13.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1627000, episode_reward=-2491.60 +/- 313.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1628000, episode_reward=-2489.60 +/- 314.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1629000, episode_reward=-2626.80 +/- 16.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1630000, episode_reward=-2640.80 +/- 10.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1631000, episode_reward=-2686.60 +/- 29.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1632000, episode_reward=-2651.60 +/- 28.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1633000, episode_reward=-2657.60 +/- 21.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1634000, episode_reward=-2632.40 +/- 14.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1635000, episode_reward=-2490.80 +/- 313.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1636000, episode_reward=-2655.20 +/- 12.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1637000, episode_reward=-2629.60 +/- 23.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1638000, episode_reward=-2628.80 +/- 13.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1639000, episode_reward=-2727.60 +/- 32.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1640000, episode_reward=-2648.00 +/- 17.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1641000, episode_reward=-2501.20 +/- 311.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1642000, episode_reward=-2654.00 +/- 15.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1643000, episode_reward=-2650.40 +/- 17.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1644000, episode_reward=-2653.60 +/- 20.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1645000, episode_reward=-2478.40 +/- 311.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1646000, episode_reward=-2644.00 +/- 25.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1647000, episode_reward=-2670.40 +/- 16.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1648000, episode_reward=-2706.80 +/- 34.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1649000, episode_reward=-2647.60 +/- 22.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1650000, episode_reward=-2644.80 +/- 9.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1651000, episode_reward=-2636.00 +/- 12.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1652000, episode_reward=-2639.20 +/- 17.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1653000, episode_reward=-2658.00 +/- 29.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1654000, episode_reward=-2658.00 +/- 12.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1655000, episode_reward=-2649.60 +/- 21.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1656000, episode_reward=-2500.80 +/- 314.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1657000, episode_reward=-2653.60 +/- 14.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1658000, episode_reward=-2649.40 +/- 24.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1659000, episode_reward=-2645.20 +/- 17.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1660000, episode_reward=-2656.80 +/- 15.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1661000, episode_reward=-2474.40 +/- 307.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1662000, episode_reward=-2645.60 +/- 13.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1663000, episode_reward=-2489.60 +/- 314.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1664000, episode_reward=-2505.60 +/- 305.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1665000, episode_reward=-2353.20 +/- 374.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1666000, episode_reward=-2206.00 +/- 313.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1667000, episode_reward=-2482.00 +/- 309.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1668000, episode_reward=-2647.80 +/- 30.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1669000, episode_reward=-2648.80 +/- 21.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1670000, episode_reward=-2712.40 +/- 29.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1671000, episode_reward=-2643.20 +/- 27.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1672000, episode_reward=-2500.80 +/- 318.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1673000, episode_reward=-2646.80 +/- 26.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1674000, episode_reward=-2647.60 +/- 11.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1675000, episode_reward=-2644.80 +/- 15.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1676000, episode_reward=-2640.80 +/- 18.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1677000, episode_reward=-2495.20 +/- 305.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1678000, episode_reward=-2639.20 +/- 11.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1679000, episode_reward=-2632.40 +/- 12.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1680000, episode_reward=-2663.60 +/- 9.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1681000, episode_reward=-2634.00 +/- 17.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1682000, episode_reward=-2653.60 +/- 19.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1683000, episode_reward=-2646.80 +/- 9.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1684000, episode_reward=-2648.40 +/- 10.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1685000, episode_reward=-2650.00 +/- 12.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1686000, episode_reward=-2648.80 +/- 17.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1687000, episode_reward=-2335.20 +/- 376.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1688000, episode_reward=-2479.60 +/- 314.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1689000, episode_reward=-2656.00 +/- 20.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1690000, episode_reward=-2652.00 +/- 13.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1691000, episode_reward=-2657.60 +/- 17.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1692000, episode_reward=-2654.00 +/- 11.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1693000, episode_reward=-2657.20 +/- 20.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1694000, episode_reward=-2374.40 +/- 45.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1695000, episode_reward=-2651.60 +/- 20.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1696000, episode_reward=-2722.40 +/- 48.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1697000, episode_reward=-2642.00 +/- 17.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1698000, episode_reward=-2659.80 +/- 15.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1699000, episode_reward=-2646.40 +/- 18.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1700000, episode_reward=-2650.00 +/- 23.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1701000, episode_reward=-2661.60 +/- 11.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1702000, episode_reward=-2632.80 +/- 22.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1703000, episode_reward=-2650.40 +/- 19.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1704000, episode_reward=-2634.00 +/- 18.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1705000, episode_reward=-2503.20 +/- 315.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1706000, episode_reward=-2650.00 +/- 23.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1707000, episode_reward=-2644.80 +/- 11.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1708000, episode_reward=-2493.60 +/- 314.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1709000, episode_reward=-2478.80 +/- 304.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1710000, episode_reward=-2495.20 +/- 315.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1711000, episode_reward=-2639.60 +/- 25.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1712000, episode_reward=-2638.40 +/- 18.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1713000, episode_reward=-2639.60 +/- 21.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1714000, episode_reward=-2657.00 +/- 28.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1715000, episode_reward=-2657.20 +/- 21.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1716000, episode_reward=-2658.00 +/- 29.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1717000, episode_reward=-2646.80 +/- 29.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1718000, episode_reward=-2648.60 +/- 22.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1719000, episode_reward=-2662.80 +/- 11.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1720000, episode_reward=-2650.80 +/- 24.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1721000, episode_reward=-2670.40 +/- 18.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1722000, episode_reward=-2636.00 +/- 20.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1723000, episode_reward=-2641.60 +/- 17.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1724000, episode_reward=-2652.40 +/- 15.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1725000, episode_reward=-2650.80 +/- 29.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1726000, episode_reward=-2651.20 +/- 14.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1727000, episode_reward=-2665.20 +/- 38.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1728000, episode_reward=-2659.60 +/- 9.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1729000, episode_reward=-2644.40 +/- 15.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1730000, episode_reward=-2654.40 +/- 15.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1731000, episode_reward=-2633.20 +/- 9.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1732000, episode_reward=-2657.60 +/- 5.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1733000, episode_reward=-2638.80 +/- 12.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1734000, episode_reward=-2648.80 +/- 15.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1735000, episode_reward=-2656.00 +/- 21.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1736000, episode_reward=-2654.00 +/- 14.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1737000, episode_reward=-2643.20 +/- 16.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1738000, episode_reward=-2553.00 +/- 304.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1739000, episode_reward=-2680.60 +/- 37.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1740000, episode_reward=-2634.40 +/- 18.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1741000, episode_reward=-2651.60 +/- 18.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1742000, episode_reward=-2680.40 +/- 16.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1743000, episode_reward=-2640.80 +/- 22.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1744000, episode_reward=-2688.00 +/- 14.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1745000, episode_reward=-2643.60 +/- 11.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1746000, episode_reward=-2647.00 +/- 18.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1747000, episode_reward=-2645.60 +/- 10.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1748000, episode_reward=-2625.20 +/- 15.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1749000, episode_reward=-2492.80 +/- 313.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1750000, episode_reward=-2658.40 +/- 5.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1751000, episode_reward=-2638.00 +/- 17.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1752000, episode_reward=-2638.00 +/- 19.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1753000, episode_reward=-2646.00 +/- 32.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1754000, episode_reward=-2498.20 +/- 300.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1755000, episode_reward=-2655.20 +/- 23.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1756000, episode_reward=-2676.20 +/- 17.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1757000, episode_reward=-2698.80 +/- 33.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1758000, episode_reward=-2631.60 +/- 7.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1759000, episode_reward=-2309.80 +/- 367.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1760000, episode_reward=-2644.00 +/- 9.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1761000, episode_reward=-2687.20 +/- 21.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1762000, episode_reward=-2642.40 +/- 18.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1763000, episode_reward=-2706.00 +/- 21.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1764000, episode_reward=-2452.80 +/- 381.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1765000, episode_reward=-2660.40 +/- 12.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1766000, episode_reward=-2648.80 +/- 14.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1767000, episode_reward=-2658.00 +/- 14.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1768000, episode_reward=-2505.60 +/- 312.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1769000, episode_reward=-2670.00 +/- 8.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1770000, episode_reward=-2651.60 +/- 13.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1771000, episode_reward=-2338.00 +/- 384.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1772000, episode_reward=-2659.20 +/- 23.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1773000, episode_reward=-2659.20 +/- 11.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1774000, episode_reward=-2755.80 +/- 45.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1775000, episode_reward=-2636.80 +/- 14.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1776000, episode_reward=-2509.80 +/- 290.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1777000, episode_reward=-2666.40 +/- 18.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1778000, episode_reward=-2403.00 +/- 365.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1779000, episode_reward=-2651.60 +/- 7.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1780000, episode_reward=-2681.20 +/- 32.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1781000, episode_reward=-2655.20 +/- 9.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1782000, episode_reward=-2654.40 +/- 15.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1783000, episode_reward=-2644.40 +/- 16.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1784000, episode_reward=-2656.40 +/- 17.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1785000, episode_reward=-2639.60 +/- 11.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1786000, episode_reward=-2883.00 +/- 109.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1787000, episode_reward=-2634.40 +/- 6.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1788000, episode_reward=-2629.60 +/- 18.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1789000, episode_reward=-2488.40 +/- 314.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1790000, episode_reward=-2636.40 +/- 19.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1791000, episode_reward=-2668.40 +/- 20.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1792000, episode_reward=-2550.60 +/- 301.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1793000, episode_reward=-2671.00 +/- 34.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1794000, episode_reward=-2714.80 +/- 5.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1795000, episode_reward=-2656.20 +/- 13.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1796000, episode_reward=-2675.00 +/- 14.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1797000, episode_reward=-2326.00 +/- 383.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1798000, episode_reward=-2754.00 +/- 57.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1799000, episode_reward=-2838.00 +/- 124.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1800000, episode_reward=-2709.60 +/- 65.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1801000, episode_reward=-2655.20 +/- 17.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1802000, episode_reward=-2664.80 +/- 9.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1803000, episode_reward=-2660.00 +/- 16.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1804000, episode_reward=-2649.20 +/- 24.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1805000, episode_reward=-2471.20 +/- 302.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1806000, episode_reward=-2651.60 +/- 7.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1807000, episode_reward=-2632.80 +/- 15.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1808000, episode_reward=-2639.60 +/- 18.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1809000, episode_reward=-2656.20 +/- 13.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1810000, episode_reward=-2643.40 +/- 22.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1811000, episode_reward=-2641.20 +/- 21.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1812000, episode_reward=-2659.20 +/- 17.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1813000, episode_reward=-2636.40 +/- 18.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1814000, episode_reward=-2503.60 +/- 300.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1815000, episode_reward=-2643.20 +/- 20.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1816000, episode_reward=-2713.20 +/- 19.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1817000, episode_reward=-2671.20 +/- 24.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1818000, episode_reward=-2653.60 +/- 5.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1819000, episode_reward=-2649.00 +/- 18.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1820000, episode_reward=-2647.60 +/- 16.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1821000, episode_reward=-2639.20 +/- 4.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1822000, episode_reward=-2649.60 +/- 21.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1823000, episode_reward=-2636.00 +/- 16.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1824000, episode_reward=-2641.60 +/- 19.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1825000, episode_reward=-2502.40 +/- 314.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1826000, episode_reward=-2647.60 +/- 14.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1827000, episode_reward=-2478.00 +/- 345.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1828000, episode_reward=-2642.40 +/- 26.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1829000, episode_reward=-2636.40 +/- 10.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1830000, episode_reward=-2641.20 +/- 25.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1831000, episode_reward=-2642.80 +/- 14.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1832000, episode_reward=-2646.40 +/- 21.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1833000, episode_reward=-2689.80 +/- 9.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1834000, episode_reward=-2654.40 +/- 16.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1835000, episode_reward=-2642.80 +/- 9.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1836000, episode_reward=-2653.60 +/- 9.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1837000, episode_reward=-2652.40 +/- 11.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1838000, episode_reward=-2651.20 +/- 15.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1839000, episode_reward=-2664.00 +/- 14.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1840000, episode_reward=-2642.00 +/- 20.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1841000, episode_reward=-2688.20 +/- 39.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1842000, episode_reward=-2638.40 +/- 13.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1843000, episode_reward=-2645.60 +/- 15.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1844000, episode_reward=-2656.80 +/- 17.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1845000, episode_reward=-2635.60 +/- 14.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1846000, episode_reward=-2632.00 +/- 23.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1847000, episode_reward=-2664.20 +/- 6.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1848000, episode_reward=-2482.80 +/- 304.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1849000, episode_reward=-2651.40 +/- 17.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1850000, episode_reward=-2652.40 +/- 2.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1851000, episode_reward=-2633.20 +/- 22.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1852000, episode_reward=-2646.00 +/- 14.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1853000, episode_reward=-2635.60 +/- 10.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1854000, episode_reward=-2664.80 +/- 26.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1855000, episode_reward=-2652.40 +/- 17.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1856000, episode_reward=-2634.40 +/- 12.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1857000, episode_reward=-2648.80 +/- 7.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1858000, episode_reward=-2482.00 +/- 304.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1859000, episode_reward=-2646.80 +/- 11.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1860000, episode_reward=-2648.40 +/- 23.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1861000, episode_reward=-2632.80 +/- 22.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1862000, episode_reward=-2652.80 +/- 6.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1863000, episode_reward=-2634.40 +/- 4.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1864000, episode_reward=-2650.00 +/- 27.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1865000, episode_reward=-2480.80 +/- 310.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1866000, episode_reward=-2669.80 +/- 9.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1867000, episode_reward=-2641.20 +/- 12.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1868000, episode_reward=-2653.20 +/- 10.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1869000, episode_reward=-2646.80 +/- 13.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1870000, episode_reward=-2671.20 +/- 22.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1871000, episode_reward=-2652.00 +/- 6.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1872000, episode_reward=-2497.20 +/- 298.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1873000, episode_reward=-2641.20 +/- 10.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1874000, episode_reward=-2493.20 +/- 306.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1875000, episode_reward=-2638.80 +/- 29.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1876000, episode_reward=-2490.00 +/- 318.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1877000, episode_reward=-2649.20 +/- 18.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1878000, episode_reward=-2631.20 +/- 16.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1879000, episode_reward=-2644.00 +/- 24.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1880000, episode_reward=-2658.00 +/- 17.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1881000, episode_reward=-2656.40 +/- 7.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1882000, episode_reward=-2651.20 +/- 21.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1883000, episode_reward=-2655.20 +/- 15.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1884000, episode_reward=-2646.80 +/- 10.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1885000, episode_reward=-2630.00 +/- 20.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1886000, episode_reward=-2487.20 +/- 301.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1887000, episode_reward=-2639.20 +/- 18.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1888000, episode_reward=-2655.20 +/- 20.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1889000, episode_reward=-2652.40 +/- 11.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1890000, episode_reward=-2336.80 +/- 372.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1891000, episode_reward=-2656.80 +/- 13.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1892000, episode_reward=-2502.00 +/- 324.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1893000, episode_reward=-2671.80 +/- 26.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1894000, episode_reward=-2490.00 +/- 319.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1895000, episode_reward=-2503.60 +/- 318.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1896000, episode_reward=-2647.60 +/- 18.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1897000, episode_reward=-2646.40 +/- 14.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1898000, episode_reward=-2638.40 +/- 7.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1899000, episode_reward=-2670.00 +/- 15.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1900000, episode_reward=-2771.80 +/- 16.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1901000, episode_reward=-2642.80 +/- 22.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1902000, episode_reward=-2660.80 +/- 14.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1903000, episode_reward=-2647.60 +/- 18.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1904000, episode_reward=-2639.40 +/- 11.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1905000, episode_reward=-2635.20 +/- 12.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1906000, episode_reward=-2662.60 +/- 26.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1907000, episode_reward=-2477.20 +/- 310.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1908000, episode_reward=-2656.80 +/- 17.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1909000, episode_reward=-2643.80 +/- 13.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1910000, episode_reward=-2652.20 +/- 30.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1911000, episode_reward=-2664.40 +/- 29.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1912000, episode_reward=-2669.20 +/- 13.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1913000, episode_reward=-2660.60 +/- 14.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1914000, episode_reward=-2508.40 +/- 307.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1915000, episode_reward=-2650.60 +/- 18.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1916000, episode_reward=-2645.60 +/- 27.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1917000, episode_reward=-2501.20 +/- 307.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1918000, episode_reward=-2667.40 +/- 7.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1919000, episode_reward=-2672.00 +/- 13.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1920000, episode_reward=-2646.40 +/- 17.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1921000, episode_reward=-2631.20 +/- 15.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1922000, episode_reward=-2640.40 +/- 22.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1923000, episode_reward=-2516.80 +/- 306.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1924000, episode_reward=-2697.00 +/- 13.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1925000, episode_reward=-2553.80 +/- 320.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1926000, episode_reward=-2643.80 +/- 16.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1927000, episode_reward=-2673.80 +/- 21.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1928000, episode_reward=-2646.20 +/- 27.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1929000, episode_reward=-2642.60 +/- 17.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1930000, episode_reward=-2508.40 +/- 319.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1931000, episode_reward=-2644.40 +/- 23.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1932000, episode_reward=-2645.60 +/- 17.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1933000, episode_reward=-2652.00 +/- 14.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1934000, episode_reward=-2653.60 +/- 7.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1935000, episode_reward=-2346.40 +/- 383.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1936000, episode_reward=-2644.80 +/- 15.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1937000, episode_reward=-2652.80 +/- 19.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1938000, episode_reward=-2638.80 +/- 22.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1939000, episode_reward=-2637.20 +/- 21.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1940000, episode_reward=-2660.00 +/- 18.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1941000, episode_reward=-2642.80 +/- 21.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1942000, episode_reward=-2656.00 +/- 17.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1943000, episode_reward=-2637.60 +/- 17.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1944000, episode_reward=-2483.20 +/- 309.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1945000, episode_reward=-2478.00 +/- 304.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1946000, episode_reward=-2652.80 +/- 8.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1947000, episode_reward=-2654.00 +/- 12.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1948000, episode_reward=-2628.00 +/- 15.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1949000, episode_reward=-2645.20 +/- 24.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1950000, episode_reward=-2640.40 +/- 16.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1951000, episode_reward=-2494.00 +/- 323.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1952000, episode_reward=-2663.40 +/- 13.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1953000, episode_reward=-2656.60 +/- 41.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1954000, episode_reward=-2640.00 +/- 8.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1955000, episode_reward=-2493.60 +/- 304.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1956000, episode_reward=-2652.00 +/- 4.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1957000, episode_reward=-2718.20 +/- 29.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1958000, episode_reward=-2653.60 +/- 10.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1959000, episode_reward=-2649.20 +/- 15.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1960000, episode_reward=-2657.60 +/- 14.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1961000, episode_reward=-2635.60 +/- 16.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1962000, episode_reward=-2642.40 +/- 15.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1963000, episode_reward=-2652.80 +/- 11.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1964000, episode_reward=-2650.80 +/- 17.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1965000, episode_reward=-2697.20 +/- 24.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1966000, episode_reward=-2474.40 +/- 307.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1967000, episode_reward=-2652.80 +/- 16.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1968000, episode_reward=-2642.00 +/- 14.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1969000, episode_reward=-2642.80 +/- 12.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1970000, episode_reward=-2650.00 +/- 28.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1971000, episode_reward=-2490.00 +/- 308.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1972000, episode_reward=-2488.00 +/- 319.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1973000, episode_reward=-2641.60 +/- 24.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1974000, episode_reward=-2641.60 +/- 10.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1975000, episode_reward=-2669.20 +/- 14.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1976000, episode_reward=-2486.00 +/- 311.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1977000, episode_reward=-2650.00 +/- 19.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1978000, episode_reward=-2491.60 +/- 313.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1979000, episode_reward=-2643.20 +/- 10.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1980000, episode_reward=-2641.60 +/- 21.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1981000, episode_reward=-2650.00 +/- 11.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1982000, episode_reward=-2494.40 +/- 312.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1983000, episode_reward=-2646.40 +/- 22.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1984000, episode_reward=-2646.00 +/- 12.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1985000, episode_reward=-2647.20 +/- 9.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1986000, episode_reward=-2642.40 +/- 18.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1987000, episode_reward=-2633.60 +/- 28.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1988000, episode_reward=-2646.80 +/- 22.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1989000, episode_reward=-2649.60 +/- 19.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1990000, episode_reward=-2650.40 +/- 7.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1991000, episode_reward=-2501.20 +/- 320.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1992000, episode_reward=-2629.20 +/- 16.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1993000, episode_reward=-2490.00 +/- 317.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1994000, episode_reward=-2519.40 +/- 309.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1995000, episode_reward=-2658.00 +/- 22.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1996000, episode_reward=-2497.60 +/- 308.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1997000, episode_reward=-2678.20 +/- 23.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1998000, episode_reward=-2649.60 +/- 20.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1999000, episode_reward=-2488.40 +/- 309.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2000000, episode_reward=-2630.40 +/- 21.41\n",
      "Episode length: 100.00 +/- 0.00\n"
     ]
    }
   ],
   "source": [
    "### REINFORCEMENT LEARNING IIIa1 ###\n",
    "### TRAIN REACTIVE MODEL ###\n",
    "import gym\n",
    "import stable_baselines3 as sb\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "import pickle\n",
    "\n",
    "# Initiate environment\n",
    "env = gym.make('Production-v0', reactive_mode = True)\n",
    "# Callback for best model\n",
    "best_callback = EvalCallback(env, best_model_save_path='./callback/',\n",
    "                             log_path='./callback/', eval_freq=1000,\n",
    "                             deterministic=True, render=False)\n",
    "\n",
    "model = sb.DQN('MlpPolicy', env, tensorboard_log=\"./tensorboard/\", gamma = 0.99, learning_rate=0.01)\n",
    "model.learn(total_timesteps=2e6, tb_log_name=\"DQN_REACT_model\", callback = best_callback)\n",
    "model.save(\"DQN_REACT_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average number of reactive maintenance interventions per episode is:  3.99\n",
      "The average number of preventive maintenance interventions per episode is:  0.0\n",
      "The average mean time between failure per episode is:  22.352\n",
      "The average sum of inventory per episode is:  0.0\n",
      "The average sum of spare parts inventory per episode is:  0.0\n",
      "The average reward per episode is:  -2612.16\n",
      "The average upper bound per episode is:  642.476\n"
     ]
    }
   ],
   "source": [
    "### REINFORCEMENT LEARNING IIIa2 ###\n",
    "### EVALUATE REACTIVE MODEL ###\n",
    "\n",
    "import pandas as pd\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3 import PPO\n",
    "import numpy as np\n",
    "\n",
    "env = gym.make('Production-v0', reactive_mode = True)\n",
    "# Best Model\n",
    "#model = DQN.load('./callback/11', env = env)\n",
    "# Initialize Reward\n",
    "result_df = pd.DataFrame([[0, 0, 0, 0, 0, 0, 0]], columns=['RM', 'PM', 'MTBF', 'Inventory', 'Spare Parts Inventory', 'Reward', 'Upper'])\n",
    "# Set iterations\n",
    "iterations = 1000\n",
    "for i in range(iterations):\n",
    "    # Initialize episode\n",
    "    store = []\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    store.append([0, env.health, env.breakdown, obs[1], obs[2], 0, done, obs[0]])\n",
    "    # Compute one episode\n",
    "    while not done:\n",
    "        # Get best action for state\n",
    "        action = obs[0]\n",
    "        # Compute next state\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        # Store results of this episode\n",
    "        store.append([action, env.health, env.breakdown, obs[1], obs[2], reward, done, obs[0]])\n",
    "    eps_df = pd.DataFrame(store, columns=['action', 'health', 'breakdown', 'inventory', 'sp_inventory', 'reward', 'done', 'next_order'])\n",
    "    # Calculate nr. of reactive maintenance interventions by counting health 'resets' and substracting PM actions\n",
    "    result_df.iloc[0]['RM'] = result_df.iloc[0]['RM'] + sum(eps_df['breakdown']==True)\n",
    "    # Calculate nr. of preventive maintenance interventions\n",
    "    result_df.iloc[0]['PM'] = result_df.iloc[0]['PM'] + sum(eps_df['action']==10)\n",
    "    # Calculate mean time between failures\n",
    "    # Cut df after last breakdown\n",
    "    eps_df_trim = eps_df.iloc[:(np.where(eps_df['breakdown'].eq(True), eps_df.index, 0).max()+1)]\n",
    "    # Calculate MTBF by dividing periods where machine is running / breakdowns\n",
    "    result_df.iloc[0]['MTBF'] = result_df.iloc[0]['MTBF'] + (len(eps_df_trim) -\n",
    "        sum(eps_df_trim['breakdown'] == True)) / sum(eps_df_trim['breakdown'] == True)\n",
    "    # Calculate inventory\n",
    "    result_df.iloc[0]['Inventory'] = result_df.iloc[0]['Inventory'] + sum(eps_df['inventory'])\n",
    "    # Calculate spare parts inventory per period\n",
    "    result_df.iloc[0]['Spare Parts Inventory'] = result_df.iloc[0]['Spare Parts Inventory'] + sum(eps_df['sp_inventory'])\n",
    "    # Calculate reward\n",
    "    result_df.iloc[0]['Reward'] = result_df.iloc[0]['Reward'] + sum(eps_df['reward'])\n",
    "    # Calculate reward with no costs and fulfillment of all orders\n",
    "    result_df.iloc[0]['Upper'] = result_df.iloc[0]['Upper'] + sum(eps_df.iloc[:-1]['next_order']) * env.order_r\n",
    "\n",
    "print(\"The average number of reactive maintenance interventions per episode is: \", result_df.iloc[0]['RM']/iterations)\n",
    "print(\"The average number of preventive maintenance interventions per episode is: \", result_df.iloc[0]['PM']/iterations)\n",
    "print(\"The average mean time between failure per episode is: \", result_df.iloc[0]['MTBF']/iterations)\n",
    "print(\"The average sum of inventory per episode is: \", result_df.iloc[0]['Inventory']/iterations)\n",
    "print(\"The average sum of spare parts inventory per episode is: \", result_df.iloc[0]['Spare Parts Inventory']/iterations)\n",
    "print(\"The average reward per episode is: \", result_df.iloc[0]['Reward']/iterations)\n",
    "print(\"The average upper bound per episode is: \", result_df.iloc[0]['Upper']/iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Maintenance Interval:  0 Coefficient:  0.0\n",
      "The average number of reactive maintenance interventions per episode is:  0.0\n",
      "The average number of preventive maintenance interventions per episode is:  100.0\n",
      "The average sum of inventory per episode is:  0.0\n",
      "The average sum of spare parts inventory per episode is:  0.0\n",
      "The average reward per episode is:  -41252.0\n",
      "The average upper bound per episode is:  626.0\n",
      "\n",
      " Maintenance Interval:  1 Coefficient:  0.05\n",
      "The average number of reactive maintenance interventions per episode is:  0.0\n",
      "The average number of preventive maintenance interventions per episode is:  50.0\n",
      "The average sum of inventory per episode is:  0.0\n",
      "The average sum of spare parts inventory per episode is:  50.0\n",
      "The average reward per episode is:  -5646.0\n",
      "The average upper bound per episode is:  624.0\n",
      "\n",
      " Maintenance Interval:  2 Coefficient:  0.1\n",
      "The average number of reactive maintenance interventions per episode is:  0.0\n",
      "The average number of preventive maintenance interventions per episode is:  33.0\n",
      "The average sum of inventory per episode is:  0.0\n",
      "The average sum of spare parts inventory per episode is:  33.0\n",
      "The average reward per episode is:  -3373.0\n",
      "The average upper bound per episode is:  616.0\n",
      "\n",
      " Maintenance Interval:  3 Coefficient:  0.15000000000000002\n",
      "The average number of reactive maintenance interventions per episode is:  0.0\n",
      "The average number of preventive maintenance interventions per episode is:  25.0\n",
      "The average sum of inventory per episode is:  0.0\n",
      "The average sum of spare parts inventory per episode is:  25.0\n",
      "The average reward per episode is:  -2295.0\n",
      "The average upper bound per episode is:  590.0\n",
      "\n",
      " Maintenance Interval:  4 Coefficient:  0.2\n",
      "The average number of reactive maintenance interventions per episode is:  0.0\n",
      "The average number of preventive maintenance interventions per episode is:  20.0\n",
      "The average sum of inventory per episode is:  0.0\n",
      "The average sum of spare parts inventory per episode is:  20.0\n",
      "The average reward per episode is:  -1648.0\n",
      "The average upper bound per episode is:  664.0\n",
      "\n",
      " Maintenance Interval:  6 Coefficient:  0.25\n",
      "The average number of reactive maintenance interventions per episode is:  0.0\n",
      "The average number of preventive maintenance interventions per episode is:  14.0\n",
      "The average sum of inventory per episode is:  0.0\n",
      "The average sum of spare parts inventory per episode is:  14.0\n",
      "The average reward per episode is:  -816.0\n",
      "The average upper bound per episode is:  668.0\n",
      "\n",
      " Maintenance Interval:  7 Coefficient:  0.30000000000000004\n",
      "The average number of reactive maintenance interventions per episode is:  0.0\n",
      "The average number of preventive maintenance interventions per episode is:  12.0\n",
      "The average sum of inventory per episode is:  0.0\n",
      "The average sum of spare parts inventory per episode is:  12.0\n",
      "The average reward per episode is:  -597.0\n",
      "The average upper bound per episode is:  646.0\n",
      "\n",
      " Maintenance Interval:  8 Coefficient:  0.35000000000000003\n",
      "The average number of reactive maintenance interventions per episode is:  0.0\n",
      "The average number of preventive maintenance interventions per episode is:  11.0\n",
      "The average sum of inventory per episode is:  0.0\n",
      "The average sum of spare parts inventory per episode is:  11.0\n",
      "The average reward per episode is:  -501.0\n",
      "The average upper bound per episode is:  602.0\n",
      "\n",
      " Maintenance Interval:  9 Coefficient:  0.4\n",
      "The average number of reactive maintenance interventions per episode is:  0.0\n",
      "The average number of preventive maintenance interventions per episode is:  10.0\n",
      "The average sum of inventory per episode is:  0.0\n",
      "The average sum of spare parts inventory per episode is:  10.0\n",
      "The average reward per episode is:  -314.0\n",
      "The average upper bound per episode is:  644.0\n",
      "\n",
      " Maintenance Interval:  10 Coefficient:  0.45\n",
      "The average number of reactive maintenance interventions per episode is:  0.0\n",
      "The average number of preventive maintenance interventions per episode is:  9.0\n",
      "The average sum of inventory per episode is:  0.0\n",
      "The average sum of spare parts inventory per episode is:  9.0\n",
      "The average reward per episode is:  -122.0\n",
      "The average upper bound per episode is:  686.0\n",
      "\n",
      " Maintenance Interval:  11 Coefficient:  0.5\n",
      "The average number of reactive maintenance interventions per episode is:  0.0\n",
      "The average number of preventive maintenance interventions per episode is:  8.0\n",
      "The average sum of inventory per episode is:  0.0\n",
      "The average sum of spare parts inventory per episode is:  8.0\n",
      "The average reward per episode is:  -34.0\n",
      "The average upper bound per episode is:  648.0\n",
      "\n",
      " Maintenance Interval:  12 Coefficient:  0.55\n",
      "The average number of reactive maintenance interventions per episode is:  0.0\n",
      "The average number of preventive maintenance interventions per episode is:  7.0\n",
      "The average sum of inventory per episode is:  0.0\n",
      "The average sum of spare parts inventory per episode is:  7.0\n",
      "The average reward per episode is:  67.0\n",
      "The average upper bound per episode is:  638.0\n",
      "\n",
      " Maintenance Interval:  13 Coefficient:  0.6000000000000001\n",
      "The average number of reactive maintenance interventions per episode is:  0.0\n",
      "The average number of preventive maintenance interventions per episode is:  7.0\n",
      "The average sum of inventory per episode is:  0.0\n",
      "The average sum of spare parts inventory per episode is:  7.0\n",
      "The average reward per episode is:  104.0\n",
      "The average upper bound per episode is:  618.0\n",
      "\n",
      " Maintenance Interval:  15 Coefficient:  0.65\n",
      "The average number of reactive maintenance interventions per episode is:  0.0\n",
      "The average number of preventive maintenance interventions per episode is:  6.0\n",
      "The average sum of inventory per episode is:  0.0\n",
      "The average sum of spare parts inventory per episode is:  6.0\n",
      "The average reward per episode is:  246.0\n",
      "The average upper bound per episode is:  646.0\n",
      "\n",
      " Maintenance Interval:  16 Coefficient:  0.7000000000000001\n",
      "The average number of reactive maintenance interventions per episode is:  0.0\n",
      "The average number of preventive maintenance interventions per episode is:  5.0\n",
      "The average sum of inventory per episode is:  0.0\n",
      "The average sum of spare parts inventory per episode is:  5.0\n",
      "The average reward per episode is:  309.0\n",
      "The average upper bound per episode is:  624.0\n",
      "\n",
      " Maintenance Interval:  17 Coefficient:  0.75\n",
      "The average number of reactive maintenance interventions per episode is:  0.0\n",
      "The average number of preventive maintenance interventions per episode is:  5.0\n",
      "The average sum of inventory per episode is:  0.0\n",
      "The average sum of spare parts inventory per episode is:  5.0\n",
      "The average reward per episode is:  363.0\n",
      "The average upper bound per episode is:  638.0\n",
      "\n",
      " Maintenance Interval:  18 Coefficient:  0.8\n",
      "The average number of reactive maintenance interventions per episode is:  0.0\n",
      "The average number of preventive maintenance interventions per episode is:  5.0\n",
      "The average sum of inventory per episode is:  0.0\n",
      "The average sum of spare parts inventory per episode is:  5.0\n",
      "The average reward per episode is:  338.0\n",
      "The average upper bound per episode is:  632.0\n",
      "\n",
      " Maintenance Interval:  19 Coefficient:  0.8500000000000001\n",
      "The average number of reactive maintenance interventions per episode is:  0.0\n",
      "The average number of preventive maintenance interventions per episode is:  5.0\n",
      "The average sum of inventory per episode is:  0.0\n",
      "The average sum of spare parts inventory per episode is:  5.0\n",
      "The average reward per episode is:  412.0\n",
      "The average upper bound per episode is:  664.0\n",
      "\n",
      " Maintenance Interval:  20 Coefficient:  0.9\n",
      "The average number of reactive maintenance interventions per episode is:  0.0\n",
      "The average number of preventive maintenance interventions per episode is:  4.0\n",
      "The average sum of inventory per episode is:  0.0\n",
      "The average sum of spare parts inventory per episode is:  4.0\n",
      "The average reward per episode is:  485.0\n",
      "The average upper bound per episode is:  678.0\n",
      "\n",
      " Maintenance Interval:  21 Coefficient:  0.9500000000000001\n",
      "The average number of reactive maintenance interventions per episode is:  0.0\n",
      "The average number of preventive maintenance interventions per episode is:  4.0\n",
      "The average sum of inventory per episode is:  0.0\n",
      "The average sum of spare parts inventory per episode is:  4.0\n",
      "The average reward per episode is:  435.0\n",
      "The average upper bound per episode is:  622.0\n",
      "\n",
      " Maintenance Interval:  22 Coefficient:  1.0\n",
      "The average number of reactive maintenance interventions per episode is:  0.0\n",
      "The average number of preventive maintenance interventions per episode is:  4.0\n",
      "The average sum of inventory per episode is:  0.0\n",
      "The average sum of spare parts inventory per episode is:  4.0\n",
      "The average reward per episode is:  415.0\n",
      "The average upper bound per episode is:  608.0\n",
      "\n",
      " Maintenance Interval:  23 Coefficient:  1.05\n",
      "The average number of reactive maintenance interventions per episode is:  2.0\n",
      "The average number of preventive maintenance interventions per episode is:  2.0\n",
      "The average sum of inventory per episode is:  0.0\n",
      "The average sum of spare parts inventory per episode is:  2.0\n",
      "The average reward per episode is:  -1087.0\n",
      "The average upper bound per episode is:  638.0\n"
     ]
    }
   ],
   "source": [
    "### REINFORCEMENT LEARNING IIIb ###\n",
    "### EVALUATE TIME-BASED PREVENTIVE MODEL ###\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "env = gym.make('Production-v0')\n",
    "#kay = range(0,7)\n",
    "kay = range (0, 22)\n",
    "# Set iterations\n",
    "iterations = 1\n",
    "\n",
    "for k in kay:\n",
    "    mtbf = round(22.352*(0+0.05*k))\n",
    "    # Initilaize Reward\n",
    "    result_df = pd.DataFrame([[0, 0, 0, 0, 0, 0]], columns=['RM', 'PM', 'Inventory', 'Spare Parts Inventory', 'Reward', 'Upper'])\n",
    "\n",
    "    for i in range(iterations):\n",
    "        # Initialize episode\n",
    "        store = []\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        store.append([0, obs[0], env.breakdown, obs[2], obs[3], 0, done, obs[1]])\n",
    "        # Compute one episode\n",
    "        while not done:\n",
    "            # One period before maintenance: action = order + spare part order\n",
    "            if env.scheduled_maintenance_counter == mtbf-1:\n",
    "                action = obs[1] + 5\n",
    "            # At period of mtbf: maintain\n",
    "            elif env.scheduled_maintenance_counter == mtbf:\n",
    "                action = 10\n",
    "            # Else: action = order    \n",
    "            else:             \n",
    "                action = obs[1]\n",
    "            # Compute next state\n",
    "            obs, reward, done, info = env.step(action)\n",
    "            # Store results of this episode\n",
    "            store.append([action, obs[0], env.breakdown, obs[2], obs[3], reward, done, obs[1]])\n",
    "        eps_df = pd.DataFrame(store, columns=['action', 'health', 'breakdown', 'inventory', 'sp_inventory', 'reward', 'done', 'next_order'])\n",
    "        # Calculate nr. of reactive maintenance interventions by counting health 'resets' and substracting PM actions\n",
    "        result_df.iloc[0]['RM'] = result_df.iloc[0]['RM'] + sum(eps_df['breakdown']==True)\n",
    "        # Calculate nr. of preventive maintenance interventions\n",
    "        result_df.iloc[0]['PM'] = result_df.iloc[0]['PM'] + sum(eps_df['action']==10)\n",
    "        # Calculate mean time between failures\n",
    "        # Cut df after last breakdown\n",
    "        eps_df_trim = eps_df.iloc[:(np.where(eps_df['breakdown'].eq(True), eps_df.index, 0).max()+1)]\n",
    "        # Calculate inventory\n",
    "        result_df.iloc[0]['Inventory'] = result_df.iloc[0]['Inventory'] + sum(eps_df['inventory'])\n",
    "        # Calculate spare parts inventory per period\n",
    "        result_df.iloc[0]['Spare Parts Inventory'] = result_df.iloc[0]['Spare Parts Inventory'] + sum(eps_df['sp_inventory'])\n",
    "        # Calculate reward\n",
    "        result_df.iloc[0]['Reward'] = result_df.iloc[0]['Reward'] + sum(eps_df['reward'])\n",
    "        # Calculate reward with no costs and fulfillment of all orders\n",
    "        result_df.iloc[0]['Upper'] = result_df.iloc[0]['Upper'] + sum(eps_df.iloc[:-1]['next_order']) * env.order_r\n",
    "\n",
    "    print(\"\\n\", \"Maintenance Interval: \", mtbf, \"Coefficient: \", 0+0.05*k)\n",
    "    print(\"The average number of reactive maintenance interventions per episode is: \", result_df.iloc[0]['RM']/iterations)\n",
    "    print(\"The average number of preventive maintenance interventions per episode is: \", result_df.iloc[0]['PM']/iterations)\n",
    "    print(\"The average sum of inventory per episode is: \", result_df.iloc[0]['Inventory']/iterations)\n",
    "    print(\"The average sum of spare parts inventory per episode is: \", result_df.iloc[0]['Spare Parts Inventory']/iterations)\n",
    "    print(\"The average reward per episode is: \", result_df.iloc[0]['Reward']/iterations)\n",
    "    print(\"The average upper bound per episode is: \", result_df.iloc[0]['Upper']/iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "### REINFORCEMENT LEARNING IV ###\n",
    "### VISUALIZE STATE-ACTION ###\n",
    "import numpy as np\n",
    "state_action = []\n",
    "\n",
    "# Define observation grid\n",
    "grid_health = np.arange(0.0, 1.01, 0.01)\n",
    "grid_order = range(0, 5)\n",
    "grid_inventory = range(0, 10)\n",
    "grid_sp_inventory = [0, 1]\n",
    "\n",
    "# Loop through grid and store best action for each state\n",
    "for hlt in grid_health:\n",
    "    for ord in grid_order:\n",
    "        for inv in grid_inventory:\n",
    "            for sin in grid_sp_inventory:\n",
    "                # Predict\n",
    "                action, _state = model.predict((hlt, ord, inv, sin), deterministic=True)\n",
    "                state_action.append([hlt, ord, inv, sin, action])\n",
    "\n",
    "state_action_df = pd.DataFrame(state_action, columns=['health', 'order', 'inventory', 'sp_inventory', 'action'])\n",
    "state_action_df.to_excel(\"visuals/state_action.xlsx\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "false\n"
     ]
    }
   ],
   "source": [
    "test = None\n",
    "if test: print(\"true\")\n",
    "else: print(\"false\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6f69c5940b32a5cbabe45c9825076a627c6cdb9ede58cf4d0fa74ca6057ffe74"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
