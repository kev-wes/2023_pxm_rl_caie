{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process noise = 0.0, Measurement noise = 0.0\n",
      "Process noise = 0.1, Measurement noise = 0.0\n",
      "Process noise = 0.2, Measurement noise = 0.0\n",
      "Process noise = 0.30000000000000004, Measurement noise = 0.0\n",
      "Process noise = 0.4, Measurement noise = 0.0\n",
      "Process noise = 0.5, Measurement noise = 0.0\n",
      "Process noise = 0.6000000000000001, Measurement noise = 0.0\n",
      "Process noise = 0.7000000000000001, Measurement noise = 0.0\n",
      "Process noise = 0.8, Measurement noise = 0.0\n",
      "Process noise = 0.9, Measurement noise = 0.0\n",
      "Process noise = 1.0, Measurement noise = 0.0\n"
     ]
    }
   ],
   "source": [
    "### DATA-DRIVEN DIAGNOSTICS I ###\n",
    "### GENERATE DATA FOR DATA-DRIVEN MODEL ###\n",
    "\n",
    "import random\n",
    "from prog_models.models import BatteryCircuit\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "# Ignore warnings when machine exceeds its end of life\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\"\"\" Method that uses a physical machine model from the prog_models package and a current (health) state of the model and\n",
    "an action (i.e., intensity), which is performed for 100 time steps\n",
    "    Parameter:\n",
    "        machine             machine model from the prog_models package\n",
    "        state               current (health) state of the model\n",
    "        action              loading of the machine for the next 100 time steps\n",
    "    Return:\n",
    "        health                               \n",
    "    \"\"\"\n",
    "def produce_model(machine, states, action):\n",
    "        \n",
    "        # Define load of battery\n",
    "        def future_loading(t, x=None):\n",
    "            return {'i': action}\n",
    "\n",
    "        # Set current state of machine\n",
    "        machine.parameters['x0'] = states\n",
    "        # Simulate 100 steps\n",
    "        options = {\n",
    "            'save_freq': 100,  # Frequency at which results are saved\n",
    "            'dt': 2  # Timestep\n",
    "        }\n",
    "        (_, _, states, outputs, event_states) = machine.simulate_to(100, future_loading, **options)\n",
    "        health = event_states[-1]['EOD']\n",
    "        return(round(health, 2), states[-1], outputs[-1]['t'], outputs[-1]['v'])\n",
    "def reset_states(machine):\n",
    "    # Returns initial states of machine, e.g., {'tb': 18.95, 'qb': 7856.3254, 'qcp': 0, 'qcs': 0} for Battery\n",
    "    return(machine.default_parameters['x0'])\n",
    "   \n",
    "for pn in [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]: \n",
    "    for mn in [0]:\n",
    "        print('Process noise = ' + str(0.1*pn) + ', Measurement noise = ' + str(0.1*mn))\n",
    "        battery = BatteryCircuit(process_noise = 0.1*pn, measurement_noise = 0.1*mn)\n",
    "        states = reset_states(battery)\n",
    "        reset_counter = 0\n",
    "        dataset = []\n",
    "        for i in range(int(1e4)):\n",
    "            # If asset failed last period, reset all historical values\n",
    "            if reset_counter == 0: t = v = t_1 = v_1 = t_2 = v_2 = t_3 = v_3 = 0 \n",
    "            # Shift history by one time period\n",
    "            v_3 = v_2\n",
    "            t_3 = t_2\n",
    "            v_2 = v_1\n",
    "            t_2 = t_1\n",
    "            v_1 = v\n",
    "            t_1 = t\n",
    "\n",
    "            # Increment reset_counter\n",
    "            reset_counter = reset_counter + 1\n",
    "            # Compute new health, states, t, and v using last battery state and a random new action\n",
    "            health, states, t, v = produce_model(machine=battery, states=states, action=random.sample((0, 1, 2, 3, 4), 1)[0])\n",
    "            \n",
    "            if health <= 0: \n",
    "                # Reset battery states to initialize battery for next produce_model call\n",
    "                states = reset_states(battery)\n",
    "                # Initialize reset_counter\n",
    "                reset_counter = 0\n",
    "                # Sometimes produce_model returns weird or negative values as the end of life is exceeded\n",
    "                # Here, we just simply set it to zero to not confuse a later learner \n",
    "                health = 0\n",
    "\n",
    "            # append to two-dimensional list\n",
    "            dataset.append([t, v, t_1, v_1, t_2, v_2, t_3, v_3, health])\n",
    "\n",
    "        # Transform two-dim list to dataframe\n",
    "        dataset = pd.DataFrame(dataset, columns=['t', 'v', 't_1', 'v_1', 't_2', 'v_2', 't_3', 'v_3', 'health'])\n",
    "        # Denote machine runs to-failure with incrementing id\n",
    "        k = 0\n",
    "        for i in range(dataset.shape[0]):\n",
    "            if (dataset.iloc[i]['t_1'] == 0 and dataset.iloc[i]['v_1'] == 0):\n",
    "                j = 1\n",
    "                k = k + 1\n",
    "            dataset.loc[i, 'time'] = j\n",
    "            j = j + 1\n",
    "            dataset.loc[i, 'ID'] = k\n",
    "        dataset = dataset.sort_values(['ID', 'time'], ascending=[True, False])\n",
    "        # Assign RUL by counting upwards per serial number in the descended data frame\n",
    "        dataset['RUL'] = dataset.groupby((dataset['ID'] != dataset['ID'].shift(1)).cumsum()).cumcount()\n",
    "        dataset = dataset.sort_values(['ID', 'time'], ascending=True)\n",
    "\n",
    "        # Save it as pickle\n",
    "        dataset.to_pickle('diagnostics/data_' + 'pn' + str(pn) + '_mn' + str(mn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model based on RUL (prognostics = True) or health (diagnostics, prognostics = False)\n",
    "prognostics = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process noise = 0.0, Measurement noise = 0.0\n",
      "R2: [0.98346924 0.97869884 0.98271119 0.98084096 0.98029931]\n",
      "mae: [-0.0186088  -0.02097155 -0.01949765 -0.0201391  -0.0208858 ]\n",
      "mape: [-7.86658635e-02 -3.23583633e+11 -9.00719925e+09 -9.20286148e-02\n",
      " -1.71587146e+11]\n",
      "rmse: [-0.03669848 -0.04135376 -0.03768967 -0.03960828 -0.04014499]\n",
      "Process noise = 0.1, Measurement noise = 0.0\n",
      "R2: [0.94511725 0.94338909 0.94308702 0.95269309 0.92781427]\n",
      "mae: [-0.04355035 -0.0430732  -0.0434886  -0.0409953  -0.04892035]\n",
      "mape: [-1.57625987e+09 -6.75539944e+09 -2.31935381e+10 -3.03992975e+10\n",
      " -3.69295169e+11]\n",
      "rmse: [-0.06585241 -0.06812947 -0.06765209 -0.06181947 -0.07649472]\n",
      "Process noise = 0.2, Measurement noise = 0.0\n",
      "R2: [0.89160295 0.90719555 0.90106071 0.89986382 0.8983238 ]\n",
      "mae: [-0.06726765 -0.0624331  -0.0637716  -0.0658622  -0.0643069 ]\n",
      "mape: [-3.90912448e+11 -3.67493730e+11 -1.03605309e+12 -7.60657977e+11\n",
      " -1.41413028e+11]\n",
      "rmse: [-0.09406841 -0.08643758 -0.08901846 -0.09017457 -0.0908437 ]\n",
      "Process noise = 0.30000000000000004, Measurement noise = 0.0\n",
      "R2: [0.87037827 0.84902257 0.84585148 0.85446821 0.85283189]\n",
      "mae: [-0.07610885 -0.0816426  -0.0808236  -0.0798498  -0.0796984 ]\n",
      "mape: [-8.21906932e+10 -2.44320280e+11 -3.61413870e+11 -2.58281439e+11\n",
      " -2.56705179e+10]\n",
      "rmse: [-0.10279177 -0.11007041 -0.11067439 -0.10907084 -0.10878756]\n",
      "Process noise = 0.4, Measurement noise = 0.0\n",
      "R2: [0.83456832 0.83012227 0.80835482 0.83903277 0.81974217]\n",
      "mae: [-0.0846392  -0.0875222  -0.09059015 -0.08526315 -0.090239  ]\n",
      "mape: [-1.02456892e+11 -4.00820367e+11 -2.39816680e+11 -2.95661316e+11\n",
      " -6.73288144e+10]\n",
      "rmse: [-0.11565337 -0.11667704 -0.12263368 -0.11445298 -0.12049927]\n",
      "Process noise = 0.5, Measurement noise = 0.0\n",
      "R2: [0.82839126 0.80480565 0.82500873 0.8179684  0.817267  ]\n",
      "mae: [-0.09095045 -0.096345   -0.0892272  -0.09109585 -0.09089715]\n",
      "mape: [-4.76480841e+11 -2.50850499e+11 -2.85978576e+11 -7.34086739e+10\n",
      " -4.02171447e+11]\n",
      "rmse: [-0.11899326 -0.12706747 -0.11847659 -0.12179301 -0.12175318]\n",
      "Process noise = 0.6000000000000001, Measurement noise = 0.0\n",
      "R2: [0.78807567 0.79536683 0.79253112 0.78549367 0.7681572 ]\n",
      "mae: [-0.09885245 -0.09946715 -0.09709435 -0.1021844  -0.1052449 ]\n",
      "mape: [-3.47452711e+11 -4.54638382e+11 -8.87209127e+11 -2.08516663e+11\n",
      " -3.28762773e+11]\n",
      "rmse: [-0.13075345 -0.13061024 -0.12871161 -0.13304715 -0.13793793]\n",
      "Process noise = 0.7000000000000001, Measurement noise = 0.0\n",
      "R2: [0.77073703 0.76979583 0.75657293 0.74444239 0.7908058 ]\n",
      "mae: [-0.1031984  -0.1025792  -0.10822995 -0.11202985 -0.09902865]\n",
      "mape: [-2.76746197e+11 -4.12529726e+11 -1.64156206e+11 -5.88395291e+11\n",
      " -3.12099454e+11]\n",
      "rmse: [-0.13438963 -0.1351357  -0.14146253 -0.14412065 -0.1310333 ]\n",
      "Process noise = 0.8, Measurement noise = 0.0\n",
      "R2: [0.75441041 0.73836646 0.72956504 0.7436836  0.74272255]\n",
      "mae: [-0.1075912  -0.1102882  -0.11299515 -0.10926115 -0.1102889 ]\n",
      "mape: [-3.29888673e+11 -1.32315757e+12 -3.39571412e+11 -9.10177485e+11\n",
      " -5.11158558e+11]\n",
      "rmse: [-0.14059851 -0.14411492 -0.14701622 -0.1434903  -0.14296051]\n",
      "Process noise = 0.9, Measurement noise = 0.0\n",
      "R2: [0.7459057  0.72225387 0.71386912 0.73079549 0.71229312]\n",
      "mae: [-0.1088923  -0.115251   -0.11461225 -0.1123348  -0.1146464 ]\n",
      "mape: [-2.86203756e+11 -1.43439648e+11 -3.62990130e+11 -8.26410532e+10\n",
      " -6.39285967e+11]\n",
      "rmse: [-0.14232725 -0.14946078 -0.14982241 -0.14679103 -0.14974282]\n",
      "Process noise = 1.0, Measurement noise = 0.0\n",
      "R2: [0.70404433 0.7015158  0.69136757 0.70469365 0.71885872]\n",
      "mae: [-0.1154981  -0.1165566  -0.11575255 -0.11541465 -0.11638255]\n",
      "mape: [-1.49744688e+11 -5.21967197e+11 -5.96051411e+11 -2.15925084e+12\n",
      " -3.37769972e+11]\n",
      "rmse: [-0.15095345 -0.15119491 -0.15424514 -0.15225114 -0.15056938]\n"
     ]
    }
   ],
   "source": [
    "### DATA-DRIVEN DIAGNOSTICS II ###\n",
    "### FIT AND TEST MODEL ###\n",
    "from sklearn import tree, linear_model, kernel_ridge, svm, neighbors, gaussian_process, ensemble, neural_network\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_validate\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "score_data = []\n",
    "for pn in [0, 1, 2, 3, 4, 5, 6, 7 , 8, 9, 10]: \n",
    "    for mn in [0]:\n",
    "        print('Process noise = ' + str(0.1*pn) + ', Measurement noise = ' + str(0.1*mn))\n",
    "        dataset = pd.read_pickle('diagnostics/data_' + 'pn' + str(pn) + '_mn' + str(mn))\n",
    "        X = dataset[['t', 'v', 't_1', 'v_1', 't_2', 'v_2', 't_3', 'v_3']]\n",
    "        if prognostics:\n",
    "            y = dataset['RUL']\n",
    "        else:\n",
    "            y = dataset['health']\n",
    "        learner = ensemble.RandomForestRegressor()\n",
    "        scoring = {'r2': 'r2',\n",
    "                    'mae': 'neg_mean_absolute_error',\n",
    "                    'mape': 'neg_mean_absolute_percentage_error',\n",
    "                    'rmse': 'neg_root_mean_squared_error'}\n",
    "        scores = cross_validate(learner, X, y, scoring=scoring, cv=5) # default scoring R2\n",
    "        print(\"R2: \" + str(scores['test_r2']))\n",
    "        print(\"mae: \" + str(scores['test_mae']))\n",
    "        print(\"mape: \" + str(scores['test_mape']))\n",
    "        print(\"rmse: \" + str(scores['test_rmse']))\n",
    "        # Store measures\n",
    "        score_data.append(['model_' + 'pn' + str(pn) + '_mn' + str(mn), np.mean(scores['test_r2']),\n",
    "                            np.mean(scores['test_mae']), np.mean(scores['test_mape']), np.mean(scores['test_rmse'])])\n",
    "\n",
    "        # Fit on all data\n",
    "        model = ensemble.RandomForestRegressor().fit(X, y)\n",
    "        if prognostics:\n",
    "            pickle.dump(model, open('prognostics/model_' + 'pn' + str(pn) + '_mn' + str(mn), 'wb'))\n",
    "        else:\n",
    "            pickle.dump(model, open('diagnostics/model_' + 'pn' + str(pn) + '_mn' + str(mn), 'wb'))\n",
    "# Transform two-dim list to dataframe\n",
    "score_df = pd.DataFrame(score_data, columns=['model', 'r2', 'mae', 'mape', 'rmse'])\n",
    "if prognostics:\n",
    "    score_df.to_excel(\"prognostics/scores.xlsx\") \n",
    "else:\n",
    "    score_df.to_excel(\"diagnostics/scores.xlsx\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process noise = 0.0, Measurement noise = 0.0\n",
      "Process noise = 0.1, Measurement noise = 0.0\n",
      "Process noise = 0.2, Measurement noise = 0.0\n",
      "Process noise = 0.30000000000000004, Measurement noise = 0.0\n",
      "Process noise = 0.4, Measurement noise = 0.0\n",
      "Process noise = 0.5, Measurement noise = 0.0\n",
      "Process noise = 0.6000000000000001, Measurement noise = 0.0\n",
      "Process noise = 0.7000000000000001, Measurement noise = 0.0\n",
      "Process noise = 0.8, Measurement noise = 0.0\n",
      "Process noise = 0.9, Measurement noise = 0.0\n",
      "Process noise = 1.0, Measurement noise = 0.0\n"
     ]
    }
   ],
   "source": [
    "### DATA-DRIVEN DIAGNOSTICS IIIa ###\n",
    "### FIT, TEST, VISUALIZE MODEL USING TRAIN AND TEST SETS ###\n",
    "\n",
    "for pn in [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]: \n",
    "    for mn in [0]:\n",
    "        print('Process noise = ' + str(0+0.1*pn) + ', Measurement noise = ' + str(0+0.1*mn))\n",
    "        if prognostics:\n",
    "            dataset = pd.read_pickle('diagnostics/data_' + 'pn' + str(pn) + '_mn' + str(mn))\n",
    "            y = dataset['RUL']\n",
    "        else:\n",
    "            dataset = pd.read_pickle('prognostics/data_' + 'pn' + str(pn) + '_mn' + str(mn))\n",
    "            y = dataset['health']\n",
    "        X = dataset[['t', 'v', 't_1', 'v_1', 't_2', 'v_2', 't_3', 'v_3']]\n",
    "        # Find index of healthy machines\n",
    "        index_df = X.index[(X['t_1'] == 0) & (X['v_1'] == 0) & (X['t_2'] == 0) & (X['v_2'] == 0) & (X['t_3'] == 0) & (X['v_3'] == 0)].tolist()\n",
    "        index_test = round(len(index_df)*0.8)\n",
    "\n",
    "        # Create train and test set without disrupting machine runs to-failure\n",
    "        X_train = X.iloc[0:(index_df[index_test])]\n",
    "        y_train = y.iloc[0:(index_df[index_test])]\n",
    "        X_test = X.iloc[index_df[index_test]:(len(X))]\n",
    "        y_test = y.iloc[index_df[index_test]:(len(y))]\n",
    "\n",
    "        ## Train\n",
    "        model = ensemble.RandomForestRegressor().fit(X_train, y_train)\n",
    "        ## Predict\n",
    "        y_pred = pd.DataFrame(model.predict(X_test), columns=['Pred'])\n",
    "        ## Analyze\n",
    "        #reset index of each DataFrame\n",
    "        X_test.reset_index(drop=True, inplace=True)\n",
    "        y_test.reset_index(drop=True, inplace=True)\n",
    "        # Concat dataframes\n",
    "        test_df = pd.concat([X_test, y_test, y_pred], axis=1)\n",
    "        # Print for visualization (e.g., in R)\n",
    "        if prognostics:\n",
    "            test_df.to_excel(\"prognostics/test_results_\" + 'pn' + str(pn) + '_mn' + str(mn) + \".xlsx\") \n",
    "        else:\n",
    "            test_df.to_excel(\"diagnostics/test_results_\" + 'pn' + str(pn) + '_mn' + str(mn) + \".xlsx\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process noise = 0.0, Measurement noise = 0.0\n",
      "Diagnostics-based RL\n",
      "Eval num_timesteps=1000, episode_reward=-2327.30 +/- 149.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=-2374.23 +/- 305.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=3000, episode_reward=-3273.85 +/- 117.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=4000, episode_reward=-3414.32 +/- 105.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=5000, episode_reward=-2369.63 +/- 86.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=6000, episode_reward=-2555.41 +/- 227.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=7000, episode_reward=-1733.20 +/- 135.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=8000, episode_reward=-1658.09 +/- 172.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=9000, episode_reward=-1507.88 +/- 100.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=10000, episode_reward=-1464.62 +/- 172.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=11000, episode_reward=-911.99 +/- 212.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=12000, episode_reward=-819.95 +/- 177.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=13000, episode_reward=-1039.81 +/- 244.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=14000, episode_reward=-897.63 +/- 160.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=15000, episode_reward=-794.37 +/- 243.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=16000, episode_reward=-953.92 +/- 201.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=17000, episode_reward=-741.63 +/- 230.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=18000, episode_reward=-913.99 +/- 144.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=19000, episode_reward=-710.36 +/- 81.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=20000, episode_reward=-996.53 +/- 249.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=21000, episode_reward=-797.06 +/- 127.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=22000, episode_reward=-790.77 +/- 83.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=23000, episode_reward=-649.04 +/- 246.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=24000, episode_reward=-713.18 +/- 299.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=25000, episode_reward=-762.82 +/- 258.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=26000, episode_reward=-784.76 +/- 146.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=27000, episode_reward=-669.71 +/- 251.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=28000, episode_reward=-788.06 +/- 159.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=29000, episode_reward=-693.01 +/- 254.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=30000, episode_reward=-826.94 +/- 102.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=31000, episode_reward=-733.03 +/- 274.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=32000, episode_reward=-923.88 +/- 51.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=33000, episode_reward=-1042.19 +/- 248.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=34000, episode_reward=-674.47 +/- 178.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=35000, episode_reward=-846.53 +/- 190.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=36000, episode_reward=-760.66 +/- 82.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=37000, episode_reward=-850.22 +/- 232.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=38000, episode_reward=-942.80 +/- 258.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=39000, episode_reward=-924.24 +/- 157.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=40000, episode_reward=-890.56 +/- 130.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=41000, episode_reward=-891.78 +/- 162.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=42000, episode_reward=-861.89 +/- 178.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=43000, episode_reward=-905.45 +/- 99.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=44000, episode_reward=-935.98 +/- 182.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=45000, episode_reward=-761.15 +/- 84.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=46000, episode_reward=-926.64 +/- 240.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=47000, episode_reward=-895.15 +/- 192.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=48000, episode_reward=-763.54 +/- 129.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=49000, episode_reward=-887.13 +/- 161.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=50000, episode_reward=-947.17 +/- 86.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=51000, episode_reward=-832.79 +/- 49.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=52000, episode_reward=-857.82 +/- 189.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=53000, episode_reward=-823.84 +/- 128.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=54000, episode_reward=-632.92 +/- 239.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=55000, episode_reward=-951.83 +/- 157.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=56000, episode_reward=-833.39 +/- 43.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=57000, episode_reward=-840.43 +/- 285.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=58000, episode_reward=-845.90 +/- 91.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=59000, episode_reward=-859.62 +/- 137.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=60000, episode_reward=-957.62 +/- 128.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=61000, episode_reward=-871.61 +/- 112.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=62000, episode_reward=-905.79 +/- 150.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=63000, episode_reward=-983.47 +/- 101.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=64000, episode_reward=-916.78 +/- 173.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=65000, episode_reward=-837.06 +/- 184.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=66000, episode_reward=-836.55 +/- 275.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=67000, episode_reward=-872.26 +/- 171.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=68000, episode_reward=-704.46 +/- 113.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=69000, episode_reward=-807.98 +/- 224.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=70000, episode_reward=-886.32 +/- 193.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=71000, episode_reward=-984.97 +/- 148.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=72000, episode_reward=-801.18 +/- 82.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=73000, episode_reward=-961.55 +/- 198.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=74000, episode_reward=-1038.43 +/- 168.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=75000, episode_reward=-875.18 +/- 169.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=76000, episode_reward=-830.46 +/- 69.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=77000, episode_reward=-820.67 +/- 153.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=78000, episode_reward=-719.43 +/- 183.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=79000, episode_reward=-965.61 +/- 179.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=80000, episode_reward=-847.98 +/- 138.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=81000, episode_reward=-854.91 +/- 355.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=82000, episode_reward=-1102.66 +/- 121.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=83000, episode_reward=-868.98 +/- 142.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=84000, episode_reward=-960.82 +/- 298.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=85000, episode_reward=-1025.53 +/- 164.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=86000, episode_reward=-993.73 +/- 97.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=87000, episode_reward=-913.57 +/- 62.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=88000, episode_reward=-1034.81 +/- 135.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=89000, episode_reward=-982.97 +/- 172.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=90000, episode_reward=-931.75 +/- 158.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=91000, episode_reward=-741.94 +/- 200.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=92000, episode_reward=-902.51 +/- 194.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=93000, episode_reward=-875.42 +/- 151.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=94000, episode_reward=-961.62 +/- 91.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=95000, episode_reward=-970.85 +/- 276.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=96000, episode_reward=-896.36 +/- 225.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=97000, episode_reward=-1077.05 +/- 131.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=98000, episode_reward=-875.39 +/- 221.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=99000, episode_reward=-838.26 +/- 142.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=100000, episode_reward=-959.76 +/- 162.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=101000, episode_reward=-971.85 +/- 202.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=102000, episode_reward=-833.96 +/- 134.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=103000, episode_reward=-794.17 +/- 189.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=104000, episode_reward=-892.82 +/- 260.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=105000, episode_reward=-752.35 +/- 216.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=106000, episode_reward=-889.98 +/- 114.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=107000, episode_reward=-787.62 +/- 232.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=108000, episode_reward=-935.63 +/- 194.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=109000, episode_reward=-914.31 +/- 55.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=110000, episode_reward=-696.12 +/- 306.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=111000, episode_reward=-831.32 +/- 266.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=112000, episode_reward=-925.22 +/- 281.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=113000, episode_reward=-812.09 +/- 76.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=114000, episode_reward=-893.30 +/- 141.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=115000, episode_reward=-824.39 +/- 247.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=116000, episode_reward=-808.57 +/- 191.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=117000, episode_reward=-803.43 +/- 120.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=118000, episode_reward=-898.62 +/- 186.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=119000, episode_reward=-829.32 +/- 68.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=120000, episode_reward=-935.86 +/- 79.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=121000, episode_reward=-789.41 +/- 214.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=122000, episode_reward=-853.55 +/- 85.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=123000, episode_reward=-886.70 +/- 210.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=124000, episode_reward=-851.21 +/- 87.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=125000, episode_reward=-881.42 +/- 153.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=126000, episode_reward=-906.68 +/- 167.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=127000, episode_reward=-1045.12 +/- 36.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=128000, episode_reward=-771.21 +/- 186.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=129000, episode_reward=-893.93 +/- 182.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=130000, episode_reward=-1065.61 +/- 125.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=131000, episode_reward=-885.21 +/- 81.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=132000, episode_reward=-777.79 +/- 109.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=133000, episode_reward=-877.90 +/- 161.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=134000, episode_reward=-852.25 +/- 212.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=135000, episode_reward=-807.47 +/- 109.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=136000, episode_reward=-867.62 +/- 156.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=137000, episode_reward=-852.02 +/- 183.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=138000, episode_reward=-931.89 +/- 118.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=139000, episode_reward=-819.64 +/- 148.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=140000, episode_reward=-754.08 +/- 182.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=141000, episode_reward=-809.74 +/- 107.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=142000, episode_reward=-833.41 +/- 113.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=143000, episode_reward=-981.24 +/- 89.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=144000, episode_reward=-869.34 +/- 104.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=145000, episode_reward=-762.86 +/- 302.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=146000, episode_reward=-919.36 +/- 336.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=147000, episode_reward=-829.14 +/- 295.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=148000, episode_reward=-847.92 +/- 178.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=149000, episode_reward=-682.80 +/- 240.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=150000, episode_reward=-970.21 +/- 43.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=151000, episode_reward=-894.59 +/- 89.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=152000, episode_reward=-971.91 +/- 187.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=153000, episode_reward=-864.16 +/- 199.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=154000, episode_reward=-806.27 +/- 231.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=155000, episode_reward=-983.67 +/- 94.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=156000, episode_reward=-992.28 +/- 128.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=157000, episode_reward=-913.70 +/- 128.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=158000, episode_reward=-968.10 +/- 189.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=159000, episode_reward=-787.39 +/- 83.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=160000, episode_reward=-959.37 +/- 71.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=161000, episode_reward=-830.37 +/- 298.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=162000, episode_reward=-903.53 +/- 79.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=163000, episode_reward=-940.52 +/- 103.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=164000, episode_reward=-752.14 +/- 135.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=165000, episode_reward=-552.69 +/- 245.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=166000, episode_reward=-937.01 +/- 134.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=167000, episode_reward=-1020.61 +/- 146.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=168000, episode_reward=-868.82 +/- 157.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=169000, episode_reward=-876.25 +/- 258.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=170000, episode_reward=-717.76 +/- 248.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=171000, episode_reward=-802.93 +/- 132.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=172000, episode_reward=-863.94 +/- 314.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=173000, episode_reward=-819.80 +/- 110.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=174000, episode_reward=-710.87 +/- 284.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=175000, episode_reward=-621.35 +/- 313.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=176000, episode_reward=-882.03 +/- 77.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=177000, episode_reward=-581.35 +/- 228.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=178000, episode_reward=-945.35 +/- 106.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=179000, episode_reward=-849.00 +/- 115.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=180000, episode_reward=-827.48 +/- 146.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=181000, episode_reward=-868.36 +/- 256.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=182000, episode_reward=-902.85 +/- 127.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=183000, episode_reward=-986.79 +/- 100.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=184000, episode_reward=-804.61 +/- 172.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=185000, episode_reward=-652.32 +/- 272.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=186000, episode_reward=-837.57 +/- 104.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=187000, episode_reward=-877.28 +/- 184.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=188000, episode_reward=-638.81 +/- 452.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=189000, episode_reward=-809.89 +/- 256.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=190000, episode_reward=-851.29 +/- 240.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=191000, episode_reward=-776.88 +/- 161.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=192000, episode_reward=-895.31 +/- 85.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=193000, episode_reward=-919.96 +/- 206.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=194000, episode_reward=-1125.16 +/- 54.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=195000, episode_reward=-945.07 +/- 86.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=196000, episode_reward=-943.93 +/- 230.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=197000, episode_reward=-647.85 +/- 307.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=198000, episode_reward=-443.21 +/- 212.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=199000, episode_reward=-938.63 +/- 72.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=200000, episode_reward=-572.94 +/- 300.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=201000, episode_reward=-630.16 +/- 399.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=202000, episode_reward=-827.26 +/- 225.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=203000, episode_reward=-410.62 +/- 87.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=204000, episode_reward=-824.87 +/- 296.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=205000, episode_reward=-854.56 +/- 245.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=206000, episode_reward=-743.57 +/- 131.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=207000, episode_reward=-605.11 +/- 358.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=208000, episode_reward=-844.70 +/- 157.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=209000, episode_reward=-788.86 +/- 245.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=210000, episode_reward=-802.32 +/- 252.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=211000, episode_reward=-512.71 +/- 502.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=212000, episode_reward=-370.93 +/- 471.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=213000, episode_reward=-805.25 +/- 144.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=214000, episode_reward=-425.81 +/- 467.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=215000, episode_reward=-685.36 +/- 392.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=216000, episode_reward=-505.40 +/- 276.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=217000, episode_reward=-303.72 +/- 96.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=218000, episode_reward=-832.48 +/- 234.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=219000, episode_reward=-603.52 +/- 262.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=220000, episode_reward=-687.24 +/- 434.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=221000, episode_reward=-834.55 +/- 257.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=222000, episode_reward=-885.30 +/- 75.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=223000, episode_reward=-781.93 +/- 163.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=224000, episode_reward=-743.04 +/- 384.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=225000, episode_reward=-796.43 +/- 375.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=226000, episode_reward=-1096.15 +/- 89.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=227000, episode_reward=-814.98 +/- 273.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=228000, episode_reward=-697.20 +/- 234.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=229000, episode_reward=-805.56 +/- 125.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=230000, episode_reward=-731.45 +/- 345.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=231000, episode_reward=-942.11 +/- 69.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=232000, episode_reward=-723.89 +/- 302.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=233000, episode_reward=-480.10 +/- 385.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=234000, episode_reward=-632.28 +/- 194.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=235000, episode_reward=-809.56 +/- 246.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=236000, episode_reward=-792.65 +/- 418.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=237000, episode_reward=-594.53 +/- 391.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=238000, episode_reward=-906.93 +/- 141.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=239000, episode_reward=-769.37 +/- 224.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=240000, episode_reward=-888.87 +/- 237.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=241000, episode_reward=-875.96 +/- 300.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=242000, episode_reward=-831.27 +/- 232.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=243000, episode_reward=-832.87 +/- 190.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=244000, episode_reward=-865.37 +/- 240.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=245000, episode_reward=-665.62 +/- 377.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=246000, episode_reward=-408.40 +/- 321.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=247000, episode_reward=-824.09 +/- 229.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=248000, episode_reward=-959.12 +/- 179.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=249000, episode_reward=-527.85 +/- 213.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=250000, episode_reward=-765.76 +/- 182.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=251000, episode_reward=-546.83 +/- 247.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=252000, episode_reward=-702.52 +/- 252.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=253000, episode_reward=-693.13 +/- 493.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=254000, episode_reward=-58.88 +/- 718.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=255000, episode_reward=-367.91 +/- 375.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=256000, episode_reward=-537.58 +/- 506.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=257000, episode_reward=-491.90 +/- 289.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=258000, episode_reward=-835.43 +/- 185.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=259000, episode_reward=-648.10 +/- 405.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=260000, episode_reward=-792.44 +/- 292.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=261000, episode_reward=-995.99 +/- 49.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=262000, episode_reward=-619.82 +/- 262.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=263000, episode_reward=-861.23 +/- 227.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=264000, episode_reward=-722.99 +/- 373.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=265000, episode_reward=-737.93 +/- 303.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=266000, episode_reward=-545.13 +/- 258.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=267000, episode_reward=-582.41 +/- 255.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=268000, episode_reward=-535.15 +/- 174.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=269000, episode_reward=-672.01 +/- 299.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=270000, episode_reward=-859.79 +/- 209.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=271000, episode_reward=-702.27 +/- 436.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=272000, episode_reward=-437.95 +/- 173.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=273000, episode_reward=-470.48 +/- 336.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=274000, episode_reward=-211.67 +/- 160.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=275000, episode_reward=-64.99 +/- 710.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=276000, episode_reward=-488.96 +/- 298.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=277000, episode_reward=-852.15 +/- 166.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=278000, episode_reward=-572.75 +/- 250.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=279000, episode_reward=-133.71 +/- 468.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=280000, episode_reward=-432.13 +/- 286.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=281000, episode_reward=-851.96 +/- 225.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=282000, episode_reward=-537.09 +/- 138.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=283000, episode_reward=-488.41 +/- 131.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=284000, episode_reward=-496.02 +/- 437.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=285000, episode_reward=-241.83 +/- 422.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=286000, episode_reward=-436.29 +/- 365.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=287000, episode_reward=-907.68 +/- 192.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=288000, episode_reward=-883.26 +/- 200.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=289000, episode_reward=-634.22 +/- 328.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=290000, episode_reward=-327.30 +/- 408.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=291000, episode_reward=-410.72 +/- 436.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=292000, episode_reward=-266.50 +/- 553.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=293000, episode_reward=114.96 +/- 268.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=294000, episode_reward=172.49 +/- 440.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=295000, episode_reward=1108.54 +/- 596.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=296000, episode_reward=-99.39 +/- 526.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=297000, episode_reward=-342.63 +/- 146.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=298000, episode_reward=-200.47 +/- 252.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=299000, episode_reward=-169.27 +/- 377.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=300000, episode_reward=442.87 +/- 411.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=301000, episode_reward=718.07 +/- 339.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=302000, episode_reward=64.87 +/- 641.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=303000, episode_reward=-367.29 +/- 615.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=304000, episode_reward=375.62 +/- 593.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=305000, episode_reward=-62.84 +/- 376.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=306000, episode_reward=28.21 +/- 558.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=307000, episode_reward=153.08 +/- 140.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=308000, episode_reward=236.98 +/- 368.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=309000, episode_reward=1081.49 +/- 490.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=310000, episode_reward=242.84 +/- 380.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=311000, episode_reward=765.11 +/- 600.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=312000, episode_reward=512.54 +/- 516.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=313000, episode_reward=640.37 +/- 700.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=314000, episode_reward=836.89 +/- 278.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=315000, episode_reward=388.00 +/- 615.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=316000, episode_reward=-17.12 +/- 773.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=317000, episode_reward=783.74 +/- 435.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=318000, episode_reward=553.48 +/- 319.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=319000, episode_reward=636.82 +/- 621.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=320000, episode_reward=475.56 +/- 221.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=321000, episode_reward=299.43 +/- 456.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=322000, episode_reward=644.38 +/- 483.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=323000, episode_reward=558.03 +/- 601.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=324000, episode_reward=397.49 +/- 644.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=325000, episode_reward=276.50 +/- 292.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=326000, episode_reward=134.28 +/- 830.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=327000, episode_reward=-97.07 +/- 422.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=328000, episode_reward=430.26 +/- 333.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=329000, episode_reward=151.68 +/- 508.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=330000, episode_reward=499.41 +/- 816.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=331000, episode_reward=436.25 +/- 699.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=332000, episode_reward=1177.96 +/- 337.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=333000, episode_reward=928.59 +/- 515.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=334000, episode_reward=671.02 +/- 756.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=335000, episode_reward=285.77 +/- 594.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=336000, episode_reward=515.57 +/- 504.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=337000, episode_reward=541.87 +/- 587.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=338000, episode_reward=954.17 +/- 709.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=339000, episode_reward=1223.72 +/- 506.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=340000, episode_reward=594.55 +/- 319.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=341000, episode_reward=1006.08 +/- 682.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=342000, episode_reward=929.05 +/- 340.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=343000, episode_reward=739.95 +/- 671.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=344000, episode_reward=938.42 +/- 547.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=345000, episode_reward=1286.67 +/- 243.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=346000, episode_reward=1056.18 +/- 532.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=347000, episode_reward=1021.91 +/- 360.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=348000, episode_reward=1182.01 +/- 295.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=349000, episode_reward=689.42 +/- 375.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=350000, episode_reward=893.44 +/- 580.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=351000, episode_reward=778.51 +/- 486.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=352000, episode_reward=633.15 +/- 366.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=353000, episode_reward=-193.10 +/- 499.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=354000, episode_reward=407.86 +/- 361.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=355000, episode_reward=836.79 +/- 619.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=356000, episode_reward=683.13 +/- 487.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=357000, episode_reward=663.64 +/- 710.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=358000, episode_reward=558.37 +/- 606.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=359000, episode_reward=1054.26 +/- 225.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=360000, episode_reward=821.00 +/- 192.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=361000, episode_reward=1181.23 +/- 153.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=362000, episode_reward=839.73 +/- 352.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=363000, episode_reward=1129.28 +/- 360.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=364000, episode_reward=905.81 +/- 481.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=365000, episode_reward=814.73 +/- 565.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=366000, episode_reward=1211.40 +/- 221.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=367000, episode_reward=1350.98 +/- 353.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=368000, episode_reward=1247.97 +/- 291.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=369000, episode_reward=1357.19 +/- 213.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=370000, episode_reward=1359.09 +/- 50.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=371000, episode_reward=1307.15 +/- 157.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=372000, episode_reward=1291.41 +/- 282.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=373000, episode_reward=1400.59 +/- 254.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=374000, episode_reward=1198.12 +/- 260.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=375000, episode_reward=1144.26 +/- 347.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=376000, episode_reward=1168.12 +/- 133.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=377000, episode_reward=1157.30 +/- 461.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=378000, episode_reward=1185.49 +/- 338.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=379000, episode_reward=522.72 +/- 295.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=380000, episode_reward=1246.47 +/- 331.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=381000, episode_reward=1215.51 +/- 476.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=382000, episode_reward=1201.47 +/- 234.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=383000, episode_reward=1402.95 +/- 115.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=384000, episode_reward=926.43 +/- 160.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=385000, episode_reward=952.26 +/- 190.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=386000, episode_reward=1326.22 +/- 238.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=387000, episode_reward=1401.71 +/- 312.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=388000, episode_reward=1402.45 +/- 222.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=389000, episode_reward=1106.29 +/- 365.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=390000, episode_reward=1241.77 +/- 221.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=391000, episode_reward=969.89 +/- 320.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=392000, episode_reward=1408.03 +/- 415.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=393000, episode_reward=1351.56 +/- 300.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=394000, episode_reward=1480.54 +/- 199.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=395000, episode_reward=1347.66 +/- 195.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=396000, episode_reward=1330.29 +/- 196.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=397000, episode_reward=1336.67 +/- 267.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=398000, episode_reward=1339.47 +/- 246.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=399000, episode_reward=1323.75 +/- 188.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=400000, episode_reward=1479.30 +/- 165.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=401000, episode_reward=1430.88 +/- 266.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=402000, episode_reward=1568.99 +/- 49.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=403000, episode_reward=1475.74 +/- 113.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=404000, episode_reward=1267.26 +/- 257.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=405000, episode_reward=1191.59 +/- 460.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=406000, episode_reward=1285.91 +/- 327.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=407000, episode_reward=1339.25 +/- 341.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=408000, episode_reward=1412.11 +/- 136.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=409000, episode_reward=1277.13 +/- 331.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=410000, episode_reward=1329.68 +/- 175.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=411000, episode_reward=1514.81 +/- 62.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=412000, episode_reward=1352.23 +/- 226.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=413000, episode_reward=1435.55 +/- 34.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=414000, episode_reward=1284.04 +/- 221.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=415000, episode_reward=1479.29 +/- 184.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=416000, episode_reward=1268.42 +/- 167.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=417000, episode_reward=1338.12 +/- 189.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=418000, episode_reward=1289.83 +/- 337.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=419000, episode_reward=1468.37 +/- 103.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=420000, episode_reward=1243.13 +/- 478.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=421000, episode_reward=1207.89 +/- 424.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=422000, episode_reward=1416.85 +/- 148.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=423000, episode_reward=1446.58 +/- 98.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=424000, episode_reward=1334.68 +/- 173.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=425000, episode_reward=1278.25 +/- 296.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=426000, episode_reward=1230.97 +/- 172.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=427000, episode_reward=1389.72 +/- 184.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=428000, episode_reward=1450.20 +/- 271.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=429000, episode_reward=1320.20 +/- 319.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=430000, episode_reward=1563.67 +/- 149.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=431000, episode_reward=1558.25 +/- 103.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=432000, episode_reward=1236.48 +/- 447.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=433000, episode_reward=1383.61 +/- 210.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=434000, episode_reward=1527.35 +/- 306.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=435000, episode_reward=1183.50 +/- 204.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=436000, episode_reward=1286.34 +/- 250.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=437000, episode_reward=1356.74 +/- 246.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=438000, episode_reward=1393.64 +/- 241.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=439000, episode_reward=1274.88 +/- 312.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=440000, episode_reward=1266.45 +/- 263.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=441000, episode_reward=1189.26 +/- 328.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=442000, episode_reward=1224.97 +/- 276.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=443000, episode_reward=1023.25 +/- 414.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=444000, episode_reward=1163.65 +/- 504.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=445000, episode_reward=1181.14 +/- 326.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=446000, episode_reward=1339.67 +/- 257.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=447000, episode_reward=1448.93 +/- 88.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=448000, episode_reward=839.15 +/- 477.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=449000, episode_reward=1427.47 +/- 336.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=450000, episode_reward=1298.19 +/- 154.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=451000, episode_reward=1492.45 +/- 213.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=452000, episode_reward=1184.91 +/- 438.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=453000, episode_reward=1406.75 +/- 288.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=454000, episode_reward=1196.39 +/- 277.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=455000, episode_reward=1373.15 +/- 147.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=456000, episode_reward=1326.95 +/- 151.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=457000, episode_reward=1428.89 +/- 93.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=458000, episode_reward=1523.90 +/- 100.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=459000, episode_reward=1352.45 +/- 266.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=460000, episode_reward=1550.85 +/- 47.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=461000, episode_reward=1380.56 +/- 101.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=462000, episode_reward=1446.13 +/- 227.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=463000, episode_reward=1558.53 +/- 240.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=464000, episode_reward=1382.71 +/- 258.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=465000, episode_reward=1316.26 +/- 192.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=466000, episode_reward=1418.65 +/- 72.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=467000, episode_reward=1154.27 +/- 250.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=468000, episode_reward=1427.08 +/- 132.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=469000, episode_reward=1366.49 +/- 86.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=470000, episode_reward=1512.09 +/- 81.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=471000, episode_reward=1349.92 +/- 228.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=472000, episode_reward=1248.11 +/- 592.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=473000, episode_reward=1122.80 +/- 363.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=474000, episode_reward=1273.08 +/- 177.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=475000, episode_reward=1284.71 +/- 292.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=476000, episode_reward=1363.53 +/- 282.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=477000, episode_reward=1471.22 +/- 286.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=478000, episode_reward=1226.98 +/- 189.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=479000, episode_reward=1578.88 +/- 212.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=480000, episode_reward=1404.32 +/- 308.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=481000, episode_reward=1227.74 +/- 256.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=482000, episode_reward=1377.80 +/- 181.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=483000, episode_reward=1109.63 +/- 257.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=484000, episode_reward=1220.28 +/- 448.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=485000, episode_reward=1330.98 +/- 138.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=486000, episode_reward=1444.61 +/- 91.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=487000, episode_reward=1464.31 +/- 297.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=488000, episode_reward=1491.47 +/- 86.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=489000, episode_reward=1418.44 +/- 132.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=490000, episode_reward=1408.76 +/- 297.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=491000, episode_reward=1292.22 +/- 202.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=492000, episode_reward=1425.58 +/- 240.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=493000, episode_reward=1296.91 +/- 333.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=494000, episode_reward=1332.23 +/- 430.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=495000, episode_reward=1324.31 +/- 363.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=496000, episode_reward=1488.73 +/- 43.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=497000, episode_reward=1307.46 +/- 280.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=498000, episode_reward=1580.85 +/- 105.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=499000, episode_reward=1503.76 +/- 98.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=500000, episode_reward=1412.72 +/- 79.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=501000, episode_reward=1594.75 +/- 147.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=502000, episode_reward=1324.94 +/- 268.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=503000, episode_reward=1528.18 +/- 135.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=504000, episode_reward=1367.16 +/- 305.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=505000, episode_reward=1493.78 +/- 110.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=506000, episode_reward=1350.33 +/- 310.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=507000, episode_reward=1444.44 +/- 125.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=508000, episode_reward=1594.57 +/- 38.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=509000, episode_reward=1466.71 +/- 167.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=510000, episode_reward=1473.31 +/- 59.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=511000, episode_reward=1339.43 +/- 234.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=512000, episode_reward=1405.07 +/- 108.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=513000, episode_reward=1519.52 +/- 210.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=514000, episode_reward=1461.87 +/- 176.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=515000, episode_reward=1533.94 +/- 70.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=516000, episode_reward=1148.32 +/- 104.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=517000, episode_reward=1536.81 +/- 162.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=518000, episode_reward=1484.02 +/- 118.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=519000, episode_reward=1229.98 +/- 280.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=520000, episode_reward=1301.12 +/- 317.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=521000, episode_reward=979.46 +/- 229.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=522000, episode_reward=1128.17 +/- 198.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=523000, episode_reward=1428.36 +/- 103.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=524000, episode_reward=1160.43 +/- 341.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=525000, episode_reward=1330.72 +/- 211.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=526000, episode_reward=1553.62 +/- 93.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=527000, episode_reward=1242.41 +/- 160.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=528000, episode_reward=1522.46 +/- 72.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=529000, episode_reward=1519.92 +/- 149.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=530000, episode_reward=1385.93 +/- 169.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=531000, episode_reward=1542.73 +/- 138.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=532000, episode_reward=1440.54 +/- 243.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=533000, episode_reward=1464.75 +/- 20.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=534000, episode_reward=1427.21 +/- 355.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=535000, episode_reward=1452.25 +/- 134.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=536000, episode_reward=1196.81 +/- 488.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=537000, episode_reward=1447.42 +/- 256.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=538000, episode_reward=1624.52 +/- 22.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=539000, episode_reward=1395.81 +/- 269.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=540000, episode_reward=1408.36 +/- 267.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=541000, episode_reward=1380.17 +/- 170.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=542000, episode_reward=1428.71 +/- 86.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=543000, episode_reward=1477.24 +/- 98.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=544000, episode_reward=1475.99 +/- 97.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=545000, episode_reward=1508.62 +/- 123.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=546000, episode_reward=1361.90 +/- 243.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=547000, episode_reward=1310.50 +/- 187.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=548000, episode_reward=1607.45 +/- 139.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=549000, episode_reward=1410.37 +/- 194.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=550000, episode_reward=1441.14 +/- 135.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=551000, episode_reward=1443.87 +/- 193.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=552000, episode_reward=1434.51 +/- 256.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=553000, episode_reward=1500.37 +/- 82.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=554000, episode_reward=1238.95 +/- 255.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=555000, episode_reward=1376.74 +/- 240.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=556000, episode_reward=1461.63 +/- 204.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=557000, episode_reward=1411.10 +/- 93.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=558000, episode_reward=1565.35 +/- 150.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=559000, episode_reward=1614.10 +/- 155.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=560000, episode_reward=1568.17 +/- 173.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=561000, episode_reward=1296.65 +/- 248.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=562000, episode_reward=1241.34 +/- 345.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=563000, episode_reward=1556.95 +/- 96.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=564000, episode_reward=1541.46 +/- 120.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=565000, episode_reward=1274.72 +/- 323.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=566000, episode_reward=1323.36 +/- 397.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=567000, episode_reward=1398.80 +/- 258.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=568000, episode_reward=1509.56 +/- 107.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=569000, episode_reward=1473.49 +/- 234.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=570000, episode_reward=1411.36 +/- 322.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=571000, episode_reward=1516.69 +/- 120.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=572000, episode_reward=1542.67 +/- 76.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=573000, episode_reward=1578.83 +/- 154.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=574000, episode_reward=1510.29 +/- 142.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=575000, episode_reward=1468.52 +/- 364.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=576000, episode_reward=1283.60 +/- 273.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=577000, episode_reward=1535.85 +/- 110.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=578000, episode_reward=1416.10 +/- 59.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=579000, episode_reward=1334.57 +/- 244.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=580000, episode_reward=1414.99 +/- 140.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=581000, episode_reward=1373.50 +/- 198.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=582000, episode_reward=1208.23 +/- 191.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=583000, episode_reward=1540.47 +/- 173.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=584000, episode_reward=1511.23 +/- 278.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=585000, episode_reward=1576.26 +/- 65.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=586000, episode_reward=1390.69 +/- 208.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=587000, episode_reward=1558.48 +/- 137.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=588000, episode_reward=1503.64 +/- 225.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=589000, episode_reward=1548.18 +/- 76.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=590000, episode_reward=1456.96 +/- 204.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=591000, episode_reward=1517.05 +/- 268.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=592000, episode_reward=1615.37 +/- 143.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=593000, episode_reward=1489.13 +/- 220.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=594000, episode_reward=1543.64 +/- 76.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=595000, episode_reward=1519.64 +/- 161.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=596000, episode_reward=1306.28 +/- 266.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=597000, episode_reward=1461.49 +/- 129.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=598000, episode_reward=1634.59 +/- 88.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=599000, episode_reward=1556.84 +/- 137.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=600000, episode_reward=1310.82 +/- 253.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=601000, episode_reward=1444.90 +/- 93.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=602000, episode_reward=1306.21 +/- 303.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=603000, episode_reward=1478.90 +/- 150.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=604000, episode_reward=1350.07 +/- 187.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=605000, episode_reward=1315.11 +/- 294.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=606000, episode_reward=1359.69 +/- 232.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=607000, episode_reward=1328.67 +/- 188.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=608000, episode_reward=1109.87 +/- 238.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=609000, episode_reward=1399.73 +/- 238.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=610000, episode_reward=1532.79 +/- 173.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=611000, episode_reward=1654.05 +/- 177.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=612000, episode_reward=1384.98 +/- 240.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=613000, episode_reward=1345.80 +/- 175.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=614000, episode_reward=1359.82 +/- 125.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=615000, episode_reward=1519.99 +/- 114.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=616000, episode_reward=1518.94 +/- 216.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=617000, episode_reward=1406.43 +/- 200.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=618000, episode_reward=1441.19 +/- 246.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=619000, episode_reward=1338.19 +/- 111.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=620000, episode_reward=1377.07 +/- 268.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=621000, episode_reward=1465.07 +/- 127.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=622000, episode_reward=1454.12 +/- 235.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=623000, episode_reward=1392.20 +/- 371.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=624000, episode_reward=1590.83 +/- 101.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=625000, episode_reward=1435.86 +/- 98.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=626000, episode_reward=1475.17 +/- 132.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=627000, episode_reward=1436.27 +/- 284.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=628000, episode_reward=1557.25 +/- 52.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=629000, episode_reward=1447.43 +/- 294.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=630000, episode_reward=1639.62 +/- 128.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=631000, episode_reward=1472.31 +/- 281.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=632000, episode_reward=1295.16 +/- 233.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=633000, episode_reward=1382.70 +/- 241.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=634000, episode_reward=1439.43 +/- 49.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=635000, episode_reward=1539.03 +/- 163.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=636000, episode_reward=1497.73 +/- 62.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=637000, episode_reward=1511.07 +/- 90.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=638000, episode_reward=1215.95 +/- 324.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=639000, episode_reward=1391.96 +/- 317.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=640000, episode_reward=1513.26 +/- 116.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=641000, episode_reward=1550.03 +/- 85.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=642000, episode_reward=1410.66 +/- 262.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=643000, episode_reward=1532.02 +/- 126.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=644000, episode_reward=1572.62 +/- 84.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=645000, episode_reward=1506.87 +/- 93.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=646000, episode_reward=1439.81 +/- 216.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=647000, episode_reward=1392.46 +/- 280.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=648000, episode_reward=1310.76 +/- 229.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=649000, episode_reward=1541.00 +/- 107.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=650000, episode_reward=1355.60 +/- 291.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=651000, episode_reward=1357.44 +/- 313.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=652000, episode_reward=1570.99 +/- 92.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=653000, episode_reward=1472.40 +/- 85.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=654000, episode_reward=1612.92 +/- 108.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=655000, episode_reward=1610.94 +/- 90.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=656000, episode_reward=1566.67 +/- 132.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=657000, episode_reward=1552.68 +/- 90.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=658000, episode_reward=1508.35 +/- 107.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=659000, episode_reward=1339.22 +/- 296.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=660000, episode_reward=1435.63 +/- 136.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=661000, episode_reward=1468.48 +/- 159.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=662000, episode_reward=1425.87 +/- 171.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=663000, episode_reward=1540.35 +/- 93.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=664000, episode_reward=1388.13 +/- 295.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=665000, episode_reward=1451.06 +/- 147.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=666000, episode_reward=1458.51 +/- 122.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=667000, episode_reward=1572.70 +/- 175.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=668000, episode_reward=1525.64 +/- 143.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=669000, episode_reward=1395.65 +/- 165.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=670000, episode_reward=1448.94 +/- 68.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=671000, episode_reward=1462.51 +/- 85.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=672000, episode_reward=1505.27 +/- 161.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=673000, episode_reward=1652.06 +/- 74.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=674000, episode_reward=1469.67 +/- 155.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=675000, episode_reward=1491.79 +/- 99.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=676000, episode_reward=1506.61 +/- 82.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=677000, episode_reward=1280.80 +/- 183.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=678000, episode_reward=1412.85 +/- 183.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=679000, episode_reward=1397.94 +/- 136.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=680000, episode_reward=1548.49 +/- 40.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=681000, episode_reward=1584.09 +/- 115.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=682000, episode_reward=1509.29 +/- 133.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=683000, episode_reward=1482.34 +/- 96.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=684000, episode_reward=1492.42 +/- 138.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=685000, episode_reward=1613.00 +/- 110.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=686000, episode_reward=1490.61 +/- 378.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=687000, episode_reward=1502.07 +/- 99.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=688000, episode_reward=1467.66 +/- 199.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=689000, episode_reward=1535.25 +/- 99.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=690000, episode_reward=1352.78 +/- 276.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=691000, episode_reward=1273.39 +/- 349.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=692000, episode_reward=1536.99 +/- 226.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=693000, episode_reward=1491.50 +/- 89.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=694000, episode_reward=1433.16 +/- 94.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=695000, episode_reward=1555.40 +/- 114.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=696000, episode_reward=1627.01 +/- 82.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=697000, episode_reward=1529.23 +/- 160.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=698000, episode_reward=1360.12 +/- 215.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=699000, episode_reward=1453.16 +/- 270.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=700000, episode_reward=1375.97 +/- 214.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=701000, episode_reward=1327.69 +/- 256.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=702000, episode_reward=1488.32 +/- 169.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=703000, episode_reward=1500.52 +/- 74.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=704000, episode_reward=1601.29 +/- 96.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=705000, episode_reward=1439.19 +/- 114.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=706000, episode_reward=1436.77 +/- 389.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=707000, episode_reward=1583.29 +/- 103.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=708000, episode_reward=1594.00 +/- 72.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=709000, episode_reward=1556.65 +/- 87.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=710000, episode_reward=1376.16 +/- 194.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=711000, episode_reward=1431.68 +/- 59.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=712000, episode_reward=1549.92 +/- 109.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=713000, episode_reward=1502.03 +/- 226.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=714000, episode_reward=1549.82 +/- 69.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=715000, episode_reward=1545.91 +/- 121.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=716000, episode_reward=1593.49 +/- 107.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=717000, episode_reward=1318.65 +/- 269.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=718000, episode_reward=1451.47 +/- 225.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=719000, episode_reward=1578.03 +/- 109.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=720000, episode_reward=1556.82 +/- 161.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=721000, episode_reward=1364.22 +/- 232.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=722000, episode_reward=1521.72 +/- 161.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=723000, episode_reward=1442.08 +/- 243.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=724000, episode_reward=1505.45 +/- 143.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=725000, episode_reward=1497.82 +/- 129.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=726000, episode_reward=1619.93 +/- 81.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=727000, episode_reward=1483.95 +/- 115.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=728000, episode_reward=1483.78 +/- 141.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=729000, episode_reward=1559.05 +/- 100.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=730000, episode_reward=1497.96 +/- 80.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=731000, episode_reward=1397.45 +/- 226.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=732000, episode_reward=1371.86 +/- 272.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=733000, episode_reward=1597.57 +/- 88.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=734000, episode_reward=1498.46 +/- 150.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=735000, episode_reward=1433.04 +/- 248.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=736000, episode_reward=1320.99 +/- 188.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=737000, episode_reward=1326.51 +/- 387.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=738000, episode_reward=1340.13 +/- 317.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=739000, episode_reward=1565.82 +/- 167.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=740000, episode_reward=1494.72 +/- 80.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=741000, episode_reward=1552.31 +/- 82.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=742000, episode_reward=1404.63 +/- 96.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=743000, episode_reward=1468.77 +/- 234.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=744000, episode_reward=1540.26 +/- 43.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=745000, episode_reward=1530.70 +/- 131.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=746000, episode_reward=1506.85 +/- 159.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=747000, episode_reward=1280.14 +/- 223.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=748000, episode_reward=1466.15 +/- 167.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=749000, episode_reward=1616.69 +/- 95.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=750000, episode_reward=1525.21 +/- 363.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=751000, episode_reward=1349.20 +/- 155.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=752000, episode_reward=1430.23 +/- 239.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=753000, episode_reward=1558.76 +/- 148.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=754000, episode_reward=1557.68 +/- 173.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=755000, episode_reward=1421.89 +/- 98.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=756000, episode_reward=1545.64 +/- 121.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=757000, episode_reward=1545.98 +/- 32.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=758000, episode_reward=1537.14 +/- 87.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=759000, episode_reward=1511.41 +/- 397.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=760000, episode_reward=1340.01 +/- 294.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=761000, episode_reward=1488.92 +/- 93.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=762000, episode_reward=1459.85 +/- 213.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=763000, episode_reward=1479.72 +/- 220.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=764000, episode_reward=1480.58 +/- 233.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=765000, episode_reward=1331.62 +/- 282.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=766000, episode_reward=1586.37 +/- 78.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=767000, episode_reward=1541.15 +/- 117.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=768000, episode_reward=1617.21 +/- 140.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=769000, episode_reward=1478.35 +/- 272.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=770000, episode_reward=1430.32 +/- 210.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=771000, episode_reward=1534.99 +/- 162.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=772000, episode_reward=1451.96 +/- 154.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=773000, episode_reward=1462.86 +/- 99.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=774000, episode_reward=1512.88 +/- 165.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=775000, episode_reward=1490.80 +/- 43.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=776000, episode_reward=1554.39 +/- 130.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=777000, episode_reward=1534.76 +/- 155.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=778000, episode_reward=1433.76 +/- 75.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=779000, episode_reward=1559.35 +/- 123.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=780000, episode_reward=1479.63 +/- 146.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=781000, episode_reward=1450.98 +/- 306.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=782000, episode_reward=1442.35 +/- 236.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=783000, episode_reward=1452.89 +/- 207.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=784000, episode_reward=1466.18 +/- 255.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=785000, episode_reward=1457.09 +/- 108.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=786000, episode_reward=1358.50 +/- 265.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=787000, episode_reward=1437.66 +/- 335.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=788000, episode_reward=1501.68 +/- 55.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=789000, episode_reward=1420.55 +/- 225.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=790000, episode_reward=1401.13 +/- 204.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=791000, episode_reward=1547.91 +/- 129.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=792000, episode_reward=1584.91 +/- 97.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=793000, episode_reward=1450.54 +/- 158.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=794000, episode_reward=1500.80 +/- 139.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=795000, episode_reward=1479.10 +/- 268.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=796000, episode_reward=1533.35 +/- 40.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=797000, episode_reward=1435.38 +/- 193.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=798000, episode_reward=1510.93 +/- 136.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=799000, episode_reward=1327.40 +/- 312.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=800000, episode_reward=1468.15 +/- 158.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=801000, episode_reward=1445.59 +/- 105.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=802000, episode_reward=1571.56 +/- 162.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=803000, episode_reward=1496.40 +/- 144.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=804000, episode_reward=1346.43 +/- 272.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=805000, episode_reward=1438.74 +/- 107.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=806000, episode_reward=1514.70 +/- 111.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=807000, episode_reward=1441.97 +/- 189.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=808000, episode_reward=1465.28 +/- 46.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=809000, episode_reward=1622.76 +/- 112.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=810000, episode_reward=1218.85 +/- 392.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=811000, episode_reward=1602.63 +/- 94.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=812000, episode_reward=1421.19 +/- 314.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=813000, episode_reward=1492.31 +/- 121.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=814000, episode_reward=1439.07 +/- 305.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=815000, episode_reward=1449.58 +/- 196.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=816000, episode_reward=1560.21 +/- 143.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=817000, episode_reward=1578.62 +/- 140.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=818000, episode_reward=1191.22 +/- 252.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=819000, episode_reward=1544.43 +/- 137.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=820000, episode_reward=1405.04 +/- 278.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=821000, episode_reward=1477.80 +/- 236.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=822000, episode_reward=1423.91 +/- 118.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=823000, episode_reward=1495.47 +/- 109.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=824000, episode_reward=1637.79 +/- 139.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=825000, episode_reward=1401.25 +/- 275.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=826000, episode_reward=1533.80 +/- 195.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=827000, episode_reward=1456.64 +/- 122.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=828000, episode_reward=1440.72 +/- 371.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=829000, episode_reward=1418.79 +/- 170.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=830000, episode_reward=1307.65 +/- 286.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=831000, episode_reward=1447.33 +/- 190.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=832000, episode_reward=1554.21 +/- 109.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=833000, episode_reward=1580.39 +/- 75.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=834000, episode_reward=1576.81 +/- 95.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=835000, episode_reward=1508.42 +/- 79.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=836000, episode_reward=1344.83 +/- 263.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=837000, episode_reward=1559.77 +/- 119.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=838000, episode_reward=1338.55 +/- 239.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=839000, episode_reward=1498.66 +/- 134.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=840000, episode_reward=1458.46 +/- 216.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=841000, episode_reward=1487.71 +/- 238.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=842000, episode_reward=1448.54 +/- 233.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=843000, episode_reward=1440.86 +/- 230.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=844000, episode_reward=1464.70 +/- 189.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=845000, episode_reward=1726.70 +/- 100.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=846000, episode_reward=1541.70 +/- 152.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=847000, episode_reward=1401.29 +/- 299.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=848000, episode_reward=1503.00 +/- 194.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=849000, episode_reward=1471.43 +/- 260.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=850000, episode_reward=1558.50 +/- 75.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=851000, episode_reward=1361.83 +/- 281.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=852000, episode_reward=1554.11 +/- 173.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=853000, episode_reward=1443.49 +/- 193.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=854000, episode_reward=1618.43 +/- 107.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=855000, episode_reward=1337.53 +/- 279.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=856000, episode_reward=1506.28 +/- 36.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=857000, episode_reward=1467.71 +/- 114.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=858000, episode_reward=1439.70 +/- 257.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=859000, episode_reward=1500.74 +/- 102.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=860000, episode_reward=1552.32 +/- 44.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=861000, episode_reward=1570.71 +/- 41.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=862000, episode_reward=1440.86 +/- 35.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=863000, episode_reward=1580.22 +/- 139.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=864000, episode_reward=1652.29 +/- 59.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=865000, episode_reward=1358.50 +/- 218.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=866000, episode_reward=1579.69 +/- 132.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=867000, episode_reward=1547.32 +/- 148.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=868000, episode_reward=1367.10 +/- 192.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=869000, episode_reward=1399.32 +/- 275.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=870000, episode_reward=1549.30 +/- 50.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=871000, episode_reward=1627.97 +/- 133.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=872000, episode_reward=1393.34 +/- 148.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=873000, episode_reward=1458.72 +/- 216.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=874000, episode_reward=1583.07 +/- 119.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=875000, episode_reward=1562.73 +/- 146.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=876000, episode_reward=1423.09 +/- 130.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=877000, episode_reward=1454.92 +/- 312.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=878000, episode_reward=1544.94 +/- 48.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=879000, episode_reward=1594.91 +/- 127.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=880000, episode_reward=1330.21 +/- 175.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=881000, episode_reward=1582.81 +/- 80.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=882000, episode_reward=1469.16 +/- 99.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=883000, episode_reward=1470.62 +/- 311.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=884000, episode_reward=1719.36 +/- 116.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=885000, episode_reward=1522.72 +/- 128.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=886000, episode_reward=1603.27 +/- 97.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=887000, episode_reward=1457.68 +/- 70.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=888000, episode_reward=1505.89 +/- 55.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=889000, episode_reward=1517.01 +/- 157.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=890000, episode_reward=1633.52 +/- 54.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=891000, episode_reward=1518.17 +/- 106.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=892000, episode_reward=1449.66 +/- 107.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=893000, episode_reward=1466.37 +/- 131.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=894000, episode_reward=1486.13 +/- 124.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=895000, episode_reward=1558.02 +/- 101.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=896000, episode_reward=1492.32 +/- 146.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=897000, episode_reward=1551.45 +/- 64.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=898000, episode_reward=1391.17 +/- 293.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=899000, episode_reward=1506.13 +/- 42.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=900000, episode_reward=1451.82 +/- 119.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=901000, episode_reward=1174.26 +/- 369.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=902000, episode_reward=1536.67 +/- 113.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=903000, episode_reward=1593.64 +/- 215.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=904000, episode_reward=1627.51 +/- 91.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=905000, episode_reward=1380.40 +/- 201.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=906000, episode_reward=1446.01 +/- 288.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=907000, episode_reward=1582.02 +/- 74.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=908000, episode_reward=1399.38 +/- 307.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=909000, episode_reward=1592.95 +/- 149.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=910000, episode_reward=1386.15 +/- 175.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=911000, episode_reward=1561.11 +/- 163.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=912000, episode_reward=1458.90 +/- 253.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=913000, episode_reward=1515.78 +/- 269.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=914000, episode_reward=1582.76 +/- 100.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=915000, episode_reward=1573.17 +/- 341.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=916000, episode_reward=1557.55 +/- 97.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=917000, episode_reward=1351.87 +/- 258.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=918000, episode_reward=1438.90 +/- 150.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=919000, episode_reward=1502.53 +/- 136.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=920000, episode_reward=1399.04 +/- 214.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=921000, episode_reward=1542.48 +/- 87.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=922000, episode_reward=1578.87 +/- 106.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=923000, episode_reward=1601.78 +/- 138.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=924000, episode_reward=1488.49 +/- 176.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=925000, episode_reward=1409.40 +/- 267.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=926000, episode_reward=1580.37 +/- 60.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=927000, episode_reward=1613.79 +/- 71.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=928000, episode_reward=1510.99 +/- 111.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=929000, episode_reward=1409.31 +/- 228.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=930000, episode_reward=1548.74 +/- 155.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=931000, episode_reward=1543.92 +/- 122.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=932000, episode_reward=1486.48 +/- 185.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=933000, episode_reward=1600.81 +/- 121.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=934000, episode_reward=1545.51 +/- 118.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=935000, episode_reward=1477.89 +/- 86.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=936000, episode_reward=1646.85 +/- 163.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=937000, episode_reward=1577.05 +/- 120.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=938000, episode_reward=1503.36 +/- 262.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=939000, episode_reward=1414.48 +/- 106.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=940000, episode_reward=1537.07 +/- 151.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=941000, episode_reward=1423.33 +/- 176.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=942000, episode_reward=1523.66 +/- 126.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=943000, episode_reward=1432.76 +/- 277.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=944000, episode_reward=1564.46 +/- 120.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=945000, episode_reward=1529.15 +/- 138.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=946000, episode_reward=1593.60 +/- 99.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=947000, episode_reward=1512.61 +/- 149.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=948000, episode_reward=1271.63 +/- 320.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=949000, episode_reward=1402.34 +/- 112.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=950000, episode_reward=1482.02 +/- 106.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=951000, episode_reward=1488.91 +/- 203.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=952000, episode_reward=1435.89 +/- 326.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=953000, episode_reward=1516.18 +/- 94.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=954000, episode_reward=1635.56 +/- 160.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=955000, episode_reward=1339.65 +/- 256.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=956000, episode_reward=1483.30 +/- 199.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=957000, episode_reward=1328.61 +/- 312.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=958000, episode_reward=1564.11 +/- 94.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=959000, episode_reward=1312.33 +/- 300.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=960000, episode_reward=1546.46 +/- 43.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=961000, episode_reward=1476.98 +/- 177.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=962000, episode_reward=1493.25 +/- 301.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=963000, episode_reward=1507.04 +/- 240.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=964000, episode_reward=1640.24 +/- 76.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=965000, episode_reward=1551.02 +/- 96.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=966000, episode_reward=1315.31 +/- 251.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=967000, episode_reward=1530.98 +/- 208.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=968000, episode_reward=1503.98 +/- 112.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=969000, episode_reward=1554.80 +/- 103.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=970000, episode_reward=1530.42 +/- 80.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=971000, episode_reward=1519.25 +/- 185.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=972000, episode_reward=1428.05 +/- 35.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=973000, episode_reward=1500.47 +/- 144.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=974000, episode_reward=1426.30 +/- 283.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=975000, episode_reward=1550.42 +/- 162.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=976000, episode_reward=1544.25 +/- 69.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=977000, episode_reward=1291.90 +/- 307.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=978000, episode_reward=1454.09 +/- 162.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=979000, episode_reward=1610.45 +/- 106.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=980000, episode_reward=1376.21 +/- 316.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=981000, episode_reward=1552.84 +/- 119.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=982000, episode_reward=1514.71 +/- 118.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=983000, episode_reward=1584.81 +/- 125.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=984000, episode_reward=1556.44 +/- 174.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=985000, episode_reward=1399.96 +/- 255.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=986000, episode_reward=1434.24 +/- 143.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=987000, episode_reward=1494.71 +/- 248.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=988000, episode_reward=1531.07 +/- 96.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=989000, episode_reward=1593.44 +/- 114.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=990000, episode_reward=1401.46 +/- 53.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=991000, episode_reward=1466.02 +/- 136.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=992000, episode_reward=1565.39 +/- 120.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=993000, episode_reward=1591.93 +/- 95.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=994000, episode_reward=1457.85 +/- 87.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=995000, episode_reward=1555.44 +/- 119.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=996000, episode_reward=1422.09 +/- 337.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=997000, episode_reward=1549.61 +/- 53.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=998000, episode_reward=1480.99 +/- 83.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=999000, episode_reward=1617.08 +/- 118.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1000000, episode_reward=1574.73 +/- 112.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1001000, episode_reward=1297.71 +/- 337.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1002000, episode_reward=1452.43 +/- 81.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1003000, episode_reward=1521.09 +/- 93.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1004000, episode_reward=1563.03 +/- 128.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1005000, episode_reward=1502.40 +/- 242.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1006000, episode_reward=1614.14 +/- 155.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1007000, episode_reward=1669.99 +/- 143.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1008000, episode_reward=1553.31 +/- 95.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1009000, episode_reward=1416.58 +/- 179.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1010000, episode_reward=1431.03 +/- 278.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1011000, episode_reward=1472.35 +/- 125.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1012000, episode_reward=1519.53 +/- 88.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1013000, episode_reward=1561.21 +/- 61.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1014000, episode_reward=1452.33 +/- 196.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1015000, episode_reward=1214.22 +/- 250.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1016000, episode_reward=1366.79 +/- 232.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1017000, episode_reward=1442.23 +/- 149.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1018000, episode_reward=1552.64 +/- 109.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1019000, episode_reward=1412.65 +/- 223.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1020000, episode_reward=1631.40 +/- 132.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1021000, episode_reward=1404.67 +/- 271.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1022000, episode_reward=1240.48 +/- 284.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1023000, episode_reward=1553.35 +/- 57.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1024000, episode_reward=1522.48 +/- 57.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1025000, episode_reward=1489.76 +/- 298.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1026000, episode_reward=1388.02 +/- 165.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1027000, episode_reward=1584.19 +/- 158.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1028000, episode_reward=1572.31 +/- 120.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1029000, episode_reward=1508.94 +/- 337.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1030000, episode_reward=1492.09 +/- 165.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1031000, episode_reward=1551.63 +/- 101.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1032000, episode_reward=1373.92 +/- 463.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1033000, episode_reward=1576.15 +/- 122.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1034000, episode_reward=1456.10 +/- 147.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1035000, episode_reward=1497.01 +/- 154.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1036000, episode_reward=1458.99 +/- 185.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1037000, episode_reward=1646.85 +/- 96.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1038000, episode_reward=1529.47 +/- 52.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1039000, episode_reward=1536.38 +/- 190.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1040000, episode_reward=1496.61 +/- 73.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1041000, episode_reward=1361.49 +/- 144.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1042000, episode_reward=1567.35 +/- 160.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1043000, episode_reward=1310.51 +/- 294.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1044000, episode_reward=1603.34 +/- 144.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1045000, episode_reward=1501.20 +/- 75.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1046000, episode_reward=1558.30 +/- 190.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1047000, episode_reward=1519.02 +/- 143.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1048000, episode_reward=1470.84 +/- 73.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1049000, episode_reward=1441.60 +/- 148.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1050000, episode_reward=1631.05 +/- 143.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1051000, episode_reward=1338.35 +/- 320.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1052000, episode_reward=1535.58 +/- 149.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1053000, episode_reward=1497.65 +/- 89.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1054000, episode_reward=1392.43 +/- 176.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1055000, episode_reward=1554.09 +/- 85.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1056000, episode_reward=1510.69 +/- 163.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1057000, episode_reward=1625.85 +/- 138.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1058000, episode_reward=1498.93 +/- 99.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1059000, episode_reward=1504.75 +/- 105.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1060000, episode_reward=1572.32 +/- 138.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1061000, episode_reward=1449.35 +/- 144.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1062000, episode_reward=1468.08 +/- 17.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1063000, episode_reward=1370.92 +/- 279.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1064000, episode_reward=1510.44 +/- 128.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1065000, episode_reward=1597.19 +/- 118.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1066000, episode_reward=1553.50 +/- 134.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1067000, episode_reward=1493.87 +/- 218.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1068000, episode_reward=1493.21 +/- 158.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1069000, episode_reward=1484.88 +/- 46.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1070000, episode_reward=1645.10 +/- 60.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1071000, episode_reward=1389.36 +/- 227.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1072000, episode_reward=1616.64 +/- 158.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1073000, episode_reward=1570.56 +/- 171.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1074000, episode_reward=1353.50 +/- 270.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1075000, episode_reward=1502.63 +/- 276.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1076000, episode_reward=1569.43 +/- 101.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1077000, episode_reward=1518.59 +/- 44.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1078000, episode_reward=1332.33 +/- 261.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1079000, episode_reward=1447.05 +/- 154.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1080000, episode_reward=1428.73 +/- 145.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1081000, episode_reward=1539.53 +/- 82.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1082000, episode_reward=1658.88 +/- 123.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1083000, episode_reward=1542.46 +/- 90.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1084000, episode_reward=1494.92 +/- 175.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1085000, episode_reward=1538.83 +/- 186.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1086000, episode_reward=1458.94 +/- 122.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1087000, episode_reward=1454.43 +/- 292.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1088000, episode_reward=1567.69 +/- 116.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1089000, episode_reward=1584.74 +/- 145.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1090000, episode_reward=1561.24 +/- 146.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1091000, episode_reward=1556.83 +/- 109.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1092000, episode_reward=1513.92 +/- 89.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1093000, episode_reward=1576.41 +/- 103.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1094000, episode_reward=1539.61 +/- 141.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1095000, episode_reward=1628.74 +/- 223.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1096000, episode_reward=1638.39 +/- 283.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1097000, episode_reward=1345.22 +/- 265.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1098000, episode_reward=1447.10 +/- 116.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1099000, episode_reward=1529.58 +/- 116.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1100000, episode_reward=1593.80 +/- 136.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1101000, episode_reward=1538.94 +/- 178.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1102000, episode_reward=1569.07 +/- 178.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1103000, episode_reward=1518.57 +/- 44.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1104000, episode_reward=1534.11 +/- 47.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1105000, episode_reward=1516.92 +/- 126.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1106000, episode_reward=1582.12 +/- 155.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1107000, episode_reward=1430.61 +/- 181.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1108000, episode_reward=1569.12 +/- 307.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1109000, episode_reward=1445.60 +/- 223.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1110000, episode_reward=1586.28 +/- 108.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1111000, episode_reward=1488.51 +/- 122.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1112000, episode_reward=1561.86 +/- 125.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1113000, episode_reward=1342.17 +/- 365.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1114000, episode_reward=1452.74 +/- 64.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1115000, episode_reward=1465.28 +/- 129.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1116000, episode_reward=1556.27 +/- 33.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1117000, episode_reward=1488.61 +/- 87.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1118000, episode_reward=1535.77 +/- 89.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1119000, episode_reward=1387.51 +/- 248.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1120000, episode_reward=1637.13 +/- 95.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1121000, episode_reward=1619.87 +/- 117.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1122000, episode_reward=1495.18 +/- 281.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1123000, episode_reward=1420.32 +/- 104.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1124000, episode_reward=1569.81 +/- 138.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1125000, episode_reward=1585.28 +/- 90.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1126000, episode_reward=1573.90 +/- 150.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1127000, episode_reward=1491.20 +/- 70.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1128000, episode_reward=1486.71 +/- 71.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1129000, episode_reward=1618.89 +/- 112.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1130000, episode_reward=1596.31 +/- 151.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1131000, episode_reward=1481.06 +/- 133.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1132000, episode_reward=1438.85 +/- 374.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1133000, episode_reward=1554.06 +/- 103.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1134000, episode_reward=1450.61 +/- 23.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1135000, episode_reward=1529.88 +/- 81.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1136000, episode_reward=1562.01 +/- 180.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1137000, episode_reward=1496.43 +/- 65.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1138000, episode_reward=1531.62 +/- 79.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1139000, episode_reward=1480.41 +/- 127.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1140000, episode_reward=1563.54 +/- 109.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1141000, episode_reward=1592.05 +/- 167.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1142000, episode_reward=1578.46 +/- 142.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1143000, episode_reward=1522.82 +/- 106.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1144000, episode_reward=1540.83 +/- 115.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1145000, episode_reward=1567.13 +/- 131.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1146000, episode_reward=1614.83 +/- 119.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1147000, episode_reward=1468.43 +/- 120.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1148000, episode_reward=1588.08 +/- 61.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1149000, episode_reward=1584.85 +/- 198.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1150000, episode_reward=1576.64 +/- 136.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1151000, episode_reward=1488.78 +/- 91.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1152000, episode_reward=1420.50 +/- 337.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1153000, episode_reward=1664.71 +/- 139.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1154000, episode_reward=1392.94 +/- 136.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1155000, episode_reward=1598.42 +/- 102.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1156000, episode_reward=1508.47 +/- 127.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1157000, episode_reward=1424.00 +/- 90.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1158000, episode_reward=1628.47 +/- 148.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1159000, episode_reward=1535.31 +/- 135.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1160000, episode_reward=1381.21 +/- 300.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1161000, episode_reward=1571.16 +/- 155.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1162000, episode_reward=1491.87 +/- 164.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1163000, episode_reward=1667.01 +/- 195.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1164000, episode_reward=1633.08 +/- 96.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1165000, episode_reward=1572.10 +/- 174.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1166000, episode_reward=1495.81 +/- 186.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1167000, episode_reward=1616.97 +/- 107.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1168000, episode_reward=1623.58 +/- 107.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1169000, episode_reward=1414.32 +/- 232.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1170000, episode_reward=1611.43 +/- 80.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1171000, episode_reward=1610.60 +/- 105.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1172000, episode_reward=1465.38 +/- 122.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1173000, episode_reward=1504.72 +/- 161.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1174000, episode_reward=1424.95 +/- 145.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1175000, episode_reward=1525.53 +/- 144.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1176000, episode_reward=1428.40 +/- 131.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1177000, episode_reward=1560.18 +/- 92.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1178000, episode_reward=1525.05 +/- 90.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1179000, episode_reward=1499.19 +/- 93.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1180000, episode_reward=1573.07 +/- 89.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1181000, episode_reward=1533.02 +/- 230.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1182000, episode_reward=1491.67 +/- 94.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1183000, episode_reward=1489.76 +/- 112.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1184000, episode_reward=1456.67 +/- 101.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1185000, episode_reward=1572.68 +/- 44.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1186000, episode_reward=1540.57 +/- 127.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1187000, episode_reward=1581.92 +/- 80.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1188000, episode_reward=1495.55 +/- 116.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1189000, episode_reward=1498.49 +/- 99.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1190000, episode_reward=1581.42 +/- 91.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1191000, episode_reward=1545.47 +/- 299.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1192000, episode_reward=1564.87 +/- 171.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1193000, episode_reward=1565.14 +/- 115.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1194000, episode_reward=1534.07 +/- 101.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1195000, episode_reward=1429.46 +/- 214.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1196000, episode_reward=1556.59 +/- 162.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1197000, episode_reward=1493.06 +/- 92.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1198000, episode_reward=1355.60 +/- 335.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1199000, episode_reward=1551.99 +/- 234.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1200000, episode_reward=1622.48 +/- 89.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1201000, episode_reward=1347.67 +/- 278.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1202000, episode_reward=1594.10 +/- 53.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1203000, episode_reward=1681.08 +/- 128.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1204000, episode_reward=1558.21 +/- 149.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1205000, episode_reward=1478.74 +/- 169.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1206000, episode_reward=1569.82 +/- 104.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1207000, episode_reward=1538.40 +/- 133.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1208000, episode_reward=1598.21 +/- 82.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1209000, episode_reward=1554.02 +/- 67.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1210000, episode_reward=1468.36 +/- 191.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1211000, episode_reward=1566.99 +/- 60.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1212000, episode_reward=1601.13 +/- 122.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1213000, episode_reward=1554.38 +/- 142.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1214000, episode_reward=1509.67 +/- 50.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1215000, episode_reward=1461.45 +/- 87.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1216000, episode_reward=1532.90 +/- 269.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1217000, episode_reward=1505.74 +/- 103.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1218000, episode_reward=1510.21 +/- 110.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1219000, episode_reward=1430.69 +/- 159.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1220000, episode_reward=1587.49 +/- 117.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1221000, episode_reward=1515.75 +/- 141.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1222000, episode_reward=1639.14 +/- 77.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1223000, episode_reward=1592.65 +/- 79.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1224000, episode_reward=1569.22 +/- 95.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1225000, episode_reward=1513.23 +/- 133.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1226000, episode_reward=1566.43 +/- 36.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1227000, episode_reward=1458.49 +/- 84.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1228000, episode_reward=1578.77 +/- 71.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1229000, episode_reward=1526.01 +/- 149.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1230000, episode_reward=1574.49 +/- 122.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1231000, episode_reward=1543.30 +/- 71.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1232000, episode_reward=1529.92 +/- 216.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1233000, episode_reward=1366.94 +/- 155.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1234000, episode_reward=1429.47 +/- 209.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1235000, episode_reward=1551.02 +/- 177.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1236000, episode_reward=1459.32 +/- 31.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1237000, episode_reward=1512.66 +/- 70.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1238000, episode_reward=1581.71 +/- 116.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1239000, episode_reward=1496.33 +/- 101.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1240000, episode_reward=1544.12 +/- 107.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1241000, episode_reward=1598.27 +/- 101.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1242000, episode_reward=1509.53 +/- 148.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1243000, episode_reward=1438.14 +/- 81.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1244000, episode_reward=1466.71 +/- 197.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1245000, episode_reward=1616.90 +/- 100.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1246000, episode_reward=1509.72 +/- 153.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1247000, episode_reward=1521.40 +/- 143.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1248000, episode_reward=1474.23 +/- 127.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1249000, episode_reward=1560.19 +/- 139.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1250000, episode_reward=1538.26 +/- 89.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1251000, episode_reward=1501.80 +/- 89.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1252000, episode_reward=1529.84 +/- 155.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1253000, episode_reward=1533.96 +/- 137.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1254000, episode_reward=1533.31 +/- 86.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1255000, episode_reward=1468.83 +/- 69.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1256000, episode_reward=1408.97 +/- 224.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1257000, episode_reward=1521.02 +/- 72.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1258000, episode_reward=1520.80 +/- 141.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1259000, episode_reward=1449.08 +/- 105.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1260000, episode_reward=1560.81 +/- 126.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1261000, episode_reward=1578.45 +/- 82.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1262000, episode_reward=1332.67 +/- 356.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1263000, episode_reward=1533.86 +/- 177.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1264000, episode_reward=1525.37 +/- 97.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1265000, episode_reward=1539.70 +/- 67.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1266000, episode_reward=1462.65 +/- 104.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1267000, episode_reward=1505.86 +/- 92.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1268000, episode_reward=1572.10 +/- 89.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1269000, episode_reward=1358.04 +/- 298.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1270000, episode_reward=1473.72 +/- 162.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1271000, episode_reward=1571.46 +/- 129.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1272000, episode_reward=1627.51 +/- 82.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1273000, episode_reward=1564.42 +/- 192.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1274000, episode_reward=1562.99 +/- 137.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1275000, episode_reward=1462.84 +/- 142.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1276000, episode_reward=1425.81 +/- 111.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1277000, episode_reward=1466.96 +/- 131.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1278000, episode_reward=1547.49 +/- 84.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1279000, episode_reward=1434.13 +/- 124.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1280000, episode_reward=1312.42 +/- 336.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1281000, episode_reward=1542.78 +/- 82.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1282000, episode_reward=1480.30 +/- 106.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1283000, episode_reward=1490.09 +/- 191.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1284000, episode_reward=1569.76 +/- 130.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1285000, episode_reward=1491.24 +/- 134.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1286000, episode_reward=1466.82 +/- 83.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1287000, episode_reward=1526.69 +/- 83.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1288000, episode_reward=1478.37 +/- 149.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1289000, episode_reward=1493.54 +/- 118.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1290000, episode_reward=1477.67 +/- 99.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1291000, episode_reward=1443.59 +/- 259.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1292000, episode_reward=1432.13 +/- 255.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1293000, episode_reward=1386.47 +/- 118.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1294000, episode_reward=1508.08 +/- 101.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1295000, episode_reward=1564.54 +/- 133.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1296000, episode_reward=1515.06 +/- 60.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1297000, episode_reward=1397.11 +/- 168.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1298000, episode_reward=1493.95 +/- 88.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1299000, episode_reward=1534.57 +/- 127.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1300000, episode_reward=1507.87 +/- 127.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1301000, episode_reward=1525.68 +/- 63.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1302000, episode_reward=1473.86 +/- 110.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1303000, episode_reward=1567.07 +/- 130.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1304000, episode_reward=1480.53 +/- 135.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1305000, episode_reward=1542.77 +/- 137.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1306000, episode_reward=1511.04 +/- 103.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1307000, episode_reward=1578.15 +/- 114.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1308000, episode_reward=1569.18 +/- 149.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1309000, episode_reward=1582.07 +/- 39.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1310000, episode_reward=1474.91 +/- 94.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1311000, episode_reward=1597.24 +/- 118.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1312000, episode_reward=1523.03 +/- 152.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1313000, episode_reward=1544.60 +/- 65.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1314000, episode_reward=1476.90 +/- 197.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1315000, episode_reward=1547.83 +/- 57.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1316000, episode_reward=1430.72 +/- 35.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1317000, episode_reward=1456.70 +/- 98.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1318000, episode_reward=1487.52 +/- 121.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1319000, episode_reward=1587.95 +/- 37.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1320000, episode_reward=1571.61 +/- 68.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1321000, episode_reward=1404.29 +/- 323.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1322000, episode_reward=1633.57 +/- 62.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1323000, episode_reward=1431.92 +/- 215.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1324000, episode_reward=1447.43 +/- 182.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1325000, episode_reward=1517.08 +/- 87.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1326000, episode_reward=1531.41 +/- 124.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1327000, episode_reward=1523.71 +/- 87.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1328000, episode_reward=1560.07 +/- 55.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1329000, episode_reward=1580.07 +/- 55.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1330000, episode_reward=1453.14 +/- 137.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1331000, episode_reward=1635.32 +/- 126.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1332000, episode_reward=1584.50 +/- 116.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1333000, episode_reward=1484.71 +/- 155.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1334000, episode_reward=1605.87 +/- 95.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1335000, episode_reward=1498.31 +/- 24.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1336000, episode_reward=1406.51 +/- 205.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1337000, episode_reward=1561.25 +/- 91.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1338000, episode_reward=1509.32 +/- 98.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1339000, episode_reward=1513.60 +/- 75.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1340000, episode_reward=1491.57 +/- 118.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1341000, episode_reward=1595.72 +/- 138.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1342000, episode_reward=1504.02 +/- 72.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1343000, episode_reward=1501.16 +/- 138.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1344000, episode_reward=1604.33 +/- 84.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1345000, episode_reward=1418.85 +/- 138.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1346000, episode_reward=1390.00 +/- 53.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1347000, episode_reward=1546.46 +/- 61.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1348000, episode_reward=1569.25 +/- 114.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1349000, episode_reward=1406.73 +/- 276.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1350000, episode_reward=1571.90 +/- 44.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1351000, episode_reward=1514.60 +/- 90.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1352000, episode_reward=1471.62 +/- 66.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1353000, episode_reward=1609.54 +/- 117.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1354000, episode_reward=1495.72 +/- 84.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1355000, episode_reward=1469.41 +/- 164.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1356000, episode_reward=1460.02 +/- 121.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1357000, episode_reward=1487.00 +/- 186.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1358000, episode_reward=1624.08 +/- 144.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1359000, episode_reward=1484.09 +/- 133.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1360000, episode_reward=1551.01 +/- 106.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1361000, episode_reward=1572.46 +/- 219.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1362000, episode_reward=1571.54 +/- 67.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1363000, episode_reward=1485.99 +/- 134.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1364000, episode_reward=1182.90 +/- 467.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1365000, episode_reward=1514.28 +/- 128.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1366000, episode_reward=1408.21 +/- 82.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1367000, episode_reward=1385.92 +/- 189.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1368000, episode_reward=1453.95 +/- 123.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1369000, episode_reward=1559.62 +/- 102.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1370000, episode_reward=1439.76 +/- 103.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1371000, episode_reward=1590.66 +/- 128.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1372000, episode_reward=1656.45 +/- 91.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1373000, episode_reward=1464.35 +/- 152.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1374000, episode_reward=1571.86 +/- 209.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1375000, episode_reward=1328.98 +/- 314.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1376000, episode_reward=1291.54 +/- 243.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1377000, episode_reward=1551.73 +/- 236.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1378000, episode_reward=1509.38 +/- 62.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1379000, episode_reward=1512.02 +/- 259.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1380000, episode_reward=1587.71 +/- 155.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1381000, episode_reward=1538.20 +/- 82.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1382000, episode_reward=1605.00 +/- 143.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1383000, episode_reward=1590.55 +/- 73.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1384000, episode_reward=1622.95 +/- 147.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1385000, episode_reward=1507.65 +/- 100.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1386000, episode_reward=1602.66 +/- 103.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1387000, episode_reward=1491.78 +/- 108.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1388000, episode_reward=1453.01 +/- 241.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1389000, episode_reward=1395.89 +/- 304.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1390000, episode_reward=1388.31 +/- 400.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1391000, episode_reward=1517.93 +/- 62.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1392000, episode_reward=1544.52 +/- 113.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1393000, episode_reward=1623.86 +/- 104.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1394000, episode_reward=1451.66 +/- 297.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1395000, episode_reward=1554.80 +/- 179.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1396000, episode_reward=1622.98 +/- 131.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1397000, episode_reward=1541.41 +/- 113.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1398000, episode_reward=1557.63 +/- 160.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1399000, episode_reward=1553.11 +/- 216.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1400000, episode_reward=1552.15 +/- 87.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1401000, episode_reward=1661.23 +/- 93.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1402000, episode_reward=1580.46 +/- 92.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1403000, episode_reward=1445.97 +/- 166.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1404000, episode_reward=1577.62 +/- 195.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1405000, episode_reward=1525.80 +/- 147.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1406000, episode_reward=1423.10 +/- 223.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1407000, episode_reward=1362.08 +/- 335.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1408000, episode_reward=1546.88 +/- 34.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1409000, episode_reward=1575.70 +/- 124.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1410000, episode_reward=1501.78 +/- 97.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1411000, episode_reward=1669.87 +/- 140.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1412000, episode_reward=1477.93 +/- 200.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1413000, episode_reward=1599.41 +/- 155.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1414000, episode_reward=1413.16 +/- 174.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1415000, episode_reward=1564.69 +/- 125.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1416000, episode_reward=1607.62 +/- 64.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1417000, episode_reward=1489.49 +/- 126.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1418000, episode_reward=1559.90 +/- 172.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1419000, episode_reward=1519.51 +/- 105.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1420000, episode_reward=1486.74 +/- 128.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1421000, episode_reward=1574.74 +/- 106.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1422000, episode_reward=1514.37 +/- 104.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1423000, episode_reward=1569.00 +/- 175.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1424000, episode_reward=1622.28 +/- 175.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1425000, episode_reward=1386.75 +/- 186.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1426000, episode_reward=1512.75 +/- 117.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1427000, episode_reward=1384.91 +/- 320.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1428000, episode_reward=1502.39 +/- 113.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1429000, episode_reward=1424.19 +/- 139.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1430000, episode_reward=1624.89 +/- 75.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1431000, episode_reward=1486.96 +/- 218.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1432000, episode_reward=1616.12 +/- 97.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1433000, episode_reward=1426.60 +/- 202.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1434000, episode_reward=1292.43 +/- 301.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1435000, episode_reward=1455.56 +/- 212.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1436000, episode_reward=1536.45 +/- 107.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1437000, episode_reward=1616.36 +/- 52.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1438000, episode_reward=1300.81 +/- 259.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1439000, episode_reward=1498.32 +/- 79.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1440000, episode_reward=1432.75 +/- 243.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1441000, episode_reward=1391.30 +/- 282.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1442000, episode_reward=1451.77 +/- 43.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1443000, episode_reward=1573.63 +/- 105.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1444000, episode_reward=1447.62 +/- 217.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1445000, episode_reward=1358.98 +/- 326.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1446000, episode_reward=1343.45 +/- 194.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1447000, episode_reward=1573.93 +/- 158.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1448000, episode_reward=1485.48 +/- 200.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1449000, episode_reward=1522.34 +/- 215.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1450000, episode_reward=1388.80 +/- 271.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1451000, episode_reward=1493.13 +/- 298.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1452000, episode_reward=1459.79 +/- 114.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1453000, episode_reward=1573.81 +/- 146.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1454000, episode_reward=1388.29 +/- 281.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1455000, episode_reward=1338.46 +/- 364.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1456000, episode_reward=1470.74 +/- 170.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1457000, episode_reward=1572.23 +/- 237.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1458000, episode_reward=1570.11 +/- 157.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1459000, episode_reward=1480.72 +/- 103.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1460000, episode_reward=1601.72 +/- 134.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1461000, episode_reward=1471.89 +/- 131.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1462000, episode_reward=1421.86 +/- 313.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1463000, episode_reward=1523.54 +/- 120.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1464000, episode_reward=1527.36 +/- 48.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1465000, episode_reward=1277.43 +/- 243.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1466000, episode_reward=1612.52 +/- 134.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1467000, episode_reward=1531.32 +/- 94.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1468000, episode_reward=1528.52 +/- 223.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1469000, episode_reward=1651.20 +/- 89.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1470000, episode_reward=1485.77 +/- 153.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1471000, episode_reward=1489.98 +/- 102.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1472000, episode_reward=1489.73 +/- 204.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1473000, episode_reward=1502.51 +/- 115.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1474000, episode_reward=1536.92 +/- 65.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1475000, episode_reward=1582.50 +/- 163.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1476000, episode_reward=1473.70 +/- 102.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1477000, episode_reward=1502.04 +/- 137.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1478000, episode_reward=1388.38 +/- 274.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1479000, episode_reward=1534.22 +/- 240.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1480000, episode_reward=1411.01 +/- 160.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1481000, episode_reward=1513.26 +/- 127.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1482000, episode_reward=1571.56 +/- 111.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1483000, episode_reward=1311.37 +/- 316.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1484000, episode_reward=1439.24 +/- 204.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1485000, episode_reward=1508.67 +/- 61.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1486000, episode_reward=1562.22 +/- 43.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1487000, episode_reward=1506.58 +/- 153.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1488000, episode_reward=1558.96 +/- 138.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1489000, episode_reward=1393.27 +/- 219.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1490000, episode_reward=1580.36 +/- 51.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1491000, episode_reward=1464.91 +/- 203.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1492000, episode_reward=1451.01 +/- 217.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1493000, episode_reward=1733.54 +/- 123.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1494000, episode_reward=1404.17 +/- 165.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1495000, episode_reward=1592.78 +/- 140.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1496000, episode_reward=1512.58 +/- 140.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1497000, episode_reward=1600.10 +/- 99.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1498000, episode_reward=1422.07 +/- 239.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1499000, episode_reward=1505.67 +/- 328.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1500000, episode_reward=1445.38 +/- 188.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1501000, episode_reward=1617.10 +/- 109.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1502000, episode_reward=1511.80 +/- 25.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1503000, episode_reward=1561.54 +/- 169.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1504000, episode_reward=1537.53 +/- 82.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1505000, episode_reward=1521.12 +/- 65.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1506000, episode_reward=1540.01 +/- 153.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1507000, episode_reward=1506.64 +/- 107.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1508000, episode_reward=1479.75 +/- 74.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1509000, episode_reward=1543.44 +/- 115.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1510000, episode_reward=1490.58 +/- 64.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1511000, episode_reward=1530.81 +/- 100.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1512000, episode_reward=1554.86 +/- 57.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1513000, episode_reward=1440.22 +/- 385.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1514000, episode_reward=1584.15 +/- 53.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1515000, episode_reward=1527.60 +/- 103.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1516000, episode_reward=1530.50 +/- 199.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1517000, episode_reward=1400.83 +/- 190.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1518000, episode_reward=1455.03 +/- 225.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1519000, episode_reward=1535.54 +/- 47.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1520000, episode_reward=1562.30 +/- 127.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1521000, episode_reward=1487.82 +/- 83.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1522000, episode_reward=1588.52 +/- 101.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1523000, episode_reward=1504.35 +/- 48.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1524000, episode_reward=1632.21 +/- 90.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1525000, episode_reward=1623.16 +/- 139.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1526000, episode_reward=1575.87 +/- 170.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1527000, episode_reward=1567.44 +/- 103.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1528000, episode_reward=1489.01 +/- 59.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1529000, episode_reward=1513.97 +/- 89.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1530000, episode_reward=1532.27 +/- 66.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1531000, episode_reward=1585.92 +/- 112.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1532000, episode_reward=1594.96 +/- 118.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1533000, episode_reward=1599.92 +/- 117.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1534000, episode_reward=1623.83 +/- 141.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1535000, episode_reward=1465.69 +/- 99.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1536000, episode_reward=1596.93 +/- 71.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1537000, episode_reward=1626.03 +/- 117.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1538000, episode_reward=1627.40 +/- 91.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1539000, episode_reward=1629.91 +/- 79.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1540000, episode_reward=1582.89 +/- 63.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1541000, episode_reward=1635.04 +/- 126.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1542000, episode_reward=1397.39 +/- 263.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1543000, episode_reward=1665.63 +/- 36.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1544000, episode_reward=1644.57 +/- 117.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1545000, episode_reward=1448.61 +/- 116.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1546000, episode_reward=1576.80 +/- 122.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1547000, episode_reward=1603.13 +/- 141.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1548000, episode_reward=1625.45 +/- 53.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1549000, episode_reward=1557.46 +/- 110.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1550000, episode_reward=1529.73 +/- 70.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1551000, episode_reward=1609.86 +/- 113.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1552000, episode_reward=1542.69 +/- 102.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1553000, episode_reward=1480.67 +/- 306.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1554000, episode_reward=1415.22 +/- 185.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1555000, episode_reward=1567.08 +/- 112.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1556000, episode_reward=1432.85 +/- 162.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1557000, episode_reward=1653.54 +/- 102.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1558000, episode_reward=1604.59 +/- 180.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1559000, episode_reward=1510.41 +/- 129.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1560000, episode_reward=1634.34 +/- 113.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1561000, episode_reward=1419.73 +/- 228.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1562000, episode_reward=1574.56 +/- 79.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1563000, episode_reward=1560.00 +/- 74.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1564000, episode_reward=1626.47 +/- 105.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1565000, episode_reward=1612.12 +/- 183.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1566000, episode_reward=1588.03 +/- 160.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1567000, episode_reward=1485.88 +/- 100.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1568000, episode_reward=1550.28 +/- 173.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1569000, episode_reward=1489.86 +/- 181.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1570000, episode_reward=1647.14 +/- 92.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1571000, episode_reward=1540.14 +/- 49.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1572000, episode_reward=1486.67 +/- 218.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1573000, episode_reward=1442.68 +/- 106.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1574000, episode_reward=1460.52 +/- 227.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1575000, episode_reward=1469.23 +/- 156.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1576000, episode_reward=1498.26 +/- 88.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1577000, episode_reward=1649.69 +/- 92.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1578000, episode_reward=1475.30 +/- 106.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1579000, episode_reward=1491.36 +/- 146.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1580000, episode_reward=1584.51 +/- 85.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1581000, episode_reward=1504.13 +/- 112.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1582000, episode_reward=1518.35 +/- 74.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1583000, episode_reward=1490.50 +/- 84.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1584000, episode_reward=1592.11 +/- 95.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1585000, episode_reward=1517.50 +/- 157.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1586000, episode_reward=1572.46 +/- 39.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1587000, episode_reward=1509.16 +/- 33.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1588000, episode_reward=1532.84 +/- 59.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1589000, episode_reward=1567.54 +/- 161.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1590000, episode_reward=1533.85 +/- 119.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1591000, episode_reward=1516.31 +/- 52.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1592000, episode_reward=1501.84 +/- 104.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1593000, episode_reward=1651.16 +/- 89.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1594000, episode_reward=1436.50 +/- 93.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1595000, episode_reward=1490.54 +/- 78.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1596000, episode_reward=1465.70 +/- 118.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1597000, episode_reward=1476.03 +/- 67.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1598000, episode_reward=1566.69 +/- 36.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1599000, episode_reward=1508.25 +/- 114.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1600000, episode_reward=1610.36 +/- 66.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1601000, episode_reward=1491.23 +/- 132.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1602000, episode_reward=1425.34 +/- 118.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1603000, episode_reward=1584.33 +/- 173.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1604000, episode_reward=1523.59 +/- 101.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1605000, episode_reward=1601.85 +/- 134.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1606000, episode_reward=1534.91 +/- 120.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1607000, episode_reward=1460.60 +/- 73.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1608000, episode_reward=1493.72 +/- 71.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1609000, episode_reward=1538.85 +/- 66.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1610000, episode_reward=1523.47 +/- 72.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1611000, episode_reward=1396.87 +/- 74.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1612000, episode_reward=1510.46 +/- 111.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1613000, episode_reward=1568.70 +/- 89.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1614000, episode_reward=1472.28 +/- 98.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1615000, episode_reward=1512.81 +/- 72.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1616000, episode_reward=1581.60 +/- 120.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1617000, episode_reward=1580.80 +/- 146.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1618000, episode_reward=1481.76 +/- 40.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1619000, episode_reward=1564.34 +/- 86.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1620000, episode_reward=1466.70 +/- 221.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1621000, episode_reward=1527.43 +/- 84.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1622000, episode_reward=1406.70 +/- 319.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1623000, episode_reward=1512.68 +/- 142.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1624000, episode_reward=1629.53 +/- 96.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1625000, episode_reward=1452.45 +/- 226.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1626000, episode_reward=1433.07 +/- 247.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1627000, episode_reward=1491.59 +/- 149.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1628000, episode_reward=1356.79 +/- 280.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1629000, episode_reward=1519.03 +/- 112.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1630000, episode_reward=1465.18 +/- 136.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1631000, episode_reward=1578.88 +/- 125.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1632000, episode_reward=1434.02 +/- 234.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1633000, episode_reward=1385.33 +/- 292.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1634000, episode_reward=1570.96 +/- 70.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1635000, episode_reward=1508.05 +/- 153.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1636000, episode_reward=1509.05 +/- 125.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1637000, episode_reward=1572.69 +/- 101.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1638000, episode_reward=1503.64 +/- 175.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1639000, episode_reward=1535.35 +/- 73.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1640000, episode_reward=1564.36 +/- 95.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1641000, episode_reward=1618.20 +/- 143.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1642000, episode_reward=1494.11 +/- 112.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1643000, episode_reward=1589.89 +/- 104.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1644000, episode_reward=1581.78 +/- 106.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1645000, episode_reward=1510.52 +/- 302.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1646000, episode_reward=1568.24 +/- 110.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1647000, episode_reward=1508.43 +/- 96.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1648000, episode_reward=1569.46 +/- 96.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1649000, episode_reward=1528.89 +/- 227.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1650000, episode_reward=1565.51 +/- 118.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1651000, episode_reward=1440.23 +/- 210.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1652000, episode_reward=1465.86 +/- 167.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1653000, episode_reward=1312.11 +/- 279.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1654000, episode_reward=1470.57 +/- 310.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1655000, episode_reward=1600.39 +/- 166.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1656000, episode_reward=1545.99 +/- 146.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1657000, episode_reward=1505.03 +/- 84.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1658000, episode_reward=1473.26 +/- 266.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1659000, episode_reward=1527.25 +/- 112.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1660000, episode_reward=1595.55 +/- 95.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1661000, episode_reward=1547.87 +/- 88.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1662000, episode_reward=1457.15 +/- 157.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1663000, episode_reward=1447.71 +/- 267.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1664000, episode_reward=1542.81 +/- 91.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1665000, episode_reward=1596.33 +/- 62.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1666000, episode_reward=1645.45 +/- 178.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1667000, episode_reward=1566.91 +/- 117.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1668000, episode_reward=1634.53 +/- 63.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1669000, episode_reward=1605.00 +/- 121.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1670000, episode_reward=1522.20 +/- 160.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1671000, episode_reward=1551.03 +/- 189.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1672000, episode_reward=1598.91 +/- 84.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1673000, episode_reward=1477.00 +/- 78.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1674000, episode_reward=1492.67 +/- 108.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1675000, episode_reward=1508.63 +/- 95.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1676000, episode_reward=1452.38 +/- 61.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1677000, episode_reward=1536.37 +/- 106.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1678000, episode_reward=1485.63 +/- 109.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1679000, episode_reward=1541.74 +/- 96.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1680000, episode_reward=1445.66 +/- 169.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1681000, episode_reward=1559.18 +/- 59.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1682000, episode_reward=1536.38 +/- 98.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1683000, episode_reward=1476.84 +/- 47.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1684000, episode_reward=1565.84 +/- 148.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1685000, episode_reward=1546.57 +/- 144.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1686000, episode_reward=1343.77 +/- 310.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1687000, episode_reward=1524.82 +/- 150.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1688000, episode_reward=1506.68 +/- 149.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1689000, episode_reward=1584.67 +/- 60.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1690000, episode_reward=1535.99 +/- 123.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1691000, episode_reward=1445.85 +/- 246.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1692000, episode_reward=1620.60 +/- 153.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1693000, episode_reward=1550.73 +/- 110.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1694000, episode_reward=1441.01 +/- 270.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1695000, episode_reward=1548.65 +/- 261.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1696000, episode_reward=1433.23 +/- 202.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1697000, episode_reward=1455.68 +/- 118.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1698000, episode_reward=1597.80 +/- 161.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1699000, episode_reward=1691.36 +/- 106.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1700000, episode_reward=1568.30 +/- 97.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1701000, episode_reward=1452.47 +/- 279.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1702000, episode_reward=1487.80 +/- 94.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1703000, episode_reward=1522.45 +/- 190.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1704000, episode_reward=1399.86 +/- 366.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1705000, episode_reward=1511.39 +/- 131.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1706000, episode_reward=1566.26 +/- 76.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1707000, episode_reward=1543.66 +/- 129.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1708000, episode_reward=1375.04 +/- 202.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1709000, episode_reward=1631.00 +/- 151.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1710000, episode_reward=1415.08 +/- 171.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1711000, episode_reward=1650.98 +/- 85.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1712000, episode_reward=1454.92 +/- 265.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1713000, episode_reward=1506.52 +/- 139.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1714000, episode_reward=1555.27 +/- 152.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1715000, episode_reward=1545.69 +/- 78.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1716000, episode_reward=1470.06 +/- 154.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1717000, episode_reward=1561.14 +/- 78.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1718000, episode_reward=1520.12 +/- 98.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1719000, episode_reward=1477.57 +/- 128.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1720000, episode_reward=1490.43 +/- 130.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1721000, episode_reward=1544.90 +/- 140.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1722000, episode_reward=1567.48 +/- 140.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1723000, episode_reward=1437.06 +/- 142.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1724000, episode_reward=1437.18 +/- 168.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1725000, episode_reward=1540.34 +/- 123.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1726000, episode_reward=1557.49 +/- 216.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1727000, episode_reward=1550.40 +/- 92.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1728000, episode_reward=1474.80 +/- 326.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1729000, episode_reward=1497.73 +/- 108.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1730000, episode_reward=1512.03 +/- 96.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1731000, episode_reward=1525.90 +/- 77.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1732000, episode_reward=1488.25 +/- 103.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1733000, episode_reward=1461.19 +/- 78.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1734000, episode_reward=1523.31 +/- 180.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1735000, episode_reward=1514.46 +/- 194.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1736000, episode_reward=1510.12 +/- 109.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1737000, episode_reward=1505.58 +/- 49.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1738000, episode_reward=1499.88 +/- 94.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1739000, episode_reward=1547.40 +/- 70.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1740000, episode_reward=1414.19 +/- 120.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1741000, episode_reward=1501.31 +/- 81.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1742000, episode_reward=1589.57 +/- 65.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1743000, episode_reward=1595.71 +/- 124.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1744000, episode_reward=1436.54 +/- 242.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1745000, episode_reward=1398.79 +/- 260.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1746000, episode_reward=1352.10 +/- 309.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1747000, episode_reward=1662.50 +/- 96.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1748000, episode_reward=1459.16 +/- 185.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1749000, episode_reward=1482.75 +/- 80.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1750000, episode_reward=1564.60 +/- 191.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1751000, episode_reward=1530.30 +/- 78.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1752000, episode_reward=1599.13 +/- 15.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1753000, episode_reward=1540.79 +/- 60.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1754000, episode_reward=1537.61 +/- 52.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1755000, episode_reward=1437.45 +/- 299.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1756000, episode_reward=1473.84 +/- 87.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1757000, episode_reward=1554.61 +/- 183.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1758000, episode_reward=1612.05 +/- 117.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1759000, episode_reward=1552.95 +/- 165.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1760000, episode_reward=1499.65 +/- 208.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1761000, episode_reward=1532.51 +/- 128.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1762000, episode_reward=1511.51 +/- 142.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1763000, episode_reward=1426.11 +/- 100.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1764000, episode_reward=1483.57 +/- 64.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1765000, episode_reward=1487.18 +/- 127.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1766000, episode_reward=1508.82 +/- 59.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1767000, episode_reward=1576.40 +/- 81.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1768000, episode_reward=1477.97 +/- 78.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1769000, episode_reward=1627.71 +/- 130.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1770000, episode_reward=1503.75 +/- 134.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1771000, episode_reward=1506.20 +/- 83.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1772000, episode_reward=1566.28 +/- 96.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1773000, episode_reward=1451.16 +/- 157.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1774000, episode_reward=1500.79 +/- 48.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1775000, episode_reward=1561.91 +/- 107.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1776000, episode_reward=1618.84 +/- 87.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1777000, episode_reward=1442.95 +/- 241.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1778000, episode_reward=1506.11 +/- 83.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1779000, episode_reward=1542.77 +/- 109.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1780000, episode_reward=1437.43 +/- 221.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1781000, episode_reward=1552.90 +/- 91.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1782000, episode_reward=1611.06 +/- 108.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1783000, episode_reward=1523.58 +/- 56.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1784000, episode_reward=1586.89 +/- 173.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1785000, episode_reward=1578.47 +/- 163.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1786000, episode_reward=1505.26 +/- 105.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1787000, episode_reward=1410.22 +/- 186.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1788000, episode_reward=1554.75 +/- 42.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1789000, episode_reward=1566.94 +/- 109.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1790000, episode_reward=1515.51 +/- 155.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1791000, episode_reward=1601.93 +/- 42.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1792000, episode_reward=1517.50 +/- 50.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1793000, episode_reward=1526.86 +/- 133.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1794000, episode_reward=1465.47 +/- 184.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1795000, episode_reward=1593.80 +/- 138.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1796000, episode_reward=1639.02 +/- 88.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1797000, episode_reward=1569.33 +/- 24.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1798000, episode_reward=1559.03 +/- 133.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1799000, episode_reward=1551.28 +/- 111.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1800000, episode_reward=1582.49 +/- 163.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1801000, episode_reward=1532.43 +/- 138.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1802000, episode_reward=1558.37 +/- 178.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1803000, episode_reward=1521.80 +/- 92.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1804000, episode_reward=1561.85 +/- 70.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1805000, episode_reward=1568.32 +/- 189.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1806000, episode_reward=1583.27 +/- 110.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1807000, episode_reward=1490.35 +/- 337.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1808000, episode_reward=1472.70 +/- 180.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1809000, episode_reward=1553.17 +/- 307.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1810000, episode_reward=1477.94 +/- 101.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1811000, episode_reward=1580.46 +/- 134.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1812000, episode_reward=1680.48 +/- 135.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1813000, episode_reward=1534.67 +/- 191.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1814000, episode_reward=1583.51 +/- 191.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1815000, episode_reward=1443.96 +/- 319.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1816000, episode_reward=1522.56 +/- 132.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1817000, episode_reward=1504.04 +/- 181.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1818000, episode_reward=1541.22 +/- 209.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1819000, episode_reward=1634.79 +/- 87.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1820000, episode_reward=1667.54 +/- 17.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1821000, episode_reward=1444.53 +/- 271.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1822000, episode_reward=1599.84 +/- 77.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1823000, episode_reward=1498.35 +/- 249.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1824000, episode_reward=1333.68 +/- 214.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1825000, episode_reward=1578.82 +/- 129.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1826000, episode_reward=1417.94 +/- 119.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1827000, episode_reward=1459.64 +/- 66.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1828000, episode_reward=1560.22 +/- 73.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1829000, episode_reward=1571.88 +/- 32.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1830000, episode_reward=1495.68 +/- 296.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1831000, episode_reward=1494.91 +/- 169.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1832000, episode_reward=1531.50 +/- 95.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1833000, episode_reward=1620.69 +/- 62.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1834000, episode_reward=1580.44 +/- 84.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1835000, episode_reward=1491.12 +/- 164.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1836000, episode_reward=1537.79 +/- 204.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1837000, episode_reward=1609.94 +/- 62.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1838000, episode_reward=1576.10 +/- 102.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1839000, episode_reward=1542.42 +/- 68.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1840000, episode_reward=1480.55 +/- 86.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1841000, episode_reward=1565.15 +/- 110.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1842000, episode_reward=1655.40 +/- 113.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1843000, episode_reward=1595.20 +/- 110.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1844000, episode_reward=1520.40 +/- 94.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1845000, episode_reward=1507.41 +/- 196.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1846000, episode_reward=1589.40 +/- 110.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1847000, episode_reward=1558.48 +/- 196.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1848000, episode_reward=1546.86 +/- 144.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1849000, episode_reward=1478.41 +/- 249.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1850000, episode_reward=1604.77 +/- 146.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1851000, episode_reward=1413.40 +/- 201.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1852000, episode_reward=1490.63 +/- 85.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1853000, episode_reward=1497.63 +/- 382.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1854000, episode_reward=1572.52 +/- 152.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1855000, episode_reward=1422.52 +/- 157.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1856000, episode_reward=1435.27 +/- 239.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1857000, episode_reward=1616.19 +/- 110.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1858000, episode_reward=1617.71 +/- 115.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1859000, episode_reward=1454.05 +/- 209.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1860000, episode_reward=1338.18 +/- 238.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1861000, episode_reward=1612.97 +/- 136.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1862000, episode_reward=1502.98 +/- 93.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1863000, episode_reward=1664.91 +/- 148.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1864000, episode_reward=1456.84 +/- 250.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1865000, episode_reward=1286.92 +/- 118.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1866000, episode_reward=1491.55 +/- 151.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1867000, episode_reward=1589.80 +/- 155.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1868000, episode_reward=1566.89 +/- 147.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1869000, episode_reward=1531.69 +/- 66.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1870000, episode_reward=1573.84 +/- 77.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1871000, episode_reward=1430.62 +/- 167.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1872000, episode_reward=1486.83 +/- 148.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1873000, episode_reward=1601.48 +/- 118.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1874000, episode_reward=1530.15 +/- 73.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1875000, episode_reward=1546.85 +/- 73.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1876000, episode_reward=1543.76 +/- 100.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1877000, episode_reward=1609.67 +/- 158.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1878000, episode_reward=1382.37 +/- 294.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1879000, episode_reward=1492.56 +/- 140.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1880000, episode_reward=1524.21 +/- 127.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1881000, episode_reward=1517.43 +/- 159.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1882000, episode_reward=1292.59 +/- 279.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1883000, episode_reward=1212.45 +/- 285.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1884000, episode_reward=1526.55 +/- 95.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1885000, episode_reward=1549.27 +/- 55.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1886000, episode_reward=1551.36 +/- 119.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1887000, episode_reward=1606.00 +/- 138.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1888000, episode_reward=1568.12 +/- 83.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1889000, episode_reward=1483.74 +/- 87.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1890000, episode_reward=1565.09 +/- 146.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1891000, episode_reward=1532.15 +/- 105.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1892000, episode_reward=1370.32 +/- 518.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1893000, episode_reward=1461.64 +/- 241.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1894000, episode_reward=1558.21 +/- 126.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1895000, episode_reward=1271.29 +/- 247.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1896000, episode_reward=1570.36 +/- 69.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1897000, episode_reward=1479.09 +/- 77.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1898000, episode_reward=1391.74 +/- 163.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1899000, episode_reward=1653.20 +/- 99.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1900000, episode_reward=1594.58 +/- 107.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1901000, episode_reward=1646.77 +/- 41.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1902000, episode_reward=1284.33 +/- 226.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1903000, episode_reward=1646.10 +/- 122.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1904000, episode_reward=1683.12 +/- 112.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1905000, episode_reward=1510.31 +/- 218.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1906000, episode_reward=1509.97 +/- 277.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1907000, episode_reward=1499.60 +/- 88.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1908000, episode_reward=1664.83 +/- 110.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1909000, episode_reward=1563.06 +/- 143.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1910000, episode_reward=1613.84 +/- 136.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1911000, episode_reward=1592.42 +/- 157.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1912000, episode_reward=1544.97 +/- 236.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1913000, episode_reward=1476.57 +/- 264.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1914000, episode_reward=1323.09 +/- 190.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1915000, episode_reward=1517.48 +/- 118.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1916000, episode_reward=1570.60 +/- 211.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1917000, episode_reward=1599.64 +/- 83.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1918000, episode_reward=1458.85 +/- 127.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1919000, episode_reward=1666.96 +/- 109.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1920000, episode_reward=1598.19 +/- 83.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1921000, episode_reward=1443.16 +/- 132.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1922000, episode_reward=1645.94 +/- 130.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1923000, episode_reward=1551.99 +/- 49.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1924000, episode_reward=1606.65 +/- 30.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1925000, episode_reward=1440.00 +/- 112.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1926000, episode_reward=1549.05 +/- 146.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1927000, episode_reward=1617.12 +/- 75.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1928000, episode_reward=1529.80 +/- 32.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1929000, episode_reward=1463.98 +/- 124.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1930000, episode_reward=1487.78 +/- 177.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1931000, episode_reward=1575.55 +/- 48.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1932000, episode_reward=1627.60 +/- 67.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1933000, episode_reward=1539.55 +/- 95.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1934000, episode_reward=1557.13 +/- 123.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1935000, episode_reward=1499.08 +/- 173.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1936000, episode_reward=1535.13 +/- 136.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1937000, episode_reward=1551.05 +/- 98.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1938000, episode_reward=1505.40 +/- 137.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1939000, episode_reward=1598.04 +/- 114.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1940000, episode_reward=1585.99 +/- 77.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1941000, episode_reward=1401.78 +/- 130.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1942000, episode_reward=1504.93 +/- 195.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1943000, episode_reward=1628.88 +/- 101.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1944000, episode_reward=1542.02 +/- 200.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1945000, episode_reward=1504.38 +/- 73.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1946000, episode_reward=1416.85 +/- 153.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1947000, episode_reward=1525.01 +/- 50.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1948000, episode_reward=1513.22 +/- 105.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1949000, episode_reward=1526.84 +/- 139.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1950000, episode_reward=1527.57 +/- 142.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1951000, episode_reward=1615.41 +/- 106.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1952000, episode_reward=1496.58 +/- 71.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1953000, episode_reward=1573.55 +/- 51.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1954000, episode_reward=1586.91 +/- 65.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1955000, episode_reward=1562.00 +/- 95.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1956000, episode_reward=1553.54 +/- 206.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1957000, episode_reward=1523.76 +/- 107.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1958000, episode_reward=1563.47 +/- 142.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1959000, episode_reward=1570.16 +/- 76.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1960000, episode_reward=1316.41 +/- 326.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1961000, episode_reward=1573.81 +/- 150.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1962000, episode_reward=1441.88 +/- 223.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1963000, episode_reward=1492.59 +/- 102.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1964000, episode_reward=1592.49 +/- 77.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1965000, episode_reward=1441.58 +/- 126.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1966000, episode_reward=1513.93 +/- 139.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1967000, episode_reward=1444.91 +/- 197.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1968000, episode_reward=1578.26 +/- 226.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1969000, episode_reward=1449.90 +/- 246.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1970000, episode_reward=1538.61 +/- 47.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1971000, episode_reward=1602.67 +/- 88.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1972000, episode_reward=1578.42 +/- 59.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1973000, episode_reward=1671.85 +/- 127.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1974000, episode_reward=1610.09 +/- 99.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1975000, episode_reward=1578.28 +/- 116.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1976000, episode_reward=1533.43 +/- 90.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1977000, episode_reward=1550.02 +/- 90.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1978000, episode_reward=1553.03 +/- 128.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1979000, episode_reward=1548.45 +/- 148.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1980000, episode_reward=1523.91 +/- 99.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1981000, episode_reward=1648.37 +/- 169.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1982000, episode_reward=1447.22 +/- 203.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1983000, episode_reward=1552.97 +/- 89.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1984000, episode_reward=1470.38 +/- 93.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1985000, episode_reward=1612.38 +/- 59.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1986000, episode_reward=1557.89 +/- 144.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1987000, episode_reward=1508.46 +/- 109.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1988000, episode_reward=1512.03 +/- 75.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1989000, episode_reward=1523.32 +/- 115.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1990000, episode_reward=1402.79 +/- 177.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1991000, episode_reward=1534.90 +/- 165.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1992000, episode_reward=1581.29 +/- 100.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1993000, episode_reward=1579.84 +/- 110.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1994000, episode_reward=1453.26 +/- 355.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1995000, episode_reward=1675.21 +/- 116.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1996000, episode_reward=1609.22 +/- 114.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1997000, episode_reward=1650.62 +/- 88.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1998000, episode_reward=1363.55 +/- 290.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1999000, episode_reward=1432.26 +/- 211.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2000000, episode_reward=1523.21 +/- 132.71\n",
      "Episode length: 100.00 +/- 0.00\n"
     ]
    }
   ],
   "source": [
    "### REINFORCEMENT LEARNING I ###\n",
    "### TRAIN, SAVE, EVALUATE MODEL ###\n",
    "\n",
    "import gym\n",
    "import stable_baselines3 as sb\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "import pickle\n",
    "\n",
    "total_timesteps = 2e6\n",
    "for pn in [0]: #[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]:\n",
    "    for mn in [0]:\n",
    "        print('Process noise = ' + str(0.1*pn) + ', Measurement noise = ' + str(0.1*mn))\n",
    "        # Load diagnostics/prognostics model from disk and initiate environment\n",
    "        if prognostics:\n",
    "            print('Prognostics-based RL')\n",
    "            prog_model = pickle.load(open('prognostics/model_' + 'pn' + str(pn) + '_mn' + str(mn), 'rb'))\n",
    "            env = gym.make('Production-v0', prog_model = prog_model, process_noise = 0.1*pn)\n",
    "            # Callback for best model\n",
    "            best_callback = EvalCallback(env, best_model_save_path='./callback/prognostics/' + 'pn' + str(pn) + '_mn' + str(mn),\n",
    "                                        log_path='./callback/prognostics/' + 'pn' + str(pn) + '_mn' + str(mn), eval_freq=1000,\n",
    "                                        deterministic=True, render=False)\n",
    "            model = sb.PPO('MlpPolicy', env, tensorboard_log=\"./tensorboard/prognostics/\")\n",
    "            model.learn(total_timesteps=total_timesteps, tb_log_name='PPO_pn' + str(pn) + '_mn' + str(mn), callback = best_callback)\n",
    "        else:\n",
    "            print('Diagnostics-based RL')\n",
    "            diag_model = pickle.load(open('diagnostics/model_' + 'pn' + str(pn) + '_mn' + str(mn), 'rb'))\n",
    "            env = gym.make('Production-v0', diag_model = diag_model, process_noise = 0.1*pn, forecast = 1, prod_levels = 6)\n",
    "            # Callback for best model\n",
    "            best_callback = EvalCallback(env, best_model_save_path='./callback/fc1_l6_' + 'pn' + str(pn) + '_mn' + str(mn),\n",
    "                                    log_path='./callback/fc1_l6_' + 'pn' + str(pn) + '_mn' + str(mn), eval_freq=1000,\n",
    "                                    deterministic=True, render=False)\n",
    "            model = sb.PPO('MlpPolicy', env, tensorboard_log=\"./tensorboard/\")\n",
    "            model.learn(total_timesteps=total_timesteps, tb_log_name='DQN_fc1_l6_pn' + str(pn) + '_mn' + str(mn), callback = best_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process noise = 0.0, Measurement noise = 0.0\n",
      "(-482.6, 52.08245558727046)\n"
     ]
    }
   ],
   "source": [
    "### REINFORCEMENT LEARNING II ###\n",
    "### LOAD MODEL ###\n",
    "import gym\n",
    "import stable_baselines3 as sb\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3 import A2C\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "import pickle\n",
    "\n",
    "for pn in [0]: #[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]:\n",
    "    for mn in [0]:\n",
    "        print('Process noise = ' + str(0.1*pn) + ', Measurement noise = ' + str(0.1*mn))\n",
    "        if prognostics:\n",
    "            prog_model = pickle.load(open('prognostics/model_' + 'pn' + str(pn) + '_mn' + str(mn), 'rb'))\n",
    "            env = gym.make('Production-v0', prog_model = prog_model, process_noise = 0.1*pn)\n",
    "            model = PPO.load('./callback/prognostics/' + 'pn' + str(pn) + '_mn' + str(mn) + '/best_model', env = env)\n",
    "        else:\n",
    "            diag_model = pickle.load(open('diagnostics/model_' + 'pn' + str(pn) + '_mn' + str(mn), 'rb'))\n",
    "            env = gym.make('Production-v0', diag_model = diag_model, process_noise = 0.1*pn, forecast = 12)\n",
    "            model = PPO.load('./callback/fc1_l6_' + 'pn' + str(pn) + '_mn' + str(mn) + '/best_model', env = env)\n",
    "        \n",
    "        # Evaluate the agent\n",
    "        print(evaluate_policy(model, model.get_env(), n_eval_episodes=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process noise = 0.0, Measurement noise = 0.0\n",
      "The average number of reactive maintenance interventions per episode is:  0.13\n",
      "The average number of preventive maintenance interventions per episode is:  3.22\n",
      "The average sum of inventory per episode is:  77.46330000000003\n",
      "The average sum of spare parts inventory per episode is:  4.19\n",
      "The average reward per episode is:  1490.7510000000004\n",
      "The average upper bound per episode is:  2712.989000000001\n"
     ]
    }
   ],
   "source": [
    "### REINFORCEMENT LEARNING III ###\n",
    "### TRY AND EVALUATE MY MODEL ###\n",
    "import pandas as pd\n",
    "from stable_baselines3 import PPO, DQN\n",
    "import gym\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "for pn in [0]:#[5, 6, 7, 8, 9, 10]: #[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]:\n",
    "    for mn in [0]:\n",
    "        print('Process noise = ' + str(0.1*pn) + ', Measurement noise = ' + str(0.1*mn))\n",
    "        if prognostics:\n",
    "            prog_model = pickle.load(open('prognostics/model_' + 'pn' + str(pn) + '_mn' + str(mn), 'rb'))\n",
    "            env = gym.make('Production-v0', prog_model = prog_model, process_noise = 0.1*pn)\n",
    "            model = PPO.load('./callback/prognostics/' + 'pn' + str(pn) + '_mn' + str(mn) + '/best_model', env = env)\n",
    "        else:\n",
    "            diag_model = pickle.load(open('diagnostics/model_' + 'pn' + str(pn) + '_mn' + str(mn), 'rb'))\n",
    "            env = gym.make('Production-v0', diag_model = diag_model, process_noise = 0.1*pn, forecast = 1, prod_levels = 6)\n",
    "            model = PPO.load('./callback/fc1_l6_' + 'pn' + str(pn) + '_mn' + str(mn) + '/best_model', env = env)\n",
    "        # Initilaize Reward\n",
    "        result_df = pd.DataFrame(np.nan, index=range(0,100), columns=['RM', 'PM', 'Inventory', 'Spare Parts Inventory', 'Reward', 'Upper'])\n",
    "        # Set iterations\n",
    "        iterations = 100\n",
    "        for i in range(iterations):\n",
    "            # Initialize episode\n",
    "            store = []\n",
    "            obs = env.reset()\n",
    "            done = False\n",
    "            store.append([0, obs[0], env.breakdown, obs[2], obs[3], 0, done, obs[1]])\n",
    "            # Compute one episode\n",
    "            while not done:\n",
    "                # Get best action for state\n",
    "                action, _state = model.predict(obs, deterministic=True)\n",
    "                # Compute next state\n",
    "                #action = random.choice([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
    "                obs, reward, done, info = env.step(action)\n",
    "                # Store results of this episode\n",
    "                store.append([action, obs[0], env.breakdown, obs[2], obs[3], reward, done, obs[1]])\n",
    "            eps_df = pd.DataFrame(store, columns=['action', 'health_rul', 'breakdown', 'inventory', 'sp_inventory', 'reward', 'done', 'next_order'])\n",
    "            # Calculate nr. of reactive maintenance interventions by counting health 'resets' and substracting PM actions\n",
    "            result_df.iloc[i]['RM'] = sum(eps_df['breakdown']==True)\n",
    "            # Calculate nr. of preventive maintenance interventions\n",
    "            result_df.iloc[i]['PM'] = sum(eps_df['action']== env.actions-1)\n",
    "            # Calculate inventory\n",
    "            result_df.iloc[i]['Inventory'] = sum(eps_df['inventory'])\n",
    "            # Calculate spare parts inventory per period\n",
    "            result_df.iloc[i]['Spare Parts Inventory'] = sum(eps_df['sp_inventory'])\n",
    "            # Calculate reward\n",
    "            result_df.iloc[i]['Reward'] = sum(eps_df['reward'])\n",
    "            # Calculate reward with no costs and fulfillment of all orders\n",
    "            result_df.iloc[i]['Upper'] = sum(eps_df.iloc[:-1]['next_order']) * env.order_r\n",
    "\n",
    "        print(\"The average number of reactive maintenance interventions per episode is: \", result_df['RM'].mean())\n",
    "        print(\"The average number of preventive maintenance interventions per episode is: \", result_df['PM'].mean())\n",
    "        print(\"The average sum of inventory per episode is: \", result_df['Inventory'].mean())\n",
    "        print(\"The average sum of spare parts inventory per episode is: \", result_df['Spare Parts Inventory'].mean())\n",
    "        print(\"The average reward per episode is: \", result_df['Reward'].mean())\n",
    "        print(\"The average upper bound per episode is: \", result_df['Upper'].mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average number of reactive maintenance interventions per episode is:  0.13\n",
      "The average number of preventive maintenance interventions per episode is:  3.22\n",
      "The average sum of inventory per episode is:  77.46330000000003\n",
      "The average sum of spare parts inventory per episode is:  4.19\n",
      "The average reward per episode is:  1490.7510000000004\n",
      "The average upper bound per episode is:  2712.989000000001\n"
     ]
    }
   ],
   "source": [
    "print(\"The average number of reactive maintenance interventions per episode is: \", result_df['RM'].mean())\n",
    "print(\"The average number of preventive maintenance interventions per episode is: \", result_df['PM'].mean())\n",
    "print(\"The average sum of inventory per episode is: \", result_df['Inventory'].mean())\n",
    "print(\"The average sum of spare parts inventory per episode is: \", result_df['Spare Parts Inventory'].mean())\n",
    "print(\"The average reward per episode is: \", result_df['Reward'].mean())\n",
    "print(\"The average upper bound per episode is: \", result_df['Upper'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average number of reactive maintenance interventions per episode is:  3.0\n",
      "The average number of preventive maintenance interventions per episode is:  0.0\n",
      "The average mean time between failure per episode is:  28.487499999999986\n",
      "The standard deviation of the mean time between failure per episode is:  2.2942669667461497\n",
      "The average sum of inventory per episode is:  40.431799999999996\n",
      "The average sum of spare parts inventory per episode is:  0.0\n",
      "The average reward per episode is:  -436.1795000000002\n",
      "The average upper bound per episode is:  2695.034999999999\n"
     ]
    }
   ],
   "source": [
    "### REINFORCEMENT LEARNING IIIa ###\n",
    "### EVALUATE REACTIVE MODEL ###\n",
    "\n",
    "import pandas as pd\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3 import PPO\n",
    "import numpy as np\n",
    "import math\n",
    "import gym\n",
    "\n",
    "env = gym.make('Production-v0', reactive_mode = True, forecast = 1, prod_levels = 6)\n",
    "# Initialize Reward\n",
    "# Set iterations\n",
    "iterations = 100\n",
    "result_df = pd.DataFrame(np.nan, index=range(0,100), columns=['RM', 'PM', 'MTBF', 'Inventory', 'Spare Parts Inventory', 'Reward', 'Upper'])\n",
    "\n",
    "for i in range(iterations):\n",
    "    # Initialize episode\n",
    "    store = []\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    store.append([0, env.true_health, env.breakdown, obs[1], obs[2], 0, done, obs[0]])\n",
    "    # Compute one episode\n",
    "    while not done:\n",
    "        # Get best action for state\n",
    "        action = min(env.prod_levels-1, max(0, math.ceil(obs[0] - obs[1])))\n",
    "        # Compute next state\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        # Store results of this episode\n",
    "        store.append([action, env.true_health, env.breakdown, obs[1], obs[2], reward, done, obs[0]])\n",
    "    eps_df = pd.DataFrame(store, columns=['action', 'health', 'breakdown', 'inventory', 'sp_inventory', 'reward', 'done', 'next_order'])\n",
    "    # Calculate nr. of reactive maintenance interventions by counting health 'resets' and substracting PM actions\n",
    "    result_df.iloc[i]['RM'] = sum(eps_df['breakdown']==True)\n",
    "    # Calculate nr. of preventive maintenance interventions\n",
    "    result_df.iloc[i]['PM'] = sum(eps_df['action']== env.actions-1)\n",
    "    # Calculate mean time between failures\n",
    "    # Cut df after last breakdown\n",
    "    eps_df_trim = eps_df.iloc[:(np.where(eps_df['breakdown'].eq(True), eps_df.index, 0).max()+1)]\n",
    "    # Calculate MTBF by dividing periods where machine is running / breakdowns\n",
    "    result_df.iloc[i]['MTBF'] = (len(eps_df_trim) - sum(eps_df_trim['breakdown'] == True)) / sum(eps_df_trim['breakdown'] == True)\n",
    "    # Calculate inventory\n",
    "    result_df.iloc[i]['Inventory'] = sum(eps_df['inventory'])\n",
    "    # Calculate spare parts inventory per period\n",
    "    result_df.iloc[i]['Spare Parts Inventory'] = sum(eps_df['sp_inventory'])\n",
    "    # Calculate reward\n",
    "    result_df.iloc[i]['Reward'] = sum(eps_df['reward'])\n",
    "    # Calculate reward with no costs and fulfillment of all orders\n",
    "    result_df.iloc[i]['Upper'] = sum(eps_df.iloc[:-1]['next_order']) * env.order_r\n",
    "\n",
    "print(\"The average number of reactive maintenance interventions per episode is: \", result_df['RM'].mean())\n",
    "print(\"The average number of preventive maintenance interventions per episode is: \", result_df['PM'].mean())\n",
    "print(\"The average mean time between failure per episode is: \", result_df['MTBF'].mean())\n",
    "print(\"The standard deviation of the mean time between failure per episode is: \", result_df['MTBF'].std())\n",
    "print(\"The average sum of inventory per episode is: \", result_df['Inventory'].mean())\n",
    "print(\"The average sum of spare parts inventory per episode is: \", result_df['Spare Parts Inventory'].mean())\n",
    "print(\"The average reward per episode is: \", result_df['Reward'].mean())\n",
    "print(\"The average upper bound per episode is: \", result_df['Upper'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Maintenance Interval:  21 Coefficient:  1.05\n",
      "The average number of reactive maintenance interventions per episode is:  0.0\n",
      "The average number of preventive maintenance interventions per episode is:  4.0\n",
      "The average sum of inventory per episode is:  42.379999999999995\n",
      "The average sum of spare parts inventory per episode is:  4.0\n",
      "The average reward per episode is:  1667.3000000000004\n",
      "The average upper bound per episode is:  2584.8\n"
     ]
    }
   ],
   "source": [
    "### REINFORCEMENT LEARNING IIIb ###\n",
    "### EVALUATE TIME-BASED PREVENTIVE MODEL ###\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gym\n",
    "import math\n",
    "\n",
    "env = gym.make('Production-v0', forecast = 0, prod_levels = 6)\n",
    "interval = range (21, 22)\n",
    "# Set iterations\n",
    "iterations = 1\n",
    "\n",
    "for k in interval:\n",
    "    # Initilaize Reward\n",
    "    result_df = pd.DataFrame(np.nan, index=range(0,100), columns=['RM', 'PM', 'Inventory', 'Spare Parts Inventory', 'Reward', 'Upper'])\n",
    "\n",
    "    for i in range(iterations):\n",
    "        # Initialize episode\n",
    "        store = []\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        store.append([0, obs[0], env.breakdown, obs[1], obs[2], 0, done, env.old_order])\n",
    "        # Compute one episode\n",
    "        while not done:\n",
    "            # One period before maintenance: action = order + spare part order\n",
    "            if env.scheduled_maintenance_counter == k-1:\n",
    "                action = min(env.prod_levels-1, max(0, round(env.old_order))) + env.prod_levels\n",
    "            # At period of mtbf: maintain\n",
    "            elif env.scheduled_maintenance_counter == k:\n",
    "                action = env.actions-1\n",
    "            # Else: action = order    \n",
    "            else:             \n",
    "                action = min(env.prod_levels-1, max(0, round(env.old_order)))\n",
    "            # Compute next state\n",
    "            obs, reward, done, info = env.step(action)\n",
    "            # Store results of this episode\n",
    "            store.append([action, obs[0], env.breakdown, obs[1], obs[2], reward, done, env.old_order])\n",
    "        eps_df = pd.DataFrame(store, columns=['action', 'health', 'breakdown', 'inventory', 'sp_inventory', 'reward', 'done', 'old_order'])\n",
    "        # Calculate nr. of reactive maintenance interventions by counting health 'resets' and substracting PM actions\n",
    "        result_df.iloc[i]['RM'] = sum(eps_df['breakdown']==True)\n",
    "        # Calculate nr. of preventive maintenance interventions\n",
    "        result_df.iloc[i]['PM'] = sum(eps_df['action']==env.actions-1)\n",
    "        # Calculate inventory\n",
    "        result_df.iloc[i]['Inventory'] = sum(eps_df['inventory'])\n",
    "        # Calculate spare parts inventory per period\n",
    "        result_df.iloc[i]['Spare Parts Inventory'] = sum(eps_df['sp_inventory'])\n",
    "        # Calculate reward\n",
    "        result_df.iloc[i]['Reward'] = sum(eps_df['reward'])\n",
    "        # Calculate reward with no costs and fulfillment of all orders\n",
    "        result_df.iloc[i]['Upper'] = sum(eps_df.iloc[:-1]['next_order']) * env.order_r\n",
    "\n",
    "    print(\"\\n\", \"Maintenance Interval: \", k, \"Coefficient: \", 0+0.05*k)\n",
    "    print(\"The average number of reactive maintenance interventions per episode is: \", result_df['RM'].mean())\n",
    "    print(\"The average number of preventive maintenance interventions per episode is: \", result_df['PM'].mean())\n",
    "    print(\"The average sum of inventory per episode is: \", result_df['Inventory'].mean())\n",
    "    print(\"The average sum of spare parts inventory per episode is: \", result_df['Spare Parts Inventory'].mean())\n",
    "    print(\"The average reward per episode is: \", result_df['Reward'].mean())\n",
    "    print(\"The average upper bound per episode is: \", result_df['Upper'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "### REINFORCEMENT LEARNING IV ###\n",
    "### VISUALIZE STATE-ACTION ###\n",
    "import numpy as np\n",
    "state_action = []\n",
    "\n",
    "# Define observation grid\n",
    "grid_health = np.arange(0.0, 1.01, 0.01)\n",
    "grid_order = range(0, 5)\n",
    "grid_inventory = range(0, 10)\n",
    "grid_sp_inventory = [0, 1]\n",
    "\n",
    "# Loop through grid and store best action for each state\n",
    "for hlt in grid_health:\n",
    "    for ord in grid_order:\n",
    "        for inv in grid_inventory:\n",
    "            for sin in grid_sp_inventory:\n",
    "                # Predict\n",
    "                action, _state = model.predict((hlt, ord, inv, sin), deterministic=True)\n",
    "                state_action.append([hlt, ord, inv, sin, action])\n",
    "\n",
    "state_action_df = pd.DataFrame(state_action, columns=['health', 'order', 'inventory', 'sp_inventory', 'action'])\n",
    "state_action_df.to_excel(\"visuals/state_action.xlsx\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### REINFORCEMENT LEARNING IVa ###\n",
    "### VISUALIZE STATE-VALUE ###\n",
    "import numpy as np\n",
    "state_action = []\n",
    "\n",
    "# Define observation grid\n",
    "grid_health = np.arange(0.0, 1.01, 0.01)\n",
    "grid_order = range(0, 5)\n",
    "grid_inventory = range(0, 10)\n",
    "grid_sp_inventory = [0, 1]\n",
    "\n",
    "# Loop through grid and store best action for each state\n",
    "for hlt in grid_health:\n",
    "    for ord in grid_order:\n",
    "        for inv in grid_inventory:\n",
    "            for sin in grid_sp_inventory:\n",
    "                # Predict\n",
    "                obs, _ = model.policy.obs_to_tensor(hlt, ord, inv, sin)\n",
    "                value = model.policy.predict_values(obs).item()\n",
    "                state_action.append([hlt, ord, inv, sin, value])\n",
    "\n",
    "state_action_df = pd.DataFrame(state_action, columns=['health', 'order', 'inventory', 'sp_inventory', 'value'])\n",
    "state_action_df.to_excel(\"visuals/state_value.xlsx\") \n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6f69c5940b32a5cbabe45c9825076a627c6cdb9ede58cf4d0fa74ca6057ffe74"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
