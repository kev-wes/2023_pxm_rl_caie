{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process noise = 0.0, Measurement noise = 0.0\n",
      "Process noise = 0.1, Measurement noise = 0.0\n",
      "Process noise = 0.2, Measurement noise = 0.0\n",
      "Process noise = 0.30000000000000004, Measurement noise = 0.0\n",
      "Process noise = 0.4, Measurement noise = 0.0\n",
      "Process noise = 0.5, Measurement noise = 0.0\n",
      "Process noise = 0.6000000000000001, Measurement noise = 0.0\n",
      "Process noise = 0.7000000000000001, Measurement noise = 0.0\n",
      "Process noise = 0.8, Measurement noise = 0.0\n",
      "Process noise = 0.9, Measurement noise = 0.0\n",
      "Process noise = 1.0, Measurement noise = 0.0\n"
     ]
    }
   ],
   "source": [
    "### DATA-DRIVEN DIAGNOSTICS I ###\n",
    "### GENERATE DATA FOR DATA-DRIVEN MODEL ###\n",
    "\n",
    "import random\n",
    "from prog_models.models import BatteryCircuit\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "# Ignore warnings when machine exceeds its end of life\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\"\"\" Method that uses a physical machine model from the prog_models package and a current (health) state of the model and\n",
    "an action (i.e., intensity), which is performed for 100 time steps\n",
    "    Parameter:\n",
    "        machine             machine model from the prog_models package\n",
    "        state               current (health) state of the model\n",
    "        action              loading of the machine for the next 100 time steps\n",
    "    Return:\n",
    "        health                               \n",
    "    \"\"\"\n",
    "def produce_model(machine, states, action):\n",
    "        \n",
    "        # Define load of battery\n",
    "        def future_loading(t, x=None):\n",
    "            return {'i': action}\n",
    "\n",
    "        # Set current state of machine\n",
    "        machine.parameters['x0'] = states\n",
    "        # Simulate 100 steps\n",
    "        options = {\n",
    "            'save_freq': 100,  # Frequency at which results are saved\n",
    "            'dt': 2  # Timestep\n",
    "        }\n",
    "        (_, _, states, outputs, event_states) = machine.simulate_to(100, future_loading, **options)\n",
    "        health = event_states[-1]['EOD']\n",
    "        return(round(health, 2), states[-1], outputs[-1]['t'], outputs[-1]['v'])\n",
    "def reset_states(machine):\n",
    "    # Returns initial states of machine, e.g., {'tb': 18.95, 'qb': 7856.3254, 'qcp': 0, 'qcs': 0} for Battery\n",
    "    return(machine.default_parameters['x0'])\n",
    "   \n",
    "for pn in [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]: \n",
    "    for mn in [0]:\n",
    "        print('Process noise = ' + str(0.1*pn) + ', Measurement noise = ' + str(0.1*mn))\n",
    "        battery = BatteryCircuit(process_noise = 0.1*pn, measurement_noise = 0.1*mn)\n",
    "        states = reset_states(battery)\n",
    "        reset_counter = 0\n",
    "        dataset = []\n",
    "        for i in range(int(1e4)):\n",
    "            # If asset failed last period, reset all historical values\n",
    "            if reset_counter == 0: t = v = t_1 = v_1 = t_2 = v_2 = t_3 = v_3 = 0 \n",
    "            # Shift history by one time period\n",
    "            v_3 = v_2\n",
    "            t_3 = t_2\n",
    "            v_2 = v_1\n",
    "            t_2 = t_1\n",
    "            v_1 = v\n",
    "            t_1 = t\n",
    "\n",
    "            # Increment reset_counter\n",
    "            reset_counter = reset_counter + 1\n",
    "            # Compute new health, states, t, and v using last battery state and a random new action\n",
    "            health, states, t, v = produce_model(machine=battery, states=states, action=random.sample((0, 1, 2, 3, 4), 1)[0])\n",
    "            \n",
    "            if health <= 0: \n",
    "                # Reset battery states to initialize battery for next produce_model call\n",
    "                states = reset_states(battery)\n",
    "                # Initialize reset_counter\n",
    "                reset_counter = 0\n",
    "                # Sometimes produce_model returns weird or negative values as the end of life is exceeded\n",
    "                # Here, we just simply set it to zero to not confuse a later learner \n",
    "                health = 0\n",
    "\n",
    "            # append to two-dimensional list\n",
    "            dataset.append([t, v, t_1, v_1, t_2, v_2, t_3, v_3, health])\n",
    "\n",
    "        # Transform two-dim list to dataframe\n",
    "        dataset = pd.DataFrame(dataset, columns=['t', 'v', 't_1', 'v_1', 't_2', 'v_2', 't_3', 'v_3', 'health'])\n",
    "        # Denote machine runs to-failure with incrementing id\n",
    "        k = 0\n",
    "        for i in range(dataset.shape[0]):\n",
    "            if (dataset.iloc[i]['t_1'] == 0 and dataset.iloc[i]['v_1'] == 0):\n",
    "                j = 1\n",
    "                k = k + 1\n",
    "            dataset.loc[i, 'time'] = j\n",
    "            j = j + 1\n",
    "            dataset.loc[i, 'ID'] = k\n",
    "        dataset = dataset.sort_values(['ID', 'time'], ascending=[True, False])\n",
    "        # Assign RUL by counting upwards per serial number in the descended data frame\n",
    "        dataset['RUL'] = dataset.groupby((dataset['ID'] != dataset['ID'].shift(1)).cumsum()).cumcount()\n",
    "        dataset = dataset.sort_values(['ID', 'time'], ascending=True)\n",
    "\n",
    "        # Save it as pickle\n",
    "        dataset.to_pickle('diagnostics/data_' + 'pn' + str(pn) + '_mn' + str(mn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model based on RUL (prognostics = True) or health (diagnostics, prognostics = False)\n",
    "prognostics = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process noise = 0.0, Measurement noise = 0.0\n",
      "R2: [0.98346924 0.97869884 0.98271119 0.98084096 0.98029931]\n",
      "mae: [-0.0186088  -0.02097155 -0.01949765 -0.0201391  -0.0208858 ]\n",
      "mape: [-7.86658635e-02 -3.23583633e+11 -9.00719925e+09 -9.20286148e-02\n",
      " -1.71587146e+11]\n",
      "rmse: [-0.03669848 -0.04135376 -0.03768967 -0.03960828 -0.04014499]\n",
      "Process noise = 0.1, Measurement noise = 0.0\n",
      "R2: [0.94511725 0.94338909 0.94308702 0.95269309 0.92781427]\n",
      "mae: [-0.04355035 -0.0430732  -0.0434886  -0.0409953  -0.04892035]\n",
      "mape: [-1.57625987e+09 -6.75539944e+09 -2.31935381e+10 -3.03992975e+10\n",
      " -3.69295169e+11]\n",
      "rmse: [-0.06585241 -0.06812947 -0.06765209 -0.06181947 -0.07649472]\n",
      "Process noise = 0.2, Measurement noise = 0.0\n",
      "R2: [0.89160295 0.90719555 0.90106071 0.89986382 0.8983238 ]\n",
      "mae: [-0.06726765 -0.0624331  -0.0637716  -0.0658622  -0.0643069 ]\n",
      "mape: [-3.90912448e+11 -3.67493730e+11 -1.03605309e+12 -7.60657977e+11\n",
      " -1.41413028e+11]\n",
      "rmse: [-0.09406841 -0.08643758 -0.08901846 -0.09017457 -0.0908437 ]\n",
      "Process noise = 0.30000000000000004, Measurement noise = 0.0\n",
      "R2: [0.87037827 0.84902257 0.84585148 0.85446821 0.85283189]\n",
      "mae: [-0.07610885 -0.0816426  -0.0808236  -0.0798498  -0.0796984 ]\n",
      "mape: [-8.21906932e+10 -2.44320280e+11 -3.61413870e+11 -2.58281439e+11\n",
      " -2.56705179e+10]\n",
      "rmse: [-0.10279177 -0.11007041 -0.11067439 -0.10907084 -0.10878756]\n",
      "Process noise = 0.4, Measurement noise = 0.0\n",
      "R2: [0.83456832 0.83012227 0.80835482 0.83903277 0.81974217]\n",
      "mae: [-0.0846392  -0.0875222  -0.09059015 -0.08526315 -0.090239  ]\n",
      "mape: [-1.02456892e+11 -4.00820367e+11 -2.39816680e+11 -2.95661316e+11\n",
      " -6.73288144e+10]\n",
      "rmse: [-0.11565337 -0.11667704 -0.12263368 -0.11445298 -0.12049927]\n",
      "Process noise = 0.5, Measurement noise = 0.0\n",
      "R2: [0.82839126 0.80480565 0.82500873 0.8179684  0.817267  ]\n",
      "mae: [-0.09095045 -0.096345   -0.0892272  -0.09109585 -0.09089715]\n",
      "mape: [-4.76480841e+11 -2.50850499e+11 -2.85978576e+11 -7.34086739e+10\n",
      " -4.02171447e+11]\n",
      "rmse: [-0.11899326 -0.12706747 -0.11847659 -0.12179301 -0.12175318]\n",
      "Process noise = 0.6000000000000001, Measurement noise = 0.0\n",
      "R2: [0.78807567 0.79536683 0.79253112 0.78549367 0.7681572 ]\n",
      "mae: [-0.09885245 -0.09946715 -0.09709435 -0.1021844  -0.1052449 ]\n",
      "mape: [-3.47452711e+11 -4.54638382e+11 -8.87209127e+11 -2.08516663e+11\n",
      " -3.28762773e+11]\n",
      "rmse: [-0.13075345 -0.13061024 -0.12871161 -0.13304715 -0.13793793]\n",
      "Process noise = 0.7000000000000001, Measurement noise = 0.0\n",
      "R2: [0.77073703 0.76979583 0.75657293 0.74444239 0.7908058 ]\n",
      "mae: [-0.1031984  -0.1025792  -0.10822995 -0.11202985 -0.09902865]\n",
      "mape: [-2.76746197e+11 -4.12529726e+11 -1.64156206e+11 -5.88395291e+11\n",
      " -3.12099454e+11]\n",
      "rmse: [-0.13438963 -0.1351357  -0.14146253 -0.14412065 -0.1310333 ]\n",
      "Process noise = 0.8, Measurement noise = 0.0\n",
      "R2: [0.75441041 0.73836646 0.72956504 0.7436836  0.74272255]\n",
      "mae: [-0.1075912  -0.1102882  -0.11299515 -0.10926115 -0.1102889 ]\n",
      "mape: [-3.29888673e+11 -1.32315757e+12 -3.39571412e+11 -9.10177485e+11\n",
      " -5.11158558e+11]\n",
      "rmse: [-0.14059851 -0.14411492 -0.14701622 -0.1434903  -0.14296051]\n",
      "Process noise = 0.9, Measurement noise = 0.0\n",
      "R2: [0.7459057  0.72225387 0.71386912 0.73079549 0.71229312]\n",
      "mae: [-0.1088923  -0.115251   -0.11461225 -0.1123348  -0.1146464 ]\n",
      "mape: [-2.86203756e+11 -1.43439648e+11 -3.62990130e+11 -8.26410532e+10\n",
      " -6.39285967e+11]\n",
      "rmse: [-0.14232725 -0.14946078 -0.14982241 -0.14679103 -0.14974282]\n",
      "Process noise = 1.0, Measurement noise = 0.0\n",
      "R2: [0.70404433 0.7015158  0.69136757 0.70469365 0.71885872]\n",
      "mae: [-0.1154981  -0.1165566  -0.11575255 -0.11541465 -0.11638255]\n",
      "mape: [-1.49744688e+11 -5.21967197e+11 -5.96051411e+11 -2.15925084e+12\n",
      " -3.37769972e+11]\n",
      "rmse: [-0.15095345 -0.15119491 -0.15424514 -0.15225114 -0.15056938]\n"
     ]
    }
   ],
   "source": [
    "### DATA-DRIVEN DIAGNOSTICS II ###\n",
    "### FIT AND TEST MODEL ###\n",
    "from sklearn import tree, linear_model, kernel_ridge, svm, neighbors, gaussian_process, ensemble, neural_network\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_validate\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "score_data = []\n",
    "for pn in [0, 1, 2, 3, 4, 5, 6, 7 , 8, 9, 10]: \n",
    "    for mn in [0]:\n",
    "        print('Process noise = ' + str(0.1*pn) + ', Measurement noise = ' + str(0.1*mn))\n",
    "        dataset = pd.read_pickle('diagnostics/data_' + 'pn' + str(pn) + '_mn' + str(mn))\n",
    "        X = dataset[['t', 'v', 't_1', 'v_1', 't_2', 'v_2', 't_3', 'v_3']]\n",
    "        if prognostics:\n",
    "            y = dataset['RUL']\n",
    "        else:\n",
    "            y = dataset['health']\n",
    "        learner = ensemble.RandomForestRegressor()\n",
    "        scoring = {'r2': 'r2',\n",
    "                    'mae': 'neg_mean_absolute_error',\n",
    "                    'mape': 'neg_mean_absolute_percentage_error',\n",
    "                    'rmse': 'neg_root_mean_squared_error'}\n",
    "        scores = cross_validate(learner, X, y, scoring=scoring, cv=5) # default scoring R2\n",
    "        print(\"R2: \" + str(scores['test_r2']))\n",
    "        print(\"mae: \" + str(scores['test_mae']))\n",
    "        print(\"mape: \" + str(scores['test_mape']))\n",
    "        print(\"rmse: \" + str(scores['test_rmse']))\n",
    "        # Store measures\n",
    "        score_data.append(['model_' + 'pn' + str(pn) + '_mn' + str(mn), np.mean(scores['test_r2']),\n",
    "                            np.mean(scores['test_mae']), np.mean(scores['test_mape']), np.mean(scores['test_rmse'])])\n",
    "\n",
    "        # Fit on all data\n",
    "        model = ensemble.RandomForestRegressor().fit(X, y)\n",
    "        if prognostics:\n",
    "            pickle.dump(model, open('prognostics/model_' + 'pn' + str(pn) + '_mn' + str(mn), 'wb'))\n",
    "        else:\n",
    "            pickle.dump(model, open('diagnostics/model_' + 'pn' + str(pn) + '_mn' + str(mn), 'wb'))\n",
    "# Transform two-dim list to dataframe\n",
    "score_df = pd.DataFrame(score_data, columns=['model', 'r2', 'mae', 'mape', 'rmse'])\n",
    "if prognostics:\n",
    "    score_df.to_excel(\"prognostics/scores.xlsx\") \n",
    "else:\n",
    "    score_df.to_excel(\"diagnostics/scores.xlsx\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process noise = 0.0, Measurement noise = 0.0\n",
      "Process noise = 0.1, Measurement noise = 0.0\n",
      "Process noise = 0.2, Measurement noise = 0.0\n",
      "Process noise = 0.30000000000000004, Measurement noise = 0.0\n",
      "Process noise = 0.4, Measurement noise = 0.0\n",
      "Process noise = 0.5, Measurement noise = 0.0\n",
      "Process noise = 0.6000000000000001, Measurement noise = 0.0\n",
      "Process noise = 0.7000000000000001, Measurement noise = 0.0\n",
      "Process noise = 0.8, Measurement noise = 0.0\n",
      "Process noise = 0.9, Measurement noise = 0.0\n",
      "Process noise = 1.0, Measurement noise = 0.0\n"
     ]
    }
   ],
   "source": [
    "### DATA-DRIVEN DIAGNOSTICS IIIa ###\n",
    "### FIT, TEST, VISUALIZE MODEL USING TRAIN AND TEST SETS ###\n",
    "\n",
    "for pn in [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]: \n",
    "    for mn in [0]:\n",
    "        print('Process noise = ' + str(0+0.1*pn) + ', Measurement noise = ' + str(0+0.1*mn))\n",
    "        if prognostics:\n",
    "            dataset = pd.read_pickle('diagnostics/data_' + 'pn' + str(pn) + '_mn' + str(mn))\n",
    "            y = dataset['RUL']\n",
    "        else:\n",
    "            dataset = pd.read_pickle('prognostics/data_' + 'pn' + str(pn) + '_mn' + str(mn))\n",
    "            y = dataset['health']\n",
    "        X = dataset[['t', 'v', 't_1', 'v_1', 't_2', 'v_2', 't_3', 'v_3']]\n",
    "        # Find index of healthy machines\n",
    "        index_df = X.index[(X['t_1'] == 0) & (X['v_1'] == 0) & (X['t_2'] == 0) & (X['v_2'] == 0) & (X['t_3'] == 0) & (X['v_3'] == 0)].tolist()\n",
    "        index_test = round(len(index_df)*0.8)\n",
    "\n",
    "        # Create train and test set without disrupting machine runs to-failure\n",
    "        X_train = X.iloc[0:(index_df[index_test])]\n",
    "        y_train = y.iloc[0:(index_df[index_test])]\n",
    "        X_test = X.iloc[index_df[index_test]:(len(X))]\n",
    "        y_test = y.iloc[index_df[index_test]:(len(y))]\n",
    "\n",
    "        ## Train\n",
    "        model = ensemble.RandomForestRegressor().fit(X_train, y_train)\n",
    "        ## Predict\n",
    "        y_pred = pd.DataFrame(model.predict(X_test), columns=['Pred'])\n",
    "        ## Analyze\n",
    "        #reset index of each DataFrame\n",
    "        X_test.reset_index(drop=True, inplace=True)\n",
    "        y_test.reset_index(drop=True, inplace=True)\n",
    "        # Concat dataframes\n",
    "        test_df = pd.concat([X_test, y_test, y_pred], axis=1)\n",
    "        # Print for visualization (e.g., in R)\n",
    "        if prognostics:\n",
    "            test_df.to_excel(\"prognostics/test_results_\" + 'pn' + str(pn) + '_mn' + str(mn) + \".xlsx\") \n",
    "        else:\n",
    "            test_df.to_excel(\"diagnostics/test_results_\" + 'pn' + str(pn) + '_mn' + str(mn) + \".xlsx\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process noise = 0.0, Measurement noise = 0.0\n",
      "Diagnostics-based RL\n",
      "Eval num_timesteps=1000, episode_reward=-4737.04 +/- 216.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=-4642.34 +/- 194.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3000, episode_reward=-1557.85 +/- 85.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4000, episode_reward=-1687.91 +/- 122.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=5000, episode_reward=-1566.61 +/- 389.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=6000, episode_reward=-1574.09 +/- 118.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=7000, episode_reward=-1906.85 +/- 161.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=8000, episode_reward=-1888.56 +/- 128.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=9000, episode_reward=-1878.29 +/- 427.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=10000, episode_reward=-1796.73 +/- 212.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=11000, episode_reward=-1326.03 +/- 134.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=12000, episode_reward=-1443.98 +/- 178.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=13000, episode_reward=-1460.12 +/- 207.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=14000, episode_reward=-1363.68 +/- 185.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=15000, episode_reward=-1465.20 +/- 307.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=16000, episode_reward=-1480.50 +/- 227.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=17000, episode_reward=-1392.57 +/- 325.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=18000, episode_reward=-1313.15 +/- 258.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=19000, episode_reward=-1314.85 +/- 275.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=20000, episode_reward=-1466.25 +/- 174.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=21000, episode_reward=-1258.19 +/- 324.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=22000, episode_reward=-1421.76 +/- 122.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=23000, episode_reward=-1515.38 +/- 116.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=24000, episode_reward=-1475.74 +/- 176.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=25000, episode_reward=-1437.53 +/- 154.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=26000, episode_reward=-1560.86 +/- 204.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=27000, episode_reward=-1425.35 +/- 114.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=28000, episode_reward=-1480.01 +/- 142.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=29000, episode_reward=-1395.54 +/- 79.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=30000, episode_reward=-1482.99 +/- 145.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=31000, episode_reward=-1212.02 +/- 105.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=32000, episode_reward=-1361.09 +/- 96.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=33000, episode_reward=-1154.95 +/- 70.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=34000, episode_reward=-1165.70 +/- 114.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=35000, episode_reward=-1271.34 +/- 120.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=36000, episode_reward=-1236.68 +/- 170.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=37000, episode_reward=-1072.29 +/- 43.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=38000, episode_reward=-1052.42 +/- 124.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=39000, episode_reward=-1093.03 +/- 91.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=40000, episode_reward=-1090.55 +/- 156.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=41000, episode_reward=-1097.17 +/- 76.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=42000, episode_reward=-1079.21 +/- 312.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=43000, episode_reward=-1079.48 +/- 75.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=44000, episode_reward=-1056.08 +/- 374.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=45000, episode_reward=-1140.43 +/- 254.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=46000, episode_reward=-1126.27 +/- 333.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=47000, episode_reward=-1250.49 +/- 317.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=48000, episode_reward=-1184.57 +/- 124.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=49000, episode_reward=-1033.37 +/- 166.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=50000, episode_reward=-1150.79 +/- 275.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=51000, episode_reward=-1326.17 +/- 271.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=52000, episode_reward=-1357.62 +/- 254.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=53000, episode_reward=-991.91 +/- 86.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=54000, episode_reward=-1119.52 +/- 267.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=55000, episode_reward=-1259.93 +/- 279.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=56000, episode_reward=-1119.10 +/- 214.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=57000, episode_reward=-1045.46 +/- 152.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=58000, episode_reward=-1231.47 +/- 418.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=59000, episode_reward=-1065.39 +/- 203.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=60000, episode_reward=-954.85 +/- 79.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=61000, episode_reward=-914.44 +/- 97.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=62000, episode_reward=-1323.19 +/- 351.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=63000, episode_reward=-1141.22 +/- 191.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=64000, episode_reward=-1351.66 +/- 257.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=65000, episode_reward=-1435.51 +/- 233.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=66000, episode_reward=-1248.82 +/- 321.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=67000, episode_reward=-1337.09 +/- 286.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=68000, episode_reward=-1319.44 +/- 185.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=69000, episode_reward=-1182.55 +/- 281.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=70000, episode_reward=-1022.81 +/- 284.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=71000, episode_reward=-1083.50 +/- 143.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=72000, episode_reward=-1471.94 +/- 146.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=73000, episode_reward=-1042.32 +/- 125.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=74000, episode_reward=-1037.34 +/- 254.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=75000, episode_reward=-1298.81 +/- 326.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=76000, episode_reward=-1137.29 +/- 248.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=77000, episode_reward=-1190.48 +/- 314.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=78000, episode_reward=-1173.76 +/- 233.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=79000, episode_reward=-1452.24 +/- 176.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=80000, episode_reward=-961.36 +/- 126.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=81000, episode_reward=-1058.47 +/- 276.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=82000, episode_reward=-1342.13 +/- 203.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=83000, episode_reward=-1368.12 +/- 279.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=84000, episode_reward=-1356.04 +/- 255.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=85000, episode_reward=-1244.01 +/- 283.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=86000, episode_reward=-1275.77 +/- 262.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=87000, episode_reward=-1069.17 +/- 196.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=88000, episode_reward=-1333.93 +/- 345.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=89000, episode_reward=-1308.88 +/- 303.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=90000, episode_reward=-1162.02 +/- 414.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=91000, episode_reward=-1255.95 +/- 360.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=92000, episode_reward=-1556.41 +/- 196.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=93000, episode_reward=-1210.16 +/- 288.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=94000, episode_reward=-1320.98 +/- 281.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=95000, episode_reward=-1079.08 +/- 326.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=96000, episode_reward=-1296.34 +/- 154.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=97000, episode_reward=-1273.56 +/- 301.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=98000, episode_reward=-1197.53 +/- 268.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=99000, episode_reward=-1179.19 +/- 286.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=100000, episode_reward=-1071.12 +/- 323.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=101000, episode_reward=-1334.69 +/- 256.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=102000, episode_reward=-1325.92 +/- 293.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=103000, episode_reward=-1016.82 +/- 275.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=104000, episode_reward=-947.74 +/- 145.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=105000, episode_reward=-1123.82 +/- 344.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=106000, episode_reward=-1456.45 +/- 137.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=107000, episode_reward=-1478.59 +/- 180.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=108000, episode_reward=-1279.33 +/- 167.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=109000, episode_reward=-1178.89 +/- 327.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=110000, episode_reward=-1369.33 +/- 371.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=111000, episode_reward=-1076.32 +/- 249.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=112000, episode_reward=-1339.44 +/- 321.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=113000, episode_reward=-1009.57 +/- 179.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=114000, episode_reward=-1322.99 +/- 383.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=115000, episode_reward=-1346.31 +/- 197.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=116000, episode_reward=-1065.72 +/- 216.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=117000, episode_reward=-1292.57 +/- 281.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=118000, episode_reward=-1190.06 +/- 320.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=119000, episode_reward=-1117.90 +/- 224.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=120000, episode_reward=-1203.52 +/- 335.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=121000, episode_reward=-1311.39 +/- 287.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=122000, episode_reward=-1391.24 +/- 345.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=123000, episode_reward=-1148.03 +/- 247.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=124000, episode_reward=-1369.95 +/- 277.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=125000, episode_reward=-1232.41 +/- 232.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=126000, episode_reward=-1290.54 +/- 285.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=127000, episode_reward=-1029.21 +/- 68.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=128000, episode_reward=-1184.59 +/- 369.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=129000, episode_reward=-1253.82 +/- 288.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=130000, episode_reward=-1219.73 +/- 264.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=131000, episode_reward=-1496.74 +/- 285.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=132000, episode_reward=-909.49 +/- 414.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=133000, episode_reward=-1297.78 +/- 265.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=134000, episode_reward=-827.48 +/- 251.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=135000, episode_reward=-1044.99 +/- 474.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=136000, episode_reward=-1101.71 +/- 262.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=137000, episode_reward=-945.25 +/- 219.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=138000, episode_reward=-909.46 +/- 319.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=139000, episode_reward=-975.58 +/- 307.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=140000, episode_reward=-1262.03 +/- 381.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=141000, episode_reward=-1046.72 +/- 413.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=142000, episode_reward=-998.10 +/- 540.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=143000, episode_reward=-956.05 +/- 409.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=144000, episode_reward=-907.12 +/- 212.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=145000, episode_reward=-1170.85 +/- 274.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=146000, episode_reward=-980.01 +/- 247.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=147000, episode_reward=-1163.21 +/- 325.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=148000, episode_reward=-709.12 +/- 480.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=149000, episode_reward=-489.28 +/- 344.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=150000, episode_reward=-462.77 +/- 103.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=151000, episode_reward=-696.90 +/- 288.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=152000, episode_reward=-1256.31 +/- 319.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=153000, episode_reward=-705.02 +/- 607.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=154000, episode_reward=-890.31 +/- 288.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=155000, episode_reward=-705.68 +/- 434.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=156000, episode_reward=-128.30 +/- 148.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=157000, episode_reward=-236.05 +/- 177.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=158000, episode_reward=-357.85 +/- 410.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=159000, episode_reward=-420.04 +/- 162.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=160000, episode_reward=153.16 +/- 370.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=161000, episode_reward=-271.55 +/- 275.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=162000, episode_reward=-386.02 +/- 165.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=163000, episode_reward=-635.90 +/- 540.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=164000, episode_reward=-56.18 +/- 450.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=165000, episode_reward=-119.19 +/- 356.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=166000, episode_reward=-127.02 +/- 328.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=167000, episode_reward=190.81 +/- 240.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=168000, episode_reward=-144.27 +/- 283.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=169000, episode_reward=-29.37 +/- 489.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=170000, episode_reward=-118.09 +/- 491.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=171000, episode_reward=-122.70 +/- 387.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=172000, episode_reward=-240.08 +/- 192.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=173000, episode_reward=18.77 +/- 336.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=174000, episode_reward=16.40 +/- 259.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=175000, episode_reward=515.51 +/- 446.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=176000, episode_reward=586.20 +/- 163.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=177000, episode_reward=226.40 +/- 320.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=178000, episode_reward=345.19 +/- 157.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=179000, episode_reward=-47.21 +/- 412.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=180000, episode_reward=94.30 +/- 322.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=181000, episode_reward=272.48 +/- 285.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=182000, episode_reward=515.82 +/- 202.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=183000, episode_reward=-176.36 +/- 470.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=184000, episode_reward=-338.54 +/- 308.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=185000, episode_reward=150.24 +/- 351.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=186000, episode_reward=406.43 +/- 347.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=187000, episode_reward=699.78 +/- 133.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=188000, episode_reward=502.48 +/- 178.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=189000, episode_reward=721.23 +/- 228.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=190000, episode_reward=730.22 +/- 127.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=191000, episode_reward=640.52 +/- 154.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=192000, episode_reward=695.46 +/- 232.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=193000, episode_reward=662.26 +/- 207.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=194000, episode_reward=638.34 +/- 215.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=195000, episode_reward=779.01 +/- 113.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=196000, episode_reward=631.73 +/- 209.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=197000, episode_reward=592.17 +/- 268.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=198000, episode_reward=913.02 +/- 234.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=199000, episode_reward=577.88 +/- 211.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=200000, episode_reward=644.63 +/- 230.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=201000, episode_reward=690.92 +/- 170.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=202000, episode_reward=794.39 +/- 151.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=203000, episode_reward=839.57 +/- 175.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=204000, episode_reward=578.00 +/- 271.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=205000, episode_reward=684.88 +/- 285.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=206000, episode_reward=696.10 +/- 140.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=207000, episode_reward=597.07 +/- 226.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=208000, episode_reward=724.91 +/- 213.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=209000, episode_reward=689.93 +/- 162.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=210000, episode_reward=632.15 +/- 161.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=211000, episode_reward=632.11 +/- 172.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=212000, episode_reward=565.51 +/- 149.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=213000, episode_reward=706.29 +/- 175.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=214000, episode_reward=750.39 +/- 157.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=215000, episode_reward=672.66 +/- 113.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=216000, episode_reward=627.05 +/- 179.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=217000, episode_reward=788.37 +/- 293.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=218000, episode_reward=867.53 +/- 260.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=219000, episode_reward=619.08 +/- 328.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=220000, episode_reward=714.03 +/- 131.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=221000, episode_reward=669.83 +/- 150.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=222000, episode_reward=638.25 +/- 338.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=223000, episode_reward=628.91 +/- 204.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=224000, episode_reward=709.92 +/- 141.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=225000, episode_reward=560.41 +/- 229.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=226000, episode_reward=658.48 +/- 135.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=227000, episode_reward=508.81 +/- 242.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=228000, episode_reward=756.57 +/- 128.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=229000, episode_reward=535.71 +/- 45.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=230000, episode_reward=601.01 +/- 313.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=231000, episode_reward=711.70 +/- 204.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=232000, episode_reward=734.96 +/- 234.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=233000, episode_reward=604.89 +/- 134.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=234000, episode_reward=750.68 +/- 218.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=235000, episode_reward=612.23 +/- 200.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=236000, episode_reward=870.91 +/- 176.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=237000, episode_reward=743.97 +/- 317.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=238000, episode_reward=867.70 +/- 166.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=239000, episode_reward=869.01 +/- 153.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=240000, episode_reward=742.44 +/- 176.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=241000, episode_reward=752.34 +/- 246.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=242000, episode_reward=713.24 +/- 85.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=243000, episode_reward=779.70 +/- 129.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=244000, episode_reward=847.84 +/- 178.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=245000, episode_reward=705.81 +/- 245.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=246000, episode_reward=567.31 +/- 109.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=247000, episode_reward=643.92 +/- 63.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=248000, episode_reward=674.71 +/- 88.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=249000, episode_reward=831.83 +/- 70.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=250000, episode_reward=553.67 +/- 124.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=251000, episode_reward=712.35 +/- 119.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=252000, episode_reward=566.31 +/- 117.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=253000, episode_reward=629.14 +/- 209.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=254000, episode_reward=565.39 +/- 296.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=255000, episode_reward=721.94 +/- 196.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=256000, episode_reward=681.10 +/- 212.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=257000, episode_reward=731.40 +/- 188.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=258000, episode_reward=761.99 +/- 140.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=259000, episode_reward=700.64 +/- 199.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=260000, episode_reward=736.16 +/- 254.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=261000, episode_reward=531.90 +/- 183.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=262000, episode_reward=703.13 +/- 138.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=263000, episode_reward=645.95 +/- 200.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=264000, episode_reward=784.57 +/- 116.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=265000, episode_reward=676.83 +/- 194.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=266000, episode_reward=599.89 +/- 150.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=267000, episode_reward=842.46 +/- 209.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=268000, episode_reward=499.42 +/- 233.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=269000, episode_reward=662.47 +/- 251.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=270000, episode_reward=668.43 +/- 172.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=271000, episode_reward=701.11 +/- 163.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=272000, episode_reward=541.03 +/- 173.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=273000, episode_reward=758.75 +/- 113.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=274000, episode_reward=700.04 +/- 196.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=275000, episode_reward=663.15 +/- 178.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=276000, episode_reward=795.84 +/- 180.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=277000, episode_reward=683.90 +/- 154.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=278000, episode_reward=554.95 +/- 66.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=279000, episode_reward=749.60 +/- 204.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=280000, episode_reward=668.14 +/- 213.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=281000, episode_reward=688.15 +/- 243.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=282000, episode_reward=689.41 +/- 213.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=283000, episode_reward=727.68 +/- 124.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=284000, episode_reward=785.81 +/- 161.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=285000, episode_reward=701.54 +/- 261.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=286000, episode_reward=743.23 +/- 82.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=287000, episode_reward=835.69 +/- 284.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=288000, episode_reward=829.05 +/- 149.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=289000, episode_reward=937.95 +/- 130.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=290000, episode_reward=874.43 +/- 78.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=291000, episode_reward=790.16 +/- 278.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=292000, episode_reward=762.13 +/- 239.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=293000, episode_reward=763.03 +/- 216.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=294000, episode_reward=707.41 +/- 219.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=295000, episode_reward=818.88 +/- 167.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=296000, episode_reward=892.10 +/- 193.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=297000, episode_reward=724.94 +/- 174.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=298000, episode_reward=606.21 +/- 275.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=299000, episode_reward=697.48 +/- 220.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=300000, episode_reward=666.70 +/- 134.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=301000, episode_reward=776.77 +/- 167.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=302000, episode_reward=671.30 +/- 262.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=303000, episode_reward=822.16 +/- 179.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=304000, episode_reward=812.25 +/- 203.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=305000, episode_reward=732.20 +/- 219.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=306000, episode_reward=730.36 +/- 129.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=307000, episode_reward=812.04 +/- 207.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=308000, episode_reward=538.37 +/- 137.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=309000, episode_reward=596.18 +/- 189.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=310000, episode_reward=700.59 +/- 308.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=311000, episode_reward=826.95 +/- 243.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=312000, episode_reward=651.72 +/- 327.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=313000, episode_reward=723.66 +/- 178.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=314000, episode_reward=837.92 +/- 181.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=315000, episode_reward=794.66 +/- 83.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=316000, episode_reward=650.67 +/- 181.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=317000, episode_reward=717.86 +/- 225.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=318000, episode_reward=778.31 +/- 166.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=319000, episode_reward=711.28 +/- 244.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=320000, episode_reward=526.57 +/- 308.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=321000, episode_reward=578.77 +/- 212.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=322000, episode_reward=625.43 +/- 199.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=323000, episode_reward=802.26 +/- 134.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=324000, episode_reward=799.91 +/- 164.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=325000, episode_reward=772.20 +/- 165.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=326000, episode_reward=767.62 +/- 192.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=327000, episode_reward=622.23 +/- 166.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=328000, episode_reward=659.98 +/- 247.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=329000, episode_reward=982.08 +/- 227.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=330000, episode_reward=756.70 +/- 175.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=331000, episode_reward=635.45 +/- 106.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=332000, episode_reward=665.69 +/- 163.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=333000, episode_reward=770.49 +/- 151.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=334000, episode_reward=869.00 +/- 229.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=335000, episode_reward=799.80 +/- 174.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=336000, episode_reward=780.90 +/- 133.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=337000, episode_reward=709.57 +/- 140.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=338000, episode_reward=627.71 +/- 222.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=339000, episode_reward=596.53 +/- 113.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=340000, episode_reward=929.24 +/- 124.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=341000, episode_reward=800.38 +/- 98.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=342000, episode_reward=769.69 +/- 221.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=343000, episode_reward=770.46 +/- 257.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=344000, episode_reward=669.52 +/- 298.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=345000, episode_reward=873.71 +/- 214.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=346000, episode_reward=654.99 +/- 84.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=347000, episode_reward=723.45 +/- 205.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=348000, episode_reward=682.57 +/- 216.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=349000, episode_reward=676.71 +/- 233.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=350000, episode_reward=726.48 +/- 228.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=351000, episode_reward=733.25 +/- 57.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=352000, episode_reward=760.93 +/- 193.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=353000, episode_reward=928.17 +/- 136.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=354000, episode_reward=753.08 +/- 44.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=355000, episode_reward=745.37 +/- 111.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=356000, episode_reward=764.03 +/- 175.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=357000, episode_reward=807.39 +/- 214.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=358000, episode_reward=753.62 +/- 368.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=359000, episode_reward=719.03 +/- 144.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=360000, episode_reward=709.85 +/- 209.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=361000, episode_reward=837.89 +/- 175.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=362000, episode_reward=554.92 +/- 288.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=363000, episode_reward=714.88 +/- 111.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=364000, episode_reward=856.81 +/- 55.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=365000, episode_reward=884.24 +/- 72.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=366000, episode_reward=662.48 +/- 136.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=367000, episode_reward=839.47 +/- 217.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=368000, episode_reward=770.24 +/- 286.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=369000, episode_reward=749.82 +/- 175.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=370000, episode_reward=831.18 +/- 142.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=371000, episode_reward=825.95 +/- 243.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=372000, episode_reward=814.81 +/- 150.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=373000, episode_reward=812.11 +/- 123.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=374000, episode_reward=700.31 +/- 148.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=375000, episode_reward=684.84 +/- 142.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=376000, episode_reward=800.62 +/- 172.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=377000, episode_reward=870.45 +/- 151.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=378000, episode_reward=755.80 +/- 202.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=379000, episode_reward=731.11 +/- 232.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=380000, episode_reward=850.33 +/- 156.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=381000, episode_reward=960.84 +/- 235.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=382000, episode_reward=702.93 +/- 211.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=383000, episode_reward=672.35 +/- 232.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=384000, episode_reward=852.61 +/- 153.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=385000, episode_reward=895.81 +/- 280.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=386000, episode_reward=816.07 +/- 302.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=387000, episode_reward=614.40 +/- 180.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=388000, episode_reward=518.61 +/- 253.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=389000, episode_reward=811.12 +/- 93.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=390000, episode_reward=921.18 +/- 106.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=391000, episode_reward=913.68 +/- 85.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=392000, episode_reward=797.63 +/- 141.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=393000, episode_reward=859.43 +/- 202.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=394000, episode_reward=858.36 +/- 102.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=395000, episode_reward=845.93 +/- 233.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=396000, episode_reward=830.94 +/- 139.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=397000, episode_reward=824.97 +/- 185.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=398000, episode_reward=775.20 +/- 312.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=399000, episode_reward=515.81 +/- 265.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=400000, episode_reward=861.56 +/- 232.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=401000, episode_reward=848.15 +/- 328.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=402000, episode_reward=650.00 +/- 292.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=403000, episode_reward=768.21 +/- 260.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=404000, episode_reward=805.99 +/- 89.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=405000, episode_reward=738.43 +/- 222.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=406000, episode_reward=817.11 +/- 112.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=407000, episode_reward=754.87 +/- 235.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=408000, episode_reward=879.39 +/- 176.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=409000, episode_reward=730.77 +/- 323.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=410000, episode_reward=616.83 +/- 171.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=411000, episode_reward=574.99 +/- 207.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=412000, episode_reward=697.77 +/- 263.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=413000, episode_reward=869.68 +/- 86.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=414000, episode_reward=807.97 +/- 177.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=415000, episode_reward=719.50 +/- 102.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=416000, episode_reward=704.90 +/- 191.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=417000, episode_reward=481.06 +/- 204.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=418000, episode_reward=812.87 +/- 192.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=419000, episode_reward=704.16 +/- 235.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=420000, episode_reward=816.79 +/- 88.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=421000, episode_reward=823.92 +/- 168.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=422000, episode_reward=665.91 +/- 127.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=423000, episode_reward=785.91 +/- 108.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=424000, episode_reward=690.03 +/- 176.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=425000, episode_reward=605.09 +/- 216.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=426000, episode_reward=718.47 +/- 121.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=427000, episode_reward=882.40 +/- 113.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=428000, episode_reward=805.54 +/- 162.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=429000, episode_reward=530.05 +/- 236.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=430000, episode_reward=784.13 +/- 141.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=431000, episode_reward=895.57 +/- 162.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=432000, episode_reward=706.90 +/- 184.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=433000, episode_reward=1020.77 +/- 139.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=434000, episode_reward=763.03 +/- 193.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=435000, episode_reward=779.73 +/- 184.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=436000, episode_reward=837.60 +/- 157.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=437000, episode_reward=741.41 +/- 262.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=438000, episode_reward=880.75 +/- 268.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=439000, episode_reward=712.77 +/- 227.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=440000, episode_reward=806.29 +/- 205.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=441000, episode_reward=612.67 +/- 191.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=442000, episode_reward=636.52 +/- 184.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=443000, episode_reward=569.14 +/- 282.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=444000, episode_reward=714.49 +/- 187.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=445000, episode_reward=889.98 +/- 140.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=446000, episode_reward=688.13 +/- 191.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=447000, episode_reward=726.27 +/- 190.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=448000, episode_reward=854.36 +/- 132.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=449000, episode_reward=603.12 +/- 125.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=450000, episode_reward=728.12 +/- 246.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=451000, episode_reward=669.87 +/- 82.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=452000, episode_reward=763.01 +/- 168.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=453000, episode_reward=728.44 +/- 213.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=454000, episode_reward=936.60 +/- 179.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=455000, episode_reward=636.02 +/- 122.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=456000, episode_reward=784.86 +/- 134.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=457000, episode_reward=893.03 +/- 293.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=458000, episode_reward=635.95 +/- 213.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=459000, episode_reward=748.57 +/- 251.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=460000, episode_reward=684.63 +/- 186.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=461000, episode_reward=591.72 +/- 182.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=462000, episode_reward=893.64 +/- 142.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=463000, episode_reward=843.99 +/- 141.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=464000, episode_reward=958.98 +/- 185.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=465000, episode_reward=922.26 +/- 112.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=466000, episode_reward=772.52 +/- 142.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=467000, episode_reward=675.97 +/- 156.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=468000, episode_reward=838.42 +/- 111.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=469000, episode_reward=557.82 +/- 244.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=470000, episode_reward=749.72 +/- 216.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=471000, episode_reward=851.65 +/- 124.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=472000, episode_reward=974.29 +/- 196.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=473000, episode_reward=868.81 +/- 166.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=474000, episode_reward=583.66 +/- 239.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=475000, episode_reward=813.68 +/- 195.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=476000, episode_reward=696.77 +/- 135.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=477000, episode_reward=728.54 +/- 146.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=478000, episode_reward=773.15 +/- 330.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=479000, episode_reward=885.81 +/- 112.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=480000, episode_reward=686.66 +/- 187.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=481000, episode_reward=882.72 +/- 245.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=482000, episode_reward=794.42 +/- 199.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=483000, episode_reward=875.43 +/- 185.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=484000, episode_reward=999.50 +/- 141.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=485000, episode_reward=827.00 +/- 339.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=486000, episode_reward=687.78 +/- 80.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=487000, episode_reward=758.58 +/- 239.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=488000, episode_reward=785.78 +/- 209.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=489000, episode_reward=685.73 +/- 186.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=490000, episode_reward=812.16 +/- 157.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=491000, episode_reward=843.22 +/- 138.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=492000, episode_reward=966.78 +/- 183.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=493000, episode_reward=846.89 +/- 179.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=494000, episode_reward=686.64 +/- 149.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=495000, episode_reward=814.51 +/- 192.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=496000, episode_reward=898.66 +/- 153.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=497000, episode_reward=771.84 +/- 180.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=498000, episode_reward=862.72 +/- 58.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=499000, episode_reward=943.87 +/- 199.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=500000, episode_reward=908.16 +/- 208.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=501000, episode_reward=805.86 +/- 243.33\n",
      "Episode length: 100.00 +/- 0.00\n"
     ]
    }
   ],
   "source": [
    "### REINFORCEMENT LEARNING I ###\n",
    "### TRAIN, SAVE, EVALUATE MODEL ###\n",
    "\n",
    "import gym\n",
    "import stable_baselines3 as sb\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "import pickle\n",
    "\n",
    "total_timesteps = 5e5\n",
    "for pn in [0]: #[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]:\n",
    "    for mn in [0]:\n",
    "        print('Process noise = ' + str(0.1*pn) + ', Measurement noise = ' + str(0.1*mn))\n",
    "        # Load diagnostics/prognostics model from disk and initiate environment\n",
    "        if prognostics:\n",
    "            print('Prognostics-based RL')\n",
    "            prog_model = pickle.load(open('prognostics/model_' + 'pn' + str(pn) + '_mn' + str(mn), 'rb'))\n",
    "            env = gym.make('Production-v0', prog_model = prog_model, process_noise = 0.1*pn)\n",
    "            # Callback for best model\n",
    "            best_callback = EvalCallback(env, best_model_save_path='./callback/prognostics/' + 'pn' + str(pn) + '_mn' + str(mn),\n",
    "                                        log_path='./callback/prognostics/' + 'pn' + str(pn) + '_mn' + str(mn), eval_freq=1000,\n",
    "                                        deterministic=True, render=False)\n",
    "            model = sb.PPO('MlpPolicy', env, tensorboard_log=\"./tensorboard/prognostics/\")\n",
    "            model.learn(total_timesteps=total_timesteps, tb_log_name='PPO_pn' + str(pn) + '_mn' + str(mn), callback = best_callback)\n",
    "        else:\n",
    "            print('Diagnostics-based RL')\n",
    "            diag_model = pickle.load(open('diagnostics/model_' + 'pn' + str(pn) + '_mn' + str(mn), 'rb'))\n",
    "            env = gym.make('Production-v0', diag_model = diag_model, process_noise = 0.1*pn, forecast = 0, prod_levels = 6)\n",
    "            # Callback for best model\n",
    "            best_callback = EvalCallback(env, best_model_save_path='./callback/fc0_l6_' + 'pn' + str(pn) + '_mn' + str(mn),\n",
    "                                    log_path='./callback/fc0_l6_' + 'pn' + str(pn) + '_mn' + str(mn), eval_freq=1000,\n",
    "                                    deterministic=True, render=False)\n",
    "            model = sb.PPO('MlpPolicy', env, tensorboard_log=\"./tensorboard/\")\n",
    "            model.learn(total_timesteps=total_timesteps, tb_log_name='DQN_fc0_l6_pn' + str(pn) + '_mn' + str(mn), callback = best_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process noise = 0.0, Measurement noise = 0.0\n",
      "(-482.6, 52.08245558727046)\n"
     ]
    }
   ],
   "source": [
    "### REINFORCEMENT LEARNING II ###\n",
    "### LOAD MODEL ###\n",
    "import gym\n",
    "import stable_baselines3 as sb\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3 import A2C\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "import pickle\n",
    "\n",
    "for pn in [0]: #[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]:\n",
    "    for mn in [0]:\n",
    "        print('Process noise = ' + str(0.1*pn) + ', Measurement noise = ' + str(0.1*mn))\n",
    "        if prognostics:\n",
    "            prog_model = pickle.load(open('prognostics/model_' + 'pn' + str(pn) + '_mn' + str(mn), 'rb'))\n",
    "            env = gym.make('Production-v0', prog_model = prog_model, process_noise = 0.1*pn)\n",
    "            model = PPO.load('./callback/prognostics/' + 'pn' + str(pn) + '_mn' + str(mn) + '/best_model', env = env)\n",
    "        else:\n",
    "            diag_model = pickle.load(open('diagnostics/model_' + 'pn' + str(pn) + '_mn' + str(mn), 'rb'))\n",
    "            env = gym.make('Production-v0', diag_model = diag_model, process_noise = 0.1*pn, forecast = 12)\n",
    "            model = PPO.load('./callback/fc1_l6_' + 'pn' + str(pn) + '_mn' + str(mn) + '/best_model', env = env)\n",
    "        \n",
    "        # Evaluate the agent\n",
    "        print(evaluate_policy(model, model.get_env(), n_eval_episodes=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process noise = 0.0, Measurement noise = 0.0\n",
      "The average number of reactive maintenance interventions per episode is:  0.0\n",
      "The average number of preventive maintenance interventions per episode is:  3.06\n",
      "The average sum of inventory per episode is:  189.84799999999996\n",
      "The average sum of spare parts inventory per episode is:  3.18\n",
      "The average reward per episode is:  769.7730000000004\n",
      "The average upper bound per episode is:  2689.9879999999994\n"
     ]
    }
   ],
   "source": [
    "### REINFORCEMENT LEARNING III ###\n",
    "### TRY AND EVALUATE MY MODEL ###\n",
    "import pandas as pd\n",
    "from stable_baselines3 import PPO, DQN\n",
    "import gym\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "for pn in [0]:#[5, 6, 7, 8, 9, 10]: #[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]:\n",
    "    for mn in [0]:\n",
    "        print('Process noise = ' + str(0.1*pn) + ', Measurement noise = ' + str(0.1*mn))\n",
    "        if prognostics:\n",
    "            prog_model = pickle.load(open('prognostics/model_' + 'pn' + str(pn) + '_mn' + str(mn), 'rb'))\n",
    "            env = gym.make('Production-v0', prog_model = prog_model, process_noise = 0.1*pn)\n",
    "            model = PPO.load('./callback/prognostics/' + 'pn' + str(pn) + '_mn' + str(mn) + '/best_model', env = env)\n",
    "        else:\n",
    "            diag_model = pickle.load(open('diagnostics/model_' + 'pn' + str(pn) + '_mn' + str(mn), 'rb'))\n",
    "            env = gym.make('Production-v0', diag_model = diag_model, process_noise = 0.1*pn, forecast = 0, prod_levels = 6)\n",
    "            model = PPO.load('./callback/fc0_l6_' + 'pn' + str(pn) + '_mn' + str(mn) + '/best_model', env = env)\n",
    "        # Initilaize Reward\n",
    "        result_df = pd.DataFrame(np.nan, index=range(0,100), columns=['RM', 'PM', 'Inventory', 'Spare Parts Inventory', 'Reward', 'Upper'])\n",
    "        # Set iterations\n",
    "        iterations = 100\n",
    "        for i in range(iterations):\n",
    "            # Initialize episode\n",
    "            store = []\n",
    "            obs = env.reset()\n",
    "            done = False\n",
    "            store.append([0, obs[0], env.breakdown, obs[1], obs[2], 0, done, env.old_order])\n",
    "            # Compute one episode\n",
    "            while not done:\n",
    "                # Get best action for state\n",
    "                action, _state = model.predict(obs, deterministic=True)\n",
    "                # Compute next state\n",
    "                #action = random.choice([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
    "                obs, reward, done, info = env.step(action)\n",
    "                # Store results of this episode\n",
    "                store.append([action, obs[0], env.breakdown, obs[1], obs[2], reward, done, env.old_order])\n",
    "            eps_df = pd.DataFrame(store, columns=['action', 'health_rul', 'breakdown', 'inventory', 'sp_inventory', 'reward', 'done', 'old_order'])\n",
    "            # Calculate nr. of reactive maintenance interventions by counting health 'resets' and substracting PM actions\n",
    "            result_df.iloc[i]['RM'] = sum(eps_df['breakdown']==True)\n",
    "            # Calculate nr. of preventive maintenance interventions\n",
    "            result_df.iloc[i]['PM'] = sum(eps_df['action']== env.actions-1)\n",
    "            # Calculate inventory\n",
    "            result_df.iloc[i]['Inventory'] = sum(eps_df['inventory'])\n",
    "            # Calculate spare parts inventory per period\n",
    "            result_df.iloc[i]['Spare Parts Inventory'] = sum(eps_df['sp_inventory'])\n",
    "            # Calculate reward\n",
    "            result_df.iloc[i]['Reward'] = sum(eps_df['reward'])\n",
    "            # Calculate reward with no costs and fulfillment of all orders\n",
    "            result_df.iloc[i]['Upper'] = sum(eps_df['old_order']) * env.order_r\n",
    "\n",
    "        print(\"The average number of reactive maintenance interventions per episode is: \", result_df['RM'].mean())\n",
    "        print(\"The average number of preventive maintenance interventions per episode is: \", result_df['PM'].mean())\n",
    "        print(\"The average sum of inventory per episode is: \", result_df['Inventory'].mean())\n",
    "        print(\"The average sum of spare parts inventory per episode is: \", result_df['Spare Parts Inventory'].mean())\n",
    "        print(\"The average reward per episode is: \", result_df['Reward'].mean())\n",
    "        print(\"The average upper bound per episode is: \", result_df['Upper'].mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average number of reactive maintenance interventions per episode is:  2.98\n",
      "The average number of preventive maintenance interventions per episode is:  0.0\n",
      "The average mean time between failure per episode is:  29.190000000000012\n",
      "The standard deviation of the mean time between failure per episode is:  2.881718957955041\n",
      "The average sum of inventory per episode is:  225.2379\n",
      "The average sum of spare parts inventory per episode is:  0.0\n",
      "The average reward per episode is:  -1532.396\n",
      "The average upper bound per episode is:  2740.876\n"
     ]
    }
   ],
   "source": [
    "### REINFORCEMENT LEARNING IIIa ###\n",
    "### EVALUATE REACTIVE MODEL ###\n",
    "\n",
    "import pandas as pd\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3 import PPO\n",
    "import numpy as np\n",
    "import math\n",
    "import gym\n",
    "\n",
    "env = gym.make('Production-v0', reactive_mode = True, forecast = 0, prod_levels = 6)\n",
    "# Initialize Reward\n",
    "# Set iterations\n",
    "iterations = 100\n",
    "result_df = pd.DataFrame(np.nan, index=range(0,100), columns=['RM', 'PM', 'MTBF', 'Inventory', 'Spare Parts Inventory', 'Reward', 'Upper'])\n",
    "\n",
    "for i in range(iterations):\n",
    "    # Initialize episode\n",
    "    store = []\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    store.append([0, env.true_health, env.breakdown, obs[0], obs[1], 0, done, env.old_order])\n",
    "    # Compute one episode\n",
    "    while not done:\n",
    "        # Get best action for state\n",
    "        action = min(env.prod_levels-1, max(0, round(env.old_order)))\n",
    "        # Compute next state\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        # Store results of this episode\n",
    "        store.append([action, env.true_health, env.breakdown, obs[0], obs[1], reward, done, env.old_order])\n",
    "    eps_df = pd.DataFrame(store, columns=['action', 'health', 'breakdown', 'inventory', 'sp_inventory', 'reward', 'done', 'old_order'])\n",
    "    # Calculate nr. of reactive maintenance interventions by counting health 'resets' and substracting PM actions\n",
    "    result_df.iloc[i]['RM'] = sum(eps_df['breakdown']==True)\n",
    "    # Calculate nr. of preventive maintenance interventions\n",
    "    result_df.iloc[i]['PM'] = sum(eps_df['action']== env.actions-1)\n",
    "    # Calculate mean time between failures\n",
    "    # Cut df after last breakdown\n",
    "    eps_df_trim = eps_df.iloc[:(np.where(eps_df['breakdown'].eq(True), eps_df.index, 0).max()+1)]\n",
    "    # Calculate MTBF by dividing periods where machine is running / breakdowns\n",
    "    result_df.iloc[i]['MTBF'] = (len(eps_df_trim) - sum(eps_df_trim['breakdown'] == True)) / sum(eps_df_trim['breakdown'] == True)\n",
    "    # Calculate inventory\n",
    "    result_df.iloc[i]['Inventory'] = sum(eps_df['inventory'])\n",
    "    # Calculate spare parts inventory per period\n",
    "    result_df.iloc[i]['Spare Parts Inventory'] = sum(eps_df['sp_inventory'])\n",
    "    # Calculate reward\n",
    "    result_df.iloc[i]['Reward'] = sum(eps_df['reward'])\n",
    "    # Calculate reward with no costs and fulfillment of all orders\n",
    "    result_df.iloc[i]['Upper'] = sum(eps_df['old_order']) * env.order_r\n",
    "\n",
    "print(\"The average number of reactive maintenance interventions per episode is: \", result_df['RM'].mean())\n",
    "print(\"The average number of preventive maintenance interventions per episode is: \", result_df['PM'].mean())\n",
    "print(\"The average mean time between failure per episode is: \", result_df['MTBF'].mean())\n",
    "print(\"The standard deviation of the mean time between failure per episode is: \", result_df['MTBF'].std())\n",
    "print(\"The average sum of inventory per episode is: \", result_df['Inventory'].mean())\n",
    "print(\"The average sum of spare parts inventory per episode is: \", result_df['Spare Parts Inventory'].mean())\n",
    "print(\"The average reward per episode is: \", result_df['Reward'].mean())\n",
    "print(\"The average upper bound per episode is: \", result_df['Upper'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Maintenance Interval:  21 Coefficient:  1.05\n",
      "The average number of reactive maintenance interventions per episode is:  0.01\n",
      "The average number of preventive maintenance interventions per episode is:  3.99\n",
      "The average sum of inventory per episode is:  225.05169999999995\n",
      "The average sum of spare parts inventory per episode is:  3.99\n",
      "The average reward per episode is:  670.2695000000001\n",
      "The average upper bound per episode is:  2723.029000000001\n"
     ]
    }
   ],
   "source": [
    "### REINFORCEMENT LEARNING IIIb ###\n",
    "### EVALUATE TIME-BASED PREVENTIVE MODEL ###\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gym\n",
    "import math\n",
    "\n",
    "env = gym.make('Production-v0', forecast = 0, prod_levels = 6)\n",
    "interval = range (21, 22)\n",
    "# Set iterations\n",
    "iterations = 100\n",
    "\n",
    "for k in interval:\n",
    "    # Initilaize Reward\n",
    "    result_df = pd.DataFrame(np.nan, index=range(0,100), columns=['RM', 'PM', 'Inventory', 'Spare Parts Inventory', 'Reward', 'Upper'])\n",
    "\n",
    "    for i in range(iterations):\n",
    "        # Initialize episode\n",
    "        store = []\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        store.append([0, obs[0], env.breakdown, obs[1], obs[2], 0, done, env.old_order])\n",
    "        # Compute one episode\n",
    "        while not done:\n",
    "            # One period before maintenance: action = order + spare part order\n",
    "            if env.scheduled_maintenance_counter == k-1:\n",
    "                action = min(env.prod_levels-1, max(0, round(env.old_order))) + env.prod_levels\n",
    "            # At period of mtbf: maintain\n",
    "            elif env.scheduled_maintenance_counter == k:\n",
    "                action = env.actions-1\n",
    "            # Else: action = order    \n",
    "            else:             \n",
    "                action = min(env.prod_levels-1, max(0, round(env.old_order)))\n",
    "            # Compute next state\n",
    "            obs, reward, done, info = env.step(action)\n",
    "            # Store results of this episode\n",
    "            store.append([action, obs[0], env.breakdown, obs[1], obs[2], reward, done, env.old_order])\n",
    "        eps_df = pd.DataFrame(store, columns=['action', 'health', 'breakdown', 'inventory', 'sp_inventory', 'reward', 'done', 'old_order'])\n",
    "        # Calculate nr. of reactive maintenance interventions by counting health 'resets' and substracting PM actions\n",
    "        result_df.iloc[i]['RM'] = sum(eps_df['breakdown']==True)\n",
    "        # Calculate nr. of preventive maintenance interventions\n",
    "        result_df.iloc[i]['PM'] = sum(eps_df['action']==env.actions-1)\n",
    "        # Calculate inventory\n",
    "        result_df.iloc[i]['Inventory'] = sum(eps_df['inventory'])\n",
    "        # Calculate spare parts inventory per period\n",
    "        result_df.iloc[i]['Spare Parts Inventory'] = sum(eps_df['sp_inventory'])\n",
    "        # Calculate reward\n",
    "        result_df.iloc[i]['Reward'] = sum(eps_df['reward'])\n",
    "        # Calculate reward with no costs and fulfillment of all orders\n",
    "        result_df.iloc[i]['Upper'] = sum(eps_df['old_order']) * env.order_r\n",
    "\n",
    "    print(\"\\n\", \"Maintenance Interval: \", k, \"Coefficient: \", 0+0.05*k)\n",
    "    print(\"The average number of reactive maintenance interventions per episode is: \", result_df['RM'].mean())\n",
    "    print(\"The average number of preventive maintenance interventions per episode is: \", result_df['PM'].mean())\n",
    "    print(\"The average sum of inventory per episode is: \", result_df['Inventory'].mean())\n",
    "    print(\"The average sum of spare parts inventory per episode is: \", result_df['Spare Parts Inventory'].mean())\n",
    "    print(\"The average reward per episode is: \", result_df['Reward'].mean())\n",
    "    print(\"The average upper bound per episode is: \", result_df['Upper'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "### REINFORCEMENT LEARNING IV ###\n",
    "### VISUALIZE STATE-ACTION ###\n",
    "import numpy as np\n",
    "state_action = []\n",
    "\n",
    "# Define observation grid\n",
    "grid_health = np.arange(0.0, 1.01, 0.01)\n",
    "grid_order = range(0, 5)\n",
    "grid_inventory = range(0, 10)\n",
    "grid_sp_inventory = [0, 1]\n",
    "\n",
    "# Loop through grid and store best action for each state\n",
    "for hlt in grid_health:\n",
    "    for ord in grid_order:\n",
    "        for inv in grid_inventory:\n",
    "            for sin in grid_sp_inventory:\n",
    "                # Predict\n",
    "                action, _state = model.predict((hlt, ord, inv, sin), deterministic=True)\n",
    "                state_action.append([hlt, ord, inv, sin, action])\n",
    "\n",
    "state_action_df = pd.DataFrame(state_action, columns=['health', 'order', 'inventory', 'sp_inventory', 'action'])\n",
    "state_action_df.to_excel(\"visuals/state_action.xlsx\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### REINFORCEMENT LEARNING IVa ###\n",
    "### VISUALIZE STATE-VALUE ###\n",
    "import numpy as np\n",
    "state_action = []\n",
    "\n",
    "# Define observation grid\n",
    "grid_health = np.arange(0.0, 1.01, 0.01)\n",
    "grid_order = range(0, 5)\n",
    "grid_inventory = range(0, 10)\n",
    "grid_sp_inventory = [0, 1]\n",
    "\n",
    "# Loop through grid and store best action for each state\n",
    "for hlt in grid_health:\n",
    "    for ord in grid_order:\n",
    "        for inv in grid_inventory:\n",
    "            for sin in grid_sp_inventory:\n",
    "                # Predict\n",
    "                obs, _ = model.policy.obs_to_tensor(hlt, ord, inv, sin)\n",
    "                value = model.policy.predict_values(obs).item()\n",
    "                state_action.append([hlt, ord, inv, sin, value])\n",
    "\n",
    "state_action_df = pd.DataFrame(state_action, columns=['health', 'order', 'inventory', 'sp_inventory', 'value'])\n",
    "state_action_df.to_excel(\"visuals/state_value.xlsx\") \n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6f69c5940b32a5cbabe45c9825076a627c6cdb9ede58cf4d0fa74ca6057ffe74"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
