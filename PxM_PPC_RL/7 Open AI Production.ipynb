{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10000\n"
     ]
    }
   ],
   "source": [
    "### DATA-DRIVEN PROGNOSTICS I ###\n",
    "### GENERATE DATA FOR DATA-DRIVEN MODEL ###\n",
    "\n",
    "import random\n",
    "from prog_models.models import BatteryCircuit\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "# Ignore warnings when machine exceeds its end of life\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\"\"\" Method that uses a physical machine model from the prog_models package and a current (health) state of the model and\n",
    "an action (i.e., intensity), which is performed for 100 time steps\n",
    "    Parameter:\n",
    "        machine             machine model from the prog_models package\n",
    "        state               current (health) state of the model\n",
    "        action              loading of the machine for the next 100 time steps\n",
    "    Return:\n",
    "        health                               \n",
    "    \"\"\"\n",
    "def produce_model(machine, states, action):\n",
    "        \n",
    "        # Define load of battery\n",
    "        def future_loading(t, x=None):\n",
    "            return {'i': action}\n",
    "\n",
    "        # Set current state of machine\n",
    "        machine.parameters['x0'] = states\n",
    "        # Simulate 100 steps\n",
    "        options = {\n",
    "            'save_freq': 100,  # Frequency at which results are saved\n",
    "            'dt': 2  # Timestep\n",
    "        }\n",
    "        (_, _, states, outputs, event_states) = machine.simulate_to(100, future_loading, **options)\n",
    "        health = event_states[-1]['EOD']\n",
    "        return(round(health, 2), states[-1], outputs[-1]['t'], outputs[-1]['v'])\n",
    "def reset_states(machine):\n",
    "    # Returns initial states of machine, e.g., {'tb': 18.95, 'qb': 7856.3254, 'qcp': 0, 'qcs': 0} for Battery\n",
    "    return(machine.default_parameters['x0'])\n",
    "\n",
    "battery = BatteryCircuit()\n",
    "states = reset_states(battery)\n",
    "reset_counter = 0\n",
    "dataset = []\n",
    "for i in range(int(1e4)):\n",
    "    # If asset failed last period, reset all historical values\n",
    "    if reset_counter == 0: t = v = t_1 = v_1 = t_2 = v_2 = t_3 = v_3 = 0 \n",
    "    # Shift history by one time period\n",
    "    v_3 = v_2\n",
    "    t_3 = t_2\n",
    "    v_2 = v_1\n",
    "    t_2 = t_1\n",
    "    v_1 = v\n",
    "    t_1 = t\n",
    "\n",
    "    # Increment reset_counter\n",
    "    reset_counter = reset_counter + 1\n",
    "    # Compute new health, states, t, and v using last battery state and a random new action\n",
    "    health, states, t, v = produce_model(machine=battery, states=states, action=random.sample((0, 1, 2, 3, 4), 1)[0])\n",
    "    \n",
    "    if health <= 0: \n",
    "        # Reset battery states to initialize battery for next produce_model call\n",
    "        states = reset_states(battery)\n",
    "        # Initialize reset_counter\n",
    "        reset_counter = 0\n",
    "        # Sometimes produce_model returns weird or negative values as the end of life is exceeded\n",
    "        # Here, we just simply set it to zero to not confuse a later learner \n",
    "        health = 0\n",
    "\n",
    "    # append to two-dimensional list\n",
    "    dataset.append([t, v, t_1, v_1, t_2, v_2, t_3, v_3, health])\n",
    "\n",
    "    # print progress every 10,000 iterations\n",
    "    if (i+1) % 10000 == 0: print(\"Iteration\", i+1)\n",
    "# Transform two-dim list to dataframe\n",
    "dataset = pd.DataFrame(dataset, columns=['t', 'v', 't_1', 'v_1', 't_2', 'v_2', 't_3', 'v_3', 'health'])\n",
    "# Save it as pickle\n",
    "dataset.to_pickle('diagnostics/data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestRegressor() : [0.92981457 0.94752602 0.94335423 0.94618864 0.93738079]\n"
     ]
    }
   ],
   "source": [
    "### DATA-DRIVEN PROGNOSTICS II ###\n",
    "### FIT AND TEST MODEL ###\n",
    "from sklearn import tree, linear_model, kernel_ridge, svm, neighbors, gaussian_process, ensemble, neural_network\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import pickle\n",
    "\n",
    "dataset = pd.read_pickle('diagnostics/data')\n",
    "X = dataset[['t', 'v', 't_1', 'v_1', 't_2', 'v_2', 't_3', 'v_3']]\n",
    "y = dataset['health']\n",
    "#learner = [linear_model.LinearRegression(), linear_model.Ridge(), linear_model.Lasso(), linear_model.BayesianRidge(), tree.DecisionTreeRegressor(), # Fast\n",
    "#        kernel_ridge.KernelRidge(), svm.SVR(), neighbors.KNeighborsRegressor(), gaussian_process.GaussianProcessRegressor(), # Slow\n",
    "#        ensemble.RandomForestRegressor(), neural_network.MLPRegressor()] # Slow\n",
    "learner = [ensemble.RandomForestRegressor()]\n",
    "for i in learner:\n",
    "    reg = i\n",
    "    print(i, \":\", cross_val_score(reg, X, y, cv=5)) # default scoring R2\n",
    "# Fit on all data\n",
    "model = learner[0].fit(X, y)\n",
    "pickle.dump(model, open('diagnostics/model', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DATA-DRIVEN PROGNOSTICS IIIa ###\n",
    "### FIT, TEST, VISUALIZE MODEL USING TRAIN AND TEST SETS ###\n",
    "\n",
    "# Find index of healthy machines\n",
    "index_df = X.index[(X['t_1'] == 0) & (X['v_1'] == 0) & (X['t_2'] == 0) & (X['v_2'] == 0) & (X['t_3'] == 0) & (X['v_3'] == 0)].tolist()\n",
    "index_test = round(len(index_df)*0.8)\n",
    "\n",
    "# Create train and test set without disrupting machine runs to-failure\n",
    "X_train = X.iloc[0:(index_df[index_test])]\n",
    "y_train = y.iloc[0:(index_df[index_test])]\n",
    "X_test = X.iloc[index_df[index_test]:(len(X))]\n",
    "y_test = y.iloc[index_df[index_test]:(len(y))]\n",
    "\n",
    "## Train\n",
    "learner = [ensemble.RandomForestRegressor()]\n",
    "model = learner[0].fit(X_train, y_train)\n",
    "## Predict\n",
    "y_pred = pd.DataFrame(model.predict(X_test), columns=['Pred'])\n",
    "## Analyze\n",
    "#reset index of each DataFrame\n",
    "X_test.reset_index(drop=True, inplace=True)\n",
    "y_test.reset_index(drop=True, inplace=True)\n",
    "# Concat dataframes\n",
    "test_df = pd.concat([X_test, y_test, y_pred], axis=1)\n",
    "# Print for visualization (e.g., in R)\n",
    "test_df.to_excel(\"diagnostics/test_results.xlsx\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=1000, episode_reward=-2777.40 +/- 31.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=-2782.20 +/- 28.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=3000, episode_reward=-4101.60 +/- 50.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=4000, episode_reward=-4102.00 +/- 73.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=5000, episode_reward=-4123.00 +/- 40.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=6000, episode_reward=-4155.00 +/- 31.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=7000, episode_reward=-4152.80 +/- 37.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=8000, episode_reward=-4139.40 +/- 22.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=9000, episode_reward=-4152.40 +/- 23.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=10000, episode_reward=-4137.80 +/- 55.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=11000, episode_reward=-4140.00 +/- 27.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=12000, episode_reward=-4121.80 +/- 46.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=13000, episode_reward=-4133.00 +/- 40.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=14000, episode_reward=-4113.40 +/- 52.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=15000, episode_reward=-4096.20 +/- 41.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=16000, episode_reward=-4153.20 +/- 30.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=17000, episode_reward=-4145.00 +/- 46.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=18000, episode_reward=-4132.00 +/- 39.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=19000, episode_reward=-4132.20 +/- 36.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=20000, episode_reward=-4154.40 +/- 28.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=21000, episode_reward=-4136.80 +/- 26.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=22000, episode_reward=-4123.80 +/- 46.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=23000, episode_reward=-4148.60 +/- 8.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=24000, episode_reward=-4171.20 +/- 21.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=25000, episode_reward=-4135.80 +/- 65.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=26000, episode_reward=-4137.60 +/- 47.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=27000, episode_reward=-4099.60 +/- 52.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=28000, episode_reward=-4152.00 +/- 24.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=29000, episode_reward=-4141.60 +/- 49.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=30000, episode_reward=-4144.20 +/- 35.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=31000, episode_reward=-4149.60 +/- 19.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=32000, episode_reward=-4114.40 +/- 44.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=33000, episode_reward=-4141.80 +/- 39.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=34000, episode_reward=-4128.00 +/- 39.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=35000, episode_reward=-4137.80 +/- 44.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=36000, episode_reward=-4176.00 +/- 40.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=37000, episode_reward=-4119.80 +/- 43.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=38000, episode_reward=-4138.00 +/- 60.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=39000, episode_reward=-4153.20 +/- 35.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=40000, episode_reward=-4141.20 +/- 38.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=41000, episode_reward=-4129.00 +/- 32.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=42000, episode_reward=-4146.80 +/- 56.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=43000, episode_reward=-4117.00 +/- 48.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=44000, episode_reward=-4176.40 +/- 31.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=45000, episode_reward=-4149.20 +/- 21.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=46000, episode_reward=-4130.60 +/- 50.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=47000, episode_reward=-4100.80 +/- 52.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=48000, episode_reward=-4091.40 +/- 42.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=49000, episode_reward=-4099.00 +/- 42.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=50000, episode_reward=-4091.40 +/- 45.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=51000, episode_reward=-4127.20 +/- 40.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=52000, episode_reward=-4160.60 +/- 16.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=53000, episode_reward=-4136.00 +/- 43.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=54000, episode_reward=-4125.80 +/- 38.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=55000, episode_reward=-4140.60 +/- 37.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=56000, episode_reward=-4146.40 +/- 28.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=57000, episode_reward=-4174.60 +/- 43.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=58000, episode_reward=-4120.40 +/- 49.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=59000, episode_reward=-4124.80 +/- 38.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=60000, episode_reward=-4073.40 +/- 66.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=61000, episode_reward=-4077.40 +/- 49.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=62000, episode_reward=-4124.80 +/- 22.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=63000, episode_reward=-4117.80 +/- 39.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=64000, episode_reward=-4101.80 +/- 51.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=65000, episode_reward=-4171.80 +/- 31.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=66000, episode_reward=-4165.20 +/- 53.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=67000, episode_reward=-4133.60 +/- 29.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=68000, episode_reward=-4111.40 +/- 50.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=69000, episode_reward=-4139.00 +/- 48.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=70000, episode_reward=-4126.00 +/- 58.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=71000, episode_reward=-4145.40 +/- 32.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=72000, episode_reward=-4115.40 +/- 44.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=73000, episode_reward=-4158.20 +/- 32.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=74000, episode_reward=-4161.80 +/- 27.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=75000, episode_reward=-4148.80 +/- 53.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=76000, episode_reward=-4140.20 +/- 56.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=77000, episode_reward=-4150.80 +/- 35.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=78000, episode_reward=-4163.20 +/- 30.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=79000, episode_reward=-4133.20 +/- 53.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=80000, episode_reward=-4155.80 +/- 34.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=81000, episode_reward=-4162.80 +/- 16.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=82000, episode_reward=-4154.00 +/- 22.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=83000, episode_reward=-4124.00 +/- 49.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=84000, episode_reward=-4146.20 +/- 36.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=85000, episode_reward=-4106.20 +/- 36.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=86000, episode_reward=-4117.40 +/- 66.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=87000, episode_reward=-4137.20 +/- 40.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=88000, episode_reward=-4117.20 +/- 28.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=89000, episode_reward=-4109.60 +/- 39.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=90000, episode_reward=-4152.40 +/- 42.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=91000, episode_reward=-4123.40 +/- 49.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=92000, episode_reward=-4121.20 +/- 36.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=93000, episode_reward=-4096.60 +/- 73.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=94000, episode_reward=-4149.40 +/- 27.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=95000, episode_reward=-4084.20 +/- 43.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=96000, episode_reward=-4124.40 +/- 72.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=97000, episode_reward=-4122.00 +/- 54.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=98000, episode_reward=-4086.60 +/- 72.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=99000, episode_reward=-4164.60 +/- 48.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=100000, episode_reward=-4106.00 +/- 60.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=101000, episode_reward=-4140.20 +/- 42.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=102000, episode_reward=-4125.00 +/- 21.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=103000, episode_reward=-4133.20 +/- 70.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=104000, episode_reward=-4142.80 +/- 29.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=105000, episode_reward=-4101.80 +/- 56.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=106000, episode_reward=-4111.00 +/- 52.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=107000, episode_reward=-4148.40 +/- 22.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=108000, episode_reward=-4106.80 +/- 44.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=109000, episode_reward=-4080.60 +/- 81.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=110000, episode_reward=-4124.00 +/- 64.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=111000, episode_reward=-4146.20 +/- 52.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=112000, episode_reward=-4133.80 +/- 57.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=113000, episode_reward=-4113.80 +/- 31.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=114000, episode_reward=-4142.40 +/- 38.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=115000, episode_reward=-4103.20 +/- 31.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=116000, episode_reward=-4113.80 +/- 31.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=117000, episode_reward=-4144.20 +/- 29.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=118000, episode_reward=-4157.00 +/- 22.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=119000, episode_reward=-4136.40 +/- 45.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=120000, episode_reward=-4146.20 +/- 33.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=121000, episode_reward=-4128.20 +/- 23.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=122000, episode_reward=-4102.60 +/- 45.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=123000, episode_reward=-4128.00 +/- 26.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=124000, episode_reward=-4115.40 +/- 20.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=125000, episode_reward=-4128.80 +/- 48.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=126000, episode_reward=-4146.00 +/- 41.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=127000, episode_reward=-4127.20 +/- 29.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=128000, episode_reward=-4122.20 +/- 55.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=129000, episode_reward=-4144.80 +/- 39.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=130000, episode_reward=-4156.40 +/- 57.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=131000, episode_reward=-4180.40 +/- 13.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=132000, episode_reward=-4131.40 +/- 15.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=133000, episode_reward=-4133.00 +/- 47.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=134000, episode_reward=-4139.20 +/- 24.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=135000, episode_reward=-4133.80 +/- 32.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=136000, episode_reward=-4131.80 +/- 64.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=137000, episode_reward=-4127.80 +/- 50.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=138000, episode_reward=-4109.20 +/- 62.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=139000, episode_reward=-4153.60 +/- 16.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=140000, episode_reward=-4130.60 +/- 62.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=141000, episode_reward=-4151.20 +/- 61.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=142000, episode_reward=-4158.60 +/- 36.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=143000, episode_reward=-4122.40 +/- 48.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=144000, episode_reward=-4117.40 +/- 37.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=145000, episode_reward=-4121.60 +/- 27.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=146000, episode_reward=-4116.40 +/- 25.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=147000, episode_reward=-4168.00 +/- 38.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=148000, episode_reward=-4114.00 +/- 67.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=149000, episode_reward=-4120.80 +/- 51.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=150000, episode_reward=-4095.60 +/- 41.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=151000, episode_reward=-4117.80 +/- 27.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=152000, episode_reward=-4146.20 +/- 51.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=153000, episode_reward=-4179.20 +/- 18.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=154000, episode_reward=-4137.20 +/- 14.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=155000, episode_reward=-4129.00 +/- 74.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=156000, episode_reward=-4164.80 +/- 54.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=157000, episode_reward=-4151.60 +/- 27.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=158000, episode_reward=-4132.60 +/- 29.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=159000, episode_reward=-4113.60 +/- 54.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=160000, episode_reward=-4127.00 +/- 45.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=161000, episode_reward=-4162.60 +/- 48.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=162000, episode_reward=-4123.20 +/- 25.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=163000, episode_reward=-4115.60 +/- 18.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=164000, episode_reward=-4113.40 +/- 15.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=165000, episode_reward=-4152.40 +/- 31.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=166000, episode_reward=-4112.40 +/- 48.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=167000, episode_reward=-4163.80 +/- 22.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=168000, episode_reward=-4153.00 +/- 38.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=169000, episode_reward=-4114.00 +/- 72.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=170000, episode_reward=-4154.60 +/- 13.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=171000, episode_reward=-4153.80 +/- 33.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=172000, episode_reward=-4109.80 +/- 44.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=173000, episode_reward=-4106.80 +/- 56.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=174000, episode_reward=-4104.00 +/- 43.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=175000, episode_reward=-4148.00 +/- 52.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=176000, episode_reward=-4094.60 +/- 75.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=177000, episode_reward=-4155.20 +/- 9.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=178000, episode_reward=-4099.60 +/- 28.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=179000, episode_reward=-4134.20 +/- 36.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=180000, episode_reward=-4098.60 +/- 87.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=181000, episode_reward=-4173.00 +/- 50.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=182000, episode_reward=-4109.20 +/- 81.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=183000, episode_reward=-4136.40 +/- 39.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=184000, episode_reward=-4116.80 +/- 9.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=185000, episode_reward=-4166.00 +/- 9.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=186000, episode_reward=-4151.00 +/- 31.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=187000, episode_reward=-4163.40 +/- 37.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=188000, episode_reward=-4139.20 +/- 44.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=189000, episode_reward=-4112.40 +/- 35.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=190000, episode_reward=-4127.80 +/- 49.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=191000, episode_reward=-4106.00 +/- 38.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=192000, episode_reward=-4140.40 +/- 31.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=193000, episode_reward=-4159.40 +/- 73.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=194000, episode_reward=-4136.40 +/- 42.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=195000, episode_reward=-4105.80 +/- 66.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=196000, episode_reward=-4092.00 +/- 37.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=197000, episode_reward=-4087.20 +/- 76.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=198000, episode_reward=-4144.20 +/- 34.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=199000, episode_reward=-4144.00 +/- 38.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=200000, episode_reward=-4139.80 +/- 44.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=201000, episode_reward=-4137.60 +/- 28.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=202000, episode_reward=-4111.00 +/- 75.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=203000, episode_reward=-4133.80 +/- 46.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=204000, episode_reward=-4080.20 +/- 37.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=205000, episode_reward=-4142.80 +/- 29.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=206000, episode_reward=-4137.60 +/- 44.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=207000, episode_reward=-4100.00 +/- 46.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=208000, episode_reward=-4150.00 +/- 41.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=209000, episode_reward=-4140.80 +/- 30.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=210000, episode_reward=-4130.80 +/- 31.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=211000, episode_reward=-4128.60 +/- 58.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=212000, episode_reward=-4167.00 +/- 25.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=213000, episode_reward=-4146.40 +/- 35.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=214000, episode_reward=-4157.40 +/- 47.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=215000, episode_reward=-4094.80 +/- 30.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=216000, episode_reward=-4086.20 +/- 56.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=217000, episode_reward=-4141.80 +/- 30.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=218000, episode_reward=-4086.60 +/- 63.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=219000, episode_reward=-4117.40 +/- 32.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=220000, episode_reward=-4117.40 +/- 29.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=221000, episode_reward=-4122.80 +/- 47.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=222000, episode_reward=-4166.20 +/- 29.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=223000, episode_reward=-4096.40 +/- 46.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=224000, episode_reward=-4167.60 +/- 20.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=225000, episode_reward=-4144.20 +/- 6.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=226000, episode_reward=-4070.00 +/- 52.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=227000, episode_reward=-4154.80 +/- 18.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=228000, episode_reward=-4100.60 +/- 30.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=229000, episode_reward=-4135.80 +/- 51.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=230000, episode_reward=-4110.80 +/- 74.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=231000, episode_reward=-4115.00 +/- 31.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=232000, episode_reward=-4140.60 +/- 34.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=233000, episode_reward=-4125.20 +/- 22.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=234000, episode_reward=-4145.40 +/- 12.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=235000, episode_reward=-4129.20 +/- 41.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=236000, episode_reward=-4165.00 +/- 45.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=237000, episode_reward=-4163.60 +/- 24.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=238000, episode_reward=-4117.80 +/- 52.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=239000, episode_reward=-4130.80 +/- 30.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=240000, episode_reward=-4125.40 +/- 27.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=241000, episode_reward=-4115.20 +/- 53.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=242000, episode_reward=-4147.20 +/- 35.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=243000, episode_reward=-4093.60 +/- 70.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=244000, episode_reward=-4133.80 +/- 70.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=245000, episode_reward=-4103.60 +/- 49.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=246000, episode_reward=-4114.00 +/- 51.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=247000, episode_reward=-4139.00 +/- 74.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=248000, episode_reward=-4143.20 +/- 52.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=249000, episode_reward=-4132.40 +/- 30.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=250000, episode_reward=-4113.00 +/- 45.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=251000, episode_reward=-4077.20 +/- 45.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=252000, episode_reward=-4136.20 +/- 31.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=253000, episode_reward=-4140.40 +/- 62.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=254000, episode_reward=-4097.00 +/- 25.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=255000, episode_reward=-4118.80 +/- 42.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=256000, episode_reward=-4127.00 +/- 14.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=257000, episode_reward=-4074.20 +/- 62.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=258000, episode_reward=-4120.00 +/- 37.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=259000, episode_reward=-4135.20 +/- 22.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=260000, episode_reward=-4136.80 +/- 37.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=261000, episode_reward=-4123.80 +/- 47.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=262000, episode_reward=-4129.60 +/- 49.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=263000, episode_reward=-4149.80 +/- 12.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=264000, episode_reward=-4138.60 +/- 44.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=265000, episode_reward=-4118.40 +/- 42.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=266000, episode_reward=-4146.60 +/- 54.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=267000, episode_reward=-4142.00 +/- 24.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=268000, episode_reward=-4144.80 +/- 30.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=269000, episode_reward=-4157.60 +/- 52.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=270000, episode_reward=-4158.20 +/- 57.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=271000, episode_reward=-4116.80 +/- 34.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=272000, episode_reward=-4140.20 +/- 16.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=273000, episode_reward=-4143.40 +/- 38.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=274000, episode_reward=-4149.40 +/- 39.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=275000, episode_reward=-4104.60 +/- 62.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=276000, episode_reward=-4177.20 +/- 13.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=277000, episode_reward=-4105.60 +/- 73.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=278000, episode_reward=-4099.80 +/- 32.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=279000, episode_reward=-4115.20 +/- 67.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=280000, episode_reward=-4093.20 +/- 41.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=281000, episode_reward=-4116.80 +/- 25.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=282000, episode_reward=-4144.80 +/- 13.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=283000, episode_reward=-4113.00 +/- 42.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=284000, episode_reward=-4089.80 +/- 74.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=285000, episode_reward=-4118.80 +/- 33.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=286000, episode_reward=-4127.20 +/- 29.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=287000, episode_reward=-4101.40 +/- 47.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=288000, episode_reward=-4136.60 +/- 34.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=289000, episode_reward=-4139.00 +/- 46.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=290000, episode_reward=-4104.00 +/- 54.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=291000, episode_reward=-4156.40 +/- 36.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=292000, episode_reward=-4144.80 +/- 42.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=293000, episode_reward=-4145.20 +/- 23.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=294000, episode_reward=-4140.20 +/- 37.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=295000, episode_reward=-4139.80 +/- 51.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=296000, episode_reward=-4137.00 +/- 25.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=297000, episode_reward=-4120.20 +/- 31.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=298000, episode_reward=-4116.60 +/- 27.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=299000, episode_reward=-4141.20 +/- 37.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=300000, episode_reward=-4155.00 +/- 44.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=301000, episode_reward=-4145.40 +/- 40.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=302000, episode_reward=-4119.60 +/- 49.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=303000, episode_reward=-4139.40 +/- 54.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=304000, episode_reward=-4137.40 +/- 41.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=305000, episode_reward=-4119.60 +/- 36.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=306000, episode_reward=-4140.60 +/- 19.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=307000, episode_reward=-4150.20 +/- 27.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=308000, episode_reward=-4183.40 +/- 49.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=309000, episode_reward=-4187.60 +/- 23.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=310000, episode_reward=-4165.20 +/- 25.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=311000, episode_reward=-4140.40 +/- 33.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=312000, episode_reward=-4093.60 +/- 50.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=313000, episode_reward=-4118.80 +/- 52.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=314000, episode_reward=-4139.40 +/- 53.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=315000, episode_reward=-4118.20 +/- 38.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=316000, episode_reward=-4103.80 +/- 61.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=317000, episode_reward=-4123.60 +/- 24.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=318000, episode_reward=-4141.00 +/- 44.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=319000, episode_reward=-4113.40 +/- 41.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=320000, episode_reward=-4117.40 +/- 33.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=321000, episode_reward=-4117.80 +/- 49.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=322000, episode_reward=-4118.40 +/- 43.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=323000, episode_reward=-4146.40 +/- 41.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=324000, episode_reward=-4068.80 +/- 104.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=325000, episode_reward=-4112.00 +/- 35.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=326000, episode_reward=-4126.80 +/- 30.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=327000, episode_reward=-4151.20 +/- 15.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=328000, episode_reward=-4108.60 +/- 59.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=329000, episode_reward=-4156.00 +/- 45.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=330000, episode_reward=-4125.40 +/- 22.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=331000, episode_reward=-4140.00 +/- 36.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=332000, episode_reward=-4112.40 +/- 50.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=333000, episode_reward=-4106.60 +/- 43.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=334000, episode_reward=-4077.60 +/- 40.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=335000, episode_reward=-4144.60 +/- 50.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=336000, episode_reward=-4158.40 +/- 29.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=337000, episode_reward=-4124.80 +/- 36.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=338000, episode_reward=-4126.60 +/- 32.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=339000, episode_reward=-4160.40 +/- 39.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=340000, episode_reward=-4137.80 +/- 40.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=341000, episode_reward=-4146.00 +/- 58.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=342000, episode_reward=-4124.60 +/- 32.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=343000, episode_reward=-4093.20 +/- 25.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=344000, episode_reward=-4130.80 +/- 32.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=345000, episode_reward=-4151.80 +/- 24.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=346000, episode_reward=-4138.40 +/- 37.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=347000, episode_reward=-4141.40 +/- 38.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=348000, episode_reward=-4169.00 +/- 24.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=349000, episode_reward=-4111.00 +/- 21.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=350000, episode_reward=-4112.40 +/- 36.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=351000, episode_reward=-4123.20 +/- 34.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=352000, episode_reward=-4124.80 +/- 46.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=353000, episode_reward=-4140.80 +/- 41.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=354000, episode_reward=-4146.20 +/- 49.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=355000, episode_reward=-4111.40 +/- 28.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=356000, episode_reward=-4148.00 +/- 42.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=357000, episode_reward=-4133.40 +/- 37.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=358000, episode_reward=-4125.80 +/- 28.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=359000, episode_reward=-4165.80 +/- 30.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=360000, episode_reward=-4152.40 +/- 17.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=361000, episode_reward=-4118.20 +/- 75.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=362000, episode_reward=-4161.20 +/- 47.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=363000, episode_reward=-4153.40 +/- 42.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=364000, episode_reward=-4162.20 +/- 43.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=365000, episode_reward=-4176.60 +/- 20.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=366000, episode_reward=-4118.60 +/- 61.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=367000, episode_reward=-4169.00 +/- 52.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=368000, episode_reward=-4141.20 +/- 79.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=369000, episode_reward=-4136.00 +/- 20.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=370000, episode_reward=-4145.20 +/- 16.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=371000, episode_reward=-4150.40 +/- 58.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=372000, episode_reward=-4121.40 +/- 89.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=373000, episode_reward=-4137.60 +/- 39.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=374000, episode_reward=-4106.80 +/- 65.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=375000, episode_reward=-4145.60 +/- 19.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=376000, episode_reward=-4082.80 +/- 31.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=377000, episode_reward=-4132.60 +/- 38.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=378000, episode_reward=-4142.00 +/- 39.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=379000, episode_reward=-4106.20 +/- 46.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=380000, episode_reward=-4124.40 +/- 41.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=381000, episode_reward=-4142.20 +/- 30.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=382000, episode_reward=-4134.00 +/- 12.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=383000, episode_reward=-4126.20 +/- 24.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=384000, episode_reward=-4120.20 +/- 22.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=385000, episode_reward=-4174.40 +/- 35.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=386000, episode_reward=-4141.60 +/- 47.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=387000, episode_reward=-4119.60 +/- 29.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=388000, episode_reward=-4082.00 +/- 32.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=389000, episode_reward=-4106.80 +/- 34.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=390000, episode_reward=-4131.60 +/- 40.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=391000, episode_reward=-4158.40 +/- 22.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=392000, episode_reward=-4104.80 +/- 42.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=393000, episode_reward=-4137.80 +/- 28.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=394000, episode_reward=-4098.80 +/- 46.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=395000, episode_reward=-4142.20 +/- 32.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=396000, episode_reward=-4134.40 +/- 43.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=397000, episode_reward=-4107.40 +/- 27.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=398000, episode_reward=-4135.80 +/- 23.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=399000, episode_reward=-4129.40 +/- 47.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=400000, episode_reward=-4101.40 +/- 49.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=401000, episode_reward=-4147.40 +/- 26.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=402000, episode_reward=-4087.40 +/- 62.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=403000, episode_reward=-4155.80 +/- 36.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=404000, episode_reward=-4121.40 +/- 61.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=405000, episode_reward=-4140.20 +/- 50.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=406000, episode_reward=-4143.00 +/- 66.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=407000, episode_reward=-4148.60 +/- 36.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=408000, episode_reward=-4147.00 +/- 52.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=409000, episode_reward=-4084.80 +/- 42.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=410000, episode_reward=-4154.60 +/- 25.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=411000, episode_reward=-4128.40 +/- 55.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=412000, episode_reward=-4156.00 +/- 58.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=413000, episode_reward=-4155.80 +/- 52.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=414000, episode_reward=-4158.80 +/- 48.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=415000, episode_reward=-4134.80 +/- 42.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=416000, episode_reward=-4117.40 +/- 38.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=417000, episode_reward=-4141.60 +/- 57.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=418000, episode_reward=-4121.40 +/- 28.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=419000, episode_reward=-4121.00 +/- 45.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=420000, episode_reward=-4146.80 +/- 23.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=421000, episode_reward=-4147.40 +/- 25.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=422000, episode_reward=-4120.80 +/- 58.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=423000, episode_reward=-4118.00 +/- 58.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=424000, episode_reward=-4072.80 +/- 46.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=425000, episode_reward=-4137.80 +/- 46.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=426000, episode_reward=-4141.80 +/- 25.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=427000, episode_reward=-4109.60 +/- 40.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=428000, episode_reward=-4159.20 +/- 29.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=429000, episode_reward=-4120.20 +/- 57.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=430000, episode_reward=-4133.60 +/- 49.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=431000, episode_reward=-4134.40 +/- 42.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=432000, episode_reward=-4159.40 +/- 30.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=433000, episode_reward=-4104.80 +/- 20.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=434000, episode_reward=-4121.40 +/- 52.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=435000, episode_reward=-4095.00 +/- 53.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=436000, episode_reward=-4106.00 +/- 41.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=437000, episode_reward=-4131.00 +/- 39.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=438000, episode_reward=-4152.80 +/- 31.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=439000, episode_reward=-4133.40 +/- 57.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=440000, episode_reward=-4138.00 +/- 49.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=441000, episode_reward=-4116.20 +/- 41.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=442000, episode_reward=-4127.20 +/- 20.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=443000, episode_reward=-4129.20 +/- 54.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=444000, episode_reward=-4122.40 +/- 64.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=445000, episode_reward=-4105.00 +/- 69.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=446000, episode_reward=-4074.20 +/- 44.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=447000, episode_reward=-4167.00 +/- 55.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=448000, episode_reward=-4121.20 +/- 40.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=449000, episode_reward=-4162.00 +/- 34.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=450000, episode_reward=-4152.40 +/- 42.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=451000, episode_reward=-4131.80 +/- 68.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=452000, episode_reward=-4145.80 +/- 50.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=453000, episode_reward=-4143.20 +/- 45.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=454000, episode_reward=-4117.40 +/- 18.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=455000, episode_reward=-4115.20 +/- 28.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=456000, episode_reward=-4127.20 +/- 45.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=457000, episode_reward=-4145.20 +/- 38.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=458000, episode_reward=-4120.40 +/- 28.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=459000, episode_reward=-4147.60 +/- 28.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=460000, episode_reward=-4120.00 +/- 41.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=461000, episode_reward=-4133.60 +/- 34.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=462000, episode_reward=-4181.80 +/- 37.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=463000, episode_reward=-4124.80 +/- 35.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=464000, episode_reward=-4126.80 +/- 73.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=465000, episode_reward=-4126.60 +/- 34.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=466000, episode_reward=-4153.20 +/- 54.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=467000, episode_reward=-4128.60 +/- 49.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=468000, episode_reward=-4142.40 +/- 58.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=469000, episode_reward=-4121.40 +/- 28.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=470000, episode_reward=-4143.80 +/- 11.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=471000, episode_reward=-4148.80 +/- 34.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=472000, episode_reward=-4147.40 +/- 32.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=473000, episode_reward=-4132.60 +/- 33.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=474000, episode_reward=-4198.00 +/- 70.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=475000, episode_reward=-4150.40 +/- 22.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=476000, episode_reward=-4119.60 +/- 63.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=477000, episode_reward=-4140.80 +/- 25.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=478000, episode_reward=-4131.80 +/- 42.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=479000, episode_reward=-4111.20 +/- 35.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=480000, episode_reward=-4165.40 +/- 50.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=481000, episode_reward=-4114.80 +/- 51.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=482000, episode_reward=-4165.80 +/- 27.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=483000, episode_reward=-4112.00 +/- 84.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=484000, episode_reward=-4137.00 +/- 56.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=485000, episode_reward=-4147.60 +/- 28.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=486000, episode_reward=-4188.60 +/- 29.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=487000, episode_reward=-4146.00 +/- 50.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=488000, episode_reward=-4121.20 +/- 78.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=489000, episode_reward=-4097.80 +/- 27.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=490000, episode_reward=-4147.20 +/- 64.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=491000, episode_reward=-4118.20 +/- 57.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=492000, episode_reward=-4162.40 +/- 33.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=493000, episode_reward=-4148.40 +/- 40.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=494000, episode_reward=-4159.20 +/- 29.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=495000, episode_reward=-4142.80 +/- 39.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=496000, episode_reward=-4164.80 +/- 27.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=497000, episode_reward=-4120.80 +/- 34.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=498000, episode_reward=-4108.20 +/- 40.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=499000, episode_reward=-4125.80 +/- 40.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=500000, episode_reward=-4129.80 +/- 35.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=501000, episode_reward=-4106.40 +/- 25.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=502000, episode_reward=-4120.60 +/- 42.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=503000, episode_reward=-4127.20 +/- 65.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=504000, episode_reward=-4144.80 +/- 38.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=505000, episode_reward=-4158.80 +/- 38.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=506000, episode_reward=-4043.20 +/- 101.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=507000, episode_reward=-4157.00 +/- 70.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=508000, episode_reward=-4101.80 +/- 62.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=509000, episode_reward=-4130.80 +/- 36.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=510000, episode_reward=-4125.60 +/- 29.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=511000, episode_reward=-4088.20 +/- 25.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=512000, episode_reward=-4095.00 +/- 57.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=513000, episode_reward=-4141.00 +/- 16.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=514000, episode_reward=-4130.80 +/- 75.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=515000, episode_reward=-4120.00 +/- 44.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=516000, episode_reward=-4153.80 +/- 51.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=517000, episode_reward=-4135.60 +/- 31.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=518000, episode_reward=-4119.20 +/- 70.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=519000, episode_reward=-4115.00 +/- 50.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=520000, episode_reward=-4129.00 +/- 69.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=521000, episode_reward=-4117.80 +/- 39.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=522000, episode_reward=-4152.80 +/- 31.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=523000, episode_reward=-4119.00 +/- 35.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=524000, episode_reward=-4161.20 +/- 34.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=525000, episode_reward=-4164.00 +/- 42.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=526000, episode_reward=-4130.00 +/- 40.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=527000, episode_reward=-4109.80 +/- 17.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=528000, episode_reward=-4126.80 +/- 48.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=529000, episode_reward=-4151.40 +/- 47.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=530000, episode_reward=-4173.80 +/- 48.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=531000, episode_reward=-4134.40 +/- 37.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=532000, episode_reward=-4128.60 +/- 51.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=533000, episode_reward=-4132.40 +/- 25.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=534000, episode_reward=-4136.80 +/- 52.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=535000, episode_reward=-4121.00 +/- 16.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=536000, episode_reward=-4104.40 +/- 27.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=537000, episode_reward=-4162.20 +/- 45.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=538000, episode_reward=-4098.80 +/- 69.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=539000, episode_reward=-4144.00 +/- 38.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=540000, episode_reward=-4134.40 +/- 36.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=541000, episode_reward=-4132.20 +/- 64.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=542000, episode_reward=-4156.20 +/- 18.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=543000, episode_reward=-4113.80 +/- 56.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=544000, episode_reward=-4152.20 +/- 41.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=545000, episode_reward=-4120.20 +/- 59.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=546000, episode_reward=-4126.40 +/- 39.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=547000, episode_reward=-4135.00 +/- 50.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=548000, episode_reward=-4144.40 +/- 39.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=549000, episode_reward=-4091.60 +/- 40.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=550000, episode_reward=-4146.20 +/- 19.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=551000, episode_reward=-4163.40 +/- 26.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=552000, episode_reward=-4147.00 +/- 29.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=553000, episode_reward=-4139.60 +/- 11.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=554000, episode_reward=-4133.60 +/- 35.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=555000, episode_reward=-4129.20 +/- 58.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=556000, episode_reward=-4131.80 +/- 27.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=557000, episode_reward=-4155.80 +/- 49.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=558000, episode_reward=-4117.80 +/- 66.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=559000, episode_reward=-4123.00 +/- 59.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=560000, episode_reward=-4153.00 +/- 37.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=561000, episode_reward=-4151.20 +/- 20.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=562000, episode_reward=-4107.20 +/- 33.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=563000, episode_reward=-4144.60 +/- 16.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=564000, episode_reward=-4104.60 +/- 38.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=565000, episode_reward=-4140.20 +/- 37.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=566000, episode_reward=-4124.00 +/- 25.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=567000, episode_reward=-4112.80 +/- 22.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=568000, episode_reward=-4129.60 +/- 62.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=569000, episode_reward=-4137.80 +/- 20.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=570000, episode_reward=-4165.80 +/- 27.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=571000, episode_reward=-4179.20 +/- 37.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=572000, episode_reward=-4148.60 +/- 28.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=573000, episode_reward=-4151.40 +/- 43.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=574000, episode_reward=-4166.60 +/- 20.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=575000, episode_reward=-4160.60 +/- 13.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=576000, episode_reward=-4163.20 +/- 41.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=577000, episode_reward=-4117.80 +/- 34.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=578000, episode_reward=-4121.00 +/- 43.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=579000, episode_reward=-4122.80 +/- 44.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=580000, episode_reward=-4138.20 +/- 25.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=581000, episode_reward=-4161.20 +/- 37.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=582000, episode_reward=-4137.60 +/- 46.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=583000, episode_reward=-4111.20 +/- 50.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=584000, episode_reward=-4161.60 +/- 31.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=585000, episode_reward=-4160.40 +/- 14.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=586000, episode_reward=-4154.00 +/- 13.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=587000, episode_reward=-4107.40 +/- 26.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=588000, episode_reward=-4165.60 +/- 35.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=589000, episode_reward=-4145.80 +/- 29.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=590000, episode_reward=-4111.60 +/- 51.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=591000, episode_reward=-4141.80 +/- 48.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=592000, episode_reward=-4136.00 +/- 52.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=593000, episode_reward=-4166.00 +/- 47.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=594000, episode_reward=-4128.20 +/- 31.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=595000, episode_reward=-4142.00 +/- 46.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=596000, episode_reward=-4154.20 +/- 35.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=597000, episode_reward=-4082.60 +/- 33.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=598000, episode_reward=-4127.00 +/- 13.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=599000, episode_reward=-4121.60 +/- 15.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=600000, episode_reward=-4166.20 +/- 36.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=601000, episode_reward=-4120.20 +/- 41.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=602000, episode_reward=-4146.20 +/- 48.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=603000, episode_reward=-4121.20 +/- 108.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=604000, episode_reward=-4163.60 +/- 47.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=605000, episode_reward=-4134.20 +/- 45.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=606000, episode_reward=-4129.60 +/- 67.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=607000, episode_reward=-4133.00 +/- 44.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=608000, episode_reward=-4122.20 +/- 42.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=609000, episode_reward=-4169.00 +/- 19.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=610000, episode_reward=-4118.00 +/- 42.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=611000, episode_reward=-4138.20 +/- 42.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=612000, episode_reward=-4121.20 +/- 33.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=613000, episode_reward=-4152.20 +/- 36.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=614000, episode_reward=-4115.40 +/- 58.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=615000, episode_reward=-4127.20 +/- 60.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=616000, episode_reward=-4125.80 +/- 22.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=617000, episode_reward=-4087.20 +/- 49.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=618000, episode_reward=-4146.40 +/- 28.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=619000, episode_reward=-4131.60 +/- 47.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=620000, episode_reward=-4136.60 +/- 92.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=621000, episode_reward=-4140.60 +/- 26.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=622000, episode_reward=-4148.00 +/- 41.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=623000, episode_reward=-4151.40 +/- 34.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=624000, episode_reward=-4116.80 +/- 37.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=625000, episode_reward=-4158.00 +/- 58.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=626000, episode_reward=-4155.00 +/- 55.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=627000, episode_reward=-4128.00 +/- 22.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=628000, episode_reward=-4145.00 +/- 21.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=629000, episode_reward=-4156.20 +/- 30.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=630000, episode_reward=-4144.40 +/- 62.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=631000, episode_reward=-4159.20 +/- 25.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=632000, episode_reward=-4132.60 +/- 51.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=633000, episode_reward=-4148.60 +/- 24.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=634000, episode_reward=-4152.40 +/- 36.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=635000, episode_reward=-4124.40 +/- 45.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=636000, episode_reward=-4157.40 +/- 34.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=637000, episode_reward=-4123.80 +/- 65.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=638000, episode_reward=-4105.00 +/- 40.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=639000, episode_reward=-4154.00 +/- 44.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=640000, episode_reward=-4132.80 +/- 14.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=641000, episode_reward=-4159.00 +/- 23.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=642000, episode_reward=-4141.20 +/- 31.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=643000, episode_reward=-4144.40 +/- 26.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=644000, episode_reward=-4166.00 +/- 43.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=645000, episode_reward=-4159.60 +/- 65.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=646000, episode_reward=-4110.80 +/- 42.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=647000, episode_reward=-4139.80 +/- 17.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=648000, episode_reward=-4123.40 +/- 26.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=649000, episode_reward=-4167.80 +/- 25.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=650000, episode_reward=-4126.40 +/- 26.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=651000, episode_reward=-4127.80 +/- 43.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=652000, episode_reward=-4126.00 +/- 28.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=653000, episode_reward=-4124.20 +/- 18.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=654000, episode_reward=-4180.40 +/- 21.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=655000, episode_reward=-4158.40 +/- 19.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=656000, episode_reward=-4126.20 +/- 44.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=657000, episode_reward=-4134.00 +/- 39.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=658000, episode_reward=-4129.80 +/- 48.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=659000, episode_reward=-4135.00 +/- 14.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=660000, episode_reward=-4111.20 +/- 80.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=661000, episode_reward=-4136.60 +/- 29.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=662000, episode_reward=-4143.60 +/- 60.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=663000, episode_reward=-4163.00 +/- 38.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=664000, episode_reward=-4139.00 +/- 16.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=665000, episode_reward=-4146.40 +/- 49.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=666000, episode_reward=-4130.40 +/- 43.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=667000, episode_reward=-4136.40 +/- 38.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=668000, episode_reward=-4122.20 +/- 32.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=669000, episode_reward=-4103.20 +/- 43.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=670000, episode_reward=-4146.20 +/- 21.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=671000, episode_reward=-4145.80 +/- 24.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=672000, episode_reward=-4143.60 +/- 30.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=673000, episode_reward=-4135.60 +/- 29.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=674000, episode_reward=-4143.60 +/- 27.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=675000, episode_reward=-4114.00 +/- 47.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=676000, episode_reward=-4114.40 +/- 39.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=677000, episode_reward=-4153.20 +/- 29.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=678000, episode_reward=-4141.60 +/- 68.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=679000, episode_reward=-4137.20 +/- 41.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=680000, episode_reward=-4181.80 +/- 40.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=681000, episode_reward=-4129.80 +/- 36.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=682000, episode_reward=-4128.80 +/- 65.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=683000, episode_reward=-4096.00 +/- 70.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=684000, episode_reward=-4129.80 +/- 31.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=685000, episode_reward=-4132.40 +/- 26.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=686000, episode_reward=-4114.40 +/- 56.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=687000, episode_reward=-4094.40 +/- 65.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=688000, episode_reward=-4142.60 +/- 19.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=689000, episode_reward=-4151.40 +/- 22.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=690000, episode_reward=-4138.60 +/- 49.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=691000, episode_reward=-4146.80 +/- 48.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=692000, episode_reward=-4112.00 +/- 48.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=693000, episode_reward=-4080.60 +/- 67.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=694000, episode_reward=-4142.20 +/- 67.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=695000, episode_reward=-4125.20 +/- 48.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=696000, episode_reward=-4130.60 +/- 27.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=697000, episode_reward=-4137.20 +/- 58.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=698000, episode_reward=-4170.80 +/- 21.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=699000, episode_reward=-4138.60 +/- 52.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=700000, episode_reward=-4170.60 +/- 26.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=701000, episode_reward=-4116.00 +/- 46.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=702000, episode_reward=-4121.20 +/- 53.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=703000, episode_reward=-4147.00 +/- 33.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=704000, episode_reward=-4128.20 +/- 28.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=705000, episode_reward=-4127.20 +/- 30.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=706000, episode_reward=-4118.80 +/- 33.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=707000, episode_reward=-4120.80 +/- 29.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=708000, episode_reward=-4101.20 +/- 58.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=709000, episode_reward=-4114.80 +/- 54.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=710000, episode_reward=-4139.60 +/- 21.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=711000, episode_reward=-4123.80 +/- 35.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=712000, episode_reward=-4152.60 +/- 56.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=713000, episode_reward=-4117.80 +/- 54.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=714000, episode_reward=-4106.40 +/- 38.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=715000, episode_reward=-4164.60 +/- 50.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=716000, episode_reward=-4130.40 +/- 24.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=717000, episode_reward=-4079.40 +/- 100.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=718000, episode_reward=-4108.80 +/- 25.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=719000, episode_reward=-4112.20 +/- 48.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=720000, episode_reward=-4109.00 +/- 51.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=721000, episode_reward=-4061.20 +/- 96.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=722000, episode_reward=-4129.00 +/- 39.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=723000, episode_reward=-4145.40 +/- 16.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=724000, episode_reward=-4130.00 +/- 35.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=725000, episode_reward=-4139.00 +/- 47.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=726000, episode_reward=-4147.40 +/- 21.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=727000, episode_reward=-4124.60 +/- 36.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=728000, episode_reward=-4113.40 +/- 19.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=729000, episode_reward=-4134.20 +/- 48.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=730000, episode_reward=-4161.00 +/- 35.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=731000, episode_reward=-4150.20 +/- 48.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=732000, episode_reward=-4149.80 +/- 51.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=733000, episode_reward=-4118.80 +/- 36.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=734000, episode_reward=-4092.20 +/- 35.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=735000, episode_reward=-4168.40 +/- 22.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=736000, episode_reward=-4140.40 +/- 30.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=737000, episode_reward=-4133.60 +/- 16.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=738000, episode_reward=-4113.80 +/- 37.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=739000, episode_reward=-4124.60 +/- 21.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=740000, episode_reward=-4094.60 +/- 51.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=741000, episode_reward=-4141.40 +/- 34.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=742000, episode_reward=-4125.80 +/- 14.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=743000, episode_reward=-4130.00 +/- 11.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=744000, episode_reward=-4119.80 +/- 9.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=745000, episode_reward=-4149.00 +/- 30.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=746000, episode_reward=-4174.00 +/- 21.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=747000, episode_reward=-4148.00 +/- 25.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=748000, episode_reward=-4107.00 +/- 42.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=749000, episode_reward=-4089.80 +/- 41.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=750000, episode_reward=-4117.60 +/- 28.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=751000, episode_reward=-4122.00 +/- 90.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=752000, episode_reward=-4140.20 +/- 27.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=753000, episode_reward=-4154.80 +/- 33.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=754000, episode_reward=-4115.20 +/- 96.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=755000, episode_reward=-4125.80 +/- 53.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=756000, episode_reward=-4160.80 +/- 18.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=757000, episode_reward=-4096.80 +/- 18.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=758000, episode_reward=-4161.20 +/- 49.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=759000, episode_reward=-4163.00 +/- 17.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=760000, episode_reward=-4115.60 +/- 45.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=761000, episode_reward=-4154.40 +/- 63.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=762000, episode_reward=-4148.00 +/- 60.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=763000, episode_reward=-4145.20 +/- 33.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=764000, episode_reward=-4120.80 +/- 46.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=765000, episode_reward=-4139.60 +/- 53.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=766000, episode_reward=-4100.40 +/- 38.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=767000, episode_reward=-4158.80 +/- 26.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=768000, episode_reward=-4079.60 +/- 65.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=769000, episode_reward=-4108.40 +/- 34.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=770000, episode_reward=-4122.40 +/- 16.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=771000, episode_reward=-4138.80 +/- 19.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=772000, episode_reward=-4133.60 +/- 43.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=773000, episode_reward=-4132.20 +/- 25.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=774000, episode_reward=-4147.60 +/- 75.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=775000, episode_reward=-4119.20 +/- 26.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=776000, episode_reward=-4085.60 +/- 41.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=777000, episode_reward=-4155.60 +/- 17.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=778000, episode_reward=-4154.80 +/- 38.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=779000, episode_reward=-4161.80 +/- 42.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=780000, episode_reward=-4148.80 +/- 13.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=781000, episode_reward=-4094.20 +/- 89.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=782000, episode_reward=-4120.00 +/- 28.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=783000, episode_reward=-4098.60 +/- 32.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=784000, episode_reward=-4119.20 +/- 43.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=785000, episode_reward=-4115.60 +/- 41.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=786000, episode_reward=-4137.00 +/- 69.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=787000, episode_reward=-4078.60 +/- 68.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=788000, episode_reward=-4136.60 +/- 66.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=789000, episode_reward=-4122.80 +/- 28.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=790000, episode_reward=-4161.60 +/- 48.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=791000, episode_reward=-4127.60 +/- 64.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=792000, episode_reward=-4144.40 +/- 61.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=793000, episode_reward=-4071.80 +/- 25.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=794000, episode_reward=-4155.60 +/- 37.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=795000, episode_reward=-4168.80 +/- 34.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=796000, episode_reward=-4071.40 +/- 88.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=797000, episode_reward=-4070.40 +/- 128.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=798000, episode_reward=-4100.80 +/- 63.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=799000, episode_reward=-4137.40 +/- 35.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=800000, episode_reward=-4093.20 +/- 54.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=801000, episode_reward=-4131.60 +/- 24.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=802000, episode_reward=-4145.80 +/- 35.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=803000, episode_reward=-4098.00 +/- 63.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=804000, episode_reward=-4121.40 +/- 30.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=805000, episode_reward=-4151.60 +/- 49.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=806000, episode_reward=-4149.00 +/- 37.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=807000, episode_reward=-4096.20 +/- 68.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=808000, episode_reward=-4127.00 +/- 32.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=809000, episode_reward=-4103.40 +/- 74.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=810000, episode_reward=-4140.00 +/- 41.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=811000, episode_reward=-4139.40 +/- 30.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=812000, episode_reward=-4079.80 +/- 61.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=813000, episode_reward=-4162.60 +/- 21.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=814000, episode_reward=-4108.60 +/- 31.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=815000, episode_reward=-4185.60 +/- 37.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=816000, episode_reward=-4132.20 +/- 54.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=817000, episode_reward=-4147.80 +/- 40.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=818000, episode_reward=-4119.80 +/- 62.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=819000, episode_reward=-4177.80 +/- 41.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=820000, episode_reward=-4113.40 +/- 48.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=821000, episode_reward=-4106.00 +/- 63.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=822000, episode_reward=-4139.40 +/- 24.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=823000, episode_reward=-4127.20 +/- 32.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=824000, episode_reward=-4156.20 +/- 53.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=825000, episode_reward=-4113.00 +/- 39.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=826000, episode_reward=-4126.80 +/- 46.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=827000, episode_reward=-4071.00 +/- 38.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=828000, episode_reward=-4109.20 +/- 60.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=829000, episode_reward=-4148.40 +/- 41.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=830000, episode_reward=-4131.80 +/- 42.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=831000, episode_reward=-4167.20 +/- 78.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=832000, episode_reward=-4129.60 +/- 36.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=833000, episode_reward=-4132.40 +/- 60.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=834000, episode_reward=-4112.40 +/- 22.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=835000, episode_reward=-4115.60 +/- 33.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=836000, episode_reward=-4142.80 +/- 24.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=837000, episode_reward=-4107.40 +/- 58.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=838000, episode_reward=-4088.80 +/- 28.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=839000, episode_reward=-4137.40 +/- 31.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=840000, episode_reward=-4111.40 +/- 51.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=841000, episode_reward=-4138.80 +/- 33.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=842000, episode_reward=-4116.00 +/- 46.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=843000, episode_reward=-4132.60 +/- 39.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=844000, episode_reward=-4094.20 +/- 58.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=845000, episode_reward=-4159.40 +/- 25.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=846000, episode_reward=-4161.00 +/- 31.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=847000, episode_reward=-4126.00 +/- 20.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=848000, episode_reward=-4142.60 +/- 41.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=849000, episode_reward=-4127.60 +/- 54.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=850000, episode_reward=-4117.00 +/- 74.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=851000, episode_reward=-4139.60 +/- 73.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=852000, episode_reward=-4112.80 +/- 21.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=853000, episode_reward=-4159.00 +/- 54.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=854000, episode_reward=-4130.80 +/- 28.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=855000, episode_reward=-4136.00 +/- 52.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=856000, episode_reward=-4088.40 +/- 90.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=857000, episode_reward=-4122.60 +/- 14.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=858000, episode_reward=-4157.20 +/- 55.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=859000, episode_reward=-4165.40 +/- 48.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=860000, episode_reward=-4138.80 +/- 26.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=861000, episode_reward=-4129.40 +/- 48.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=862000, episode_reward=-4137.40 +/- 52.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=863000, episode_reward=-4085.60 +/- 32.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=864000, episode_reward=-4170.80 +/- 4.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=865000, episode_reward=-4148.80 +/- 42.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=866000, episode_reward=-4162.00 +/- 58.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=867000, episode_reward=-4139.80 +/- 50.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=868000, episode_reward=-4169.20 +/- 20.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=869000, episode_reward=-4151.60 +/- 43.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=870000, episode_reward=-4153.60 +/- 46.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=871000, episode_reward=-4083.00 +/- 52.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=872000, episode_reward=-4147.00 +/- 59.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=873000, episode_reward=-4147.20 +/- 51.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=874000, episode_reward=-4139.20 +/- 40.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=875000, episode_reward=-4125.60 +/- 62.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=876000, episode_reward=-4135.00 +/- 27.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=877000, episode_reward=-4141.00 +/- 17.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=878000, episode_reward=-4161.00 +/- 33.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=879000, episode_reward=-4132.60 +/- 36.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=880000, episode_reward=-4095.20 +/- 19.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=881000, episode_reward=-4120.40 +/- 50.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=882000, episode_reward=-4129.00 +/- 58.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=883000, episode_reward=-4157.40 +/- 28.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=884000, episode_reward=-4089.40 +/- 63.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=885000, episode_reward=-4168.40 +/- 44.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=886000, episode_reward=-4113.60 +/- 26.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=887000, episode_reward=-4148.60 +/- 17.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=888000, episode_reward=-4131.00 +/- 52.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=889000, episode_reward=-4136.40 +/- 29.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=890000, episode_reward=-4085.00 +/- 34.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=891000, episode_reward=-4111.60 +/- 58.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=892000, episode_reward=-4116.20 +/- 34.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=893000, episode_reward=-4146.20 +/- 37.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=894000, episode_reward=-4134.00 +/- 19.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=895000, episode_reward=-4128.00 +/- 55.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=896000, episode_reward=-4150.00 +/- 81.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=897000, episode_reward=-4133.80 +/- 65.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=898000, episode_reward=-4163.00 +/- 51.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=899000, episode_reward=-4129.40 +/- 25.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=900000, episode_reward=-4123.00 +/- 24.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=901000, episode_reward=-4119.80 +/- 71.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=902000, episode_reward=-4102.60 +/- 52.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=903000, episode_reward=-4138.80 +/- 34.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=904000, episode_reward=-4136.80 +/- 57.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=905000, episode_reward=-4097.00 +/- 40.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=906000, episode_reward=-4086.00 +/- 41.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=907000, episode_reward=-4146.60 +/- 23.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=908000, episode_reward=-4162.40 +/- 41.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=909000, episode_reward=-4130.20 +/- 33.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=910000, episode_reward=-4121.40 +/- 26.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=911000, episode_reward=-4152.80 +/- 34.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=912000, episode_reward=-4101.40 +/- 43.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=913000, episode_reward=-4124.40 +/- 25.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=914000, episode_reward=-4156.00 +/- 25.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=915000, episode_reward=-4172.00 +/- 35.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=916000, episode_reward=-4114.60 +/- 36.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=917000, episode_reward=-4178.80 +/- 27.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=918000, episode_reward=-4183.00 +/- 45.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=919000, episode_reward=-4133.40 +/- 26.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=920000, episode_reward=-4161.40 +/- 15.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=921000, episode_reward=-4090.60 +/- 34.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=922000, episode_reward=-4139.40 +/- 46.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=923000, episode_reward=-4142.20 +/- 23.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=924000, episode_reward=-4084.00 +/- 62.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=925000, episode_reward=-4114.60 +/- 42.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=926000, episode_reward=-4137.20 +/- 78.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=927000, episode_reward=-4146.40 +/- 42.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=928000, episode_reward=-4122.40 +/- 19.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=929000, episode_reward=-4110.00 +/- 55.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=930000, episode_reward=-4123.40 +/- 61.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=931000, episode_reward=-4167.40 +/- 30.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=932000, episode_reward=-4170.80 +/- 22.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=933000, episode_reward=-4096.60 +/- 87.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=934000, episode_reward=-4129.00 +/- 31.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=935000, episode_reward=-4089.80 +/- 56.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=936000, episode_reward=-4141.00 +/- 52.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=937000, episode_reward=-4171.00 +/- 51.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=938000, episode_reward=-4128.00 +/- 29.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=939000, episode_reward=-4128.40 +/- 38.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=940000, episode_reward=-4112.20 +/- 52.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=941000, episode_reward=-4136.60 +/- 35.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=942000, episode_reward=-4154.00 +/- 27.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=943000, episode_reward=-4110.80 +/- 31.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=944000, episode_reward=-4170.60 +/- 26.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=945000, episode_reward=-4091.00 +/- 28.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=946000, episode_reward=-4130.60 +/- 47.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=947000, episode_reward=-4135.40 +/- 30.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=948000, episode_reward=-4105.40 +/- 82.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=949000, episode_reward=-4144.60 +/- 16.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=950000, episode_reward=-4162.40 +/- 33.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=951000, episode_reward=-4113.00 +/- 47.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=952000, episode_reward=-4166.60 +/- 23.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=953000, episode_reward=-4149.00 +/- 24.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=954000, episode_reward=-4128.00 +/- 47.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=955000, episode_reward=-4130.60 +/- 37.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=956000, episode_reward=-4138.00 +/- 26.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=957000, episode_reward=-4116.00 +/- 40.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=958000, episode_reward=-4137.80 +/- 15.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=959000, episode_reward=-4151.80 +/- 24.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=960000, episode_reward=-4143.20 +/- 44.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=961000, episode_reward=-4125.60 +/- 47.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=962000, episode_reward=-4113.20 +/- 16.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=963000, episode_reward=-4140.60 +/- 50.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=964000, episode_reward=-4111.40 +/- 47.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=965000, episode_reward=-4126.00 +/- 31.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=966000, episode_reward=-4170.20 +/- 30.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=967000, episode_reward=-4106.60 +/- 53.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=968000, episode_reward=-4119.40 +/- 74.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=969000, episode_reward=-4148.40 +/- 43.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=970000, episode_reward=-4138.20 +/- 39.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=971000, episode_reward=-4150.40 +/- 36.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=972000, episode_reward=-4126.80 +/- 28.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=973000, episode_reward=-4136.20 +/- 56.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=974000, episode_reward=-4145.80 +/- 30.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=975000, episode_reward=-4126.00 +/- 26.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=976000, episode_reward=-4073.40 +/- 41.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=977000, episode_reward=-4185.00 +/- 37.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=978000, episode_reward=-4122.20 +/- 2.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=979000, episode_reward=-4131.40 +/- 34.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=980000, episode_reward=-4144.40 +/- 37.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=981000, episode_reward=-4137.80 +/- 57.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=982000, episode_reward=-4113.40 +/- 73.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=983000, episode_reward=-4164.20 +/- 23.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=984000, episode_reward=-4128.20 +/- 61.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=985000, episode_reward=-4143.80 +/- 59.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=986000, episode_reward=-4154.20 +/- 41.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=987000, episode_reward=-4104.80 +/- 43.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=988000, episode_reward=-4148.20 +/- 46.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=989000, episode_reward=-4085.20 +/- 62.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=990000, episode_reward=-4129.80 +/- 46.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=991000, episode_reward=-4145.80 +/- 43.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=992000, episode_reward=-4129.20 +/- 18.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=993000, episode_reward=-4124.60 +/- 51.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=994000, episode_reward=-4164.00 +/- 21.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=995000, episode_reward=-4131.80 +/- 15.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=996000, episode_reward=-4138.80 +/- 14.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=997000, episode_reward=-4177.40 +/- 35.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=998000, episode_reward=-4161.40 +/- 46.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=999000, episode_reward=-4135.20 +/- 45.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1000000, episode_reward=-4125.20 +/- 55.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1001000, episode_reward=-4147.40 +/- 25.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1002000, episode_reward=-4165.20 +/- 43.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1003000, episode_reward=-4135.00 +/- 28.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1004000, episode_reward=-4113.60 +/- 51.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1005000, episode_reward=-4191.80 +/- 45.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1006000, episode_reward=-4128.20 +/- 30.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1007000, episode_reward=-4086.00 +/- 26.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1008000, episode_reward=-4112.60 +/- 49.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1009000, episode_reward=-4148.20 +/- 53.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1010000, episode_reward=-4141.80 +/- 18.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1011000, episode_reward=-4156.80 +/- 17.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1012000, episode_reward=-4131.20 +/- 32.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1013000, episode_reward=-4127.20 +/- 64.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1014000, episode_reward=-4165.80 +/- 64.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1015000, episode_reward=-4139.00 +/- 17.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1016000, episode_reward=-4153.40 +/- 24.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1017000, episode_reward=-4096.20 +/- 72.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1018000, episode_reward=-4157.60 +/- 28.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1019000, episode_reward=-4156.60 +/- 37.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1020000, episode_reward=-4119.60 +/- 47.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1021000, episode_reward=-4109.00 +/- 46.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1022000, episode_reward=-4124.00 +/- 71.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1023000, episode_reward=-4087.40 +/- 70.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1024000, episode_reward=-4126.40 +/- 19.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1025000, episode_reward=-4127.20 +/- 36.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1026000, episode_reward=-4096.20 +/- 40.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1027000, episode_reward=-4139.00 +/- 41.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1028000, episode_reward=-4169.00 +/- 34.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1029000, episode_reward=-4153.20 +/- 89.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1030000, episode_reward=-4144.00 +/- 32.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1031000, episode_reward=-4147.00 +/- 36.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1032000, episode_reward=-4160.40 +/- 7.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1033000, episode_reward=-4121.80 +/- 28.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1034000, episode_reward=-4116.20 +/- 76.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1035000, episode_reward=-4133.60 +/- 24.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1036000, episode_reward=-4155.00 +/- 45.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1037000, episode_reward=-4089.00 +/- 49.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1038000, episode_reward=-4129.40 +/- 60.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1039000, episode_reward=-4111.20 +/- 29.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1040000, episode_reward=-4116.40 +/- 46.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1041000, episode_reward=-4131.60 +/- 19.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1042000, episode_reward=-4112.80 +/- 27.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1043000, episode_reward=-4102.00 +/- 16.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1044000, episode_reward=-4128.20 +/- 42.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1045000, episode_reward=-4107.80 +/- 40.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1046000, episode_reward=-4147.40 +/- 49.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1047000, episode_reward=-4130.20 +/- 52.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1048000, episode_reward=-4138.40 +/- 16.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1049000, episode_reward=-4156.00 +/- 38.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1050000, episode_reward=-4173.40 +/- 25.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1051000, episode_reward=-4108.40 +/- 60.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1052000, episode_reward=-4143.00 +/- 27.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1053000, episode_reward=-4130.60 +/- 40.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1054000, episode_reward=-4101.40 +/- 61.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1055000, episode_reward=-4169.60 +/- 55.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1056000, episode_reward=-4169.80 +/- 45.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1057000, episode_reward=-4117.00 +/- 109.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1058000, episode_reward=-4113.60 +/- 41.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1059000, episode_reward=-4149.00 +/- 58.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1060000, episode_reward=-4131.20 +/- 40.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1061000, episode_reward=-4193.60 +/- 56.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1062000, episode_reward=-4164.80 +/- 14.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1063000, episode_reward=-4097.40 +/- 52.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1064000, episode_reward=-4108.00 +/- 93.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1065000, episode_reward=-4097.00 +/- 29.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1066000, episode_reward=-4136.60 +/- 61.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1067000, episode_reward=-4117.20 +/- 49.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1068000, episode_reward=-4119.80 +/- 37.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1069000, episode_reward=-4118.00 +/- 67.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1070000, episode_reward=-4134.00 +/- 27.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1071000, episode_reward=-4129.20 +/- 71.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1072000, episode_reward=-4122.00 +/- 22.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1073000, episode_reward=-4125.00 +/- 85.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1074000, episode_reward=-4114.80 +/- 60.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1075000, episode_reward=-4130.40 +/- 37.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1076000, episode_reward=-4116.80 +/- 11.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1077000, episode_reward=-4123.60 +/- 55.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1078000, episode_reward=-4144.60 +/- 40.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1079000, episode_reward=-4153.60 +/- 27.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1080000, episode_reward=-4134.20 +/- 48.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1081000, episode_reward=-4103.80 +/- 28.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1082000, episode_reward=-4126.20 +/- 54.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1083000, episode_reward=-4098.80 +/- 61.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1084000, episode_reward=-4112.40 +/- 14.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1085000, episode_reward=-4133.00 +/- 18.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1086000, episode_reward=-4096.80 +/- 28.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1087000, episode_reward=-4116.40 +/- 28.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1088000, episode_reward=-4148.40 +/- 34.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1089000, episode_reward=-4123.80 +/- 19.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1090000, episode_reward=-4155.00 +/- 23.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1091000, episode_reward=-4135.40 +/- 44.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1092000, episode_reward=-4155.80 +/- 16.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1093000, episode_reward=-4139.40 +/- 15.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1094000, episode_reward=-4211.40 +/- 10.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1095000, episode_reward=-4124.00 +/- 37.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1096000, episode_reward=-4164.80 +/- 54.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1097000, episode_reward=-4171.20 +/- 32.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1098000, episode_reward=-4162.80 +/- 46.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1099000, episode_reward=-4174.40 +/- 38.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1100000, episode_reward=-4107.20 +/- 42.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1101000, episode_reward=-4150.80 +/- 16.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1102000, episode_reward=-4081.20 +/- 38.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1103000, episode_reward=-4123.80 +/- 30.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1104000, episode_reward=-4147.00 +/- 41.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1105000, episode_reward=-4135.20 +/- 25.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1106000, episode_reward=-4161.20 +/- 42.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1107000, episode_reward=-4110.60 +/- 46.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1108000, episode_reward=-4128.00 +/- 39.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1109000, episode_reward=-4131.20 +/- 48.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1110000, episode_reward=-4154.20 +/- 44.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1111000, episode_reward=-4090.00 +/- 37.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1112000, episode_reward=-4161.80 +/- 79.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1113000, episode_reward=-4134.60 +/- 15.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1114000, episode_reward=-4163.60 +/- 49.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1115000, episode_reward=-4104.60 +/- 52.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1116000, episode_reward=-4154.60 +/- 27.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1117000, episode_reward=-4109.40 +/- 56.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1118000, episode_reward=-4129.00 +/- 39.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1119000, episode_reward=-4165.60 +/- 35.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1120000, episode_reward=-4099.80 +/- 34.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1121000, episode_reward=-4140.80 +/- 41.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1122000, episode_reward=-4133.40 +/- 67.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1123000, episode_reward=-4149.80 +/- 21.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1124000, episode_reward=-4162.20 +/- 58.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1125000, episode_reward=-4112.20 +/- 121.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1126000, episode_reward=-4147.00 +/- 37.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1127000, episode_reward=-4144.60 +/- 28.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1128000, episode_reward=-4153.20 +/- 41.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1129000, episode_reward=-4156.20 +/- 30.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1130000, episode_reward=-4182.40 +/- 21.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1131000, episode_reward=-4158.00 +/- 14.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1132000, episode_reward=-4142.60 +/- 52.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1133000, episode_reward=-4167.60 +/- 45.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1134000, episode_reward=-4107.20 +/- 52.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1135000, episode_reward=-4112.80 +/- 34.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1136000, episode_reward=-4110.60 +/- 78.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1137000, episode_reward=-4163.60 +/- 30.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1138000, episode_reward=-4101.40 +/- 70.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1139000, episode_reward=-4095.80 +/- 42.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1140000, episode_reward=-4140.00 +/- 26.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1141000, episode_reward=-4146.00 +/- 45.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1142000, episode_reward=-4158.80 +/- 8.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1143000, episode_reward=-4133.20 +/- 43.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1144000, episode_reward=-4159.40 +/- 33.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1145000, episode_reward=-4152.00 +/- 27.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1146000, episode_reward=-4121.80 +/- 59.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1147000, episode_reward=-4132.20 +/- 57.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1148000, episode_reward=-4168.00 +/- 21.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1149000, episode_reward=-4104.00 +/- 19.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1150000, episode_reward=-4149.60 +/- 28.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1151000, episode_reward=-4126.00 +/- 42.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1152000, episode_reward=-4119.40 +/- 37.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1153000, episode_reward=-4145.60 +/- 41.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1154000, episode_reward=-4128.20 +/- 52.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1155000, episode_reward=-4129.40 +/- 31.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1156000, episode_reward=-4144.80 +/- 24.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1157000, episode_reward=-4084.80 +/- 45.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1158000, episode_reward=-4117.00 +/- 31.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1159000, episode_reward=-4143.00 +/- 64.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1160000, episode_reward=-4149.40 +/- 43.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1161000, episode_reward=-4123.60 +/- 50.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1162000, episode_reward=-4118.20 +/- 40.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1163000, episode_reward=-4164.60 +/- 14.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1164000, episode_reward=-4124.00 +/- 49.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1165000, episode_reward=-4089.40 +/- 72.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1166000, episode_reward=-4139.80 +/- 28.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1167000, episode_reward=-4092.60 +/- 45.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1168000, episode_reward=-4144.00 +/- 28.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1169000, episode_reward=-4104.00 +/- 56.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1170000, episode_reward=-4141.20 +/- 51.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1171000, episode_reward=-4185.00 +/- 43.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1172000, episode_reward=-4117.00 +/- 36.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1173000, episode_reward=-4155.80 +/- 36.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1174000, episode_reward=-4095.00 +/- 68.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1175000, episode_reward=-4117.60 +/- 19.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1176000, episode_reward=-4147.40 +/- 45.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1177000, episode_reward=-4122.80 +/- 49.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1178000, episode_reward=-4100.20 +/- 79.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1179000, episode_reward=-4149.20 +/- 49.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1180000, episode_reward=-4142.80 +/- 53.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1181000, episode_reward=-4102.20 +/- 23.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1182000, episode_reward=-4096.40 +/- 51.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1183000, episode_reward=-4173.60 +/- 31.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1184000, episode_reward=-4135.60 +/- 31.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1185000, episode_reward=-4093.00 +/- 54.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1186000, episode_reward=-4151.80 +/- 39.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1187000, episode_reward=-4134.60 +/- 44.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1188000, episode_reward=-4131.20 +/- 34.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1189000, episode_reward=-4173.20 +/- 36.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1190000, episode_reward=-4133.40 +/- 22.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1191000, episode_reward=-4147.20 +/- 32.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1192000, episode_reward=-4129.40 +/- 65.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1193000, episode_reward=-4168.60 +/- 38.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1194000, episode_reward=-4119.00 +/- 22.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1195000, episode_reward=-4154.00 +/- 28.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1196000, episode_reward=-4108.40 +/- 59.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1197000, episode_reward=-4120.00 +/- 51.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1198000, episode_reward=-4110.20 +/- 35.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1199000, episode_reward=-4155.00 +/- 15.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1200000, episode_reward=-4126.80 +/- 48.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1201000, episode_reward=-4114.20 +/- 30.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1202000, episode_reward=-4072.60 +/- 72.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1203000, episode_reward=-4135.60 +/- 32.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1204000, episode_reward=-4108.20 +/- 27.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1205000, episode_reward=-4127.00 +/- 40.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1206000, episode_reward=-4146.40 +/- 47.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1207000, episode_reward=-4123.80 +/- 36.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1208000, episode_reward=-4130.80 +/- 48.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1209000, episode_reward=-4152.40 +/- 20.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1210000, episode_reward=-4143.20 +/- 46.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1211000, episode_reward=-4098.80 +/- 90.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1212000, episode_reward=-4121.60 +/- 43.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1213000, episode_reward=-4136.20 +/- 61.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1214000, episode_reward=-4115.60 +/- 27.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1215000, episode_reward=-4117.40 +/- 72.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1216000, episode_reward=-4141.20 +/- 41.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1217000, episode_reward=-4177.20 +/- 48.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1218000, episode_reward=-4144.00 +/- 36.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1219000, episode_reward=-4108.40 +/- 22.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1220000, episode_reward=-4151.00 +/- 41.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1221000, episode_reward=-4154.60 +/- 17.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1222000, episode_reward=-4091.20 +/- 46.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1223000, episode_reward=-4134.00 +/- 23.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1224000, episode_reward=-4124.60 +/- 31.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1225000, episode_reward=-4143.80 +/- 43.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1226000, episode_reward=-4143.60 +/- 35.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1227000, episode_reward=-4114.40 +/- 56.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1228000, episode_reward=-4177.00 +/- 31.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1229000, episode_reward=-4127.60 +/- 39.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1230000, episode_reward=-4116.20 +/- 24.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1231000, episode_reward=-4101.20 +/- 48.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1232000, episode_reward=-4125.60 +/- 84.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1233000, episode_reward=-4177.80 +/- 27.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1234000, episode_reward=-4096.00 +/- 36.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1235000, episode_reward=-4121.40 +/- 39.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1236000, episode_reward=-4138.20 +/- 21.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1237000, episode_reward=-4112.20 +/- 57.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1238000, episode_reward=-4137.60 +/- 35.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1239000, episode_reward=-4150.60 +/- 28.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1240000, episode_reward=-4162.20 +/- 22.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1241000, episode_reward=-4105.00 +/- 49.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1242000, episode_reward=-4107.00 +/- 32.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1243000, episode_reward=-4138.20 +/- 46.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1244000, episode_reward=-4134.20 +/- 39.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1245000, episode_reward=-4128.40 +/- 38.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1246000, episode_reward=-4155.80 +/- 47.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1247000, episode_reward=-4144.40 +/- 57.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1248000, episode_reward=-4158.20 +/- 72.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1249000, episode_reward=-4143.20 +/- 29.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1250000, episode_reward=-4094.40 +/- 44.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1251000, episode_reward=-4090.40 +/- 74.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1252000, episode_reward=-4075.60 +/- 45.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1253000, episode_reward=-4118.00 +/- 50.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1254000, episode_reward=-4084.60 +/- 39.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1255000, episode_reward=-4102.20 +/- 37.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1256000, episode_reward=-4118.60 +/- 46.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1257000, episode_reward=-4115.40 +/- 28.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1258000, episode_reward=-4150.60 +/- 49.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1259000, episode_reward=-4113.80 +/- 61.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1260000, episode_reward=-4107.60 +/- 65.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1261000, episode_reward=-4176.00 +/- 25.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1262000, episode_reward=-4137.80 +/- 22.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1263000, episode_reward=-4115.40 +/- 40.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1264000, episode_reward=-4108.60 +/- 47.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1265000, episode_reward=-4095.20 +/- 38.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1266000, episode_reward=-4130.60 +/- 23.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1267000, episode_reward=-4161.00 +/- 21.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1268000, episode_reward=-4146.40 +/- 36.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1269000, episode_reward=-4150.60 +/- 33.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1270000, episode_reward=-4112.80 +/- 17.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1271000, episode_reward=-4114.00 +/- 27.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1272000, episode_reward=-4121.60 +/- 53.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1273000, episode_reward=-4082.60 +/- 63.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1274000, episode_reward=-4115.20 +/- 49.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1275000, episode_reward=-4112.80 +/- 37.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1276000, episode_reward=-4136.60 +/- 52.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1277000, episode_reward=-4166.60 +/- 39.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1278000, episode_reward=-4135.20 +/- 51.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1279000, episode_reward=-4063.20 +/- 44.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1280000, episode_reward=-4130.40 +/- 49.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1281000, episode_reward=-4165.20 +/- 25.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1282000, episode_reward=-4124.00 +/- 22.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1283000, episode_reward=-4140.00 +/- 11.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1284000, episode_reward=-4114.00 +/- 50.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1285000, episode_reward=-4135.80 +/- 28.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1286000, episode_reward=-4132.00 +/- 15.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1287000, episode_reward=-4148.00 +/- 22.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1288000, episode_reward=-4126.40 +/- 14.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1289000, episode_reward=-4109.60 +/- 60.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1290000, episode_reward=-4157.60 +/- 10.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1291000, episode_reward=-4149.80 +/- 56.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1292000, episode_reward=-4147.20 +/- 56.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1293000, episode_reward=-4155.00 +/- 30.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1294000, episode_reward=-4164.40 +/- 42.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1295000, episode_reward=-4102.00 +/- 79.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1296000, episode_reward=-4117.60 +/- 42.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1297000, episode_reward=-4125.40 +/- 10.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1298000, episode_reward=-4122.60 +/- 38.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1299000, episode_reward=-4128.60 +/- 39.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1300000, episode_reward=-4170.40 +/- 52.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1301000, episode_reward=-4159.20 +/- 45.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1302000, episode_reward=-4130.20 +/- 64.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1303000, episode_reward=-4142.20 +/- 24.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1304000, episode_reward=-4142.80 +/- 57.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1305000, episode_reward=-4164.40 +/- 39.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1306000, episode_reward=-4127.60 +/- 72.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1307000, episode_reward=-4099.60 +/- 70.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1308000, episode_reward=-4148.80 +/- 46.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1309000, episode_reward=-4142.60 +/- 28.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1310000, episode_reward=-4129.40 +/- 36.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1311000, episode_reward=-4126.20 +/- 38.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1312000, episode_reward=-4147.40 +/- 36.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1313000, episode_reward=-4139.60 +/- 26.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1314000, episode_reward=-4175.20 +/- 18.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1315000, episode_reward=-4104.80 +/- 37.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1316000, episode_reward=-4152.20 +/- 22.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1317000, episode_reward=-4152.00 +/- 19.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1318000, episode_reward=-4113.40 +/- 64.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1319000, episode_reward=-4133.60 +/- 44.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1320000, episode_reward=-4098.00 +/- 33.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1321000, episode_reward=-4134.00 +/- 43.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1322000, episode_reward=-4142.40 +/- 50.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1323000, episode_reward=-4137.00 +/- 35.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1324000, episode_reward=-4126.00 +/- 64.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1325000, episode_reward=-4138.20 +/- 61.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1326000, episode_reward=-4097.40 +/- 37.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1327000, episode_reward=-4134.40 +/- 33.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1328000, episode_reward=-4148.80 +/- 30.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1329000, episode_reward=-4136.80 +/- 22.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1330000, episode_reward=-4133.00 +/- 39.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1331000, episode_reward=-4139.00 +/- 34.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1332000, episode_reward=-4140.80 +/- 62.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1333000, episode_reward=-4141.40 +/- 48.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1334000, episode_reward=-4125.40 +/- 42.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1335000, episode_reward=-4089.20 +/- 28.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1336000, episode_reward=-4143.00 +/- 64.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1337000, episode_reward=-4145.20 +/- 38.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1338000, episode_reward=-4184.00 +/- 15.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1339000, episode_reward=-4125.60 +/- 4.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1340000, episode_reward=-4126.80 +/- 28.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1341000, episode_reward=-4150.40 +/- 36.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1342000, episode_reward=-4146.80 +/- 43.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1343000, episode_reward=-4116.40 +/- 60.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1344000, episode_reward=-4131.80 +/- 61.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1345000, episode_reward=-4133.60 +/- 45.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1346000, episode_reward=-4144.20 +/- 48.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1347000, episode_reward=-4129.40 +/- 63.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1348000, episode_reward=-4146.00 +/- 34.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1349000, episode_reward=-4140.00 +/- 46.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1350000, episode_reward=-4101.40 +/- 44.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1351000, episode_reward=-4134.20 +/- 15.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1352000, episode_reward=-4123.80 +/- 51.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1353000, episode_reward=-4111.80 +/- 56.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1354000, episode_reward=-4127.40 +/- 27.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1355000, episode_reward=-4128.80 +/- 26.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1356000, episode_reward=-4148.60 +/- 38.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1357000, episode_reward=-4161.80 +/- 33.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1358000, episode_reward=-4103.80 +/- 52.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1359000, episode_reward=-4123.00 +/- 50.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1360000, episode_reward=-4138.20 +/- 58.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1361000, episode_reward=-4102.00 +/- 54.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1362000, episode_reward=-4155.60 +/- 16.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1363000, episode_reward=-4132.60 +/- 32.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1364000, episode_reward=-4145.40 +/- 28.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1365000, episode_reward=-4105.60 +/- 31.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1366000, episode_reward=-4153.80 +/- 46.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1367000, episode_reward=-4131.60 +/- 77.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1368000, episode_reward=-4115.80 +/- 52.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1369000, episode_reward=-4165.40 +/- 25.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1370000, episode_reward=-4084.60 +/- 83.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1371000, episode_reward=-4109.00 +/- 62.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1372000, episode_reward=-4142.20 +/- 15.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1373000, episode_reward=-4145.00 +/- 36.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1374000, episode_reward=-4118.40 +/- 37.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1375000, episode_reward=-4150.20 +/- 21.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1376000, episode_reward=-4106.80 +/- 43.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1377000, episode_reward=-4164.60 +/- 13.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1378000, episode_reward=-4133.40 +/- 43.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1379000, episode_reward=-4099.80 +/- 45.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1380000, episode_reward=-4165.80 +/- 32.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1381000, episode_reward=-4097.00 +/- 48.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1382000, episode_reward=-4104.80 +/- 53.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1383000, episode_reward=-4130.00 +/- 32.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1384000, episode_reward=-4167.40 +/- 17.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1385000, episode_reward=-4112.00 +/- 41.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1386000, episode_reward=-4084.80 +/- 69.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1387000, episode_reward=-4137.20 +/- 59.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1388000, episode_reward=-4112.40 +/- 59.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1389000, episode_reward=-4154.20 +/- 34.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1390000, episode_reward=-4142.80 +/- 35.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1391000, episode_reward=-4138.80 +/- 26.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1392000, episode_reward=-4175.00 +/- 9.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1393000, episode_reward=-4159.60 +/- 44.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1394000, episode_reward=-4140.40 +/- 50.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1395000, episode_reward=-4123.00 +/- 59.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1396000, episode_reward=-4130.20 +/- 24.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1397000, episode_reward=-4148.20 +/- 45.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1398000, episode_reward=-4167.60 +/- 46.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1399000, episode_reward=-4145.40 +/- 17.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1400000, episode_reward=-4141.00 +/- 40.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1401000, episode_reward=-4128.80 +/- 24.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1402000, episode_reward=-4114.80 +/- 35.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1403000, episode_reward=-4129.40 +/- 39.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1404000, episode_reward=-4133.80 +/- 49.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1405000, episode_reward=-4130.60 +/- 38.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1406000, episode_reward=-4138.80 +/- 36.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1407000, episode_reward=-4157.00 +/- 46.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1408000, episode_reward=-4141.60 +/- 36.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1409000, episode_reward=-4135.40 +/- 26.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1410000, episode_reward=-4110.80 +/- 35.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1411000, episode_reward=-4121.80 +/- 19.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1412000, episode_reward=-4113.20 +/- 40.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1413000, episode_reward=-4142.40 +/- 58.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1414000, episode_reward=-4149.00 +/- 26.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1415000, episode_reward=-4128.80 +/- 45.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1416000, episode_reward=-4134.80 +/- 33.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1417000, episode_reward=-4119.40 +/- 39.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1418000, episode_reward=-4083.80 +/- 43.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1419000, episode_reward=-4128.60 +/- 25.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1420000, episode_reward=-4129.00 +/- 45.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1421000, episode_reward=-4141.60 +/- 59.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1422000, episode_reward=-4052.00 +/- 33.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1423000, episode_reward=-4140.60 +/- 53.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1424000, episode_reward=-4145.20 +/- 41.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1425000, episode_reward=-4099.80 +/- 75.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1426000, episode_reward=-4107.20 +/- 90.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1427000, episode_reward=-4110.60 +/- 26.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1428000, episode_reward=-4108.60 +/- 51.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1429000, episode_reward=-4158.20 +/- 65.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1430000, episode_reward=-4108.60 +/- 52.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1431000, episode_reward=-4153.40 +/- 12.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1432000, episode_reward=-4133.80 +/- 48.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1433000, episode_reward=-4111.00 +/- 61.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1434000, episode_reward=-4119.20 +/- 69.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1435000, episode_reward=-4116.80 +/- 48.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1436000, episode_reward=-4103.60 +/- 49.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1437000, episode_reward=-4107.60 +/- 67.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1438000, episode_reward=-4137.40 +/- 19.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1439000, episode_reward=-4148.80 +/- 48.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1440000, episode_reward=-4158.60 +/- 36.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1441000, episode_reward=-4136.40 +/- 40.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1442000, episode_reward=-4127.40 +/- 33.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1443000, episode_reward=-4138.20 +/- 69.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1444000, episode_reward=-4116.80 +/- 31.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1445000, episode_reward=-4169.20 +/- 38.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1446000, episode_reward=-4108.80 +/- 34.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1447000, episode_reward=-4138.00 +/- 32.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1448000, episode_reward=-4158.40 +/- 19.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1449000, episode_reward=-4134.00 +/- 11.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1450000, episode_reward=-4143.80 +/- 48.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1451000, episode_reward=-4111.00 +/- 51.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1452000, episode_reward=-4140.40 +/- 60.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1453000, episode_reward=-4144.80 +/- 40.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1454000, episode_reward=-4137.00 +/- 58.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1455000, episode_reward=-4125.40 +/- 32.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1456000, episode_reward=-4107.00 +/- 61.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1457000, episode_reward=-4119.60 +/- 42.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1458000, episode_reward=-4124.00 +/- 51.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1459000, episode_reward=-4126.40 +/- 32.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1460000, episode_reward=-4167.80 +/- 24.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1461000, episode_reward=-4137.20 +/- 33.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1462000, episode_reward=-4105.80 +/- 44.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1463000, episode_reward=-4154.80 +/- 32.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1464000, episode_reward=-4118.20 +/- 54.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1465000, episode_reward=-4138.40 +/- 30.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1466000, episode_reward=-4162.60 +/- 20.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1467000, episode_reward=-4136.20 +/- 50.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1468000, episode_reward=-4139.40 +/- 38.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1469000, episode_reward=-4130.60 +/- 14.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1470000, episode_reward=-4120.60 +/- 33.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1471000, episode_reward=-4138.80 +/- 25.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1472000, episode_reward=-4142.00 +/- 56.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1473000, episode_reward=-4142.00 +/- 51.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1474000, episode_reward=-4152.80 +/- 47.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1475000, episode_reward=-4159.20 +/- 30.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1476000, episode_reward=-4106.20 +/- 31.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1477000, episode_reward=-4129.00 +/- 68.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1478000, episode_reward=-4147.40 +/- 33.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1479000, episode_reward=-4122.60 +/- 26.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1480000, episode_reward=-4180.80 +/- 23.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1481000, episode_reward=-4091.00 +/- 83.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1482000, episode_reward=-4124.60 +/- 30.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1483000, episode_reward=-4126.20 +/- 92.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1484000, episode_reward=-4169.40 +/- 15.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1485000, episode_reward=-4159.40 +/- 33.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1486000, episode_reward=-4138.40 +/- 38.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1487000, episode_reward=-4124.20 +/- 37.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1488000, episode_reward=-4087.60 +/- 34.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1489000, episode_reward=-4133.60 +/- 36.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1490000, episode_reward=-4133.80 +/- 26.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1491000, episode_reward=-4157.40 +/- 22.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1492000, episode_reward=-4165.60 +/- 42.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1493000, episode_reward=-4145.00 +/- 38.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1494000, episode_reward=-4120.60 +/- 53.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1495000, episode_reward=-4130.60 +/- 39.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1496000, episode_reward=-4137.00 +/- 51.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1497000, episode_reward=-4102.40 +/- 22.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1498000, episode_reward=-4135.20 +/- 30.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1499000, episode_reward=-4142.40 +/- 59.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1500000, episode_reward=-4164.20 +/- 26.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1501000, episode_reward=-4134.80 +/- 51.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1502000, episode_reward=-4147.80 +/- 39.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1503000, episode_reward=-4123.80 +/- 39.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1504000, episode_reward=-4138.80 +/- 36.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1505000, episode_reward=-4104.20 +/- 52.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1506000, episode_reward=-4106.20 +/- 78.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1507000, episode_reward=-4124.40 +/- 27.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1508000, episode_reward=-4120.80 +/- 53.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1509000, episode_reward=-4150.40 +/- 45.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1510000, episode_reward=-4135.20 +/- 34.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1511000, episode_reward=-4125.00 +/- 22.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1512000, episode_reward=-4126.60 +/- 28.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1513000, episode_reward=-4133.80 +/- 54.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1514000, episode_reward=-4172.60 +/- 38.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1515000, episode_reward=-4125.60 +/- 52.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1516000, episode_reward=-4126.20 +/- 33.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1517000, episode_reward=-4123.20 +/- 36.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1518000, episode_reward=-4129.80 +/- 53.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1519000, episode_reward=-4135.20 +/- 52.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1520000, episode_reward=-4149.40 +/- 28.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1521000, episode_reward=-4156.00 +/- 44.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1522000, episode_reward=-4155.20 +/- 11.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1523000, episode_reward=-4130.80 +/- 35.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1524000, episode_reward=-4125.80 +/- 29.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1525000, episode_reward=-4127.80 +/- 53.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1526000, episode_reward=-4127.60 +/- 13.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1527000, episode_reward=-4132.20 +/- 33.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1528000, episode_reward=-4171.80 +/- 19.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1529000, episode_reward=-4178.00 +/- 24.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1530000, episode_reward=-4094.20 +/- 41.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1531000, episode_reward=-4106.40 +/- 32.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1532000, episode_reward=-4159.60 +/- 28.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1533000, episode_reward=-4127.40 +/- 37.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1534000, episode_reward=-4124.40 +/- 20.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1535000, episode_reward=-4145.40 +/- 53.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1536000, episode_reward=-4080.60 +/- 62.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1537000, episode_reward=-4116.20 +/- 44.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1538000, episode_reward=-4167.80 +/- 32.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1539000, episode_reward=-4144.40 +/- 29.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1540000, episode_reward=-4127.20 +/- 32.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1541000, episode_reward=-4123.80 +/- 39.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1542000, episode_reward=-4139.20 +/- 56.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1543000, episode_reward=-4152.00 +/- 61.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1544000, episode_reward=-4130.80 +/- 29.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1545000, episode_reward=-4154.60 +/- 36.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1546000, episode_reward=-4155.80 +/- 34.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1547000, episode_reward=-4106.60 +/- 39.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1548000, episode_reward=-4127.80 +/- 27.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1549000, episode_reward=-4147.40 +/- 39.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1550000, episode_reward=-4091.00 +/- 51.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1551000, episode_reward=-4119.40 +/- 49.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1552000, episode_reward=-4132.00 +/- 27.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1553000, episode_reward=-4158.60 +/- 41.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1554000, episode_reward=-4127.20 +/- 26.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1555000, episode_reward=-4141.60 +/- 40.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1556000, episode_reward=-4103.20 +/- 56.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1557000, episode_reward=-4132.80 +/- 31.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1558000, episode_reward=-4118.00 +/- 72.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1559000, episode_reward=-4127.00 +/- 31.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1560000, episode_reward=-4106.00 +/- 33.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1561000, episode_reward=-4147.80 +/- 24.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1562000, episode_reward=-4114.00 +/- 24.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1563000, episode_reward=-4106.60 +/- 40.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1564000, episode_reward=-4135.40 +/- 47.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1565000, episode_reward=-4127.80 +/- 25.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1566000, episode_reward=-4109.00 +/- 61.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1567000, episode_reward=-4170.60 +/- 52.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1568000, episode_reward=-4125.20 +/- 29.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1569000, episode_reward=-4141.40 +/- 30.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1570000, episode_reward=-4132.60 +/- 27.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1571000, episode_reward=-4167.20 +/- 38.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1572000, episode_reward=-4178.20 +/- 18.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1573000, episode_reward=-4115.80 +/- 35.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1574000, episode_reward=-4139.80 +/- 26.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1575000, episode_reward=-4144.20 +/- 11.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1576000, episode_reward=-4161.00 +/- 34.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1577000, episode_reward=-4104.60 +/- 24.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1578000, episode_reward=-4152.40 +/- 16.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1579000, episode_reward=-4117.00 +/- 48.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1580000, episode_reward=-4164.80 +/- 27.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1581000, episode_reward=-4135.60 +/- 49.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1582000, episode_reward=-4133.80 +/- 46.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1583000, episode_reward=-4148.80 +/- 37.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1584000, episode_reward=-4131.00 +/- 36.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1585000, episode_reward=-4172.60 +/- 9.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1586000, episode_reward=-4117.80 +/- 39.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1587000, episode_reward=-4134.80 +/- 52.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1588000, episode_reward=-4115.20 +/- 61.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1589000, episode_reward=-4124.40 +/- 35.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1590000, episode_reward=-4137.80 +/- 37.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1591000, episode_reward=-4131.20 +/- 33.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1592000, episode_reward=-4139.00 +/- 22.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1593000, episode_reward=-4142.20 +/- 23.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1594000, episode_reward=-4132.40 +/- 35.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1595000, episode_reward=-4128.80 +/- 49.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1596000, episode_reward=-4127.20 +/- 35.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1597000, episode_reward=-4141.40 +/- 56.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1598000, episode_reward=-4138.00 +/- 33.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1599000, episode_reward=-4167.40 +/- 26.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1600000, episode_reward=-4108.60 +/- 43.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1601000, episode_reward=-4121.00 +/- 39.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1602000, episode_reward=-4105.40 +/- 67.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1603000, episode_reward=-4163.40 +/- 18.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1604000, episode_reward=-4140.80 +/- 69.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1605000, episode_reward=-4145.60 +/- 73.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1606000, episode_reward=-4107.60 +/- 62.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1607000, episode_reward=-4115.20 +/- 50.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1608000, episode_reward=-4142.00 +/- 28.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1609000, episode_reward=-4113.40 +/- 21.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1610000, episode_reward=-4125.60 +/- 29.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1611000, episode_reward=-4135.20 +/- 27.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1612000, episode_reward=-4114.00 +/- 45.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1613000, episode_reward=-4149.00 +/- 20.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1614000, episode_reward=-4126.40 +/- 53.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1615000, episode_reward=-4140.40 +/- 50.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1616000, episode_reward=-4128.40 +/- 20.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1617000, episode_reward=-4150.00 +/- 30.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1618000, episode_reward=-4153.40 +/- 28.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1619000, episode_reward=-4126.80 +/- 38.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1620000, episode_reward=-4095.20 +/- 57.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1621000, episode_reward=-4134.00 +/- 49.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1622000, episode_reward=-4136.00 +/- 26.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1623000, episode_reward=-4133.20 +/- 32.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1624000, episode_reward=-4157.80 +/- 26.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1625000, episode_reward=-4127.60 +/- 48.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1626000, episode_reward=-4139.40 +/- 30.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1627000, episode_reward=-4129.60 +/- 38.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1628000, episode_reward=-4154.80 +/- 45.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1629000, episode_reward=-4118.40 +/- 29.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1630000, episode_reward=-4136.40 +/- 39.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1631000, episode_reward=-4125.20 +/- 33.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1632000, episode_reward=-4097.00 +/- 65.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1633000, episode_reward=-4129.40 +/- 38.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1634000, episode_reward=-4161.80 +/- 31.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1635000, episode_reward=-4144.80 +/- 32.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1636000, episode_reward=-4114.20 +/- 56.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1637000, episode_reward=-4151.00 +/- 49.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1638000, episode_reward=-4093.00 +/- 59.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1639000, episode_reward=-4105.80 +/- 53.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1640000, episode_reward=-4126.20 +/- 36.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1641000, episode_reward=-4151.20 +/- 28.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1642000, episode_reward=-4134.20 +/- 60.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1643000, episode_reward=-4129.00 +/- 63.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1644000, episode_reward=-4152.00 +/- 43.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1645000, episode_reward=-4118.40 +/- 23.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1646000, episode_reward=-4164.60 +/- 48.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1647000, episode_reward=-4156.40 +/- 42.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1648000, episode_reward=-4125.20 +/- 40.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1649000, episode_reward=-4107.20 +/- 33.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1650000, episode_reward=-4127.60 +/- 62.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1651000, episode_reward=-4138.40 +/- 31.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1652000, episode_reward=-4120.80 +/- 57.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1653000, episode_reward=-4123.60 +/- 53.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1654000, episode_reward=-4140.40 +/- 30.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1655000, episode_reward=-4160.60 +/- 37.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1656000, episode_reward=-4144.00 +/- 18.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1657000, episode_reward=-4114.00 +/- 59.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1658000, episode_reward=-4130.20 +/- 30.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1659000, episode_reward=-4150.40 +/- 87.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1660000, episode_reward=-4152.20 +/- 35.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1661000, episode_reward=-4157.00 +/- 20.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1662000, episode_reward=-4100.80 +/- 25.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1663000, episode_reward=-4116.80 +/- 55.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1664000, episode_reward=-4111.60 +/- 34.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1665000, episode_reward=-4152.40 +/- 37.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1666000, episode_reward=-4131.80 +/- 27.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1667000, episode_reward=-4133.20 +/- 46.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1668000, episode_reward=-4114.20 +/- 51.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1669000, episode_reward=-4137.80 +/- 33.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1670000, episode_reward=-4115.80 +/- 72.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1671000, episode_reward=-4122.80 +/- 18.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1672000, episode_reward=-4143.40 +/- 51.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1673000, episode_reward=-4115.20 +/- 12.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1674000, episode_reward=-4088.80 +/- 63.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1675000, episode_reward=-4113.80 +/- 27.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1676000, episode_reward=-4134.60 +/- 60.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1677000, episode_reward=-4069.40 +/- 86.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1678000, episode_reward=-4079.20 +/- 63.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1679000, episode_reward=-4138.00 +/- 41.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1680000, episode_reward=-4145.20 +/- 17.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1681000, episode_reward=-4160.80 +/- 53.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1682000, episode_reward=-4149.40 +/- 38.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1683000, episode_reward=-4180.80 +/- 20.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1684000, episode_reward=-4126.20 +/- 13.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1685000, episode_reward=-4108.60 +/- 26.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1686000, episode_reward=-4139.60 +/- 16.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1687000, episode_reward=-4141.40 +/- 37.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1688000, episode_reward=-4127.40 +/- 35.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1689000, episode_reward=-4086.00 +/- 62.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1690000, episode_reward=-4141.80 +/- 31.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1691000, episode_reward=-4113.60 +/- 48.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1692000, episode_reward=-4120.40 +/- 21.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1693000, episode_reward=-4101.40 +/- 27.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1694000, episode_reward=-4116.20 +/- 26.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1695000, episode_reward=-4142.80 +/- 58.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1696000, episode_reward=-4136.60 +/- 47.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1697000, episode_reward=-4101.80 +/- 84.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1698000, episode_reward=-4130.80 +/- 31.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1699000, episode_reward=-4143.20 +/- 21.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1700000, episode_reward=-4087.20 +/- 83.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1701000, episode_reward=-4129.40 +/- 29.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1702000, episode_reward=-4140.00 +/- 49.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1703000, episode_reward=-4121.40 +/- 55.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1704000, episode_reward=-4141.80 +/- 58.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1705000, episode_reward=-4102.20 +/- 37.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1706000, episode_reward=-4120.40 +/- 73.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1707000, episode_reward=-4065.40 +/- 66.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1708000, episode_reward=-4126.60 +/- 43.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1709000, episode_reward=-4119.20 +/- 39.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1710000, episode_reward=-4153.20 +/- 37.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1711000, episode_reward=-4075.60 +/- 65.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1712000, episode_reward=-4100.40 +/- 58.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1713000, episode_reward=-4152.80 +/- 25.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1714000, episode_reward=-4070.00 +/- 73.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1715000, episode_reward=-4102.80 +/- 32.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1716000, episode_reward=-4167.60 +/- 36.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1717000, episode_reward=-4180.00 +/- 12.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1718000, episode_reward=-4137.20 +/- 48.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1719000, episode_reward=-4135.80 +/- 60.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1720000, episode_reward=-4141.80 +/- 31.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1721000, episode_reward=-4091.40 +/- 62.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1722000, episode_reward=-4168.20 +/- 45.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1723000, episode_reward=-4145.20 +/- 26.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1724000, episode_reward=-4147.40 +/- 15.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1725000, episode_reward=-4125.60 +/- 76.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1726000, episode_reward=-4120.60 +/- 51.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1727000, episode_reward=-4110.80 +/- 52.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1728000, episode_reward=-4145.40 +/- 53.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1729000, episode_reward=-4117.60 +/- 39.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1730000, episode_reward=-4107.20 +/- 42.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1731000, episode_reward=-4134.40 +/- 69.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1732000, episode_reward=-4132.80 +/- 43.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1733000, episode_reward=-4128.20 +/- 47.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1734000, episode_reward=-4129.60 +/- 44.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1735000, episode_reward=-4181.20 +/- 47.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1736000, episode_reward=-4118.80 +/- 30.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1737000, episode_reward=-4118.60 +/- 51.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1738000, episode_reward=-4112.20 +/- 32.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1739000, episode_reward=-4150.00 +/- 26.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1740000, episode_reward=-4153.60 +/- 32.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1741000, episode_reward=-4155.40 +/- 33.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1742000, episode_reward=-4126.20 +/- 32.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1743000, episode_reward=-4145.60 +/- 27.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1744000, episode_reward=-4105.20 +/- 58.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1745000, episode_reward=-4138.20 +/- 30.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1746000, episode_reward=-4116.40 +/- 27.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1747000, episode_reward=-4143.60 +/- 40.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1748000, episode_reward=-4133.00 +/- 43.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1749000, episode_reward=-4144.00 +/- 49.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1750000, episode_reward=-4104.80 +/- 31.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1751000, episode_reward=-4119.40 +/- 33.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1752000, episode_reward=-4157.80 +/- 35.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1753000, episode_reward=-4153.40 +/- 38.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1754000, episode_reward=-4139.80 +/- 35.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1755000, episode_reward=-4177.20 +/- 27.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1756000, episode_reward=-4144.20 +/- 27.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1757000, episode_reward=-4119.40 +/- 33.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1758000, episode_reward=-4155.20 +/- 38.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1759000, episode_reward=-4173.00 +/- 26.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1760000, episode_reward=-4110.80 +/- 70.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1761000, episode_reward=-4102.60 +/- 38.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1762000, episode_reward=-4125.60 +/- 52.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1763000, episode_reward=-4100.20 +/- 61.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1764000, episode_reward=-4147.20 +/- 25.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1765000, episode_reward=-4130.60 +/- 57.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1766000, episode_reward=-4126.60 +/- 53.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1767000, episode_reward=-4171.40 +/- 30.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1768000, episode_reward=-4126.80 +/- 64.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1769000, episode_reward=-4129.00 +/- 36.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1770000, episode_reward=-4128.80 +/- 24.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1771000, episode_reward=-4132.60 +/- 10.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1772000, episode_reward=-4114.40 +/- 11.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1773000, episode_reward=-4116.60 +/- 31.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1774000, episode_reward=-4160.00 +/- 38.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1775000, episode_reward=-4109.40 +/- 46.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1776000, episode_reward=-4150.00 +/- 32.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1777000, episode_reward=-4146.20 +/- 42.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1778000, episode_reward=-4157.60 +/- 31.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1779000, episode_reward=-4132.20 +/- 39.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1780000, episode_reward=-4109.20 +/- 43.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1781000, episode_reward=-4134.00 +/- 43.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1782000, episode_reward=-4127.20 +/- 29.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1783000, episode_reward=-4122.20 +/- 19.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1784000, episode_reward=-4176.60 +/- 39.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1785000, episode_reward=-4152.80 +/- 21.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1786000, episode_reward=-4124.20 +/- 43.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1787000, episode_reward=-4139.40 +/- 41.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1788000, episode_reward=-4095.20 +/- 20.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1789000, episode_reward=-4162.40 +/- 61.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1790000, episode_reward=-4092.20 +/- 60.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1791000, episode_reward=-4157.60 +/- 26.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1792000, episode_reward=-4136.80 +/- 32.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1793000, episode_reward=-4109.40 +/- 50.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1794000, episode_reward=-4153.20 +/- 43.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1795000, episode_reward=-4151.00 +/- 22.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1796000, episode_reward=-4165.80 +/- 27.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1797000, episode_reward=-4133.60 +/- 69.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1798000, episode_reward=-4131.80 +/- 54.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1799000, episode_reward=-4113.60 +/- 34.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1800000, episode_reward=-4122.80 +/- 74.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1801000, episode_reward=-4114.00 +/- 31.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1802000, episode_reward=-4116.60 +/- 31.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1803000, episode_reward=-4099.00 +/- 56.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1804000, episode_reward=-4148.80 +/- 25.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1805000, episode_reward=-4108.60 +/- 63.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1806000, episode_reward=-4111.60 +/- 81.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1807000, episode_reward=-4118.80 +/- 104.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1808000, episode_reward=-4183.00 +/- 39.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1809000, episode_reward=-4158.60 +/- 50.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1810000, episode_reward=-4145.40 +/- 29.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1811000, episode_reward=-4154.40 +/- 39.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1812000, episode_reward=-4113.80 +/- 19.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1813000, episode_reward=-4140.80 +/- 27.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1814000, episode_reward=-4131.80 +/- 30.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1815000, episode_reward=-4126.40 +/- 81.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1816000, episode_reward=-4162.60 +/- 41.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1817000, episode_reward=-4124.60 +/- 56.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1818000, episode_reward=-4130.40 +/- 43.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1819000, episode_reward=-4122.00 +/- 74.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1820000, episode_reward=-4160.80 +/- 17.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1821000, episode_reward=-4110.20 +/- 48.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1822000, episode_reward=-4151.20 +/- 66.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1823000, episode_reward=-4123.80 +/- 43.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1824000, episode_reward=-4134.60 +/- 51.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1825000, episode_reward=-4101.60 +/- 35.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1826000, episode_reward=-4125.80 +/- 40.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1827000, episode_reward=-4071.80 +/- 45.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1828000, episode_reward=-4108.80 +/- 26.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1829000, episode_reward=-4154.40 +/- 26.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1830000, episode_reward=-4147.20 +/- 21.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1831000, episode_reward=-4124.80 +/- 45.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1832000, episode_reward=-4133.80 +/- 35.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1833000, episode_reward=-4104.40 +/- 59.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1834000, episode_reward=-4104.40 +/- 46.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1835000, episode_reward=-4134.80 +/- 36.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1836000, episode_reward=-4126.40 +/- 43.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1837000, episode_reward=-4158.80 +/- 35.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1838000, episode_reward=-4177.60 +/- 26.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1839000, episode_reward=-4154.40 +/- 40.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1840000, episode_reward=-4103.40 +/- 40.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1841000, episode_reward=-4135.80 +/- 30.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1842000, episode_reward=-4125.00 +/- 45.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1843000, episode_reward=-4142.00 +/- 31.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1844000, episode_reward=-4161.60 +/- 49.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1845000, episode_reward=-4161.20 +/- 18.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1846000, episode_reward=-4157.00 +/- 45.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1847000, episode_reward=-4154.60 +/- 42.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1848000, episode_reward=-4104.60 +/- 54.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1849000, episode_reward=-4137.80 +/- 29.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1850000, episode_reward=-4126.60 +/- 25.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1851000, episode_reward=-4145.40 +/- 25.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1852000, episode_reward=-4096.20 +/- 44.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1853000, episode_reward=-4132.80 +/- 73.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1854000, episode_reward=-4115.00 +/- 25.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1855000, episode_reward=-4165.80 +/- 34.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1856000, episode_reward=-4155.00 +/- 48.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1857000, episode_reward=-4111.40 +/- 16.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1858000, episode_reward=-4118.00 +/- 21.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1859000, episode_reward=-4151.00 +/- 53.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1860000, episode_reward=-4142.20 +/- 28.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1861000, episode_reward=-4134.20 +/- 32.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1862000, episode_reward=-4152.00 +/- 51.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1863000, episode_reward=-4127.80 +/- 33.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1864000, episode_reward=-4123.60 +/- 83.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1865000, episode_reward=-4111.00 +/- 46.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1866000, episode_reward=-4106.00 +/- 65.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1867000, episode_reward=-4150.60 +/- 30.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1868000, episode_reward=-4147.80 +/- 44.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1869000, episode_reward=-4155.80 +/- 30.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1870000, episode_reward=-4101.40 +/- 56.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1871000, episode_reward=-4109.80 +/- 65.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1872000, episode_reward=-4150.60 +/- 32.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1873000, episode_reward=-4148.80 +/- 61.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1874000, episode_reward=-4146.60 +/- 39.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1875000, episode_reward=-4134.80 +/- 34.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1876000, episode_reward=-4114.40 +/- 70.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1877000, episode_reward=-4135.00 +/- 41.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1878000, episode_reward=-4107.40 +/- 40.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1879000, episode_reward=-4118.80 +/- 79.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1880000, episode_reward=-4133.60 +/- 60.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1881000, episode_reward=-4099.60 +/- 21.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1882000, episode_reward=-4106.00 +/- 65.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1883000, episode_reward=-4144.20 +/- 65.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1884000, episode_reward=-4134.40 +/- 54.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1885000, episode_reward=-4126.40 +/- 39.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1886000, episode_reward=-4129.40 +/- 62.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1887000, episode_reward=-4165.40 +/- 26.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1888000, episode_reward=-4128.00 +/- 51.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1889000, episode_reward=-4148.00 +/- 52.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1890000, episode_reward=-4167.20 +/- 21.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1891000, episode_reward=-4141.80 +/- 43.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1892000, episode_reward=-4119.40 +/- 28.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1893000, episode_reward=-4135.60 +/- 68.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1894000, episode_reward=-4132.00 +/- 38.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1895000, episode_reward=-4160.20 +/- 27.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1896000, episode_reward=-4102.60 +/- 58.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1897000, episode_reward=-4122.60 +/- 50.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1898000, episode_reward=-4097.60 +/- 64.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1899000, episode_reward=-4122.20 +/- 47.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1900000, episode_reward=-4139.60 +/- 26.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1901000, episode_reward=-4119.20 +/- 46.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1902000, episode_reward=-4157.60 +/- 22.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1903000, episode_reward=-4119.60 +/- 51.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1904000, episode_reward=-4176.60 +/- 28.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1905000, episode_reward=-4150.40 +/- 48.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1906000, episode_reward=-4103.40 +/- 33.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1907000, episode_reward=-4141.00 +/- 23.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1908000, episode_reward=-4162.40 +/- 19.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1909000, episode_reward=-4148.00 +/- 32.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1910000, episode_reward=-4091.60 +/- 85.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1911000, episode_reward=-4151.40 +/- 28.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1912000, episode_reward=-4113.20 +/- 30.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1913000, episode_reward=-4139.40 +/- 43.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1914000, episode_reward=-4147.00 +/- 50.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1915000, episode_reward=-4146.40 +/- 34.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1916000, episode_reward=-4112.60 +/- 30.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1917000, episode_reward=-4140.00 +/- 55.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1918000, episode_reward=-4114.00 +/- 41.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1919000, episode_reward=-4105.80 +/- 63.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1920000, episode_reward=-4136.20 +/- 37.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1921000, episode_reward=-4084.00 +/- 45.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1922000, episode_reward=-4144.60 +/- 37.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1923000, episode_reward=-4152.60 +/- 52.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1924000, episode_reward=-4112.40 +/- 50.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1925000, episode_reward=-4143.60 +/- 46.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1926000, episode_reward=-4129.20 +/- 41.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1927000, episode_reward=-4116.60 +/- 108.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1928000, episode_reward=-4142.80 +/- 45.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1929000, episode_reward=-4127.80 +/- 30.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1930000, episode_reward=-4146.60 +/- 35.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1931000, episode_reward=-4156.20 +/- 40.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1932000, episode_reward=-4098.40 +/- 34.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1933000, episode_reward=-4124.60 +/- 15.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1934000, episode_reward=-4145.60 +/- 35.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1935000, episode_reward=-4145.80 +/- 42.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1936000, episode_reward=-4137.20 +/- 44.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1937000, episode_reward=-4118.40 +/- 17.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1938000, episode_reward=-4129.20 +/- 54.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1939000, episode_reward=-4142.20 +/- 28.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1940000, episode_reward=-4128.20 +/- 62.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1941000, episode_reward=-4102.40 +/- 53.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1942000, episode_reward=-4166.20 +/- 30.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1943000, episode_reward=-4138.80 +/- 38.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1944000, episode_reward=-4084.60 +/- 30.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1945000, episode_reward=-4119.20 +/- 21.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1946000, episode_reward=-4112.80 +/- 53.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1947000, episode_reward=-4109.60 +/- 41.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1948000, episode_reward=-4118.60 +/- 60.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1949000, episode_reward=-4140.00 +/- 57.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1950000, episode_reward=-4112.80 +/- 29.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1951000, episode_reward=-4133.40 +/- 38.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1952000, episode_reward=-4114.20 +/- 47.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1953000, episode_reward=-4136.60 +/- 50.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1954000, episode_reward=-4138.00 +/- 23.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1955000, episode_reward=-4137.40 +/- 19.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1956000, episode_reward=-4154.80 +/- 55.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1957000, episode_reward=-4100.20 +/- 46.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1958000, episode_reward=-4110.40 +/- 61.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1959000, episode_reward=-4130.20 +/- 15.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1960000, episode_reward=-4130.00 +/- 42.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1961000, episode_reward=-4136.60 +/- 31.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1962000, episode_reward=-4155.40 +/- 40.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1963000, episode_reward=-4108.60 +/- 17.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1964000, episode_reward=-4115.60 +/- 38.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1965000, episode_reward=-4122.60 +/- 40.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1966000, episode_reward=-4148.60 +/- 29.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1967000, episode_reward=-4123.40 +/- 31.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1968000, episode_reward=-4113.60 +/- 45.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1969000, episode_reward=-4124.40 +/- 40.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1970000, episode_reward=-4118.40 +/- 23.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1971000, episode_reward=-4136.00 +/- 62.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1972000, episode_reward=-4131.00 +/- 50.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1973000, episode_reward=-4107.40 +/- 79.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1974000, episode_reward=-4152.80 +/- 45.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1975000, episode_reward=-4137.60 +/- 68.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1976000, episode_reward=-4158.60 +/- 45.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1977000, episode_reward=-4103.40 +/- 49.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1978000, episode_reward=-4145.00 +/- 18.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1979000, episode_reward=-4093.40 +/- 22.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1980000, episode_reward=-4137.00 +/- 54.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1981000, episode_reward=-4145.20 +/- 39.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1982000, episode_reward=-4133.00 +/- 74.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1983000, episode_reward=-4146.60 +/- 68.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1984000, episode_reward=-4072.20 +/- 60.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1985000, episode_reward=-4123.40 +/- 26.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1986000, episode_reward=-4145.60 +/- 44.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1987000, episode_reward=-4104.60 +/- 66.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1988000, episode_reward=-4156.20 +/- 43.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1989000, episode_reward=-4134.00 +/- 50.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1990000, episode_reward=-4119.60 +/- 33.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1991000, episode_reward=-4129.60 +/- 37.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1992000, episode_reward=-4141.00 +/- 40.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1993000, episode_reward=-4110.80 +/- 67.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1994000, episode_reward=-4138.80 +/- 39.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1995000, episode_reward=-4143.20 +/- 36.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1996000, episode_reward=-4139.80 +/- 23.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1997000, episode_reward=-4143.40 +/- 41.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1998000, episode_reward=-4127.60 +/- 38.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1999000, episode_reward=-4143.80 +/- 38.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2000000, episode_reward=-4118.00 +/- 48.92\n",
      "Episode length: 100.00 +/- 0.00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(-4113.9, 46.828303407234394)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### REINFORCEMENT LEARNING I ###\n",
    "### TRAIN, SAVE, EVALUATE MODEL ###\n",
    "\n",
    "import gym\n",
    "import stable_baselines3 as sb\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "import pickle\n",
    "\n",
    "# Load diagnostics model from disk\n",
    "diag_model = pickle.load(open('diagnostics/model', 'rb'))\n",
    "# Initiate environment\n",
    "env = gym.make('Production-v0', diag_model = diag_model)\n",
    "# Callback for best model\n",
    "best_callback = EvalCallback(env, best_model_save_path='./callback/',\n",
    "                             log_path='./callback/', eval_freq=1000,\n",
    "                             deterministic=True, render=False)\n",
    "\n",
    "#model = sb.DQN('MlpPolicy', env, tensorboard_log=\"./tensorboard/\", gamma = 0.99, learning_rate=0.01)\n",
    "#model.learn(total_timesteps=2e6, tb_log_name=\"DQN_DIAG_model\", callback = best_callback)\n",
    "#model.save(\"DQN_DIAG_model\")\n",
    "\n",
    "#model = sb.A2C('MlpPolicy', env, tensorboard_log=\"./tensorboard/\", gamma = 0.99, learning_rate=0.01)\n",
    "#model.learn(total_timesteps=2e6, tb_log_name=\"A2C_DIAG_model\", callback = best_callback)\n",
    "#model.save(\"A2C_DIAG_model\")\n",
    "\n",
    "model = sb.PPO('MlpPolicy', env, tensorboard_log=\"./tensorboard/\")\n",
    "model.learn(total_timesteps=2e6, tb_log_name=\"PPO_DIAG_model\", callback = best_callback)\n",
    "model.save(\"PPO_DIAG_model\")\n",
    "\n",
    "# Evaluate the agent\n",
    "evaluate_policy(model, model.get_env(), n_eval_episodes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(497.3, 31.508887635078455)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### REINFORCEMENT LEARNING II ###\n",
    "### LOAD MODEL ###\n",
    "import gym\n",
    "import stable_baselines3 as sb\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "import pickle\n",
    "\n",
    "diag_model = pickle.load(open('diagnostics/model', 'rb'))\n",
    "#env = gym.make('Production-v0', diag_model = diag_model)\n",
    "env = gym.make('Production-v0')\n",
    "# Best Model\n",
    "model = DQN.load('./callback/best_model_truehealth', env = env)\n",
    "# Last Model\n",
    "#model = DQN.load('DQN_1_model', env = env)\n",
    "\n",
    "# Evaluate the agent\n",
    "evaluate_policy(model, model.get_env(), n_eval_episodes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average number of reactive maintenance interventions per episode is:  0.007\n",
      "The average number of preventive maintenance interventions per episode is:  3.99\n",
      "The average sum of inventory per episode is:  1.341\n",
      "The average sum of spare parts inventory per episode is:  8.541\n",
      "The average reward per episode is:  487.492\n",
      "The average upper bound per episode is:  644.47\n"
     ]
    }
   ],
   "source": [
    "### REINFORCEMENT LEARNING III ###\n",
    "### TRY AND EVALUATE MY MODEL ###\n",
    "import pandas as pd\n",
    "\n",
    "# Initilaize Reward\n",
    "result_df = pd.DataFrame([[0, 0, 0, 0, 0, 0]], columns=['RM', 'PM', 'Inventory', 'Spare Parts Inventory', 'Reward', 'Upper'])\n",
    "# Set iterations\n",
    "iterations = 1000\n",
    "for i in range(iterations):\n",
    "    # Initialize episode\n",
    "    store = []\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    store.append([0, obs[0], env.breakdown, obs[2], obs[3], 0, done, obs[1]])\n",
    "    # Compute one episode\n",
    "    while not done:\n",
    "        # Get best action for state\n",
    "        action, _state = model.predict(obs, deterministic=True)\n",
    "        # Compute next state\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        # Store results of this episode\n",
    "        store.append([action, obs[0], env.breakdown, obs[2], obs[3], reward, done, obs[1]])\n",
    "    eps_df = pd.DataFrame(store, columns=['action', 'health', 'breakdown', 'inventory', 'sp_inventory', 'reward', 'done', 'next_order'])\n",
    "    # Calculate nr. of reactive maintenance interventions by counting health 'resets' and substracting PM actions\n",
    "    result_df.iloc[0]['RM'] = result_df.iloc[0]['RM'] + sum(eps_df['breakdown']==True)\n",
    "    # Calculate nr. of preventive maintenance interventions\n",
    "    result_df.iloc[0]['PM'] = result_df.iloc[0]['PM'] + sum(eps_df['action']==10)\n",
    "    # Calculate inventory\n",
    "    result_df.iloc[0]['Inventory'] = result_df.iloc[0]['Inventory'] + sum(eps_df['inventory'])\n",
    "    # Calculate spare parts inventory per period\n",
    "    result_df.iloc[0]['Spare Parts Inventory'] = result_df.iloc[0]['Spare Parts Inventory'] + sum(eps_df['sp_inventory'])\n",
    "    # Calculate reward\n",
    "    result_df.iloc[0]['Reward'] = result_df.iloc[0]['Reward'] + sum(eps_df['reward'])\n",
    "    # Calculate reward with no costs and fulfillment of all orders\n",
    "    result_df.iloc[0]['Upper'] = result_df.iloc[0]['Upper'] + sum(eps_df.iloc[:-1]['next_order']) * env.order_r\n",
    "\n",
    "print(\"The average number of reactive maintenance interventions per episode is: \", result_df.iloc[0]['RM']/iterations)\n",
    "print(\"The average number of preventive maintenance interventions per episode is: \", result_df.iloc[0]['PM']/iterations) \n",
    "print(\"The average sum of inventory per episode is: \", result_df.iloc[0]['Inventory']/iterations)\n",
    "print(\"The average sum of spare parts inventory per episode is: \", result_df.iloc[0]['Spare Parts Inventory']/iterations)\n",
    "print(\"The average reward per episode is: \", result_df.iloc[0]['Reward']/iterations)\n",
    "print(\"The average upper bound per episode is: \", result_df.iloc[0]['Upper']/iterations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=1000, episode_reward=-2849.80 +/- 648.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n"
     ]
    }
   ],
   "source": [
    "### REINFORCEMENT LEARNING IIIa1 ###\n",
    "### TRAIN REACTIVE MODEL ###\n",
    "import gym\n",
    "import stable_baselines3 as sb\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "import pickle\n",
    "\n",
    "# Initiate environment\n",
    "env = gym.make('Production-v0', reactive_mode = True)\n",
    "# Callback for best model\n",
    "best_callback = EvalCallback(env, best_model_save_path='./callback/',\n",
    "                             log_path='./callback/', eval_freq=1000,\n",
    "                             deterministic=True, render=False)\n",
    "\n",
    "model = sb.DQN('MlpPolicy', env, tensorboard_log=\"./tensorboard/\", gamma = 0.99, learning_rate=0.01)\n",
    "model.learn(total_timesteps=1e3, tb_log_name=\"DQN_REACT_model\", callback = best_callback)\n",
    "model.save(\"DQN_REACT_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average number of reactive maintenance interventions per episode is:  2.0\n",
      "The average number of preventive maintenance interventions per episode is:  0.0\n",
      "The average mean time between failure per episode is:  44.0\n",
      "The average sum of inventory per episode is:  29.0\n",
      "The average sum of spare parts inventory per episode is:  52.0\n",
      "The average reward per episode is:  -1901.0\n",
      "The average upper bound per episode is:  624.0\n"
     ]
    }
   ],
   "source": [
    "### REINFORCEMENT LEARNING IIIa2 ###\n",
    "### EVALUATE REACTIVE MODEL ###\n",
    "\n",
    "import pandas as pd\n",
    "from stable_baselines3 import DQN\n",
    "import numpy as np\n",
    "\n",
    "env = gym.make('Production-v0', reactive_mode = True)\n",
    "# Best Model\n",
    "model = DQN.load('./callback/best_model', env = env)\n",
    "# Initilaize Reward\n",
    "result_df = pd.DataFrame([[0, 0, 0, 0, 0, 0, 0]], columns=['RM', 'PM', 'MTBF', 'Inventory', 'Spare Parts Inventory', 'Reward', 'Upper'])\n",
    "# Set iterations\n",
    "iterations = 1000\n",
    "for i in range(iterations):\n",
    "    # Initialize episode\n",
    "    store = []\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    store.append([0, obs[0], env.breakdown, obs[2], obs[3], 0, done, obs[1]])\n",
    "    # Compute one episode\n",
    "    while not done:\n",
    "        # Get best action for state\n",
    "        action, _state = model.predict(obs, deterministic=True)\n",
    "        # Compute next state\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        # Store results of this episode\n",
    "        store.append([action, obs[0], env.breakdown, obs[2], obs[3], reward, done, obs[1]])\n",
    "    eps_df = pd.DataFrame(store, columns=['action', 'health', 'breakdown', 'inventory', 'sp_inventory', 'reward', 'done', 'next_order'])\n",
    "    # Calculate nr. of reactive maintenance interventions by counting health 'resets' and substracting PM actions\n",
    "    result_df.iloc[0]['RM'] = result_df.iloc[0]['RM'] + sum(eps_df['breakdown']==True)\n",
    "    # Calculate nr. of preventive maintenance interventions\n",
    "    result_df.iloc[0]['PM'] = result_df.iloc[0]['PM'] + sum(eps_df['action']==10)\n",
    "    # Calculate mean time between failures\n",
    "    # Cut df after last breakdown\n",
    "    eps_df_trim = eps_df.iloc[:(np.where(eps_df['breakdown'].eq(True), eps_df.index, 0).max()+1)]\n",
    "    # Calculate MTBF by dividing running periods / breakdowns\n",
    "    result_df.iloc[0]['MTBF'] = result_df.iloc[0]['MTBF'] + (len(eps_df_trim) -\n",
    "        sum(eps_df_trim['breakdown'] == True)) / sum(eps_df_trim['breakdown'] == True)\n",
    "    # Calculate inventory\n",
    "    result_df.iloc[0]['Inventory'] = result_df.iloc[0]['Inventory'] + sum(eps_df['inventory'])\n",
    "    # Calculate spare parts inventory per period\n",
    "    result_df.iloc[0]['Spare Parts Inventory'] = result_df.iloc[0]['Spare Parts Inventory'] + sum(eps_df['sp_inventory'])\n",
    "    # Calculate reward\n",
    "    result_df.iloc[0]['Reward'] = result_df.iloc[0]['Reward'] + sum(eps_df['reward'])\n",
    "    # Calculate reward with no costs and fulfillment of all orders\n",
    "    result_df.iloc[0]['Upper'] = result_df.iloc[0]['Upper'] + sum(eps_df.iloc[:-1]['next_order']) * env.order_r\n",
    "\n",
    "print(\"The average number of reactive maintenance interventions per episode is: \", result_df.iloc[0]['RM']/iterations)\n",
    "print(\"The average number of preventive maintenance interventions per episode is: \", result_df.iloc[0]['PM']/iterations)\n",
    "print(\"The average mean time between failure per episode is: \", result_df.iloc[0]['MTBF']/iterations)\n",
    "print(\"The average sum of inventory per episode is: \", result_df.iloc[0]['Inventory']/iterations)\n",
    "print(\"The average sum of spare parts inventory per episode is: \", result_df.iloc[0]['Spare Parts Inventory']/iterations)\n",
    "print(\"The average reward per episode is: \", result_df.iloc[0]['Reward']/iterations)\n",
    "print(\"The average upper bound per episode is: \", result_df.iloc[0]['Upper']/iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21.5"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "### REINFORCEMENT LEARNING IIIb1 ###\n",
    "### TRAIN TIME-BASED PREVENTIVE MODEL ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### REINFORCEMENT LEARNING IIIb2 ###\n",
    "### EVALUATE TIME-BASED PREVENTIVE MODEL ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "### REINFORCEMENT LEARNING IV ###\n",
    "### VISUALIZE STATE-ACTION ###\n",
    "import numpy as np\n",
    "state_action = []\n",
    "\n",
    "# Define observation grid\n",
    "grid_health = np.arange(0.0, 1.01, 0.01)\n",
    "grid_order = range(0, 5)\n",
    "grid_inventory = range(0, 10)\n",
    "grid_sp_inventory = [0, 1]\n",
    "\n",
    "# Loop through grid and store best action for each state\n",
    "for hlt in grid_health:\n",
    "    for ord in grid_order:\n",
    "        for inv in grid_inventory:\n",
    "            for sin in grid_sp_inventory:\n",
    "                # Predict\n",
    "                action, _state = model.predict((hlt, ord, inv, sin), deterministic=True)\n",
    "                state_action.append([hlt, ord, inv, sin, action])\n",
    "\n",
    "state_action_df = pd.DataFrame(state_action, columns=['health', 'order', 'inventory', 'sp_inventory', 'action'])\n",
    "state_action_df.to_excel(\"visuals/state_action.xlsx\") "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6f69c5940b32a5cbabe45c9825076a627c6cdb9ede58cf4d0fa74ca6057ffe74"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
