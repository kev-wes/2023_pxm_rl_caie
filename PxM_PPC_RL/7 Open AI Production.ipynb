{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10000\n"
     ]
    }
   ],
   "source": [
    "### DATA-DRIVEN PROGNOSTICS I ###\n",
    "### GENERATE DATA FOR DATA-DRIVEN MODEL ###\n",
    "\n",
    "import random\n",
    "from prog_models.models import BatteryCircuit\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "# Ignore warnings when machine exceeds its end of life\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\"\"\" Method that uses a physical machine model from the prog_models package and a current (health) state of the model and\n",
    "an action (i.e., intensity), which is performed for 100 time steps\n",
    "    Parameter:\n",
    "        machine             machine model from the prog_models package\n",
    "        state               current (health) state of the model\n",
    "        action              loading of the machine for the next 100 time steps\n",
    "    Return:\n",
    "        health                               \n",
    "    \"\"\"\n",
    "def produce_model(machine, states, action):\n",
    "        \n",
    "        # Define load of battery\n",
    "        def future_loading(t, x=None):\n",
    "            return {'i': action}\n",
    "\n",
    "        # Set current state of machine\n",
    "        machine.parameters['x0'] = states\n",
    "        # Simulate 100 steps\n",
    "        options = {\n",
    "            'save_freq': 100,  # Frequency at which results are saved\n",
    "            'dt': 2  # Timestep\n",
    "        }\n",
    "        (_, _, states, outputs, event_states) = machine.simulate_to(100, future_loading, **options)\n",
    "        health = event_states[-1]['EOD']\n",
    "        return(round(health, 2), states[-1], outputs[-1]['t'], outputs[-1]['v'])\n",
    "def reset_states(machine):\n",
    "    # Returns initial states of machine, e.g., {'tb': 18.95, 'qb': 7856.3254, 'qcp': 0, 'qcs': 0} for Battery\n",
    "    return(machine.default_parameters['x0'])\n",
    "\n",
    "battery = BatteryCircuit()\n",
    "states = reset_states(battery)\n",
    "reset_counter = 0\n",
    "dataset = []\n",
    "for i in range(int(1e4)):\n",
    "    # If asset failed last period, reset all historical values\n",
    "    if reset_counter == 0: t = v = t_1 = v_1 = t_2 = v_2 = t_3 = v_3 = 0 \n",
    "    # Shift history by one time period\n",
    "    v_3 = v_2\n",
    "    t_3 = t_2\n",
    "    v_2 = v_1\n",
    "    t_2 = t_1\n",
    "    v_1 = v\n",
    "    t_1 = t\n",
    "\n",
    "    # Increment reset_counter\n",
    "    reset_counter = reset_counter + 1\n",
    "    # Compute new health, states, t, and v using last battery state and a random new action\n",
    "    health, states, t, v = produce_model(machine=battery, states=states, action=random.sample((0, 1, 2, 3, 4), 1)[0])\n",
    "    \n",
    "    if health <= 0: \n",
    "        # Reset battery states to initialize battery for next produce_model call\n",
    "        states = reset_states(battery)\n",
    "        # Initialize reset_counter\n",
    "        reset_counter = 0\n",
    "        # Sometimes produce_model returns weird or negative values as the end of life is exceeded\n",
    "        # Here, we just simply set it to zero to not confuse a later learner \n",
    "        health = 0\n",
    "\n",
    "    # append to two-dimensional list\n",
    "    dataset.append([t, v, t_1, v_1, t_2, v_2, t_3, v_3, health])\n",
    "\n",
    "    # print progress every 10,000 iterations\n",
    "    if (i+1) % 10000 == 0: print(\"Iteration\", i+1)\n",
    "# Transform two-dim list to dataframe\n",
    "dataset = pd.DataFrame(dataset, columns=['t', 'v', 't_1', 'v_1', 't_2', 'v_2', 't_3', 'v_3', 'health'])\n",
    "# Save it as pickle\n",
    "dataset.to_pickle('diagnostics/data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestRegressor() : [0.92981457 0.94752602 0.94335423 0.94618864 0.93738079]\n"
     ]
    }
   ],
   "source": [
    "### DATA-DRIVEN PROGNOSTICS II ###\n",
    "### FIT AND TEST MODEL ###\n",
    "from sklearn import tree, linear_model, kernel_ridge, svm, neighbors, gaussian_process, ensemble, neural_network\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import pickle\n",
    "\n",
    "dataset = pd.read_pickle('diagnostics/data')\n",
    "X = dataset[['t', 'v', 't_1', 'v_1', 't_2', 'v_2', 't_3', 'v_3']]\n",
    "y = dataset['health']\n",
    "#learner = [linear_model.LinearRegression(), linear_model.Ridge(), linear_model.Lasso(), linear_model.BayesianRidge(), tree.DecisionTreeRegressor(), # Fast\n",
    "#        kernel_ridge.KernelRidge(), svm.SVR(), neighbors.KNeighborsRegressor(), gaussian_process.GaussianProcessRegressor(), # Slow\n",
    "#        ensemble.RandomForestRegressor(), neural_network.MLPRegressor()] # Slow\n",
    "learner = [ensemble.RandomForestRegressor()]\n",
    "for i in learner:\n",
    "    reg = i\n",
    "    print(i, \":\", cross_val_score(reg, X, y, cv=5)) # default scoring R2\n",
    "# Fit on all data\n",
    "model = learner[0].fit(X, y)\n",
    "pickle.dump(model, open('diagnostics/model', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DATA-DRIVEN PROGNOSTICS IIIa ###\n",
    "### FIT, TEST, VISUALIZE MODEL USING TRAIN AND TEST SETS ###\n",
    "\n",
    "# Find index of healthy machines\n",
    "index_df = X.index[(X['t_1'] == 0) & (X['v_1'] == 0) & (X['t_2'] == 0) & (X['v_2'] == 0) & (X['t_3'] == 0) & (X['v_3'] == 0)].tolist()\n",
    "index_test = round(len(index_df)*0.8)\n",
    "\n",
    "# Create train and test set without disrupting machine runs to-failure\n",
    "X_train = X.iloc[0:(index_df[index_test])]\n",
    "y_train = y.iloc[0:(index_df[index_test])]\n",
    "X_test = X.iloc[index_df[index_test]:(len(X))]\n",
    "y_test = y.iloc[index_df[index_test]:(len(y))]\n",
    "\n",
    "## Train\n",
    "learner = [ensemble.RandomForestRegressor()]\n",
    "model = learner[0].fit(X_train, y_train)\n",
    "## Predict\n",
    "y_pred = pd.DataFrame(model.predict(X_test), columns=['Pred'])\n",
    "## Analyze\n",
    "#reset index of each DataFrame\n",
    "X_test.reset_index(drop=True, inplace=True)\n",
    "y_test.reset_index(drop=True, inplace=True)\n",
    "# Concat dataframes\n",
    "test_df = pd.concat([X_test, y_test, y_pred], axis=1)\n",
    "# Print for visualization (e.g., in R)\n",
    "test_df.to_excel(\"diagnostics/test_results.xlsx\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=1000, episode_reward=-1453.40 +/- 13.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=-1479.80 +/- 22.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=3000, episode_reward=-2292.40 +/- 97.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=4000, episode_reward=-2248.60 +/- 107.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=5000, episode_reward=-2219.60 +/- 67.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=6000, episode_reward=-2317.80 +/- 72.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=7000, episode_reward=-2098.80 +/- 79.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=8000, episode_reward=-2082.40 +/- 68.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=9000, episode_reward=-3157.60 +/- 53.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=10000, episode_reward=-3183.60 +/- 51.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=11000, episode_reward=-3129.60 +/- 75.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=12000, episode_reward=-3162.00 +/- 36.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=13000, episode_reward=-3498.80 +/- 292.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=14000, episode_reward=-3647.00 +/- 358.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=15000, episode_reward=-3120.40 +/- 88.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=16000, episode_reward=-3107.80 +/- 37.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=17000, episode_reward=-2892.00 +/- 39.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=18000, episode_reward=-2934.40 +/- 64.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=19000, episode_reward=-2926.80 +/- 36.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=20000, episode_reward=-2908.40 +/- 29.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=21000, episode_reward=-2859.60 +/- 38.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=22000, episode_reward=-2887.80 +/- 39.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=23000, episode_reward=-2816.00 +/- 24.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=24000, episode_reward=-2825.60 +/- 32.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=25000, episode_reward=-2770.40 +/- 27.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=26000, episode_reward=-2777.00 +/- 23.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=27000, episode_reward=-2804.20 +/- 16.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=28000, episode_reward=-2792.40 +/- 27.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=29000, episode_reward=-2774.20 +/- 19.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=30000, episode_reward=-2650.00 +/- 284.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=31000, episode_reward=-2860.60 +/- 20.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=32000, episode_reward=-2848.00 +/- 30.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=33000, episode_reward=-2802.40 +/- 17.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=34000, episode_reward=-2785.40 +/- 22.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=35000, episode_reward=-2824.40 +/- 37.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=36000, episode_reward=-2829.60 +/- 23.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=37000, episode_reward=-2850.40 +/- 30.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=38000, episode_reward=-2832.00 +/- 28.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=39000, episode_reward=-2868.00 +/- 34.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=40000, episode_reward=-2869.40 +/- 13.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=41000, episode_reward=-2904.80 +/- 40.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=42000, episode_reward=-2902.20 +/- 20.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=43000, episode_reward=-2887.20 +/- 52.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=44000, episode_reward=-2933.60 +/- 25.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=45000, episode_reward=-2915.80 +/- 37.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=46000, episode_reward=-2913.60 +/- 60.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=47000, episode_reward=-2945.80 +/- 24.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=48000, episode_reward=-2959.40 +/- 53.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=49000, episode_reward=-2919.00 +/- 40.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=50000, episode_reward=-2962.20 +/- 23.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=51000, episode_reward=-2974.20 +/- 41.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=52000, episode_reward=-2934.80 +/- 9.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=53000, episode_reward=-2944.80 +/- 31.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=54000, episode_reward=-2738.40 +/- 298.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=55000, episode_reward=-2913.80 +/- 28.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=56000, episode_reward=-2886.00 +/- 42.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=57000, episode_reward=-2938.60 +/- 25.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=58000, episode_reward=-2923.80 +/- 17.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=59000, episode_reward=-2899.20 +/- 62.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=60000, episode_reward=-2992.40 +/- 22.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=61000, episode_reward=-2995.60 +/- 17.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=62000, episode_reward=-2988.80 +/- 49.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=63000, episode_reward=-2944.40 +/- 57.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=64000, episode_reward=-2969.20 +/- 35.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=65000, episode_reward=-2937.40 +/- 32.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=66000, episode_reward=-2961.40 +/- 13.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=67000, episode_reward=-2936.00 +/- 32.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=68000, episode_reward=-2933.60 +/- 26.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=69000, episode_reward=-2929.80 +/- 57.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=70000, episode_reward=-2910.60 +/- 43.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=71000, episode_reward=-2945.40 +/- 29.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=72000, episode_reward=-2907.00 +/- 50.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=73000, episode_reward=-2893.20 +/- 10.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=74000, episode_reward=-2941.20 +/- 47.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=75000, episode_reward=-2935.40 +/- 45.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=76000, episode_reward=-2893.00 +/- 23.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=77000, episode_reward=-2872.40 +/- 21.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=78000, episode_reward=-2950.60 +/- 21.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=79000, episode_reward=-2896.00 +/- 34.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=80000, episode_reward=-2758.40 +/- 333.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=81000, episode_reward=-2767.40 +/- 322.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=82000, episode_reward=-2875.60 +/- 32.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=83000, episode_reward=-2929.40 +/- 35.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=84000, episode_reward=-2874.40 +/- 28.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=85000, episode_reward=-2901.20 +/- 29.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=86000, episode_reward=-2912.60 +/- 22.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=87000, episode_reward=-2450.20 +/- 637.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=88000, episode_reward=-1993.40 +/- 274.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=89000, episode_reward=-2941.20 +/- 15.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=90000, episode_reward=-2787.60 +/- 343.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=91000, episode_reward=108.40 +/- 34.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=92000, episode_reward=132.60 +/- 19.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=93000, episode_reward=117.80 +/- 28.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=94000, episode_reward=161.60 +/- 34.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=95000, episode_reward=-760.60 +/- 595.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=96000, episode_reward=-600.00 +/- 499.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=97000, episode_reward=-734.00 +/- 917.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=98000, episode_reward=-1020.00 +/- 632.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=99000, episode_reward=-474.00 +/- 392.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=100000, episode_reward=-862.40 +/- 343.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=101000, episode_reward=-2030.20 +/- 242.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=102000, episode_reward=-1890.60 +/- 435.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=103000, episode_reward=141.00 +/- 38.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=104000, episode_reward=145.00 +/- 31.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=105000, episode_reward=165.00 +/- 17.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=106000, episode_reward=143.60 +/- 28.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=107000, episode_reward=181.00 +/- 34.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=108000, episode_reward=167.40 +/- 36.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=109000, episode_reward=167.40 +/- 22.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=110000, episode_reward=196.40 +/- 12.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=111000, episode_reward=160.60 +/- 42.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=112000, episode_reward=158.40 +/- 46.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=113000, episode_reward=184.60 +/- 16.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=114000, episode_reward=167.00 +/- 14.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=115000, episode_reward=178.40 +/- 24.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=116000, episode_reward=170.80 +/- 36.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=117000, episode_reward=200.00 +/- 16.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=118000, episode_reward=176.60 +/- 17.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=119000, episode_reward=227.80 +/- 25.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=120000, episode_reward=246.80 +/- 43.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=121000, episode_reward=284.80 +/- 27.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=122000, episode_reward=188.40 +/- 179.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=123000, episode_reward=262.20 +/- 22.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=124000, episode_reward=275.60 +/- 38.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=125000, episode_reward=286.60 +/- 17.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=126000, episode_reward=292.40 +/- 33.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=127000, episode_reward=281.00 +/- 27.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=128000, episode_reward=313.60 +/- 19.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=129000, episode_reward=300.60 +/- 38.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=130000, episode_reward=297.60 +/- 27.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=131000, episode_reward=282.80 +/- 24.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=132000, episode_reward=288.00 +/- 33.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=133000, episode_reward=276.40 +/- 23.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=134000, episode_reward=336.00 +/- 34.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=135000, episode_reward=315.40 +/- 43.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=136000, episode_reward=-51.20 +/- 349.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=137000, episode_reward=129.60 +/- 242.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=138000, episode_reward=261.60 +/- 194.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=139000, episode_reward=266.20 +/- 180.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=140000, episode_reward=314.20 +/- 23.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=141000, episode_reward=254.20 +/- 202.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=142000, episode_reward=247.00 +/- 215.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=143000, episode_reward=325.80 +/- 25.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=144000, episode_reward=300.80 +/- 30.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=145000, episode_reward=335.80 +/- 18.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=146000, episode_reward=329.40 +/- 24.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=147000, episode_reward=326.80 +/- 21.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=148000, episode_reward=327.80 +/- 23.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=149000, episode_reward=334.20 +/- 14.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=150000, episode_reward=-1453.80 +/- 245.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=151000, episode_reward=-828.00 +/- 732.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=152000, episode_reward=-386.20 +/- 401.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=153000, episode_reward=-1220.00 +/- 343.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=154000, episode_reward=352.00 +/- 24.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=155000, episode_reward=360.40 +/- 31.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=156000, episode_reward=-891.00 +/- 506.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=157000, episode_reward=-991.80 +/- 498.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=158000, episode_reward=-1111.20 +/- 466.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=159000, episode_reward=-702.00 +/- 454.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=160000, episode_reward=328.20 +/- 19.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=161000, episode_reward=342.40 +/- 30.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=162000, episode_reward=345.20 +/- 22.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=163000, episode_reward=330.40 +/- 17.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=164000, episode_reward=352.00 +/- 11.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=165000, episode_reward=331.60 +/- 24.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=166000, episode_reward=360.60 +/- 21.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=167000, episode_reward=375.20 +/- 30.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=168000, episode_reward=356.60 +/- 26.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=169000, episode_reward=350.60 +/- 25.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=170000, episode_reward=351.00 +/- 22.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=171000, episode_reward=351.80 +/- 20.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=172000, episode_reward=358.60 +/- 9.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=173000, episode_reward=352.60 +/- 8.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=174000, episode_reward=348.20 +/- 12.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=175000, episode_reward=326.00 +/- 34.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=176000, episode_reward=349.20 +/- 15.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=177000, episode_reward=367.20 +/- 27.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=178000, episode_reward=378.80 +/- 12.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=179000, episode_reward=367.40 +/- 19.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=180000, episode_reward=365.60 +/- 22.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=181000, episode_reward=341.80 +/- 21.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=182000, episode_reward=348.00 +/- 33.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=183000, episode_reward=365.60 +/- 13.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=184000, episode_reward=368.40 +/- 23.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=185000, episode_reward=366.00 +/- 15.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=186000, episode_reward=365.80 +/- 18.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=187000, episode_reward=387.80 +/- 18.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=188000, episode_reward=382.20 +/- 35.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=189000, episode_reward=337.60 +/- 26.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=190000, episode_reward=341.20 +/- 28.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=191000, episode_reward=385.00 +/- 9.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=192000, episode_reward=361.00 +/- 15.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=193000, episode_reward=352.40 +/- 34.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=194000, episode_reward=374.60 +/- 21.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=195000, episode_reward=358.40 +/- 8.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=196000, episode_reward=342.60 +/- 23.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=197000, episode_reward=361.00 +/- 26.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=198000, episode_reward=360.80 +/- 27.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=199000, episode_reward=374.80 +/- 19.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=200000, episode_reward=366.80 +/- 33.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=201000, episode_reward=374.20 +/- 15.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=202000, episode_reward=368.20 +/- 15.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=203000, episode_reward=362.40 +/- 26.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=204000, episode_reward=376.60 +/- 19.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=205000, episode_reward=363.00 +/- 26.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=206000, episode_reward=374.60 +/- 18.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=207000, episode_reward=365.60 +/- 19.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=208000, episode_reward=350.80 +/- 16.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=209000, episode_reward=349.80 +/- 17.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=210000, episode_reward=370.20 +/- 15.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=211000, episode_reward=355.60 +/- 11.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=212000, episode_reward=355.40 +/- 15.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=213000, episode_reward=363.40 +/- 18.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=214000, episode_reward=349.40 +/- 17.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=215000, episode_reward=340.00 +/- 9.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=216000, episode_reward=377.80 +/- 34.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=217000, episode_reward=360.60 +/- 15.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=218000, episode_reward=371.80 +/- 17.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=219000, episode_reward=346.00 +/- 10.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=220000, episode_reward=338.80 +/- 14.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=221000, episode_reward=366.00 +/- 23.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=222000, episode_reward=366.00 +/- 14.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=223000, episode_reward=338.40 +/- 11.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=224000, episode_reward=356.00 +/- 13.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=225000, episode_reward=344.60 +/- 25.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=226000, episode_reward=348.80 +/- 9.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=227000, episode_reward=353.60 +/- 15.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=228000, episode_reward=353.40 +/- 24.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=229000, episode_reward=362.40 +/- 16.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=230000, episode_reward=358.80 +/- 12.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=231000, episode_reward=365.60 +/- 19.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=232000, episode_reward=363.20 +/- 4.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=233000, episode_reward=357.80 +/- 15.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=234000, episode_reward=380.20 +/- 19.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=235000, episode_reward=356.20 +/- 16.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=236000, episode_reward=353.00 +/- 29.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=237000, episode_reward=365.20 +/- 8.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=238000, episode_reward=360.40 +/- 16.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=239000, episode_reward=379.20 +/- 11.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=240000, episode_reward=358.40 +/- 21.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=241000, episode_reward=377.80 +/- 16.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=242000, episode_reward=372.40 +/- 11.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=243000, episode_reward=351.60 +/- 26.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=244000, episode_reward=364.60 +/- 13.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=245000, episode_reward=366.00 +/- 11.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=246000, episode_reward=356.80 +/- 17.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=247000, episode_reward=361.40 +/- 11.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=248000, episode_reward=377.20 +/- 12.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=249000, episode_reward=353.40 +/- 12.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=250000, episode_reward=381.00 +/- 40.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=251000, episode_reward=366.40 +/- 16.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=252000, episode_reward=357.40 +/- 29.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=253000, episode_reward=365.20 +/- 22.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=254000, episode_reward=374.40 +/- 13.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=255000, episode_reward=368.40 +/- 10.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=256000, episode_reward=354.20 +/- 9.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=257000, episode_reward=360.00 +/- 12.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=258000, episode_reward=357.80 +/- 16.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=259000, episode_reward=374.20 +/- 31.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=260000, episode_reward=378.40 +/- 18.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=261000, episode_reward=363.20 +/- 20.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=262000, episode_reward=352.80 +/- 6.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=263000, episode_reward=341.80 +/- 34.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=264000, episode_reward=344.40 +/- 8.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=265000, episode_reward=364.40 +/- 21.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=266000, episode_reward=362.60 +/- 6.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=267000, episode_reward=376.00 +/- 13.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=268000, episode_reward=359.80 +/- 19.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=269000, episode_reward=364.80 +/- 18.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=270000, episode_reward=384.20 +/- 13.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=271000, episode_reward=375.80 +/- 13.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=272000, episode_reward=378.40 +/- 24.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=273000, episode_reward=380.00 +/- 17.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=274000, episode_reward=378.20 +/- 21.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=275000, episode_reward=390.80 +/- 17.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=276000, episode_reward=376.00 +/- 15.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=277000, episode_reward=375.20 +/- 21.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=278000, episode_reward=397.20 +/- 10.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=279000, episode_reward=308.80 +/- 209.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=280000, episode_reward=398.40 +/- 17.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=281000, episode_reward=399.60 +/- 23.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=282000, episode_reward=394.80 +/- 20.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=283000, episode_reward=409.60 +/- 14.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=284000, episode_reward=386.20 +/- 24.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=285000, episode_reward=401.80 +/- 14.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=286000, episode_reward=373.00 +/- 21.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=287000, episode_reward=376.80 +/- 21.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=288000, episode_reward=400.60 +/- 8.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=289000, episode_reward=399.40 +/- 14.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=290000, episode_reward=397.60 +/- 34.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=291000, episode_reward=410.60 +/- 10.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=292000, episode_reward=391.60 +/- 27.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=293000, episode_reward=384.00 +/- 15.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=294000, episode_reward=376.60 +/- 20.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=295000, episode_reward=370.60 +/- 15.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=296000, episode_reward=382.40 +/- 8.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=297000, episode_reward=382.00 +/- 20.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=298000, episode_reward=387.60 +/- 1.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=299000, episode_reward=385.80 +/- 17.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=300000, episode_reward=366.00 +/- 15.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=301000, episode_reward=381.80 +/- 13.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=302000, episode_reward=370.60 +/- 11.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=303000, episode_reward=401.80 +/- 25.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=304000, episode_reward=394.00 +/- 26.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=305000, episode_reward=402.20 +/- 13.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=306000, episode_reward=378.20 +/- 15.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=307000, episode_reward=403.40 +/- 16.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=308000, episode_reward=394.60 +/- 15.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=309000, episode_reward=425.00 +/- 16.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=310000, episode_reward=398.80 +/- 33.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=311000, episode_reward=400.80 +/- 14.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=312000, episode_reward=369.80 +/- 29.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=313000, episode_reward=391.40 +/- 14.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=314000, episode_reward=384.20 +/- 17.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=315000, episode_reward=394.80 +/- 12.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=316000, episode_reward=396.00 +/- 26.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=317000, episode_reward=401.60 +/- 24.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=318000, episode_reward=308.20 +/- 202.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=319000, episode_reward=416.40 +/- 30.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=320000, episode_reward=408.60 +/- 22.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=321000, episode_reward=402.00 +/- 27.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=322000, episode_reward=418.40 +/- 5.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=323000, episode_reward=408.40 +/- 21.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=324000, episode_reward=407.80 +/- 4.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=325000, episode_reward=315.80 +/- 168.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=326000, episode_reward=209.40 +/- 231.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=327000, episode_reward=397.60 +/- 17.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=328000, episode_reward=401.20 +/- 24.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=329000, episode_reward=385.40 +/- 19.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=330000, episode_reward=404.80 +/- 9.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=331000, episode_reward=402.60 +/- 24.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=332000, episode_reward=405.80 +/- 7.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=333000, episode_reward=400.00 +/- 14.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=334000, episode_reward=406.00 +/- 22.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=335000, episode_reward=390.80 +/- 17.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=336000, episode_reward=422.00 +/- 22.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=337000, episode_reward=405.40 +/- 20.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=338000, episode_reward=385.60 +/- 11.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=339000, episode_reward=397.40 +/- 29.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=340000, episode_reward=386.60 +/- 21.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=341000, episode_reward=390.00 +/- 16.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=342000, episode_reward=385.80 +/- 28.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=343000, episode_reward=399.40 +/- 15.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=344000, episode_reward=384.60 +/- 32.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=345000, episode_reward=397.00 +/- 25.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=346000, episode_reward=414.00 +/- 11.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=347000, episode_reward=434.20 +/- 18.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=348000, episode_reward=393.40 +/- 18.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=349000, episode_reward=404.80 +/- 15.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=350000, episode_reward=402.80 +/- 15.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=351000, episode_reward=398.00 +/- 32.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=352000, episode_reward=414.80 +/- 13.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=353000, episode_reward=392.20 +/- 17.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=354000, episode_reward=413.60 +/- 19.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=355000, episode_reward=412.60 +/- 21.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=356000, episode_reward=413.40 +/- 15.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=357000, episode_reward=398.80 +/- 17.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=358000, episode_reward=408.00 +/- 28.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=359000, episode_reward=406.80 +/- 12.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=360000, episode_reward=400.60 +/- 8.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=361000, episode_reward=401.60 +/- 35.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=362000, episode_reward=401.40 +/- 31.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=363000, episode_reward=401.40 +/- 30.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=364000, episode_reward=401.80 +/- 18.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=365000, episode_reward=405.40 +/- 21.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=366000, episode_reward=396.20 +/- 30.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=367000, episode_reward=403.40 +/- 22.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=368000, episode_reward=413.60 +/- 9.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=369000, episode_reward=407.00 +/- 20.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=370000, episode_reward=433.80 +/- 28.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=371000, episode_reward=421.00 +/- 9.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=372000, episode_reward=409.40 +/- 20.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=373000, episode_reward=409.40 +/- 20.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=374000, episode_reward=413.20 +/- 8.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=375000, episode_reward=403.00 +/- 9.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=376000, episode_reward=407.00 +/- 19.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=377000, episode_reward=408.80 +/- 33.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=378000, episode_reward=419.60 +/- 36.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=379000, episode_reward=422.00 +/- 20.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=380000, episode_reward=422.20 +/- 23.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=381000, episode_reward=409.00 +/- 9.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=382000, episode_reward=409.20 +/- 20.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=383000, episode_reward=382.00 +/- 27.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=384000, episode_reward=412.80 +/- 21.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=385000, episode_reward=386.00 +/- 25.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=386000, episode_reward=399.60 +/- 35.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=387000, episode_reward=395.40 +/- 23.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=388000, episode_reward=425.60 +/- 19.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=389000, episode_reward=401.20 +/- 20.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=390000, episode_reward=400.20 +/- 23.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=391000, episode_reward=390.80 +/- 13.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=392000, episode_reward=403.40 +/- 32.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=393000, episode_reward=400.60 +/- 19.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=394000, episode_reward=410.60 +/- 23.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=395000, episode_reward=381.80 +/- 22.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=396000, episode_reward=403.80 +/- 28.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=397000, episode_reward=405.20 +/- 23.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=398000, episode_reward=399.00 +/- 23.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=399000, episode_reward=417.40 +/- 13.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=400000, episode_reward=384.40 +/- 24.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=401000, episode_reward=392.20 +/- 18.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=402000, episode_reward=389.00 +/- 17.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=403000, episode_reward=387.00 +/- 21.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=404000, episode_reward=402.00 +/- 24.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=405000, episode_reward=386.20 +/- 32.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=406000, episode_reward=391.20 +/- 14.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=407000, episode_reward=393.80 +/- 17.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=408000, episode_reward=403.80 +/- 21.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=409000, episode_reward=410.20 +/- 26.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=410000, episode_reward=386.80 +/- 22.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=411000, episode_reward=406.40 +/- 17.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=412000, episode_reward=397.20 +/- 9.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=413000, episode_reward=396.60 +/- 12.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=414000, episode_reward=409.40 +/- 33.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=415000, episode_reward=394.80 +/- 21.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=416000, episode_reward=413.80 +/- 31.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=417000, episode_reward=401.60 +/- 21.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=418000, episode_reward=405.20 +/- 15.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=419000, episode_reward=400.00 +/- 18.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=420000, episode_reward=391.20 +/- 32.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=421000, episode_reward=413.60 +/- 16.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=422000, episode_reward=390.20 +/- 19.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=423000, episode_reward=408.00 +/- 22.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=424000, episode_reward=392.40 +/- 16.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=425000, episode_reward=398.60 +/- 19.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=426000, episode_reward=411.40 +/- 20.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=427000, episode_reward=416.00 +/- 16.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=428000, episode_reward=415.00 +/- 7.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=429000, episode_reward=408.20 +/- 8.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=430000, episode_reward=423.20 +/- 22.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=431000, episode_reward=402.20 +/- 9.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=432000, episode_reward=406.80 +/- 9.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=433000, episode_reward=410.40 +/- 30.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=434000, episode_reward=408.60 +/- 20.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=435000, episode_reward=416.20 +/- 12.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=436000, episode_reward=419.00 +/- 19.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=437000, episode_reward=416.60 +/- 32.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=438000, episode_reward=416.60 +/- 23.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=439000, episode_reward=409.80 +/- 24.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=440000, episode_reward=405.40 +/- 17.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=441000, episode_reward=431.00 +/- 33.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=442000, episode_reward=420.20 +/- 21.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=443000, episode_reward=416.80 +/- 11.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=444000, episode_reward=410.80 +/- 11.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=445000, episode_reward=415.80 +/- 8.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=446000, episode_reward=416.60 +/- 24.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=447000, episode_reward=413.40 +/- 10.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=448000, episode_reward=412.40 +/- 12.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=449000, episode_reward=407.20 +/- 13.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=450000, episode_reward=412.80 +/- 18.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=451000, episode_reward=401.80 +/- 16.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=452000, episode_reward=414.80 +/- 25.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=453000, episode_reward=397.20 +/- 18.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=454000, episode_reward=422.00 +/- 12.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=455000, episode_reward=428.20 +/- 21.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=456000, episode_reward=426.20 +/- 23.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=457000, episode_reward=411.20 +/- 20.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=458000, episode_reward=433.60 +/- 31.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=459000, episode_reward=447.40 +/- 11.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=460000, episode_reward=418.60 +/- 11.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=461000, episode_reward=422.00 +/- 25.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=462000, episode_reward=405.60 +/- 16.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=463000, episode_reward=418.00 +/- 11.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=464000, episode_reward=429.20 +/- 21.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=465000, episode_reward=420.00 +/- 16.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=466000, episode_reward=429.80 +/- 21.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=467000, episode_reward=422.20 +/- 11.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=468000, episode_reward=420.40 +/- 12.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=469000, episode_reward=400.80 +/- 17.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=470000, episode_reward=397.40 +/- 15.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=471000, episode_reward=420.40 +/- 11.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=472000, episode_reward=399.00 +/- 17.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=473000, episode_reward=415.60 +/- 9.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=474000, episode_reward=431.00 +/- 10.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=475000, episode_reward=414.60 +/- 27.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=476000, episode_reward=413.20 +/- 12.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=477000, episode_reward=405.60 +/- 16.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=478000, episode_reward=406.20 +/- 4.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=479000, episode_reward=409.80 +/- 16.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=480000, episode_reward=404.20 +/- 35.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=481000, episode_reward=411.00 +/- 20.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=482000, episode_reward=432.40 +/- 8.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=483000, episode_reward=384.40 +/- 23.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=484000, episode_reward=406.20 +/- 18.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=485000, episode_reward=408.40 +/- 13.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=486000, episode_reward=407.40 +/- 23.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=487000, episode_reward=427.40 +/- 14.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=488000, episode_reward=408.60 +/- 17.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=489000, episode_reward=408.80 +/- 32.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=490000, episode_reward=318.80 +/- 191.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=491000, episode_reward=438.80 +/- 17.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=492000, episode_reward=408.60 +/- 10.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=493000, episode_reward=318.00 +/- 197.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=494000, episode_reward=414.40 +/- 14.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=495000, episode_reward=410.20 +/- 19.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=496000, episode_reward=413.20 +/- 12.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=497000, episode_reward=416.40 +/- 7.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=498000, episode_reward=421.40 +/- 16.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=499000, episode_reward=415.40 +/- 20.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=500000, episode_reward=421.00 +/- 15.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=501000, episode_reward=408.40 +/- 13.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=502000, episode_reward=404.40 +/- 20.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=503000, episode_reward=415.60 +/- 14.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=504000, episode_reward=407.40 +/- 21.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=505000, episode_reward=422.20 +/- 23.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=506000, episode_reward=416.40 +/- 25.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=507000, episode_reward=400.40 +/- 33.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=508000, episode_reward=435.00 +/- 8.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=509000, episode_reward=443.00 +/- 21.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=510000, episode_reward=435.00 +/- 22.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=511000, episode_reward=437.00 +/- 26.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=512000, episode_reward=441.40 +/- 7.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=513000, episode_reward=435.80 +/- 20.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=514000, episode_reward=444.00 +/- 24.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=515000, episode_reward=450.00 +/- 8.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=516000, episode_reward=443.00 +/- 27.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=517000, episode_reward=430.40 +/- 29.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=518000, episode_reward=436.20 +/- 12.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=519000, episode_reward=424.80 +/- 20.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=520000, episode_reward=329.20 +/- 208.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=521000, episode_reward=441.60 +/- 34.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=522000, episode_reward=431.60 +/- 21.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=523000, episode_reward=325.80 +/- 188.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=524000, episode_reward=415.40 +/- 22.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=525000, episode_reward=280.40 +/- 313.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=526000, episode_reward=448.80 +/- 29.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=527000, episode_reward=386.20 +/- 105.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=528000, episode_reward=333.60 +/- 232.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=529000, episode_reward=427.20 +/- 21.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=530000, episode_reward=416.00 +/- 20.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=531000, episode_reward=436.80 +/- 9.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=532000, episode_reward=421.60 +/- 17.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=533000, episode_reward=419.60 +/- 14.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=534000, episode_reward=435.60 +/- 27.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=535000, episode_reward=434.80 +/- 17.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=536000, episode_reward=431.60 +/- 16.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=537000, episode_reward=431.80 +/- 28.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=538000, episode_reward=429.00 +/- 20.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=539000, episode_reward=431.60 +/- 24.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=540000, episode_reward=441.40 +/- 23.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=541000, episode_reward=411.20 +/- 23.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=542000, episode_reward=449.00 +/- 29.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=543000, episode_reward=435.20 +/- 10.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=544000, episode_reward=431.20 +/- 8.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=545000, episode_reward=429.60 +/- 25.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=546000, episode_reward=406.60 +/- 12.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=547000, episode_reward=429.80 +/- 16.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=548000, episode_reward=411.40 +/- 18.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=549000, episode_reward=430.80 +/- 25.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=550000, episode_reward=431.60 +/- 13.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=551000, episode_reward=415.60 +/- 19.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=552000, episode_reward=429.00 +/- 13.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=553000, episode_reward=436.00 +/- 16.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=554000, episode_reward=427.00 +/- 28.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=555000, episode_reward=420.20 +/- 11.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=556000, episode_reward=424.20 +/- 23.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=557000, episode_reward=424.40 +/- 16.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=558000, episode_reward=413.60 +/- 16.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=559000, episode_reward=422.20 +/- 28.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=560000, episode_reward=430.40 +/- 19.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=561000, episode_reward=429.60 +/- 28.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=562000, episode_reward=427.00 +/- 20.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=563000, episode_reward=428.20 +/- 17.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=564000, episode_reward=433.60 +/- 10.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=565000, episode_reward=414.20 +/- 20.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=566000, episode_reward=423.20 +/- 22.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=567000, episode_reward=432.80 +/- 21.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=568000, episode_reward=346.80 +/- 188.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=569000, episode_reward=446.00 +/- 27.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=570000, episode_reward=433.20 +/- 20.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=571000, episode_reward=439.00 +/- 26.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=572000, episode_reward=452.80 +/- 19.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=573000, episode_reward=425.00 +/- 15.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=574000, episode_reward=405.20 +/- 26.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=575000, episode_reward=439.00 +/- 15.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=576000, episode_reward=447.60 +/- 7.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=577000, episode_reward=425.20 +/- 16.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=578000, episode_reward=410.80 +/- 16.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=579000, episode_reward=450.60 +/- 16.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=580000, episode_reward=437.40 +/- 37.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=581000, episode_reward=449.20 +/- 12.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=582000, episode_reward=435.40 +/- 19.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=583000, episode_reward=423.20 +/- 15.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=584000, episode_reward=421.00 +/- 21.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=585000, episode_reward=431.60 +/- 15.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=586000, episode_reward=427.40 +/- 15.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=587000, episode_reward=435.40 +/- 18.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=588000, episode_reward=467.60 +/- 30.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=589000, episode_reward=432.00 +/- 14.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=590000, episode_reward=439.60 +/- 17.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=591000, episode_reward=437.00 +/- 20.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=592000, episode_reward=439.20 +/- 32.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=593000, episode_reward=416.60 +/- 19.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=594000, episode_reward=419.00 +/- 27.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=595000, episode_reward=445.20 +/- 28.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=596000, episode_reward=438.40 +/- 9.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=597000, episode_reward=445.60 +/- 42.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=598000, episode_reward=426.20 +/- 19.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=599000, episode_reward=419.20 +/- 17.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=600000, episode_reward=438.40 +/- 21.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=601000, episode_reward=439.00 +/- 30.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=602000, episode_reward=437.80 +/- 27.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=603000, episode_reward=449.20 +/- 39.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=604000, episode_reward=453.60 +/- 31.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=605000, episode_reward=441.20 +/- 20.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=606000, episode_reward=431.20 +/- 27.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=607000, episode_reward=455.20 +/- 27.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=608000, episode_reward=441.60 +/- 18.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=609000, episode_reward=444.00 +/- 25.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=610000, episode_reward=429.80 +/- 24.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=611000, episode_reward=429.80 +/- 22.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=612000, episode_reward=448.40 +/- 23.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=613000, episode_reward=434.40 +/- 39.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=614000, episode_reward=447.40 +/- 16.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=615000, episode_reward=415.40 +/- 24.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=616000, episode_reward=408.20 +/- 9.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=617000, episode_reward=435.20 +/- 41.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=618000, episode_reward=431.60 +/- 17.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=619000, episode_reward=444.00 +/- 6.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=620000, episode_reward=439.00 +/- 24.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=621000, episode_reward=420.20 +/- 11.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=622000, episode_reward=438.80 +/- 17.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=623000, episode_reward=422.80 +/- 16.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=624000, episode_reward=433.00 +/- 26.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=625000, episode_reward=420.60 +/- 11.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=626000, episode_reward=420.80 +/- 28.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=627000, episode_reward=422.00 +/- 30.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=628000, episode_reward=419.00 +/- 19.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=629000, episode_reward=439.80 +/- 10.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=630000, episode_reward=421.80 +/- 25.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=631000, episode_reward=453.40 +/- 13.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=632000, episode_reward=436.20 +/- 14.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=633000, episode_reward=406.20 +/- 13.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=634000, episode_reward=437.80 +/- 21.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=635000, episode_reward=416.00 +/- 28.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=636000, episode_reward=444.20 +/- 10.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=637000, episode_reward=433.20 +/- 24.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=638000, episode_reward=443.60 +/- 10.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=639000, episode_reward=421.60 +/- 24.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=640000, episode_reward=414.60 +/- 15.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=641000, episode_reward=430.20 +/- 21.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=642000, episode_reward=440.20 +/- 24.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=643000, episode_reward=427.60 +/- 27.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=644000, episode_reward=428.80 +/- 16.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=645000, episode_reward=427.60 +/- 21.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=646000, episode_reward=428.60 +/- 19.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=647000, episode_reward=422.20 +/- 8.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=648000, episode_reward=439.20 +/- 11.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=649000, episode_reward=417.20 +/- 29.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=650000, episode_reward=410.60 +/- 31.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=651000, episode_reward=337.20 +/- 183.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=652000, episode_reward=439.40 +/- 22.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=653000, episode_reward=422.60 +/- 13.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=654000, episode_reward=425.20 +/- 20.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=655000, episode_reward=434.00 +/- 23.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=656000, episode_reward=429.80 +/- 21.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=657000, episode_reward=429.60 +/- 19.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=658000, episode_reward=421.40 +/- 21.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=659000, episode_reward=413.60 +/- 18.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=660000, episode_reward=425.20 +/- 26.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=661000, episode_reward=417.00 +/- 16.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=662000, episode_reward=424.60 +/- 26.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=663000, episode_reward=424.20 +/- 17.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=664000, episode_reward=422.40 +/- 25.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=665000, episode_reward=411.40 +/- 19.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=666000, episode_reward=414.20 +/- 11.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=667000, episode_reward=430.20 +/- 17.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=668000, episode_reward=415.80 +/- 14.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=669000, episode_reward=425.80 +/- 19.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=670000, episode_reward=422.40 +/- 17.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=671000, episode_reward=416.60 +/- 22.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=672000, episode_reward=398.80 +/- 13.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=673000, episode_reward=419.60 +/- 13.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=674000, episode_reward=404.60 +/- 14.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=675000, episode_reward=398.20 +/- 20.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=676000, episode_reward=400.20 +/- 11.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=677000, episode_reward=385.20 +/- 20.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=678000, episode_reward=398.80 +/- 13.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=679000, episode_reward=416.00 +/- 19.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=680000, episode_reward=399.60 +/- 23.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=681000, episode_reward=394.60 +/- 18.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=682000, episode_reward=407.60 +/- 22.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=683000, episode_reward=403.80 +/- 10.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=684000, episode_reward=400.80 +/- 28.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=685000, episode_reward=402.20 +/- 20.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=686000, episode_reward=413.40 +/- 12.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=687000, episode_reward=419.40 +/- 21.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=688000, episode_reward=429.00 +/- 26.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=689000, episode_reward=418.00 +/- 23.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=690000, episode_reward=419.20 +/- 16.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=691000, episode_reward=414.20 +/- 11.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=692000, episode_reward=414.20 +/- 13.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=693000, episode_reward=417.20 +/- 21.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=694000, episode_reward=404.20 +/- 27.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=695000, episode_reward=417.00 +/- 19.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=696000, episode_reward=418.00 +/- 13.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=697000, episode_reward=428.60 +/- 20.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=698000, episode_reward=441.00 +/- 21.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=699000, episode_reward=427.20 +/- 13.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=700000, episode_reward=427.60 +/- 15.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=701000, episode_reward=426.00 +/- 15.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=702000, episode_reward=435.20 +/- 31.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=703000, episode_reward=421.60 +/- 21.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=704000, episode_reward=420.80 +/- 14.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=705000, episode_reward=438.80 +/- 26.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=706000, episode_reward=420.80 +/- 12.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=707000, episode_reward=421.00 +/- 14.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=708000, episode_reward=412.60 +/- 6.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=709000, episode_reward=430.80 +/- 3.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=710000, episode_reward=432.60 +/- 16.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=711000, episode_reward=420.60 +/- 12.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=712000, episode_reward=421.20 +/- 16.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=713000, episode_reward=409.80 +/- 24.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=714000, episode_reward=413.60 +/- 22.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=715000, episode_reward=424.00 +/- 8.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=716000, episode_reward=401.40 +/- 20.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=717000, episode_reward=439.20 +/- 6.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=718000, episode_reward=421.80 +/- 14.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=719000, episode_reward=411.40 +/- 24.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=720000, episode_reward=420.80 +/- 30.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=721000, episode_reward=399.60 +/- 23.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=722000, episode_reward=422.60 +/- 18.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=723000, episode_reward=399.80 +/- 21.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=724000, episode_reward=399.40 +/- 8.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=725000, episode_reward=408.00 +/- 27.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=726000, episode_reward=395.20 +/- 14.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=727000, episode_reward=417.60 +/- 19.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=728000, episode_reward=403.20 +/- 19.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=729000, episode_reward=423.80 +/- 27.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=730000, episode_reward=413.00 +/- 16.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=731000, episode_reward=418.60 +/- 27.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=732000, episode_reward=391.40 +/- 12.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=733000, episode_reward=402.80 +/- 19.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=734000, episode_reward=420.20 +/- 21.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=735000, episode_reward=426.80 +/- 23.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=736000, episode_reward=433.60 +/- 29.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=737000, episode_reward=424.00 +/- 20.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=738000, episode_reward=450.80 +/- 20.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=739000, episode_reward=422.40 +/- 11.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=740000, episode_reward=457.40 +/- 12.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=741000, episode_reward=444.80 +/- 24.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=742000, episode_reward=424.80 +/- 15.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=743000, episode_reward=435.00 +/- 19.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=744000, episode_reward=419.20 +/- 17.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=745000, episode_reward=441.00 +/- 12.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=746000, episode_reward=440.60 +/- 16.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=747000, episode_reward=445.80 +/- 17.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=748000, episode_reward=425.40 +/- 22.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=749000, episode_reward=405.00 +/- 9.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=750000, episode_reward=430.00 +/- 20.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=751000, episode_reward=408.40 +/- 24.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=752000, episode_reward=408.40 +/- 26.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=753000, episode_reward=425.20 +/- 24.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=754000, episode_reward=425.80 +/- 5.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=755000, episode_reward=398.40 +/- 36.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=756000, episode_reward=410.00 +/- 14.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=757000, episode_reward=414.40 +/- 27.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=758000, episode_reward=404.00 +/- 6.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=759000, episode_reward=404.20 +/- 11.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=760000, episode_reward=419.00 +/- 30.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=761000, episode_reward=433.20 +/- 13.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=762000, episode_reward=430.00 +/- 34.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=763000, episode_reward=431.80 +/- 12.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=764000, episode_reward=415.40 +/- 9.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=765000, episode_reward=437.40 +/- 15.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=766000, episode_reward=420.00 +/- 21.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=767000, episode_reward=432.00 +/- 12.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=768000, episode_reward=434.60 +/- 19.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=769000, episode_reward=425.00 +/- 14.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=770000, episode_reward=411.40 +/- 15.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=771000, episode_reward=432.20 +/- 28.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=772000, episode_reward=427.20 +/- 19.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=773000, episode_reward=415.40 +/- 22.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=774000, episode_reward=399.40 +/- 13.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=775000, episode_reward=424.00 +/- 19.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=776000, episode_reward=426.60 +/- 16.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=777000, episode_reward=407.80 +/- 14.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=778000, episode_reward=420.60 +/- 23.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=779000, episode_reward=446.00 +/- 20.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=780000, episode_reward=420.00 +/- 4.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=781000, episode_reward=447.20 +/- 28.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=782000, episode_reward=433.60 +/- 13.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=783000, episode_reward=425.00 +/- 13.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=784000, episode_reward=350.20 +/- 186.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=785000, episode_reward=438.60 +/- 10.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=786000, episode_reward=426.00 +/- 28.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=787000, episode_reward=454.00 +/- 8.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=788000, episode_reward=442.60 +/- 15.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=789000, episode_reward=440.00 +/- 18.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=790000, episode_reward=448.80 +/- 18.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=791000, episode_reward=436.20 +/- 21.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=792000, episode_reward=453.20 +/- 22.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=793000, episode_reward=442.40 +/- 17.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=794000, episode_reward=445.80 +/- 11.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=795000, episode_reward=440.20 +/- 19.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=796000, episode_reward=423.60 +/- 19.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=797000, episode_reward=443.00 +/- 27.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=798000, episode_reward=438.20 +/- 20.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=799000, episode_reward=449.80 +/- 27.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=800000, episode_reward=437.20 +/- 16.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=801000, episode_reward=431.60 +/- 25.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=802000, episode_reward=428.60 +/- 11.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=803000, episode_reward=449.60 +/- 14.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=804000, episode_reward=450.40 +/- 24.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=805000, episode_reward=446.60 +/- 9.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=806000, episode_reward=459.20 +/- 20.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=807000, episode_reward=445.80 +/- 25.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=808000, episode_reward=459.60 +/- 18.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=809000, episode_reward=457.40 +/- 21.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=810000, episode_reward=435.00 +/- 13.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=811000, episode_reward=446.80 +/- 11.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=812000, episode_reward=452.40 +/- 15.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=813000, episode_reward=423.00 +/- 18.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=814000, episode_reward=405.00 +/- 17.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=815000, episode_reward=419.20 +/- 15.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=816000, episode_reward=429.40 +/- 15.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=817000, episode_reward=439.20 +/- 14.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=818000, episode_reward=420.60 +/- 25.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=819000, episode_reward=401.80 +/- 20.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=820000, episode_reward=430.00 +/- 14.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=821000, episode_reward=420.80 +/- 18.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=822000, episode_reward=419.20 +/- 20.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=823000, episode_reward=437.20 +/- 12.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=824000, episode_reward=449.20 +/- 30.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=825000, episode_reward=422.60 +/- 13.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=826000, episode_reward=429.60 +/- 11.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=827000, episode_reward=436.80 +/- 18.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=828000, episode_reward=416.00 +/- 11.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=829000, episode_reward=448.00 +/- 13.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=830000, episode_reward=425.40 +/- 7.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=831000, episode_reward=430.60 +/- 12.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=832000, episode_reward=425.40 +/- 16.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=833000, episode_reward=416.20 +/- 22.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=834000, episode_reward=443.20 +/- 13.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=835000, episode_reward=426.60 +/- 14.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=836000, episode_reward=442.00 +/- 7.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=837000, episode_reward=433.60 +/- 13.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=838000, episode_reward=435.40 +/- 4.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=839000, episode_reward=438.00 +/- 19.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=840000, episode_reward=446.60 +/- 10.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=841000, episode_reward=437.40 +/- 19.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=842000, episode_reward=445.00 +/- 17.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=843000, episode_reward=427.20 +/- 17.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=844000, episode_reward=447.80 +/- 35.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=845000, episode_reward=441.40 +/- 7.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=846000, episode_reward=451.40 +/- 16.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=847000, episode_reward=449.00 +/- 18.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=848000, episode_reward=450.20 +/- 11.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=849000, episode_reward=442.40 +/- 17.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=850000, episode_reward=451.40 +/- 25.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=851000, episode_reward=468.40 +/- 30.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=852000, episode_reward=457.40 +/- 9.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=853000, episode_reward=439.80 +/- 18.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=854000, episode_reward=457.80 +/- 11.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=855000, episode_reward=437.20 +/- 22.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=856000, episode_reward=436.00 +/- 17.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=857000, episode_reward=449.00 +/- 21.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=858000, episode_reward=449.20 +/- 21.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=859000, episode_reward=450.60 +/- 8.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=860000, episode_reward=459.20 +/- 3.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=861000, episode_reward=459.40 +/- 8.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=862000, episode_reward=442.60 +/- 21.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=863000, episode_reward=451.60 +/- 18.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=864000, episode_reward=452.80 +/- 22.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=865000, episode_reward=457.00 +/- 23.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=866000, episode_reward=460.20 +/- 45.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=867000, episode_reward=434.60 +/- 24.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=868000, episode_reward=464.00 +/- 26.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=869000, episode_reward=460.20 +/- 26.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=870000, episode_reward=447.20 +/- 14.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=871000, episode_reward=449.80 +/- 40.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=872000, episode_reward=460.60 +/- 17.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=873000, episode_reward=429.40 +/- 6.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=874000, episode_reward=429.80 +/- 20.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=875000, episode_reward=449.60 +/- 17.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=876000, episode_reward=445.60 +/- 27.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=877000, episode_reward=462.60 +/- 20.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=878000, episode_reward=455.20 +/- 22.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=879000, episode_reward=417.40 +/- 17.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=880000, episode_reward=441.20 +/- 32.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=881000, episode_reward=442.00 +/- 14.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=882000, episode_reward=457.00 +/- 16.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=883000, episode_reward=453.00 +/- 11.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=884000, episode_reward=463.40 +/- 12.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=885000, episode_reward=453.40 +/- 24.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=886000, episode_reward=454.40 +/- 14.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=887000, episode_reward=471.20 +/- 16.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=888000, episode_reward=455.00 +/- 27.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=889000, episode_reward=450.40 +/- 16.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=890000, episode_reward=462.80 +/- 23.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=891000, episode_reward=466.60 +/- 11.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=892000, episode_reward=444.20 +/- 22.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=893000, episode_reward=439.60 +/- 23.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=894000, episode_reward=445.40 +/- 19.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=895000, episode_reward=452.40 +/- 9.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=896000, episode_reward=464.20 +/- 30.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=897000, episode_reward=445.60 +/- 13.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=898000, episode_reward=454.40 +/- 7.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=899000, episode_reward=442.20 +/- 33.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=900000, episode_reward=451.60 +/- 12.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=901000, episode_reward=453.40 +/- 15.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=902000, episode_reward=464.40 +/- 21.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=903000, episode_reward=452.00 +/- 20.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=904000, episode_reward=460.60 +/- 11.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=905000, episode_reward=460.40 +/- 24.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=906000, episode_reward=453.60 +/- 16.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=907000, episode_reward=444.00 +/- 35.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=908000, episode_reward=466.20 +/- 17.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=909000, episode_reward=470.00 +/- 14.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=910000, episode_reward=439.60 +/- 7.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=911000, episode_reward=465.00 +/- 12.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=912000, episode_reward=454.20 +/- 15.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=913000, episode_reward=443.80 +/- 23.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=914000, episode_reward=453.60 +/- 10.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=915000, episode_reward=460.60 +/- 22.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=916000, episode_reward=455.60 +/- 25.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=917000, episode_reward=450.00 +/- 31.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=918000, episode_reward=459.00 +/- 25.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=919000, episode_reward=460.20 +/- 14.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=920000, episode_reward=457.00 +/- 25.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=921000, episode_reward=460.20 +/- 4.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=922000, episode_reward=454.60 +/- 11.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=923000, episode_reward=464.80 +/- 6.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=924000, episode_reward=470.80 +/- 16.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=925000, episode_reward=457.20 +/- 17.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=926000, episode_reward=453.00 +/- 25.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=927000, episode_reward=463.60 +/- 17.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=928000, episode_reward=464.40 +/- 24.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=929000, episode_reward=454.40 +/- 9.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=930000, episode_reward=453.60 +/- 14.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=931000, episode_reward=428.20 +/- 10.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=932000, episode_reward=459.20 +/- 16.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=933000, episode_reward=458.40 +/- 12.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=934000, episode_reward=451.60 +/- 15.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=935000, episode_reward=448.40 +/- 19.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=936000, episode_reward=461.00 +/- 23.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=937000, episode_reward=456.80 +/- 5.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=938000, episode_reward=446.40 +/- 26.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=939000, episode_reward=462.20 +/- 24.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=940000, episode_reward=454.00 +/- 25.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=941000, episode_reward=456.60 +/- 10.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=942000, episode_reward=448.60 +/- 7.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=943000, episode_reward=423.40 +/- 13.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=944000, episode_reward=440.80 +/- 16.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=945000, episode_reward=441.80 +/- 11.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=946000, episode_reward=451.40 +/- 25.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=947000, episode_reward=453.00 +/- 17.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=948000, episode_reward=457.00 +/- 24.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=949000, episode_reward=471.20 +/- 19.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=950000, episode_reward=448.40 +/- 20.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=951000, episode_reward=469.40 +/- 14.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=952000, episode_reward=432.40 +/- 22.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=953000, episode_reward=458.20 +/- 4.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=954000, episode_reward=478.00 +/- 20.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=955000, episode_reward=438.00 +/- 28.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=956000, episode_reward=440.20 +/- 8.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=957000, episode_reward=455.20 +/- 18.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=958000, episode_reward=463.60 +/- 6.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=959000, episode_reward=464.20 +/- 20.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=960000, episode_reward=458.20 +/- 17.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=961000, episode_reward=453.80 +/- 39.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=962000, episode_reward=459.20 +/- 35.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=963000, episode_reward=472.40 +/- 24.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=964000, episode_reward=446.20 +/- 17.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=965000, episode_reward=480.20 +/- 11.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=966000, episode_reward=467.20 +/- 19.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=967000, episode_reward=445.00 +/- 28.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=968000, episode_reward=434.80 +/- 10.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=969000, episode_reward=456.20 +/- 17.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=970000, episode_reward=360.40 +/- 211.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=971000, episode_reward=462.80 +/- 38.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=972000, episode_reward=456.00 +/- 21.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=973000, episode_reward=482.60 +/- 22.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=974000, episode_reward=471.60 +/- 14.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=975000, episode_reward=462.40 +/- 20.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=976000, episode_reward=462.00 +/- 21.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=977000, episode_reward=477.60 +/- 16.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=978000, episode_reward=457.20 +/- 14.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=979000, episode_reward=461.80 +/- 14.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=980000, episode_reward=479.60 +/- 25.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=981000, episode_reward=469.20 +/- 32.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=982000, episode_reward=468.80 +/- 10.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=983000, episode_reward=473.00 +/- 23.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=984000, episode_reward=463.60 +/- 22.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=985000, episode_reward=463.20 +/- 33.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=986000, episode_reward=487.20 +/- 37.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=987000, episode_reward=456.20 +/- 32.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=988000, episode_reward=451.00 +/- 23.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=989000, episode_reward=465.40 +/- 14.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=990000, episode_reward=463.80 +/- 17.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=991000, episode_reward=476.00 +/- 17.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=992000, episode_reward=475.80 +/- 14.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=993000, episode_reward=448.00 +/- 22.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=994000, episode_reward=462.00 +/- 18.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=995000, episode_reward=458.80 +/- 7.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=996000, episode_reward=472.00 +/- 19.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=997000, episode_reward=457.80 +/- 21.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=998000, episode_reward=449.60 +/- 29.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=999000, episode_reward=448.60 +/- 18.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1000000, episode_reward=458.20 +/- 20.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1001000, episode_reward=469.60 +/- 26.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1002000, episode_reward=438.40 +/- 26.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1003000, episode_reward=442.60 +/- 17.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1004000, episode_reward=456.20 +/- 16.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1005000, episode_reward=454.00 +/- 26.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1006000, episode_reward=467.20 +/- 26.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1007000, episode_reward=456.80 +/- 32.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1008000, episode_reward=435.60 +/- 19.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1009000, episode_reward=451.20 +/- 27.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1010000, episode_reward=449.00 +/- 14.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1011000, episode_reward=454.40 +/- 28.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1012000, episode_reward=457.00 +/- 10.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1013000, episode_reward=463.20 +/- 12.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1014000, episode_reward=457.00 +/- 9.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1015000, episode_reward=432.00 +/- 16.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1016000, episode_reward=457.80 +/- 35.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1017000, episode_reward=454.00 +/- 9.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1018000, episode_reward=450.00 +/- 27.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1019000, episode_reward=449.60 +/- 28.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1020000, episode_reward=471.00 +/- 11.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1021000, episode_reward=473.20 +/- 19.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1022000, episode_reward=462.40 +/- 13.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1023000, episode_reward=458.60 +/- 18.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1024000, episode_reward=476.00 +/- 42.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1025000, episode_reward=451.80 +/- 18.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1026000, episode_reward=463.60 +/- 24.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1027000, episode_reward=447.60 +/- 16.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1028000, episode_reward=440.80 +/- 20.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1029000, episode_reward=452.00 +/- 22.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1030000, episode_reward=461.00 +/- 17.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1031000, episode_reward=456.60 +/- 37.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1032000, episode_reward=450.80 +/- 13.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1033000, episode_reward=449.60 +/- 44.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1034000, episode_reward=450.80 +/- 25.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1035000, episode_reward=431.00 +/- 17.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1036000, episode_reward=436.40 +/- 41.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1037000, episode_reward=459.60 +/- 18.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1038000, episode_reward=450.00 +/- 16.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1039000, episode_reward=432.60 +/- 35.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1040000, episode_reward=435.20 +/- 29.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1041000, episode_reward=448.40 +/- 18.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1042000, episode_reward=451.00 +/- 29.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1043000, episode_reward=447.80 +/- 29.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1044000, episode_reward=455.80 +/- 11.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1045000, episode_reward=465.60 +/- 20.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1046000, episode_reward=448.00 +/- 27.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1047000, episode_reward=447.60 +/- 25.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1048000, episode_reward=452.60 +/- 14.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1049000, episode_reward=447.20 +/- 15.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1050000, episode_reward=430.40 +/- 17.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1051000, episode_reward=457.20 +/- 13.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1052000, episode_reward=431.20 +/- 15.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1053000, episode_reward=438.80 +/- 9.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1054000, episode_reward=423.40 +/- 19.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1055000, episode_reward=432.20 +/- 36.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1056000, episode_reward=430.40 +/- 30.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1057000, episode_reward=429.80 +/- 33.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1058000, episode_reward=443.80 +/- 28.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1059000, episode_reward=430.80 +/- 24.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1060000, episode_reward=460.00 +/- 25.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1061000, episode_reward=429.40 +/- 13.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1062000, episode_reward=435.00 +/- 24.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1063000, episode_reward=427.20 +/- 25.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1064000, episode_reward=428.00 +/- 23.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1065000, episode_reward=433.00 +/- 20.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1066000, episode_reward=421.60 +/- 21.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1067000, episode_reward=430.20 +/- 14.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1068000, episode_reward=441.40 +/- 7.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1069000, episode_reward=425.00 +/- 22.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1070000, episode_reward=447.60 +/- 13.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1071000, episode_reward=449.80 +/- 18.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1072000, episode_reward=430.40 +/- 7.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1073000, episode_reward=448.20 +/- 7.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1074000, episode_reward=443.00 +/- 14.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1075000, episode_reward=420.40 +/- 15.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1076000, episode_reward=454.40 +/- 8.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1077000, episode_reward=441.80 +/- 27.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1078000, episode_reward=435.60 +/- 29.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1079000, episode_reward=441.80 +/- 19.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1080000, episode_reward=440.60 +/- 25.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1081000, episode_reward=336.80 +/- 202.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1082000, episode_reward=462.60 +/- 23.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1083000, episode_reward=461.20 +/- 18.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1084000, episode_reward=450.20 +/- 8.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1085000, episode_reward=453.40 +/- 20.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1086000, episode_reward=442.80 +/- 21.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1087000, episode_reward=449.20 +/- 33.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1088000, episode_reward=458.00 +/- 32.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1089000, episode_reward=441.40 +/- 21.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1090000, episode_reward=453.80 +/- 23.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1091000, episode_reward=457.20 +/- 27.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1092000, episode_reward=457.80 +/- 24.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1093000, episode_reward=466.60 +/- 22.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1094000, episode_reward=447.40 +/- 32.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1095000, episode_reward=467.00 +/- 12.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1096000, episode_reward=478.60 +/- 34.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1097000, episode_reward=469.20 +/- 10.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1098000, episode_reward=479.20 +/- 15.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1099000, episode_reward=458.00 +/- 15.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1100000, episode_reward=455.00 +/- 24.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1101000, episode_reward=473.00 +/- 13.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1102000, episode_reward=472.40 +/- 12.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1103000, episode_reward=441.20 +/- 10.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1104000, episode_reward=464.60 +/- 22.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1105000, episode_reward=472.20 +/- 8.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1106000, episode_reward=461.00 +/- 28.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1107000, episode_reward=453.40 +/- 18.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1108000, episode_reward=471.60 +/- 7.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1109000, episode_reward=454.20 +/- 20.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1110000, episode_reward=463.40 +/- 20.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1111000, episode_reward=460.40 +/- 29.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1112000, episode_reward=442.60 +/- 12.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1113000, episode_reward=458.00 +/- 17.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1114000, episode_reward=460.40 +/- 19.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1115000, episode_reward=441.80 +/- 12.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1116000, episode_reward=448.40 +/- 15.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1117000, episode_reward=473.80 +/- 18.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1118000, episode_reward=455.80 +/- 8.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1119000, episode_reward=458.40 +/- 17.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1120000, episode_reward=467.80 +/- 29.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1121000, episode_reward=465.20 +/- 19.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1122000, episode_reward=450.80 +/- 11.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1123000, episode_reward=433.60 +/- 12.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1124000, episode_reward=463.80 +/- 22.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1125000, episode_reward=457.00 +/- 13.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1126000, episode_reward=471.00 +/- 23.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1127000, episode_reward=452.40 +/- 22.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1128000, episode_reward=457.80 +/- 14.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1129000, episode_reward=471.20 +/- 35.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1130000, episode_reward=436.00 +/- 21.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1131000, episode_reward=453.20 +/- 21.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1132000, episode_reward=456.40 +/- 26.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1133000, episode_reward=468.60 +/- 22.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1134000, episode_reward=466.40 +/- 23.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1135000, episode_reward=451.60 +/- 21.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1136000, episode_reward=457.60 +/- 13.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1137000, episode_reward=465.00 +/- 28.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1138000, episode_reward=468.60 +/- 23.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1139000, episode_reward=442.60 +/- 21.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1140000, episode_reward=455.00 +/- 26.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1141000, episode_reward=487.20 +/- 20.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1142000, episode_reward=461.00 +/- 18.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1143000, episode_reward=453.80 +/- 12.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1144000, episode_reward=463.60 +/- 14.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1145000, episode_reward=466.40 +/- 21.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1146000, episode_reward=447.00 +/- 25.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1147000, episode_reward=472.60 +/- 18.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1148000, episode_reward=452.20 +/- 24.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1149000, episode_reward=465.20 +/- 17.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1150000, episode_reward=444.80 +/- 17.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1151000, episode_reward=463.80 +/- 20.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1152000, episode_reward=461.00 +/- 21.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1153000, episode_reward=447.60 +/- 13.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1154000, episode_reward=471.20 +/- 24.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1155000, episode_reward=453.20 +/- 33.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1156000, episode_reward=456.80 +/- 9.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1157000, episode_reward=467.00 +/- 29.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1158000, episode_reward=456.20 +/- 25.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1159000, episode_reward=461.00 +/- 17.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1160000, episode_reward=477.00 +/- 20.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1161000, episode_reward=444.80 +/- 16.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1162000, episode_reward=457.20 +/- 31.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1163000, episode_reward=444.60 +/- 17.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1164000, episode_reward=475.80 +/- 18.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1165000, episode_reward=463.60 +/- 11.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1166000, episode_reward=466.00 +/- 20.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1167000, episode_reward=477.40 +/- 25.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1168000, episode_reward=463.20 +/- 11.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1169000, episode_reward=469.40 +/- 27.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1170000, episode_reward=465.60 +/- 17.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1171000, episode_reward=470.80 +/- 20.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1172000, episode_reward=468.80 +/- 23.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1173000, episode_reward=469.40 +/- 18.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1174000, episode_reward=460.60 +/- 13.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1175000, episode_reward=440.20 +/- 17.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1176000, episode_reward=466.20 +/- 8.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1177000, episode_reward=461.00 +/- 18.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1178000, episode_reward=449.40 +/- 17.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1179000, episode_reward=459.20 +/- 20.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1180000, episode_reward=457.20 +/- 18.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1181000, episode_reward=472.80 +/- 14.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1182000, episode_reward=450.80 +/- 19.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1183000, episode_reward=471.40 +/- 25.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1184000, episode_reward=458.60 +/- 13.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1185000, episode_reward=459.80 +/- 17.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1186000, episode_reward=471.60 +/- 20.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1187000, episode_reward=447.20 +/- 12.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1188000, episode_reward=448.80 +/- 9.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1189000, episode_reward=464.20 +/- 20.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1190000, episode_reward=449.80 +/- 37.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1191000, episode_reward=456.00 +/- 15.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1192000, episode_reward=465.40 +/- 14.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1193000, episode_reward=464.80 +/- 24.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1194000, episode_reward=458.60 +/- 26.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1195000, episode_reward=454.20 +/- 26.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1196000, episode_reward=477.20 +/- 12.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1197000, episode_reward=445.00 +/- 12.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1198000, episode_reward=455.00 +/- 27.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1199000, episode_reward=460.80 +/- 31.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1200000, episode_reward=471.40 +/- 28.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1201000, episode_reward=453.80 +/- 24.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1202000, episode_reward=450.60 +/- 21.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1203000, episode_reward=476.00 +/- 30.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1204000, episode_reward=454.20 +/- 16.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1205000, episode_reward=450.20 +/- 13.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1206000, episode_reward=462.80 +/- 11.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1207000, episode_reward=444.00 +/- 16.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1208000, episode_reward=450.80 +/- 14.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1209000, episode_reward=466.60 +/- 15.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1210000, episode_reward=473.20 +/- 30.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1211000, episode_reward=446.20 +/- 14.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1212000, episode_reward=463.20 +/- 21.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1213000, episode_reward=464.80 +/- 28.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1214000, episode_reward=458.00 +/- 13.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1215000, episode_reward=450.40 +/- 19.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1216000, episode_reward=466.00 +/- 11.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1217000, episode_reward=455.20 +/- 9.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1218000, episode_reward=469.80 +/- 18.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1219000, episode_reward=489.40 +/- 11.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1220000, episode_reward=456.40 +/- 25.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1221000, episode_reward=463.00 +/- 15.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1222000, episode_reward=475.60 +/- 6.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1223000, episode_reward=465.40 +/- 23.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1224000, episode_reward=483.80 +/- 12.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1225000, episode_reward=462.40 +/- 12.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1226000, episode_reward=452.80 +/- 17.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1227000, episode_reward=462.60 +/- 23.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1228000, episode_reward=452.60 +/- 10.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1229000, episode_reward=465.80 +/- 24.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1230000, episode_reward=457.40 +/- 10.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1231000, episode_reward=464.80 +/- 17.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1232000, episode_reward=469.00 +/- 22.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1233000, episode_reward=460.20 +/- 24.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1234000, episode_reward=465.20 +/- 11.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1235000, episode_reward=458.80 +/- 23.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1236000, episode_reward=459.00 +/- 27.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1237000, episode_reward=464.00 +/- 18.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1238000, episode_reward=467.60 +/- 10.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1239000, episode_reward=458.40 +/- 13.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1240000, episode_reward=469.80 +/- 15.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1241000, episode_reward=438.00 +/- 15.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1242000, episode_reward=450.40 +/- 17.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1243000, episode_reward=465.60 +/- 19.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1244000, episode_reward=459.00 +/- 14.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1245000, episode_reward=452.60 +/- 23.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1246000, episode_reward=467.40 +/- 17.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1247000, episode_reward=464.80 +/- 15.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1248000, episode_reward=462.60 +/- 18.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1249000, episode_reward=467.20 +/- 17.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1250000, episode_reward=467.80 +/- 30.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1251000, episode_reward=444.60 +/- 9.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1252000, episode_reward=464.60 +/- 19.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1253000, episode_reward=468.40 +/- 6.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1254000, episode_reward=440.60 +/- 13.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1255000, episode_reward=448.80 +/- 22.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1256000, episode_reward=478.40 +/- 19.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1257000, episode_reward=467.40 +/- 16.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1258000, episode_reward=462.40 +/- 29.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1259000, episode_reward=488.00 +/- 26.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1260000, episode_reward=469.20 +/- 9.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1261000, episode_reward=485.60 +/- 8.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1262000, episode_reward=483.00 +/- 19.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1263000, episode_reward=467.60 +/- 29.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1264000, episode_reward=461.20 +/- 21.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1265000, episode_reward=479.60 +/- 32.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1266000, episode_reward=457.80 +/- 12.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1267000, episode_reward=482.60 +/- 11.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1268000, episode_reward=451.40 +/- 29.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1269000, episode_reward=443.00 +/- 17.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1270000, episode_reward=470.00 +/- 16.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1271000, episode_reward=458.20 +/- 25.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1272000, episode_reward=464.60 +/- 7.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1273000, episode_reward=465.80 +/- 18.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1274000, episode_reward=466.40 +/- 13.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1275000, episode_reward=486.60 +/- 10.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1276000, episode_reward=453.20 +/- 13.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1277000, episode_reward=453.60 +/- 17.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1278000, episode_reward=461.00 +/- 14.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1279000, episode_reward=451.60 +/- 25.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1280000, episode_reward=465.60 +/- 28.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1281000, episode_reward=464.80 +/- 8.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1282000, episode_reward=467.80 +/- 33.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1283000, episode_reward=441.80 +/- 19.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1284000, episode_reward=454.40 +/- 28.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1285000, episode_reward=470.80 +/- 22.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1286000, episode_reward=457.00 +/- 24.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1287000, episode_reward=466.80 +/- 24.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1288000, episode_reward=448.40 +/- 21.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1289000, episode_reward=466.60 +/- 4.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1290000, episode_reward=463.60 +/- 20.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1291000, episode_reward=457.40 +/- 14.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1292000, episode_reward=462.40 +/- 9.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1293000, episode_reward=464.40 +/- 10.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1294000, episode_reward=473.60 +/- 22.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1295000, episode_reward=451.40 +/- 16.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1296000, episode_reward=446.80 +/- 10.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1297000, episode_reward=476.60 +/- 28.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1298000, episode_reward=465.60 +/- 5.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1299000, episode_reward=473.20 +/- 20.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1300000, episode_reward=443.60 +/- 23.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1301000, episode_reward=454.20 +/- 7.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1302000, episode_reward=464.80 +/- 28.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1303000, episode_reward=457.00 +/- 12.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1304000, episode_reward=453.00 +/- 8.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1305000, episode_reward=454.40 +/- 19.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1306000, episode_reward=461.60 +/- 14.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1307000, episode_reward=455.80 +/- 19.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1308000, episode_reward=464.80 +/- 17.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1309000, episode_reward=456.80 +/- 15.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1310000, episode_reward=468.40 +/- 20.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1311000, episode_reward=453.00 +/- 17.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1312000, episode_reward=439.60 +/- 14.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1313000, episode_reward=460.00 +/- 21.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1314000, episode_reward=467.00 +/- 20.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1315000, episode_reward=470.00 +/- 12.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1316000, episode_reward=459.40 +/- 23.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1317000, episode_reward=464.20 +/- 21.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1318000, episode_reward=460.40 +/- 26.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1319000, episode_reward=468.20 +/- 19.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1320000, episode_reward=443.60 +/- 15.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1321000, episode_reward=477.00 +/- 30.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1322000, episode_reward=469.00 +/- 11.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1323000, episode_reward=464.60 +/- 21.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1324000, episode_reward=472.00 +/- 22.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1325000, episode_reward=448.20 +/- 22.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1326000, episode_reward=475.80 +/- 20.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1327000, episode_reward=463.80 +/- 14.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1328000, episode_reward=472.20 +/- 27.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1329000, episode_reward=466.20 +/- 9.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1330000, episode_reward=451.00 +/- 15.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1331000, episode_reward=460.20 +/- 27.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1332000, episode_reward=448.00 +/- 16.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1333000, episode_reward=455.20 +/- 17.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1334000, episode_reward=453.00 +/- 18.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1335000, episode_reward=443.60 +/- 19.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1336000, episode_reward=462.40 +/- 28.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1337000, episode_reward=467.00 +/- 14.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1338000, episode_reward=453.80 +/- 28.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1339000, episode_reward=460.40 +/- 11.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1340000, episode_reward=450.40 +/- 24.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1341000, episode_reward=466.80 +/- 20.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1342000, episode_reward=460.00 +/- 12.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1343000, episode_reward=464.80 +/- 13.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1344000, episode_reward=451.60 +/- 21.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1345000, episode_reward=460.80 +/- 10.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1346000, episode_reward=465.60 +/- 9.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1347000, episode_reward=463.20 +/- 9.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1348000, episode_reward=446.80 +/- 27.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1349000, episode_reward=460.20 +/- 14.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1350000, episode_reward=451.80 +/- 15.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1351000, episode_reward=454.40 +/- 15.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1352000, episode_reward=457.20 +/- 40.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1353000, episode_reward=467.60 +/- 11.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1354000, episode_reward=465.80 +/- 11.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1355000, episode_reward=470.00 +/- 13.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1356000, episode_reward=456.00 +/- 19.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1357000, episode_reward=463.00 +/- 22.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1358000, episode_reward=470.40 +/- 21.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1359000, episode_reward=451.20 +/- 20.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1360000, episode_reward=463.00 +/- 21.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1361000, episode_reward=462.60 +/- 12.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1362000, episode_reward=466.00 +/- 22.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1363000, episode_reward=463.40 +/- 22.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1364000, episode_reward=474.80 +/- 11.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1365000, episode_reward=457.80 +/- 12.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1366000, episode_reward=458.80 +/- 15.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1367000, episode_reward=458.20 +/- 16.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1368000, episode_reward=456.20 +/- 24.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1369000, episode_reward=457.00 +/- 20.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1370000, episode_reward=452.60 +/- 25.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1371000, episode_reward=466.20 +/- 15.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1372000, episode_reward=458.00 +/- 11.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1373000, episode_reward=451.40 +/- 23.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1374000, episode_reward=450.00 +/- 10.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1375000, episode_reward=471.00 +/- 16.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1376000, episode_reward=447.60 +/- 29.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1377000, episode_reward=436.60 +/- 38.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1378000, episode_reward=443.40 +/- 15.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1379000, episode_reward=458.20 +/- 21.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1380000, episode_reward=437.20 +/- 17.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1381000, episode_reward=462.20 +/- 19.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1382000, episode_reward=457.40 +/- 12.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1383000, episode_reward=477.40 +/- 19.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1384000, episode_reward=473.20 +/- 27.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1385000, episode_reward=457.60 +/- 20.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1386000, episode_reward=467.40 +/- 27.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1387000, episode_reward=475.40 +/- 21.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1388000, episode_reward=460.60 +/- 26.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1389000, episode_reward=472.80 +/- 18.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1390000, episode_reward=472.40 +/- 14.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1391000, episode_reward=465.80 +/- 8.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1392000, episode_reward=450.00 +/- 23.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1393000, episode_reward=480.80 +/- 13.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1394000, episode_reward=459.40 +/- 22.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1395000, episode_reward=479.00 +/- 9.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1396000, episode_reward=472.00 +/- 24.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1397000, episode_reward=444.60 +/- 26.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1398000, episode_reward=469.20 +/- 23.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1399000, episode_reward=472.20 +/- 26.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1400000, episode_reward=468.60 +/- 31.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1401000, episode_reward=492.00 +/- 8.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1402000, episode_reward=466.40 +/- 27.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1403000, episode_reward=471.20 +/- 21.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1404000, episode_reward=448.20 +/- 29.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1405000, episode_reward=463.40 +/- 24.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1406000, episode_reward=471.60 +/- 16.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1407000, episode_reward=455.60 +/- 19.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1408000, episode_reward=473.80 +/- 25.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1409000, episode_reward=470.00 +/- 20.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1410000, episode_reward=462.00 +/- 14.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1411000, episode_reward=467.20 +/- 17.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1412000, episode_reward=483.00 +/- 20.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1413000, episode_reward=470.40 +/- 24.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1414000, episode_reward=470.40 +/- 24.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1415000, episode_reward=477.20 +/- 22.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1416000, episode_reward=440.80 +/- 30.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1417000, episode_reward=468.20 +/- 35.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1418000, episode_reward=473.80 +/- 7.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1419000, episode_reward=454.60 +/- 27.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1420000, episode_reward=474.80 +/- 12.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1421000, episode_reward=466.20 +/- 29.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1422000, episode_reward=461.00 +/- 26.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1423000, episode_reward=453.40 +/- 14.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1424000, episode_reward=439.60 +/- 17.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1425000, episode_reward=461.40 +/- 20.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1426000, episode_reward=448.40 +/- 30.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1427000, episode_reward=467.40 +/- 10.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1428000, episode_reward=462.60 +/- 22.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1429000, episode_reward=459.80 +/- 23.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1430000, episode_reward=453.00 +/- 41.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1431000, episode_reward=464.60 +/- 14.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1432000, episode_reward=463.00 +/- 8.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1433000, episode_reward=477.60 +/- 25.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1434000, episode_reward=448.00 +/- 23.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1435000, episode_reward=474.40 +/- 20.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1436000, episode_reward=454.80 +/- 8.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1437000, episode_reward=441.60 +/- 12.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1438000, episode_reward=458.20 +/- 7.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1439000, episode_reward=467.40 +/- 8.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1440000, episode_reward=470.20 +/- 18.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1441000, episode_reward=423.20 +/- 15.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1442000, episode_reward=461.40 +/- 10.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1443000, episode_reward=490.60 +/- 9.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1444000, episode_reward=473.40 +/- 23.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1445000, episode_reward=458.80 +/- 27.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1446000, episode_reward=466.60 +/- 8.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1447000, episode_reward=466.00 +/- 19.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1448000, episode_reward=449.20 +/- 26.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1449000, episode_reward=450.40 +/- 22.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1450000, episode_reward=471.40 +/- 30.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1451000, episode_reward=462.60 +/- 17.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1452000, episode_reward=476.20 +/- 10.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1453000, episode_reward=465.40 +/- 18.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1454000, episode_reward=464.00 +/- 13.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1455000, episode_reward=450.20 +/- 23.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1456000, episode_reward=465.00 +/- 24.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1457000, episode_reward=472.80 +/- 1.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1458000, episode_reward=461.60 +/- 11.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1459000, episode_reward=462.80 +/- 13.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1460000, episode_reward=469.00 +/- 27.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1461000, episode_reward=465.40 +/- 27.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1462000, episode_reward=459.80 +/- 25.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1463000, episode_reward=470.00 +/- 10.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1464000, episode_reward=468.80 +/- 11.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1465000, episode_reward=472.80 +/- 22.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1466000, episode_reward=450.80 +/- 15.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1467000, episode_reward=471.80 +/- 26.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1468000, episode_reward=476.40 +/- 14.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1469000, episode_reward=474.20 +/- 31.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1470000, episode_reward=466.40 +/- 24.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1471000, episode_reward=447.80 +/- 29.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1472000, episode_reward=484.00 +/- 26.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1473000, episode_reward=474.00 +/- 26.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1474000, episode_reward=472.20 +/- 18.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1475000, episode_reward=462.40 +/- 11.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1476000, episode_reward=456.80 +/- 25.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1477000, episode_reward=459.40 +/- 19.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1478000, episode_reward=457.00 +/- 32.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1479000, episode_reward=489.80 +/- 35.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1480000, episode_reward=472.20 +/- 33.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1481000, episode_reward=461.80 +/- 17.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1482000, episode_reward=491.40 +/- 15.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1483000, episode_reward=468.80 +/- 14.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1484000, episode_reward=468.80 +/- 20.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1485000, episode_reward=472.60 +/- 27.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1486000, episode_reward=475.00 +/- 27.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1487000, episode_reward=469.80 +/- 12.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1488000, episode_reward=476.20 +/- 34.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1489000, episode_reward=458.20 +/- 9.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1490000, episode_reward=461.80 +/- 11.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1491000, episode_reward=468.40 +/- 13.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1492000, episode_reward=460.80 +/- 32.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1493000, episode_reward=464.40 +/- 24.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1494000, episode_reward=464.80 +/- 14.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1495000, episode_reward=466.60 +/- 22.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1496000, episode_reward=469.00 +/- 14.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1497000, episode_reward=467.60 +/- 13.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1498000, episode_reward=478.60 +/- 19.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1499000, episode_reward=470.40 +/- 23.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1500000, episode_reward=469.80 +/- 16.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1501000, episode_reward=467.80 +/- 30.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1502000, episode_reward=474.80 +/- 13.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1503000, episode_reward=463.60 +/- 26.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1504000, episode_reward=471.40 +/- 20.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1505000, episode_reward=469.40 +/- 10.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1506000, episode_reward=464.80 +/- 10.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1507000, episode_reward=468.80 +/- 12.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1508000, episode_reward=485.20 +/- 19.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1509000, episode_reward=471.60 +/- 19.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1510000, episode_reward=475.20 +/- 20.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1511000, episode_reward=486.40 +/- 19.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1512000, episode_reward=472.40 +/- 19.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1513000, episode_reward=464.60 +/- 12.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1514000, episode_reward=465.80 +/- 26.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1515000, episode_reward=489.40 +/- 10.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1516000, episode_reward=458.80 +/- 15.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1517000, episode_reward=478.00 +/- 16.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1518000, episode_reward=454.60 +/- 17.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1519000, episode_reward=466.20 +/- 21.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1520000, episode_reward=458.20 +/- 19.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1521000, episode_reward=485.40 +/- 15.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1522000, episode_reward=484.40 +/- 6.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1523000, episode_reward=466.40 +/- 11.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1524000, episode_reward=484.40 +/- 26.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1525000, episode_reward=452.20 +/- 11.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1526000, episode_reward=465.60 +/- 24.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1527000, episode_reward=476.00 +/- 17.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1528000, episode_reward=456.20 +/- 29.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1529000, episode_reward=467.60 +/- 27.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1530000, episode_reward=463.00 +/- 22.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1531000, episode_reward=462.40 +/- 10.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1532000, episode_reward=465.80 +/- 24.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1533000, episode_reward=466.40 +/- 16.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1534000, episode_reward=460.20 +/- 24.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1535000, episode_reward=457.80 +/- 15.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1536000, episode_reward=464.60 +/- 15.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1537000, episode_reward=475.40 +/- 18.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1538000, episode_reward=455.00 +/- 24.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1539000, episode_reward=465.00 +/- 22.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1540000, episode_reward=470.80 +/- 28.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1541000, episode_reward=459.20 +/- 15.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1542000, episode_reward=506.40 +/- 17.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1543000, episode_reward=457.60 +/- 30.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1544000, episode_reward=437.60 +/- 14.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1545000, episode_reward=455.20 +/- 16.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1546000, episode_reward=460.00 +/- 13.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1547000, episode_reward=443.80 +/- 14.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1548000, episode_reward=449.40 +/- 19.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1549000, episode_reward=455.00 +/- 16.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1550000, episode_reward=440.40 +/- 17.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1551000, episode_reward=446.00 +/- 9.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1552000, episode_reward=452.40 +/- 7.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1553000, episode_reward=445.40 +/- 20.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1554000, episode_reward=464.20 +/- 23.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1555000, episode_reward=469.60 +/- 26.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1556000, episode_reward=461.60 +/- 12.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1557000, episode_reward=452.40 +/- 27.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1558000, episode_reward=455.20 +/- 31.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1559000, episode_reward=462.80 +/- 18.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1560000, episode_reward=462.20 +/- 23.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1561000, episode_reward=449.20 +/- 16.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1562000, episode_reward=470.00 +/- 17.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1563000, episode_reward=468.00 +/- 26.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1564000, episode_reward=443.40 +/- 21.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1565000, episode_reward=443.40 +/- 30.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1566000, episode_reward=472.20 +/- 20.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1567000, episode_reward=474.40 +/- 25.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1568000, episode_reward=458.80 +/- 20.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1569000, episode_reward=468.80 +/- 24.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1570000, episode_reward=467.80 +/- 25.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1571000, episode_reward=477.20 +/- 17.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1572000, episode_reward=470.80 +/- 25.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1573000, episode_reward=454.40 +/- 20.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1574000, episode_reward=470.40 +/- 18.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1575000, episode_reward=456.40 +/- 13.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1576000, episode_reward=453.60 +/- 7.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1577000, episode_reward=471.40 +/- 22.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1578000, episode_reward=469.00 +/- 19.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1579000, episode_reward=478.60 +/- 17.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1580000, episode_reward=481.80 +/- 26.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1581000, episode_reward=486.40 +/- 17.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1582000, episode_reward=458.20 +/- 21.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1583000, episode_reward=464.40 +/- 12.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1584000, episode_reward=472.80 +/- 25.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1585000, episode_reward=455.00 +/- 29.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1586000, episode_reward=465.00 +/- 24.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1587000, episode_reward=476.20 +/- 10.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1588000, episode_reward=476.80 +/- 14.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1589000, episode_reward=456.20 +/- 20.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1590000, episode_reward=450.80 +/- 34.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1591000, episode_reward=453.80 +/- 18.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1592000, episode_reward=464.00 +/- 14.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1593000, episode_reward=472.40 +/- 31.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1594000, episode_reward=475.80 +/- 18.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1595000, episode_reward=470.80 +/- 23.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1596000, episode_reward=469.60 +/- 23.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1597000, episode_reward=469.20 +/- 25.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1598000, episode_reward=468.00 +/- 23.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1599000, episode_reward=459.60 +/- 13.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1600000, episode_reward=459.00 +/- 17.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1601000, episode_reward=476.80 +/- 15.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1602000, episode_reward=466.60 +/- 10.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1603000, episode_reward=479.00 +/- 29.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1604000, episode_reward=439.00 +/- 21.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1605000, episode_reward=477.60 +/- 18.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1606000, episode_reward=461.00 +/- 31.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1607000, episode_reward=450.00 +/- 12.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1608000, episode_reward=486.40 +/- 34.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1609000, episode_reward=451.80 +/- 20.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1610000, episode_reward=470.60 +/- 26.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1611000, episode_reward=459.40 +/- 17.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1612000, episode_reward=487.40 +/- 14.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1613000, episode_reward=473.40 +/- 27.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1614000, episode_reward=474.20 +/- 5.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1615000, episode_reward=448.40 +/- 14.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1616000, episode_reward=474.20 +/- 16.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1617000, episode_reward=443.00 +/- 34.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1618000, episode_reward=458.80 +/- 19.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1619000, episode_reward=445.60 +/- 9.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1620000, episode_reward=453.20 +/- 26.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1621000, episode_reward=486.40 +/- 30.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1622000, episode_reward=454.40 +/- 16.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1623000, episode_reward=466.00 +/- 17.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1624000, episode_reward=472.00 +/- 18.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1625000, episode_reward=472.60 +/- 12.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1626000, episode_reward=473.40 +/- 24.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1627000, episode_reward=451.20 +/- 12.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1628000, episode_reward=471.60 +/- 22.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1629000, episode_reward=458.40 +/- 27.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1630000, episode_reward=457.40 +/- 27.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1631000, episode_reward=460.20 +/- 17.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1632000, episode_reward=466.20 +/- 26.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1633000, episode_reward=455.40 +/- 19.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1634000, episode_reward=485.40 +/- 17.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1635000, episode_reward=458.20 +/- 36.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1636000, episode_reward=454.40 +/- 7.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1637000, episode_reward=464.00 +/- 21.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1638000, episode_reward=470.20 +/- 27.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1639000, episode_reward=466.20 +/- 20.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1640000, episode_reward=482.20 +/- 25.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1641000, episode_reward=467.00 +/- 7.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1642000, episode_reward=475.80 +/- 20.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1643000, episode_reward=470.00 +/- 14.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1644000, episode_reward=460.80 +/- 12.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1645000, episode_reward=465.80 +/- 18.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1646000, episode_reward=481.40 +/- 17.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1647000, episode_reward=465.80 +/- 16.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1648000, episode_reward=472.80 +/- 11.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1649000, episode_reward=457.00 +/- 8.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1650000, episode_reward=470.60 +/- 24.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1651000, episode_reward=498.20 +/- 26.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1652000, episode_reward=442.60 +/- 7.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1653000, episode_reward=476.40 +/- 28.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1654000, episode_reward=478.20 +/- 17.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1655000, episode_reward=480.60 +/- 35.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1656000, episode_reward=466.80 +/- 23.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1657000, episode_reward=462.40 +/- 26.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1658000, episode_reward=473.00 +/- 20.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1659000, episode_reward=480.40 +/- 16.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1660000, episode_reward=487.20 +/- 15.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1661000, episode_reward=479.40 +/- 30.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1662000, episode_reward=475.40 +/- 3.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1663000, episode_reward=461.00 +/- 16.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1664000, episode_reward=465.00 +/- 10.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1665000, episode_reward=483.60 +/- 23.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1666000, episode_reward=478.80 +/- 8.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1667000, episode_reward=463.20 +/- 11.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1668000, episode_reward=465.60 +/- 16.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1669000, episode_reward=471.20 +/- 19.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1670000, episode_reward=472.40 +/- 15.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1671000, episode_reward=482.20 +/- 15.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1672000, episode_reward=477.80 +/- 8.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1673000, episode_reward=472.20 +/- 19.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1674000, episode_reward=469.40 +/- 19.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1675000, episode_reward=459.80 +/- 20.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1676000, episode_reward=453.80 +/- 19.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1677000, episode_reward=465.00 +/- 26.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1678000, episode_reward=469.40 +/- 3.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1679000, episode_reward=478.00 +/- 26.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1680000, episode_reward=468.00 +/- 36.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1681000, episode_reward=455.80 +/- 22.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1682000, episode_reward=472.60 +/- 24.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1683000, episode_reward=474.60 +/- 20.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1684000, episode_reward=456.60 +/- 18.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1685000, episode_reward=464.40 +/- 29.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1686000, episode_reward=483.00 +/- 13.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1687000, episode_reward=461.20 +/- 25.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1688000, episode_reward=455.00 +/- 12.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1689000, episode_reward=471.80 +/- 8.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1690000, episode_reward=471.00 +/- 21.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1691000, episode_reward=475.40 +/- 14.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1692000, episode_reward=484.60 +/- 18.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1693000, episode_reward=495.40 +/- 20.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1694000, episode_reward=492.20 +/- 11.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1695000, episode_reward=482.80 +/- 7.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1696000, episode_reward=476.20 +/- 27.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1697000, episode_reward=485.60 +/- 20.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1698000, episode_reward=459.80 +/- 21.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1699000, episode_reward=468.60 +/- 9.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1700000, episode_reward=464.60 +/- 17.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1701000, episode_reward=470.60 +/- 14.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1702000, episode_reward=478.80 +/- 12.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1703000, episode_reward=461.20 +/- 29.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1704000, episode_reward=471.40 +/- 32.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1705000, episode_reward=467.20 +/- 14.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1706000, episode_reward=472.80 +/- 26.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1707000, episode_reward=474.20 +/- 19.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1708000, episode_reward=477.20 +/- 27.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1709000, episode_reward=473.20 +/- 24.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1710000, episode_reward=471.60 +/- 21.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1711000, episode_reward=478.40 +/- 14.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1712000, episode_reward=465.20 +/- 30.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1713000, episode_reward=459.20 +/- 11.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1714000, episode_reward=453.00 +/- 14.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1715000, episode_reward=477.80 +/- 14.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1716000, episode_reward=461.20 +/- 21.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1717000, episode_reward=455.00 +/- 33.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1718000, episode_reward=468.20 +/- 21.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1719000, episode_reward=483.80 +/- 22.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1720000, episode_reward=474.60 +/- 16.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1721000, episode_reward=482.80 +/- 14.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1722000, episode_reward=485.60 +/- 21.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1723000, episode_reward=493.00 +/- 13.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1724000, episode_reward=467.40 +/- 30.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1725000, episode_reward=492.00 +/- 9.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1726000, episode_reward=461.40 +/- 18.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1727000, episode_reward=493.00 +/- 20.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1728000, episode_reward=472.60 +/- 14.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1729000, episode_reward=468.20 +/- 41.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1730000, episode_reward=461.20 +/- 17.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1731000, episode_reward=485.60 +/- 24.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1732000, episode_reward=476.40 +/- 33.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1733000, episode_reward=474.40 +/- 11.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1734000, episode_reward=481.80 +/- 27.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1735000, episode_reward=480.80 +/- 17.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1736000, episode_reward=464.40 +/- 14.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1737000, episode_reward=470.40 +/- 10.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1738000, episode_reward=462.60 +/- 9.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1739000, episode_reward=467.60 +/- 24.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1740000, episode_reward=482.40 +/- 24.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1741000, episode_reward=468.60 +/- 16.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1742000, episode_reward=471.40 +/- 10.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1743000, episode_reward=451.80 +/- 19.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1744000, episode_reward=484.60 +/- 24.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1745000, episode_reward=460.40 +/- 18.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1746000, episode_reward=466.60 +/- 20.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1747000, episode_reward=485.00 +/- 10.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1748000, episode_reward=482.00 +/- 23.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1749000, episode_reward=465.80 +/- 18.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1750000, episode_reward=459.20 +/- 18.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1751000, episode_reward=485.60 +/- 12.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1752000, episode_reward=450.20 +/- 21.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1753000, episode_reward=465.80 +/- 15.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1754000, episode_reward=464.80 +/- 26.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1755000, episode_reward=475.60 +/- 32.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1756000, episode_reward=460.20 +/- 15.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1757000, episode_reward=463.40 +/- 25.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1758000, episode_reward=463.20 +/- 38.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1759000, episode_reward=463.80 +/- 26.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1760000, episode_reward=474.40 +/- 13.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1761000, episode_reward=469.60 +/- 12.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1762000, episode_reward=482.40 +/- 25.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1763000, episode_reward=483.40 +/- 19.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1764000, episode_reward=475.40 +/- 21.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1765000, episode_reward=470.60 +/- 7.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1766000, episode_reward=488.40 +/- 10.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1767000, episode_reward=479.60 +/- 13.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1768000, episode_reward=492.40 +/- 8.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1769000, episode_reward=495.60 +/- 18.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1770000, episode_reward=479.80 +/- 3.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1771000, episode_reward=472.20 +/- 33.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1772000, episode_reward=466.40 +/- 18.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1773000, episode_reward=482.00 +/- 28.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1774000, episode_reward=488.40 +/- 10.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1775000, episode_reward=479.80 +/- 16.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1776000, episode_reward=482.80 +/- 21.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1777000, episode_reward=485.60 +/- 22.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1778000, episode_reward=470.80 +/- 16.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1779000, episode_reward=459.20 +/- 12.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1780000, episode_reward=463.40 +/- 15.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1781000, episode_reward=452.40 +/- 29.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1782000, episode_reward=466.40 +/- 27.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1783000, episode_reward=461.80 +/- 20.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1784000, episode_reward=471.40 +/- 24.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1785000, episode_reward=465.40 +/- 19.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1786000, episode_reward=475.80 +/- 19.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1787000, episode_reward=473.40 +/- 26.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1788000, episode_reward=458.80 +/- 12.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1789000, episode_reward=444.40 +/- 19.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1790000, episode_reward=472.00 +/- 14.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1791000, episode_reward=463.00 +/- 12.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1792000, episode_reward=466.00 +/- 25.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1793000, episode_reward=454.80 +/- 11.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1794000, episode_reward=450.80 +/- 29.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1795000, episode_reward=475.00 +/- 26.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1796000, episode_reward=481.00 +/- 24.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1797000, episode_reward=488.00 +/- 24.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1798000, episode_reward=457.00 +/- 16.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1799000, episode_reward=478.00 +/- 15.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1800000, episode_reward=455.80 +/- 12.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1801000, episode_reward=479.20 +/- 19.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1802000, episode_reward=458.00 +/- 29.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1803000, episode_reward=468.60 +/- 10.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1804000, episode_reward=468.20 +/- 20.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1805000, episode_reward=468.80 +/- 21.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1806000, episode_reward=468.60 +/- 14.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1807000, episode_reward=476.20 +/- 18.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1808000, episode_reward=489.20 +/- 16.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1809000, episode_reward=464.80 +/- 31.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1810000, episode_reward=476.80 +/- 25.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1811000, episode_reward=483.60 +/- 29.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1812000, episode_reward=486.20 +/- 13.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1813000, episode_reward=489.20 +/- 11.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1814000, episode_reward=488.00 +/- 33.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1815000, episode_reward=489.20 +/- 29.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1816000, episode_reward=479.00 +/- 29.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1817000, episode_reward=476.60 +/- 23.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1818000, episode_reward=481.20 +/- 8.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1819000, episode_reward=482.80 +/- 13.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1820000, episode_reward=496.60 +/- 9.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1821000, episode_reward=468.20 +/- 30.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1822000, episode_reward=475.40 +/- 28.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1823000, episode_reward=481.40 +/- 23.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1824000, episode_reward=470.20 +/- 20.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1825000, episode_reward=480.40 +/- 26.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1826000, episode_reward=462.40 +/- 14.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1827000, episode_reward=471.40 +/- 19.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1828000, episode_reward=486.80 +/- 12.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1829000, episode_reward=472.80 +/- 25.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1830000, episode_reward=461.20 +/- 31.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1831000, episode_reward=473.40 +/- 20.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1832000, episode_reward=479.00 +/- 25.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1833000, episode_reward=465.80 +/- 37.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1834000, episode_reward=485.80 +/- 16.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1835000, episode_reward=473.80 +/- 16.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1836000, episode_reward=458.60 +/- 27.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1837000, episode_reward=462.60 +/- 31.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1838000, episode_reward=462.80 +/- 15.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1839000, episode_reward=472.20 +/- 8.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1840000, episode_reward=482.60 +/- 25.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1841000, episode_reward=475.60 +/- 20.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1842000, episode_reward=471.20 +/- 19.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1843000, episode_reward=465.60 +/- 14.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1844000, episode_reward=495.00 +/- 21.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1845000, episode_reward=465.60 +/- 30.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1846000, episode_reward=465.00 +/- 12.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1847000, episode_reward=477.60 +/- 26.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1848000, episode_reward=471.60 +/- 20.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1849000, episode_reward=460.60 +/- 15.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1850000, episode_reward=488.80 +/- 16.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1851000, episode_reward=468.40 +/- 18.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1852000, episode_reward=473.40 +/- 11.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1853000, episode_reward=472.80 +/- 15.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1854000, episode_reward=456.40 +/- 18.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1855000, episode_reward=462.20 +/- 13.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1856000, episode_reward=472.80 +/- 5.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1857000, episode_reward=470.80 +/- 17.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1858000, episode_reward=457.20 +/- 16.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1859000, episode_reward=473.00 +/- 9.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1860000, episode_reward=471.80 +/- 13.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1861000, episode_reward=446.20 +/- 19.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1862000, episode_reward=464.00 +/- 11.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1863000, episode_reward=488.60 +/- 18.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1864000, episode_reward=486.20 +/- 30.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1865000, episode_reward=472.60 +/- 18.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1866000, episode_reward=440.20 +/- 17.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1867000, episode_reward=471.80 +/- 21.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1868000, episode_reward=468.40 +/- 21.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1869000, episode_reward=469.80 +/- 13.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1870000, episode_reward=466.80 +/- 17.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1871000, episode_reward=472.80 +/- 18.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1872000, episode_reward=458.40 +/- 19.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1873000, episode_reward=461.80 +/- 22.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1874000, episode_reward=477.00 +/- 17.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1875000, episode_reward=471.40 +/- 19.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1876000, episode_reward=474.00 +/- 14.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1877000, episode_reward=456.40 +/- 6.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1878000, episode_reward=460.60 +/- 13.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1879000, episode_reward=481.80 +/- 20.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1880000, episode_reward=488.80 +/- 15.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1881000, episode_reward=484.00 +/- 18.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1882000, episode_reward=492.40 +/- 16.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1883000, episode_reward=472.40 +/- 14.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1884000, episode_reward=479.60 +/- 14.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1885000, episode_reward=478.80 +/- 14.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1886000, episode_reward=484.20 +/- 8.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1887000, episode_reward=475.00 +/- 18.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1888000, episode_reward=461.40 +/- 10.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1889000, episode_reward=486.60 +/- 13.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1890000, episode_reward=490.40 +/- 19.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1891000, episode_reward=489.40 +/- 3.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1892000, episode_reward=483.40 +/- 25.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1893000, episode_reward=478.00 +/- 9.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1894000, episode_reward=483.20 +/- 6.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1895000, episode_reward=446.20 +/- 18.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1896000, episode_reward=470.80 +/- 15.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1897000, episode_reward=477.00 +/- 18.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1898000, episode_reward=472.40 +/- 17.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1899000, episode_reward=463.60 +/- 25.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1900000, episode_reward=466.80 +/- 12.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1901000, episode_reward=464.00 +/- 20.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1902000, episode_reward=467.40 +/- 19.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1903000, episode_reward=470.00 +/- 17.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1904000, episode_reward=471.20 +/- 19.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1905000, episode_reward=484.40 +/- 13.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1906000, episode_reward=487.20 +/- 13.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1907000, episode_reward=464.00 +/- 24.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1908000, episode_reward=456.80 +/- 19.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1909000, episode_reward=486.60 +/- 26.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1910000, episode_reward=464.40 +/- 24.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1911000, episode_reward=475.20 +/- 17.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1912000, episode_reward=485.60 +/- 26.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1913000, episode_reward=466.40 +/- 21.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1914000, episode_reward=471.20 +/- 24.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1915000, episode_reward=480.40 +/- 29.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1916000, episode_reward=477.40 +/- 20.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1917000, episode_reward=485.80 +/- 15.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1918000, episode_reward=466.80 +/- 13.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1919000, episode_reward=475.40 +/- 31.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1920000, episode_reward=470.60 +/- 16.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1921000, episode_reward=462.40 +/- 10.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1922000, episode_reward=469.40 +/- 20.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1923000, episode_reward=481.40 +/- 35.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1924000, episode_reward=492.40 +/- 31.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1925000, episode_reward=481.60 +/- 17.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1926000, episode_reward=492.20 +/- 13.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1927000, episode_reward=471.40 +/- 28.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1928000, episode_reward=493.80 +/- 26.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1929000, episode_reward=465.60 +/- 30.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1930000, episode_reward=475.60 +/- 15.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1931000, episode_reward=481.80 +/- 12.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1932000, episode_reward=480.00 +/- 30.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1933000, episode_reward=467.60 +/- 20.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1934000, episode_reward=473.80 +/- 33.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1935000, episode_reward=480.20 +/- 23.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1936000, episode_reward=479.80 +/- 28.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1937000, episode_reward=475.80 +/- 14.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1938000, episode_reward=472.00 +/- 15.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1939000, episode_reward=476.00 +/- 17.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1940000, episode_reward=468.60 +/- 19.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1941000, episode_reward=462.00 +/- 19.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1942000, episode_reward=455.80 +/- 11.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1943000, episode_reward=473.60 +/- 11.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1944000, episode_reward=488.00 +/- 17.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1945000, episode_reward=474.20 +/- 15.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1946000, episode_reward=479.60 +/- 18.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1947000, episode_reward=460.60 +/- 25.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1948000, episode_reward=483.80 +/- 26.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1949000, episode_reward=468.60 +/- 13.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1950000, episode_reward=471.60 +/- 31.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1951000, episode_reward=472.40 +/- 11.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1952000, episode_reward=473.80 +/- 17.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1953000, episode_reward=479.20 +/- 30.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1954000, episode_reward=489.40 +/- 23.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1955000, episode_reward=469.00 +/- 23.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1956000, episode_reward=484.40 +/- 26.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1957000, episode_reward=465.00 +/- 16.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1958000, episode_reward=475.40 +/- 19.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1959000, episode_reward=467.80 +/- 4.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1960000, episode_reward=471.60 +/- 25.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1961000, episode_reward=470.00 +/- 23.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1962000, episode_reward=484.40 +/- 19.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1963000, episode_reward=474.80 +/- 15.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1964000, episode_reward=483.20 +/- 23.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1965000, episode_reward=465.80 +/- 11.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1966000, episode_reward=475.00 +/- 14.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1967000, episode_reward=466.20 +/- 24.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1968000, episode_reward=464.00 +/- 17.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1969000, episode_reward=452.20 +/- 24.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1970000, episode_reward=464.20 +/- 31.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1971000, episode_reward=460.80 +/- 26.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1972000, episode_reward=474.80 +/- 27.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1973000, episode_reward=485.20 +/- 11.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1974000, episode_reward=475.60 +/- 23.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1975000, episode_reward=490.00 +/- 24.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1976000, episode_reward=456.80 +/- 25.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1977000, episode_reward=456.80 +/- 5.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1978000, episode_reward=475.80 +/- 23.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1979000, episode_reward=469.20 +/- 20.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1980000, episode_reward=454.20 +/- 20.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1981000, episode_reward=447.40 +/- 12.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1982000, episode_reward=481.00 +/- 11.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1983000, episode_reward=460.60 +/- 20.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1984000, episode_reward=467.20 +/- 15.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1985000, episode_reward=461.80 +/- 17.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1986000, episode_reward=473.00 +/- 23.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1987000, episode_reward=475.60 +/- 12.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1988000, episode_reward=471.80 +/- 25.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1989000, episode_reward=467.00 +/- 17.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1990000, episode_reward=461.60 +/- 10.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1991000, episode_reward=462.80 +/- 21.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1992000, episode_reward=490.60 +/- 19.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1993000, episode_reward=502.60 +/- 26.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1994000, episode_reward=486.60 +/- 14.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1995000, episode_reward=452.00 +/- 19.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1996000, episode_reward=474.20 +/- 28.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1997000, episode_reward=460.60 +/- 13.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1998000, episode_reward=476.80 +/- 26.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1999000, episode_reward=475.60 +/- 18.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2000000, episode_reward=466.40 +/- 17.74\n",
      "Episode length: 100.00 +/- 0.00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(471.3, 20.65453945262397)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### REINFORCEMENT LEARNING I ###\n",
    "### TRAIN, SAVE, EVALUATE MODEL ###\n",
    "\n",
    "import gym\n",
    "import stable_baselines3 as sb\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "import pickle\n",
    "\n",
    "# Load diagnostics model from disk\n",
    "diag_model = pickle.load(open('diagnostics/model', 'rb'))\n",
    "# Initiate environment\n",
    "env = gym.make('Production-v0')\n",
    "# Callback for best model\n",
    "best_callback = EvalCallback(env, best_model_save_path='./callback/',\n",
    "                             log_path='./callback/', eval_freq=1000,\n",
    "                             deterministic=True, render=False)\n",
    "\n",
    "#model = sb.DQN('MlpPolicy', env, tensorboard_log=\"./tensorboard/\", gamma = 0.99, learning_rate=0.01)\n",
    "#model.learn(total_timesteps=2e6, tb_log_name=\"DQN_DIAG_model\", callback = best_callback)\n",
    "#model.save(\"DQN_DIAG_model\")\n",
    "\n",
    "model = sb.PPO('MlpPolicy', env, tensorboard_log=\"./tensorboard/\")\n",
    "model.learn(total_timesteps=2e6, tb_log_name=\"PPO_TRUE_model\", callback = best_callback)\n",
    "model.save(\"PPO_TRUE_model\")\n",
    "\n",
    "#model = sb.A2C('MlpPolicy', env, tensorboard_log=\"./tensorboard/\")\n",
    "#model.learn(total_timesteps=2e6, tb_log_name=\"A2C_DIAG_model\", callback = best_callback)\n",
    "#model.save(\"A2C_DIAG_model\")\n",
    "\n",
    "# Evaluate the agent\n",
    "evaluate_policy(model, model.get_env(), n_eval_episodes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(460.1, 23.847222060441336)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### REINFORCEMENT LEARNING II ###\n",
    "### LOAD MODEL ###\n",
    "import gym\n",
    "import stable_baselines3 as sb\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3 import A2C\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "import pickle\n",
    "\n",
    "diag_model = pickle.load(open('diagnostics/model', 'rb'))\n",
    "env = gym.make('Production-v0')\n",
    "# Best Model\n",
    "model = PPO.load('./callback/best_PPO_truehealth', env = env)\n",
    "# Last Model\n",
    "#model = DQN.load('DQN_1_model', env = env)\n",
    "\n",
    "# Evaluate the agent\n",
    "evaluate_policy(model, model.get_env(), n_eval_episodes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average number of reactive maintenance interventions per episode is:  0.004\n",
      "The average number of preventive maintenance interventions per episode is:  3.996\n",
      "The average sum of inventory per episode is:  36.937\n",
      "The average sum of spare parts inventory per episode is:  10.485\n",
      "The average reward per episode is:  461.476\n",
      "The average upper bound per episode is:  643.408\n"
     ]
    }
   ],
   "source": [
    "### REINFORCEMENT LEARNING III ###\n",
    "### TRY AND EVALUATE MY MODEL ###\n",
    "import pandas as pd\n",
    "\n",
    "# Initilaize Reward\n",
    "result_df = pd.DataFrame([[0, 0, 0, 0, 0, 0]], columns=['RM', 'PM', 'Inventory', 'Spare Parts Inventory', 'Reward', 'Upper'])\n",
    "# Set iterations\n",
    "iterations = 1000\n",
    "for i in range(iterations):\n",
    "    # Initialize episode\n",
    "    store = []\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    store.append([0, obs[0], env.breakdown, obs[2], obs[3], 0, done, obs[1]])\n",
    "    # Compute one episode\n",
    "    while not done:\n",
    "        # Get best action for state\n",
    "        action, _state = model.predict(obs, deterministic=True)\n",
    "        # Compute next state\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        # Store results of this episode\n",
    "        store.append([action, obs[0], env.breakdown, obs[2], obs[3], reward, done, obs[1]])\n",
    "    eps_df = pd.DataFrame(store, columns=['action', 'health', 'breakdown', 'inventory', 'sp_inventory', 'reward', 'done', 'next_order'])\n",
    "    # Calculate nr. of reactive maintenance interventions by counting health 'resets' and substracting PM actions\n",
    "    result_df.iloc[0]['RM'] = result_df.iloc[0]['RM'] + sum(eps_df['breakdown']==True)\n",
    "    # Calculate nr. of preventive maintenance interventions\n",
    "    result_df.iloc[0]['PM'] = result_df.iloc[0]['PM'] + sum(eps_df['action']==10)\n",
    "    # Calculate inventory\n",
    "    result_df.iloc[0]['Inventory'] = result_df.iloc[0]['Inventory'] + sum(eps_df['inventory'])\n",
    "    # Calculate spare parts inventory per period\n",
    "    result_df.iloc[0]['Spare Parts Inventory'] = result_df.iloc[0]['Spare Parts Inventory'] + sum(eps_df['sp_inventory'])\n",
    "    # Calculate reward\n",
    "    result_df.iloc[0]['Reward'] = result_df.iloc[0]['Reward'] + sum(eps_df['reward'])\n",
    "    # Calculate reward with no costs and fulfillment of all orders\n",
    "    result_df.iloc[0]['Upper'] = result_df.iloc[0]['Upper'] + sum(eps_df.iloc[:-1]['next_order']) * env.order_r\n",
    "\n",
    "print(\"The average number of reactive maintenance interventions per episode is: \", result_df.iloc[0]['RM']/iterations)\n",
    "print(\"The average number of preventive maintenance interventions per episode is: \", result_df.iloc[0]['PM']/iterations) \n",
    "print(\"The average sum of inventory per episode is: \", result_df.iloc[0]['Inventory']/iterations)\n",
    "print(\"The average sum of spare parts inventory per episode is: \", result_df.iloc[0]['Spare Parts Inventory']/iterations)\n",
    "print(\"The average reward per episode is: \", result_df.iloc[0]['Reward']/iterations)\n",
    "print(\"The average upper bound per episode is: \", result_df.iloc[0]['Upper']/iterations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=1000, episode_reward=-2693.40 +/- 18.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=-2707.20 +/- 20.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=3000, episode_reward=-1183.60 +/- 21.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4000, episode_reward=-1216.00 +/- 30.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=5000, episode_reward=-1285.00 +/- 4.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=6000, episode_reward=-1273.00 +/- 24.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=7000, episode_reward=-1319.20 +/- 37.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=8000, episode_reward=-1323.60 +/- 15.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=9000, episode_reward=-1198.00 +/- 27.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=10000, episode_reward=-1224.20 +/- 61.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=11000, episode_reward=-1213.00 +/- 34.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=12000, episode_reward=-1254.80 +/- 26.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=13000, episode_reward=-1257.80 +/- 36.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=14000, episode_reward=-1261.00 +/- 39.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=15000, episode_reward=-1301.40 +/- 28.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=16000, episode_reward=-1252.40 +/- 57.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=17000, episode_reward=-1148.40 +/- 66.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=18000, episode_reward=-1179.40 +/- 26.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=19000, episode_reward=-1235.60 +/- 23.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=20000, episode_reward=-1258.40 +/- 67.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=21000, episode_reward=-1260.20 +/- 37.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=22000, episode_reward=-1309.60 +/- 32.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=23000, episode_reward=-1276.40 +/- 32.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=24000, episode_reward=-1243.60 +/- 37.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=25000, episode_reward=-1293.20 +/- 43.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=26000, episode_reward=-1253.80 +/- 32.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=27000, episode_reward=-1261.60 +/- 57.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=28000, episode_reward=-1268.80 +/- 42.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=29000, episode_reward=-1337.80 +/- 18.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=30000, episode_reward=-1256.60 +/- 53.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=31000, episode_reward=-1293.80 +/- 31.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=32000, episode_reward=-1301.80 +/- 40.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=33000, episode_reward=-1276.40 +/- 14.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=34000, episode_reward=-1260.00 +/- 42.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=35000, episode_reward=-1254.40 +/- 23.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=36000, episode_reward=-1228.80 +/- 48.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=37000, episode_reward=-1143.20 +/- 58.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=38000, episode_reward=-1154.80 +/- 75.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=39000, episode_reward=-1235.60 +/- 38.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=40000, episode_reward=-1268.20 +/- 29.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=41000, episode_reward=-1265.80 +/- 58.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=42000, episode_reward=-1300.20 +/- 11.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=43000, episode_reward=-1226.80 +/- 32.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=44000, episode_reward=-1237.00 +/- 62.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=45000, episode_reward=-1253.80 +/- 38.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=46000, episode_reward=-1163.80 +/- 55.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=47000, episode_reward=-1225.20 +/- 69.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=48000, episode_reward=-1207.00 +/- 64.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=49000, episode_reward=-1224.40 +/- 72.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=50000, episode_reward=-1260.20 +/- 51.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=51000, episode_reward=-1239.80 +/- 49.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=52000, episode_reward=-1298.40 +/- 91.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=53000, episode_reward=-1237.40 +/- 35.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=54000, episode_reward=-1217.80 +/- 56.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=55000, episode_reward=-1272.20 +/- 46.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=56000, episode_reward=-1208.00 +/- 55.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=57000, episode_reward=-1242.80 +/- 84.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=58000, episode_reward=-1247.40 +/- 39.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=59000, episode_reward=-1268.80 +/- 74.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=60000, episode_reward=-1283.00 +/- 28.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=61000, episode_reward=-1263.80 +/- 36.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=62000, episode_reward=-1271.60 +/- 32.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=63000, episode_reward=-1301.80 +/- 55.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=64000, episode_reward=-1169.20 +/- 55.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=65000, episode_reward=-1219.80 +/- 62.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=66000, episode_reward=-1196.00 +/- 71.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=67000, episode_reward=-1242.00 +/- 46.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=68000, episode_reward=-1127.80 +/- 60.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=69000, episode_reward=-1202.40 +/- 44.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=70000, episode_reward=-1038.00 +/- 33.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=71000, episode_reward=-1053.60 +/- 38.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=72000, episode_reward=-965.40 +/- 33.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=73000, episode_reward=-996.40 +/- 32.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=74000, episode_reward=-1079.80 +/- 30.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=75000, episode_reward=-1047.20 +/- 67.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=76000, episode_reward=-1098.20 +/- 39.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=77000, episode_reward=-1136.60 +/- 63.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=78000, episode_reward=-1109.00 +/- 58.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=79000, episode_reward=-1089.80 +/- 66.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=80000, episode_reward=-1102.40 +/- 66.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=81000, episode_reward=-1097.80 +/- 39.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=82000, episode_reward=-1061.40 +/- 29.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=83000, episode_reward=-1033.00 +/- 51.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=84000, episode_reward=-1135.60 +/- 46.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=85000, episode_reward=-1114.40 +/- 54.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=86000, episode_reward=-1133.40 +/- 114.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=87000, episode_reward=-1105.80 +/- 78.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=88000, episode_reward=-1145.20 +/- 36.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=89000, episode_reward=-1128.80 +/- 62.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=90000, episode_reward=-1109.60 +/- 38.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=91000, episode_reward=-1096.20 +/- 44.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=92000, episode_reward=-1083.00 +/- 53.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=93000, episode_reward=-1084.60 +/- 13.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=94000, episode_reward=-1072.60 +/- 29.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=95000, episode_reward=-1007.40 +/- 27.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=96000, episode_reward=-976.80 +/- 48.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=97000, episode_reward=-1073.20 +/- 38.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=98000, episode_reward=-1001.40 +/- 46.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=99000, episode_reward=-1058.40 +/- 55.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=100000, episode_reward=-1054.80 +/- 56.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=101000, episode_reward=-1033.00 +/- 91.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=102000, episode_reward=-1080.20 +/- 29.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=103000, episode_reward=-1063.20 +/- 52.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=104000, episode_reward=-1074.60 +/- 49.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=105000, episode_reward=-1108.40 +/- 27.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=106000, episode_reward=-1069.60 +/- 41.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=107000, episode_reward=-1084.00 +/- 50.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=108000, episode_reward=-1110.20 +/- 44.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=109000, episode_reward=-1055.60 +/- 37.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=110000, episode_reward=-1049.80 +/- 28.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=111000, episode_reward=-1007.80 +/- 54.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=112000, episode_reward=-1058.60 +/- 47.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=113000, episode_reward=-1033.60 +/- 37.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=114000, episode_reward=-1032.80 +/- 30.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=115000, episode_reward=-1034.20 +/- 5.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=116000, episode_reward=-967.40 +/- 59.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=117000, episode_reward=-976.80 +/- 13.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=118000, episode_reward=-966.60 +/- 29.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=119000, episode_reward=-975.00 +/- 80.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=120000, episode_reward=-985.60 +/- 22.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=121000, episode_reward=-1012.60 +/- 54.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=122000, episode_reward=-943.20 +/- 79.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=123000, episode_reward=-956.20 +/- 54.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=124000, episode_reward=-996.80 +/- 38.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=125000, episode_reward=-1020.80 +/- 24.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=126000, episode_reward=-992.20 +/- 24.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=127000, episode_reward=-958.00 +/- 53.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=128000, episode_reward=-933.00 +/- 28.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=129000, episode_reward=-949.80 +/- 36.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=130000, episode_reward=-971.20 +/- 31.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=131000, episode_reward=-979.60 +/- 52.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=132000, episode_reward=-950.20 +/- 50.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=133000, episode_reward=-966.20 +/- 82.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=134000, episode_reward=-939.20 +/- 54.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=135000, episode_reward=-921.60 +/- 20.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=136000, episode_reward=-924.20 +/- 33.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=137000, episode_reward=-953.60 +/- 13.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=138000, episode_reward=-982.60 +/- 35.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=139000, episode_reward=-972.00 +/- 42.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=140000, episode_reward=-974.60 +/- 27.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=141000, episode_reward=-970.60 +/- 41.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=142000, episode_reward=-962.40 +/- 19.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=143000, episode_reward=-947.00 +/- 27.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=144000, episode_reward=-945.00 +/- 25.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=145000, episode_reward=-985.20 +/- 42.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=146000, episode_reward=-925.20 +/- 46.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=147000, episode_reward=-891.20 +/- 31.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=148000, episode_reward=-949.20 +/- 62.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=149000, episode_reward=-975.80 +/- 67.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=150000, episode_reward=-1051.20 +/- 264.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=151000, episode_reward=-1035.40 +/- 308.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=152000, episode_reward=-998.40 +/- 165.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=153000, episode_reward=-1002.60 +/- 179.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=154000, episode_reward=-906.80 +/- 33.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=155000, episode_reward=-914.80 +/- 58.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=156000, episode_reward=-901.40 +/- 46.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=157000, episode_reward=-916.20 +/- 44.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=158000, episode_reward=-903.60 +/- 42.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=159000, episode_reward=-930.00 +/- 47.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=160000, episode_reward=-915.80 +/- 39.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=161000, episode_reward=-912.40 +/- 15.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=162000, episode_reward=-923.20 +/- 31.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=163000, episode_reward=-962.60 +/- 21.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=164000, episode_reward=-934.00 +/- 9.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=165000, episode_reward=-918.60 +/- 29.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=166000, episode_reward=-929.80 +/- 46.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=167000, episode_reward=-918.80 +/- 74.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=168000, episode_reward=-952.60 +/- 31.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=169000, episode_reward=-916.40 +/- 80.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=170000, episode_reward=-932.00 +/- 30.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=171000, episode_reward=-960.40 +/- 58.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=172000, episode_reward=-967.20 +/- 49.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=173000, episode_reward=-975.60 +/- 43.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=174000, episode_reward=-960.40 +/- 45.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=175000, episode_reward=-956.80 +/- 56.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=176000, episode_reward=-978.80 +/- 37.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=177000, episode_reward=-963.20 +/- 9.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=178000, episode_reward=-924.60 +/- 32.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=179000, episode_reward=-967.20 +/- 39.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=180000, episode_reward=-967.40 +/- 49.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=181000, episode_reward=-994.80 +/- 34.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=182000, episode_reward=-940.40 +/- 29.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=183000, episode_reward=-965.60 +/- 33.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=184000, episode_reward=-940.40 +/- 28.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=185000, episode_reward=-955.80 +/- 28.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=186000, episode_reward=-920.60 +/- 59.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=187000, episode_reward=-929.60 +/- 60.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=188000, episode_reward=-946.60 +/- 32.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=189000, episode_reward=-926.00 +/- 22.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=190000, episode_reward=-947.60 +/- 39.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=191000, episode_reward=-959.60 +/- 48.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=192000, episode_reward=-995.40 +/- 50.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=193000, episode_reward=-966.20 +/- 29.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=194000, episode_reward=-942.40 +/- 38.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=195000, episode_reward=-974.60 +/- 44.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=196000, episode_reward=-916.20 +/- 26.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=197000, episode_reward=-970.40 +/- 42.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=198000, episode_reward=-980.60 +/- 38.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=199000, episode_reward=-989.00 +/- 50.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=200000, episode_reward=-953.40 +/- 24.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=201000, episode_reward=-977.00 +/- 45.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=202000, episode_reward=-977.60 +/- 38.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=203000, episode_reward=-993.40 +/- 23.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=204000, episode_reward=-985.00 +/- 52.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=205000, episode_reward=-975.00 +/- 40.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=206000, episode_reward=-929.60 +/- 23.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=207000, episode_reward=-979.00 +/- 40.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=208000, episode_reward=-937.20 +/- 28.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=209000, episode_reward=-986.60 +/- 35.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=210000, episode_reward=-927.40 +/- 41.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=211000, episode_reward=-980.80 +/- 27.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=212000, episode_reward=-958.00 +/- 26.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=213000, episode_reward=-952.60 +/- 31.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=214000, episode_reward=-956.20 +/- 26.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=215000, episode_reward=-970.20 +/- 17.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=216000, episode_reward=-938.40 +/- 26.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=217000, episode_reward=-958.20 +/- 33.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=218000, episode_reward=-964.20 +/- 33.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=219000, episode_reward=-957.80 +/- 56.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=220000, episode_reward=-941.20 +/- 36.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=221000, episode_reward=-976.80 +/- 46.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=222000, episode_reward=-985.20 +/- 39.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=223000, episode_reward=-1021.60 +/- 51.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=224000, episode_reward=-971.20 +/- 37.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=225000, episode_reward=-957.20 +/- 41.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=226000, episode_reward=-996.00 +/- 36.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=227000, episode_reward=-1007.20 +/- 50.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=228000, episode_reward=-977.80 +/- 40.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=229000, episode_reward=-960.00 +/- 25.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=230000, episode_reward=-970.60 +/- 32.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=231000, episode_reward=-962.00 +/- 31.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=232000, episode_reward=-982.60 +/- 38.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=233000, episode_reward=-948.20 +/- 30.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=234000, episode_reward=-949.00 +/- 13.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=235000, episode_reward=-942.60 +/- 35.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=236000, episode_reward=-958.00 +/- 34.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=237000, episode_reward=-967.80 +/- 20.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=238000, episode_reward=-996.80 +/- 39.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=239000, episode_reward=-1021.60 +/- 27.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=240000, episode_reward=-992.40 +/- 26.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=241000, episode_reward=-1013.00 +/- 32.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=242000, episode_reward=-971.40 +/- 40.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=243000, episode_reward=-980.40 +/- 42.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=244000, episode_reward=-988.60 +/- 32.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=245000, episode_reward=-982.20 +/- 30.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=246000, episode_reward=-968.60 +/- 38.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=247000, episode_reward=-991.20 +/- 40.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=248000, episode_reward=-1012.60 +/- 25.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=249000, episode_reward=-993.20 +/- 31.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=250000, episode_reward=-959.20 +/- 35.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=251000, episode_reward=-963.00 +/- 39.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=252000, episode_reward=-989.40 +/- 6.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=253000, episode_reward=-990.60 +/- 73.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=254000, episode_reward=-942.80 +/- 38.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=255000, episode_reward=-995.00 +/- 16.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=256000, episode_reward=-972.00 +/- 54.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=257000, episode_reward=-975.60 +/- 39.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=258000, episode_reward=-1021.40 +/- 37.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=259000, episode_reward=-955.80 +/- 23.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=260000, episode_reward=-937.80 +/- 55.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=261000, episode_reward=-1005.40 +/- 36.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=262000, episode_reward=-967.60 +/- 28.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=263000, episode_reward=-972.00 +/- 38.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=264000, episode_reward=-976.40 +/- 21.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=265000, episode_reward=-974.60 +/- 70.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=266000, episode_reward=-988.20 +/- 31.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=267000, episode_reward=-970.40 +/- 21.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=268000, episode_reward=-957.80 +/- 32.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=269000, episode_reward=-997.20 +/- 35.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=270000, episode_reward=-984.60 +/- 38.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=271000, episode_reward=-986.40 +/- 44.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=272000, episode_reward=-978.20 +/- 28.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=273000, episode_reward=-1012.00 +/- 35.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=274000, episode_reward=-984.80 +/- 35.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=275000, episode_reward=-974.00 +/- 18.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=276000, episode_reward=-974.20 +/- 18.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=277000, episode_reward=-940.00 +/- 16.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=278000, episode_reward=-971.00 +/- 24.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=279000, episode_reward=-942.20 +/- 50.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=280000, episode_reward=-975.40 +/- 27.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=281000, episode_reward=-939.00 +/- 15.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=282000, episode_reward=-1005.80 +/- 20.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=283000, episode_reward=-969.60 +/- 23.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=284000, episode_reward=-965.40 +/- 24.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=285000, episode_reward=-961.00 +/- 19.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=286000, episode_reward=-999.60 +/- 28.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=287000, episode_reward=-983.20 +/- 47.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=288000, episode_reward=-985.40 +/- 19.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=289000, episode_reward=-979.20 +/- 47.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=290000, episode_reward=-950.60 +/- 42.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=291000, episode_reward=-968.80 +/- 43.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=292000, episode_reward=-978.80 +/- 30.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=293000, episode_reward=-998.20 +/- 34.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=294000, episode_reward=-969.80 +/- 33.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=295000, episode_reward=-966.00 +/- 33.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=296000, episode_reward=-985.00 +/- 45.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=297000, episode_reward=-990.20 +/- 18.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=298000, episode_reward=-961.00 +/- 45.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=299000, episode_reward=-970.60 +/- 27.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=300000, episode_reward=-977.60 +/- 37.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=301000, episode_reward=-1008.80 +/- 35.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=302000, episode_reward=-1034.60 +/- 19.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=303000, episode_reward=-979.40 +/- 42.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=304000, episode_reward=-964.80 +/- 36.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=305000, episode_reward=-969.60 +/- 22.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=306000, episode_reward=-981.80 +/- 27.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=307000, episode_reward=-970.80 +/- 32.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=308000, episode_reward=-965.80 +/- 35.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=309000, episode_reward=-969.20 +/- 37.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=310000, episode_reward=-974.00 +/- 24.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=311000, episode_reward=-971.00 +/- 36.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=312000, episode_reward=-964.20 +/- 37.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=313000, episode_reward=-998.40 +/- 26.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=314000, episode_reward=-972.40 +/- 44.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=315000, episode_reward=-969.40 +/- 47.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=316000, episode_reward=-963.00 +/- 38.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=317000, episode_reward=-1012.80 +/- 19.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=318000, episode_reward=-959.80 +/- 35.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=319000, episode_reward=-978.20 +/- 34.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=320000, episode_reward=-985.20 +/- 57.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=321000, episode_reward=-967.00 +/- 60.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=322000, episode_reward=-955.80 +/- 40.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=323000, episode_reward=-958.20 +/- 15.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=324000, episode_reward=-955.60 +/- 29.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=325000, episode_reward=-970.60 +/- 42.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=326000, episode_reward=-943.20 +/- 52.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=327000, episode_reward=-980.40 +/- 17.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=328000, episode_reward=-971.80 +/- 39.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=329000, episode_reward=-975.80 +/- 55.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=330000, episode_reward=-965.60 +/- 49.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=331000, episode_reward=-975.40 +/- 40.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=332000, episode_reward=-964.20 +/- 27.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=333000, episode_reward=-978.40 +/- 14.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=334000, episode_reward=-987.20 +/- 42.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=335000, episode_reward=-965.80 +/- 51.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=336000, episode_reward=-973.00 +/- 40.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=337000, episode_reward=-987.00 +/- 53.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=338000, episode_reward=-939.20 +/- 15.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=339000, episode_reward=-987.60 +/- 38.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=340000, episode_reward=-949.00 +/- 53.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=341000, episode_reward=-945.80 +/- 56.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=342000, episode_reward=-979.60 +/- 52.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=343000, episode_reward=-982.20 +/- 55.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=344000, episode_reward=-934.20 +/- 28.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=345000, episode_reward=-968.80 +/- 30.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=346000, episode_reward=-971.00 +/- 16.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=347000, episode_reward=-941.20 +/- 32.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=348000, episode_reward=-961.80 +/- 32.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=349000, episode_reward=-1010.60 +/- 29.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=350000, episode_reward=-964.60 +/- 20.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=351000, episode_reward=-961.20 +/- 50.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=352000, episode_reward=-950.00 +/- 31.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=353000, episode_reward=-948.00 +/- 4.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=354000, episode_reward=-961.20 +/- 40.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=355000, episode_reward=-978.60 +/- 47.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=356000, episode_reward=-970.00 +/- 64.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=357000, episode_reward=-962.20 +/- 38.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=358000, episode_reward=-1018.40 +/- 7.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=359000, episode_reward=-967.20 +/- 23.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=360000, episode_reward=-990.80 +/- 42.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=361000, episode_reward=-962.80 +/- 29.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=362000, episode_reward=-983.80 +/- 38.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=363000, episode_reward=-1004.20 +/- 25.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=364000, episode_reward=-954.80 +/- 31.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=365000, episode_reward=-936.20 +/- 39.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=366000, episode_reward=-962.40 +/- 27.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=367000, episode_reward=-953.40 +/- 36.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=368000, episode_reward=-977.00 +/- 20.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=369000, episode_reward=-940.00 +/- 39.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=370000, episode_reward=-930.80 +/- 40.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=371000, episode_reward=-953.80 +/- 33.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=372000, episode_reward=-959.80 +/- 20.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=373000, episode_reward=-922.20 +/- 42.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=374000, episode_reward=-977.80 +/- 38.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=375000, episode_reward=-921.00 +/- 37.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=376000, episode_reward=-942.60 +/- 35.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=377000, episode_reward=-965.80 +/- 26.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=378000, episode_reward=-970.80 +/- 43.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=379000, episode_reward=-950.40 +/- 48.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=380000, episode_reward=-956.20 +/- 28.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=381000, episode_reward=-946.80 +/- 51.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=382000, episode_reward=-974.60 +/- 35.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=383000, episode_reward=-979.20 +/- 32.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=384000, episode_reward=-977.80 +/- 39.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=385000, episode_reward=-958.80 +/- 27.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=386000, episode_reward=-993.20 +/- 39.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=387000, episode_reward=-955.00 +/- 37.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=388000, episode_reward=-983.60 +/- 24.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=389000, episode_reward=-968.60 +/- 25.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=390000, episode_reward=-980.00 +/- 17.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=391000, episode_reward=-966.00 +/- 59.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=392000, episode_reward=-958.20 +/- 36.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=393000, episode_reward=-951.80 +/- 38.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=394000, episode_reward=-971.60 +/- 47.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=395000, episode_reward=-950.20 +/- 22.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=396000, episode_reward=-967.00 +/- 20.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=397000, episode_reward=-937.40 +/- 27.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=398000, episode_reward=-954.60 +/- 24.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=399000, episode_reward=-971.00 +/- 31.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=400000, episode_reward=-990.80 +/- 38.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=401000, episode_reward=-972.00 +/- 34.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=402000, episode_reward=-995.80 +/- 35.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=403000, episode_reward=-984.20 +/- 28.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=404000, episode_reward=-954.40 +/- 42.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=405000, episode_reward=-934.40 +/- 39.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=406000, episode_reward=-982.20 +/- 32.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=407000, episode_reward=-960.40 +/- 38.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=408000, episode_reward=-951.40 +/- 10.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=409000, episode_reward=-942.80 +/- 39.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=410000, episode_reward=-948.80 +/- 42.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=411000, episode_reward=-959.00 +/- 25.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=412000, episode_reward=-970.00 +/- 45.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=413000, episode_reward=-964.20 +/- 29.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=414000, episode_reward=-972.40 +/- 33.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=415000, episode_reward=-974.40 +/- 57.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=416000, episode_reward=-949.80 +/- 31.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=417000, episode_reward=-946.60 +/- 33.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=418000, episode_reward=-941.00 +/- 26.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=419000, episode_reward=-970.20 +/- 52.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=420000, episode_reward=-959.60 +/- 34.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=421000, episode_reward=-953.40 +/- 29.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=422000, episode_reward=-978.40 +/- 11.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=423000, episode_reward=-933.20 +/- 14.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=424000, episode_reward=-933.40 +/- 27.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=425000, episode_reward=-922.20 +/- 34.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=426000, episode_reward=-939.40 +/- 38.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=427000, episode_reward=-971.40 +/- 23.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=428000, episode_reward=-955.20 +/- 34.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=429000, episode_reward=-952.00 +/- 28.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=430000, episode_reward=-932.00 +/- 28.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=431000, episode_reward=-943.40 +/- 22.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=432000, episode_reward=-944.40 +/- 24.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=433000, episode_reward=-994.20 +/- 37.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=434000, episode_reward=-954.00 +/- 26.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=435000, episode_reward=-891.80 +/- 31.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=436000, episode_reward=-942.00 +/- 20.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=437000, episode_reward=-950.80 +/- 33.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=438000, episode_reward=-950.20 +/- 30.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=439000, episode_reward=-935.20 +/- 29.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=440000, episode_reward=-926.40 +/- 55.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=441000, episode_reward=-917.20 +/- 35.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=442000, episode_reward=-939.20 +/- 33.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=443000, episode_reward=-969.80 +/- 38.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=444000, episode_reward=-941.40 +/- 48.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=445000, episode_reward=-964.80 +/- 42.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=446000, episode_reward=-994.60 +/- 24.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=447000, episode_reward=-983.80 +/- 36.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=448000, episode_reward=-968.80 +/- 28.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=449000, episode_reward=-960.00 +/- 14.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=450000, episode_reward=-976.80 +/- 44.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=451000, episode_reward=-964.40 +/- 11.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=452000, episode_reward=-935.80 +/- 38.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=453000, episode_reward=-966.00 +/- 21.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=454000, episode_reward=-952.40 +/- 21.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=455000, episode_reward=-945.20 +/- 28.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=456000, episode_reward=-966.40 +/- 38.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=457000, episode_reward=-933.20 +/- 41.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=458000, episode_reward=-941.20 +/- 47.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=459000, episode_reward=-953.40 +/- 37.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=460000, episode_reward=-963.60 +/- 42.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=461000, episode_reward=-963.80 +/- 33.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=462000, episode_reward=-946.20 +/- 23.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=463000, episode_reward=-933.40 +/- 16.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=464000, episode_reward=-955.40 +/- 32.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=465000, episode_reward=-939.40 +/- 26.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=466000, episode_reward=-963.20 +/- 48.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=467000, episode_reward=-922.60 +/- 43.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=468000, episode_reward=-928.80 +/- 43.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=469000, episode_reward=-947.80 +/- 25.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=470000, episode_reward=-933.80 +/- 25.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=471000, episode_reward=-958.80 +/- 51.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=472000, episode_reward=-954.40 +/- 29.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=473000, episode_reward=-976.80 +/- 24.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=474000, episode_reward=-951.60 +/- 23.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=475000, episode_reward=-975.00 +/- 28.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=476000, episode_reward=-962.80 +/- 27.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=477000, episode_reward=-913.00 +/- 10.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=478000, episode_reward=-920.20 +/- 25.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=479000, episode_reward=-947.60 +/- 32.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=480000, episode_reward=-948.80 +/- 28.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=481000, episode_reward=-944.40 +/- 52.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=482000, episode_reward=-949.60 +/- 54.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=483000, episode_reward=-974.40 +/- 38.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=484000, episode_reward=-951.60 +/- 14.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=485000, episode_reward=-933.40 +/- 47.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=486000, episode_reward=-953.80 +/- 34.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=487000, episode_reward=-948.00 +/- 13.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=488000, episode_reward=-947.60 +/- 46.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=489000, episode_reward=-937.80 +/- 21.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=490000, episode_reward=-930.20 +/- 39.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=491000, episode_reward=-962.00 +/- 30.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=492000, episode_reward=-938.40 +/- 38.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=493000, episode_reward=-950.60 +/- 51.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=494000, episode_reward=-956.60 +/- 34.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=495000, episode_reward=-990.00 +/- 54.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=496000, episode_reward=-948.40 +/- 29.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=497000, episode_reward=-929.60 +/- 19.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=498000, episode_reward=-980.20 +/- 15.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=499000, episode_reward=-922.20 +/- 33.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=500000, episode_reward=-933.60 +/- 43.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=501000, episode_reward=-931.80 +/- 54.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=502000, episode_reward=-917.00 +/- 19.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=503000, episode_reward=-940.60 +/- 37.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=504000, episode_reward=-917.20 +/- 12.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=505000, episode_reward=-903.20 +/- 35.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=506000, episode_reward=-945.40 +/- 55.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=507000, episode_reward=-911.20 +/- 32.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=508000, episode_reward=-924.60 +/- 50.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=509000, episode_reward=-923.80 +/- 50.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=510000, episode_reward=-956.00 +/- 24.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=511000, episode_reward=-917.80 +/- 25.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=512000, episode_reward=-945.20 +/- 17.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=513000, episode_reward=-927.00 +/- 51.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=514000, episode_reward=-933.60 +/- 25.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=515000, episode_reward=-959.60 +/- 26.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=516000, episode_reward=-934.20 +/- 12.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=517000, episode_reward=-916.80 +/- 25.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=518000, episode_reward=-922.60 +/- 31.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=519000, episode_reward=-961.00 +/- 22.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=520000, episode_reward=-982.00 +/- 16.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=521000, episode_reward=-959.20 +/- 25.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=522000, episode_reward=-977.60 +/- 10.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=523000, episode_reward=-915.80 +/- 45.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=524000, episode_reward=-962.00 +/- 28.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=525000, episode_reward=-936.40 +/- 51.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=526000, episode_reward=-944.80 +/- 19.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=527000, episode_reward=-943.40 +/- 37.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=528000, episode_reward=-936.40 +/- 20.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=529000, episode_reward=-969.20 +/- 30.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=530000, episode_reward=-915.60 +/- 31.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=531000, episode_reward=-925.60 +/- 20.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=532000, episode_reward=-940.20 +/- 25.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=533000, episode_reward=-955.20 +/- 44.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=534000, episode_reward=-916.20 +/- 41.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=535000, episode_reward=-974.80 +/- 32.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=536000, episode_reward=-925.60 +/- 25.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=537000, episode_reward=-972.60 +/- 48.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=538000, episode_reward=-951.60 +/- 22.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=539000, episode_reward=-943.20 +/- 27.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=540000, episode_reward=-939.80 +/- 57.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=541000, episode_reward=-942.60 +/- 59.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=542000, episode_reward=-935.00 +/- 25.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=543000, episode_reward=-927.00 +/- 21.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=544000, episode_reward=-976.00 +/- 14.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=545000, episode_reward=-963.80 +/- 31.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=546000, episode_reward=-917.40 +/- 38.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=547000, episode_reward=-968.80 +/- 24.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=548000, episode_reward=-963.80 +/- 6.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=549000, episode_reward=-980.40 +/- 38.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=550000, episode_reward=-935.60 +/- 69.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=551000, episode_reward=-955.60 +/- 54.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=552000, episode_reward=-946.20 +/- 32.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=553000, episode_reward=-926.40 +/- 44.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=554000, episode_reward=-936.60 +/- 51.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=555000, episode_reward=-975.20 +/- 29.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=556000, episode_reward=-929.60 +/- 48.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=557000, episode_reward=-945.00 +/- 47.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=558000, episode_reward=-914.80 +/- 35.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=559000, episode_reward=-953.80 +/- 38.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=560000, episode_reward=-952.40 +/- 18.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=561000, episode_reward=-952.40 +/- 55.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=562000, episode_reward=-945.80 +/- 25.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=563000, episode_reward=-961.60 +/- 30.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=564000, episode_reward=-939.40 +/- 56.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=565000, episode_reward=-958.60 +/- 39.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=566000, episode_reward=-939.60 +/- 40.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=567000, episode_reward=-963.00 +/- 21.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=568000, episode_reward=-943.00 +/- 46.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=569000, episode_reward=-915.80 +/- 28.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=570000, episode_reward=-943.60 +/- 23.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=571000, episode_reward=-932.40 +/- 20.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=572000, episode_reward=-956.80 +/- 47.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=573000, episode_reward=-939.80 +/- 34.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=574000, episode_reward=-938.00 +/- 21.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=575000, episode_reward=-941.00 +/- 49.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=576000, episode_reward=-909.80 +/- 34.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=577000, episode_reward=-966.00 +/- 54.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=578000, episode_reward=-952.60 +/- 61.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=579000, episode_reward=-942.60 +/- 17.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=580000, episode_reward=-938.00 +/- 57.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=581000, episode_reward=-948.60 +/- 20.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=582000, episode_reward=-950.40 +/- 41.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=583000, episode_reward=-928.20 +/- 18.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=584000, episode_reward=-959.80 +/- 29.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=585000, episode_reward=-946.60 +/- 15.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=586000, episode_reward=-954.80 +/- 18.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=587000, episode_reward=-925.00 +/- 25.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=588000, episode_reward=-912.20 +/- 38.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=589000, episode_reward=-940.40 +/- 25.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=590000, episode_reward=-914.80 +/- 22.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=591000, episode_reward=-942.80 +/- 26.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=592000, episode_reward=-946.00 +/- 22.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=593000, episode_reward=-946.00 +/- 53.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=594000, episode_reward=-966.80 +/- 54.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=595000, episode_reward=-946.20 +/- 42.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=596000, episode_reward=-926.60 +/- 37.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=597000, episode_reward=-934.00 +/- 32.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=598000, episode_reward=-947.00 +/- 28.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=599000, episode_reward=-918.40 +/- 25.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=600000, episode_reward=-943.40 +/- 34.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=601000, episode_reward=-914.20 +/- 28.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=602000, episode_reward=-907.60 +/- 57.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=603000, episode_reward=-928.00 +/- 20.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=604000, episode_reward=-909.00 +/- 37.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=605000, episode_reward=-909.00 +/- 30.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=606000, episode_reward=-927.80 +/- 22.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=607000, episode_reward=-977.00 +/- 49.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=608000, episode_reward=-935.00 +/- 44.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=609000, episode_reward=-941.00 +/- 30.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=610000, episode_reward=-953.80 +/- 24.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=611000, episode_reward=-958.00 +/- 45.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=612000, episode_reward=-958.80 +/- 54.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=613000, episode_reward=-933.40 +/- 27.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=614000, episode_reward=-946.80 +/- 40.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=615000, episode_reward=-907.80 +/- 19.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=616000, episode_reward=-919.20 +/- 45.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=617000, episode_reward=-941.60 +/- 27.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=618000, episode_reward=-946.60 +/- 64.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=619000, episode_reward=-953.40 +/- 25.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=620000, episode_reward=-952.20 +/- 64.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=621000, episode_reward=-936.20 +/- 23.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=622000, episode_reward=-931.00 +/- 24.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=623000, episode_reward=-909.60 +/- 26.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=624000, episode_reward=-937.80 +/- 52.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=625000, episode_reward=-938.00 +/- 51.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=626000, episode_reward=-949.60 +/- 33.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=627000, episode_reward=-932.60 +/- 20.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=628000, episode_reward=-949.40 +/- 56.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=629000, episode_reward=-936.00 +/- 38.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=630000, episode_reward=-945.80 +/- 27.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=631000, episode_reward=-972.00 +/- 27.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=632000, episode_reward=-914.60 +/- 25.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=633000, episode_reward=-924.40 +/- 23.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=634000, episode_reward=-940.00 +/- 38.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=635000, episode_reward=-918.40 +/- 40.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=636000, episode_reward=-965.00 +/- 25.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=637000, episode_reward=-957.40 +/- 68.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=638000, episode_reward=-965.80 +/- 54.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=639000, episode_reward=-920.20 +/- 43.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=640000, episode_reward=-906.80 +/- 27.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=641000, episode_reward=-926.60 +/- 30.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=642000, episode_reward=-943.40 +/- 34.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=643000, episode_reward=-928.00 +/- 26.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=644000, episode_reward=-942.00 +/- 9.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=645000, episode_reward=-937.20 +/- 44.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=646000, episode_reward=-944.20 +/- 40.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=647000, episode_reward=-910.20 +/- 10.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=648000, episode_reward=-935.00 +/- 22.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=649000, episode_reward=-923.80 +/- 22.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=650000, episode_reward=-919.80 +/- 19.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=651000, episode_reward=-957.20 +/- 45.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=652000, episode_reward=-929.60 +/- 40.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=653000, episode_reward=-965.60 +/- 45.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=654000, episode_reward=-941.80 +/- 70.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=655000, episode_reward=-921.80 +/- 77.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=656000, episode_reward=-931.60 +/- 39.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=657000, episode_reward=-935.20 +/- 26.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=658000, episode_reward=-962.60 +/- 30.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=659000, episode_reward=-932.20 +/- 40.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=660000, episode_reward=-934.40 +/- 32.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=661000, episode_reward=-970.40 +/- 22.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=662000, episode_reward=-924.00 +/- 26.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=663000, episode_reward=-935.20 +/- 18.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=664000, episode_reward=-973.20 +/- 26.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=665000, episode_reward=-926.00 +/- 39.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=666000, episode_reward=-961.20 +/- 32.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=667000, episode_reward=-973.80 +/- 11.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=668000, episode_reward=-939.40 +/- 27.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=669000, episode_reward=-918.40 +/- 22.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=670000, episode_reward=-949.40 +/- 35.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=671000, episode_reward=-915.60 +/- 28.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=672000, episode_reward=-939.60 +/- 41.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=673000, episode_reward=-932.80 +/- 45.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=674000, episode_reward=-944.00 +/- 19.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=675000, episode_reward=-952.60 +/- 43.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=676000, episode_reward=-962.40 +/- 27.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=677000, episode_reward=-926.80 +/- 49.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=678000, episode_reward=-945.20 +/- 29.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=679000, episode_reward=-932.00 +/- 26.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=680000, episode_reward=-930.40 +/- 51.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=681000, episode_reward=-934.20 +/- 98.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=682000, episode_reward=-919.00 +/- 29.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=683000, episode_reward=-921.80 +/- 37.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=684000, episode_reward=-946.20 +/- 43.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=685000, episode_reward=-920.00 +/- 44.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=686000, episode_reward=-917.20 +/- 27.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=687000, episode_reward=-913.80 +/- 21.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=688000, episode_reward=-936.00 +/- 23.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=689000, episode_reward=-912.60 +/- 45.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=690000, episode_reward=-936.00 +/- 38.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=691000, episode_reward=-916.80 +/- 61.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=692000, episode_reward=-919.60 +/- 10.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=693000, episode_reward=-918.20 +/- 11.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=694000, episode_reward=-950.60 +/- 29.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=695000, episode_reward=-921.80 +/- 14.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=696000, episode_reward=-969.00 +/- 18.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=697000, episode_reward=-918.20 +/- 36.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=698000, episode_reward=-918.20 +/- 38.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=699000, episode_reward=-951.60 +/- 38.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=700000, episode_reward=-942.20 +/- 53.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=701000, episode_reward=-967.00 +/- 36.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=702000, episode_reward=-945.80 +/- 39.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=703000, episode_reward=-932.00 +/- 38.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=704000, episode_reward=-935.40 +/- 48.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=705000, episode_reward=-957.20 +/- 47.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=706000, episode_reward=-931.20 +/- 38.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=707000, episode_reward=-973.60 +/- 30.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=708000, episode_reward=-938.80 +/- 31.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=709000, episode_reward=-950.20 +/- 20.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=710000, episode_reward=-944.80 +/- 20.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=711000, episode_reward=-939.00 +/- 22.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=712000, episode_reward=-961.40 +/- 52.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=713000, episode_reward=-935.40 +/- 24.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=714000, episode_reward=-920.00 +/- 53.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=715000, episode_reward=-918.20 +/- 22.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=716000, episode_reward=-912.80 +/- 51.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=717000, episode_reward=-957.20 +/- 43.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=718000, episode_reward=-919.00 +/- 52.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=719000, episode_reward=-940.00 +/- 24.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=720000, episode_reward=-897.60 +/- 43.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=721000, episode_reward=-920.00 +/- 18.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=722000, episode_reward=-952.80 +/- 34.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=723000, episode_reward=-918.40 +/- 39.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=724000, episode_reward=-923.60 +/- 69.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=725000, episode_reward=-937.00 +/- 46.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=726000, episode_reward=-955.40 +/- 20.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=727000, episode_reward=-921.00 +/- 50.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=728000, episode_reward=-925.40 +/- 14.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=729000, episode_reward=-912.40 +/- 47.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=730000, episode_reward=-944.60 +/- 23.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=731000, episode_reward=-920.40 +/- 20.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=732000, episode_reward=-930.60 +/- 12.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=733000, episode_reward=-934.60 +/- 43.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=734000, episode_reward=-879.60 +/- 34.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=735000, episode_reward=-960.20 +/- 57.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=736000, episode_reward=-950.00 +/- 48.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=737000, episode_reward=-922.80 +/- 30.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=738000, episode_reward=-894.80 +/- 45.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=739000, episode_reward=-948.60 +/- 41.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=740000, episode_reward=-942.00 +/- 21.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=741000, episode_reward=-956.60 +/- 34.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=742000, episode_reward=-918.20 +/- 51.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=743000, episode_reward=-950.80 +/- 14.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=744000, episode_reward=-925.40 +/- 13.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=745000, episode_reward=-925.40 +/- 23.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=746000, episode_reward=-936.60 +/- 46.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=747000, episode_reward=-932.20 +/- 30.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=748000, episode_reward=-931.40 +/- 43.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=749000, episode_reward=-938.80 +/- 27.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=750000, episode_reward=-934.40 +/- 36.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=751000, episode_reward=-921.40 +/- 37.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=752000, episode_reward=-933.00 +/- 13.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=753000, episode_reward=-900.60 +/- 43.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=754000, episode_reward=-937.00 +/- 27.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=755000, episode_reward=-936.40 +/- 35.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=756000, episode_reward=-894.20 +/- 28.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=757000, episode_reward=-935.60 +/- 38.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=758000, episode_reward=-924.00 +/- 30.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=759000, episode_reward=-921.80 +/- 22.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=760000, episode_reward=-974.40 +/- 42.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=761000, episode_reward=-927.80 +/- 23.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=762000, episode_reward=-905.00 +/- 16.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=763000, episode_reward=-962.60 +/- 28.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=764000, episode_reward=-939.40 +/- 10.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=765000, episode_reward=-919.00 +/- 29.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=766000, episode_reward=-935.80 +/- 30.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=767000, episode_reward=-891.20 +/- 36.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=768000, episode_reward=-929.80 +/- 46.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=769000, episode_reward=-932.80 +/- 12.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=770000, episode_reward=-948.20 +/- 9.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=771000, episode_reward=-940.00 +/- 19.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=772000, episode_reward=-921.00 +/- 26.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=773000, episode_reward=-916.00 +/- 54.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=774000, episode_reward=-940.40 +/- 28.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=775000, episode_reward=-912.00 +/- 25.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=776000, episode_reward=-911.40 +/- 54.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=777000, episode_reward=-959.20 +/- 42.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=778000, episode_reward=-948.00 +/- 33.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=779000, episode_reward=-910.40 +/- 46.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=780000, episode_reward=-948.80 +/- 67.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=781000, episode_reward=-970.20 +/- 21.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=782000, episode_reward=-931.60 +/- 20.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=783000, episode_reward=-946.00 +/- 27.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=784000, episode_reward=-930.20 +/- 25.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=785000, episode_reward=-946.20 +/- 23.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=786000, episode_reward=-916.00 +/- 35.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=787000, episode_reward=-929.40 +/- 32.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=788000, episode_reward=-916.20 +/- 54.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=789000, episode_reward=-936.20 +/- 14.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=790000, episode_reward=-947.40 +/- 20.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=791000, episode_reward=-908.20 +/- 39.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=792000, episode_reward=-926.40 +/- 36.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=793000, episode_reward=-871.00 +/- 26.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=794000, episode_reward=-913.00 +/- 20.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=795000, episode_reward=-934.60 +/- 59.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=796000, episode_reward=-953.60 +/- 29.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=797000, episode_reward=-939.00 +/- 14.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=798000, episode_reward=-942.00 +/- 27.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=799000, episode_reward=-938.20 +/- 57.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=800000, episode_reward=-928.60 +/- 35.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=801000, episode_reward=-880.80 +/- 34.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=802000, episode_reward=-933.40 +/- 31.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=803000, episode_reward=-924.20 +/- 46.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=804000, episode_reward=-931.60 +/- 37.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=805000, episode_reward=-914.40 +/- 41.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=806000, episode_reward=-933.60 +/- 29.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=807000, episode_reward=-941.80 +/- 42.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=808000, episode_reward=-926.80 +/- 33.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=809000, episode_reward=-904.00 +/- 45.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=810000, episode_reward=-893.40 +/- 33.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=811000, episode_reward=-935.20 +/- 31.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=812000, episode_reward=-927.00 +/- 30.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=813000, episode_reward=-921.80 +/- 37.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=814000, episode_reward=-868.80 +/- 33.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=815000, episode_reward=-933.80 +/- 45.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=816000, episode_reward=-955.00 +/- 23.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=817000, episode_reward=-954.40 +/- 26.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=818000, episode_reward=-899.60 +/- 35.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=819000, episode_reward=-902.00 +/- 27.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=820000, episode_reward=-919.60 +/- 24.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=821000, episode_reward=-927.20 +/- 53.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=822000, episode_reward=-960.60 +/- 48.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=823000, episode_reward=-937.20 +/- 45.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=824000, episode_reward=-935.20 +/- 33.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=825000, episode_reward=-939.60 +/- 21.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=826000, episode_reward=-906.40 +/- 29.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=827000, episode_reward=-902.60 +/- 22.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=828000, episode_reward=-934.20 +/- 24.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=829000, episode_reward=-947.40 +/- 38.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=830000, episode_reward=-937.40 +/- 43.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=831000, episode_reward=-930.60 +/- 20.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=832000, episode_reward=-909.00 +/- 28.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=833000, episode_reward=-945.20 +/- 17.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=834000, episode_reward=-923.20 +/- 59.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=835000, episode_reward=-953.40 +/- 14.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=836000, episode_reward=-951.20 +/- 31.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=837000, episode_reward=-936.40 +/- 57.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=838000, episode_reward=-914.40 +/- 55.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=839000, episode_reward=-939.20 +/- 41.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=840000, episode_reward=-927.80 +/- 54.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=841000, episode_reward=-926.80 +/- 27.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=842000, episode_reward=-926.00 +/- 23.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=843000, episode_reward=-930.20 +/- 20.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=844000, episode_reward=-933.40 +/- 28.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=845000, episode_reward=-908.00 +/- 34.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=846000, episode_reward=-892.00 +/- 20.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=847000, episode_reward=-905.00 +/- 9.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=848000, episode_reward=-921.80 +/- 24.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=849000, episode_reward=-895.00 +/- 24.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=850000, episode_reward=-921.20 +/- 41.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=851000, episode_reward=-914.80 +/- 33.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=852000, episode_reward=-915.60 +/- 38.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=853000, episode_reward=-923.40 +/- 52.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=854000, episode_reward=-956.60 +/- 26.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=855000, episode_reward=-905.60 +/- 17.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=856000, episode_reward=-907.60 +/- 27.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=857000, episode_reward=-951.60 +/- 12.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=858000, episode_reward=-918.20 +/- 28.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=859000, episode_reward=-920.40 +/- 29.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=860000, episode_reward=-936.00 +/- 31.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=861000, episode_reward=-934.80 +/- 31.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=862000, episode_reward=-925.80 +/- 43.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=863000, episode_reward=-923.20 +/- 38.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=864000, episode_reward=-895.20 +/- 41.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=865000, episode_reward=-926.80 +/- 25.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=866000, episode_reward=-950.00 +/- 31.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=867000, episode_reward=-927.60 +/- 18.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=868000, episode_reward=-903.80 +/- 62.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=869000, episode_reward=-920.00 +/- 24.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=870000, episode_reward=-941.80 +/- 29.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=871000, episode_reward=-948.20 +/- 49.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=872000, episode_reward=-914.80 +/- 57.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=873000, episode_reward=-935.00 +/- 37.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=874000, episode_reward=-906.80 +/- 22.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=875000, episode_reward=-911.00 +/- 45.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=876000, episode_reward=-984.80 +/- 24.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=877000, episode_reward=-925.60 +/- 31.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=878000, episode_reward=-923.20 +/- 33.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=879000, episode_reward=-936.80 +/- 28.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=880000, episode_reward=-920.40 +/- 30.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=881000, episode_reward=-953.40 +/- 31.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=882000, episode_reward=-933.60 +/- 35.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=883000, episode_reward=-900.20 +/- 67.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=884000, episode_reward=-911.40 +/- 16.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=885000, episode_reward=-926.40 +/- 28.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=886000, episode_reward=-920.20 +/- 40.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=887000, episode_reward=-932.00 +/- 33.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=888000, episode_reward=-897.20 +/- 41.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=889000, episode_reward=-914.20 +/- 29.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=890000, episode_reward=-924.60 +/- 61.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=891000, episode_reward=-914.40 +/- 14.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=892000, episode_reward=-918.20 +/- 43.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=893000, episode_reward=-922.60 +/- 9.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=894000, episode_reward=-945.60 +/- 28.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=895000, episode_reward=-890.80 +/- 19.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=896000, episode_reward=-928.60 +/- 24.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=897000, episode_reward=-936.80 +/- 36.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=898000, episode_reward=-904.00 +/- 23.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=899000, episode_reward=-903.40 +/- 19.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=900000, episode_reward=-924.20 +/- 37.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=901000, episode_reward=-908.20 +/- 40.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=902000, episode_reward=-931.00 +/- 35.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=903000, episode_reward=-908.20 +/- 41.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=904000, episode_reward=-898.40 +/- 16.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=905000, episode_reward=-918.00 +/- 17.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=906000, episode_reward=-925.80 +/- 32.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=907000, episode_reward=-929.60 +/- 40.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=908000, episode_reward=-914.20 +/- 31.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=909000, episode_reward=-937.40 +/- 32.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=910000, episode_reward=-905.80 +/- 30.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=911000, episode_reward=-910.00 +/- 25.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=912000, episode_reward=-912.40 +/- 20.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=913000, episode_reward=-929.00 +/- 21.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=914000, episode_reward=-935.00 +/- 45.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=915000, episode_reward=-929.20 +/- 26.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=916000, episode_reward=-930.00 +/- 52.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=917000, episode_reward=-951.80 +/- 45.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=918000, episode_reward=-937.80 +/- 28.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=919000, episode_reward=-930.00 +/- 39.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=920000, episode_reward=-924.60 +/- 26.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=921000, episode_reward=-939.80 +/- 29.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=922000, episode_reward=-914.40 +/- 28.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=923000, episode_reward=-902.20 +/- 28.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=924000, episode_reward=-909.00 +/- 50.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=925000, episode_reward=-920.00 +/- 31.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=926000, episode_reward=-941.60 +/- 41.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=927000, episode_reward=-928.60 +/- 32.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=928000, episode_reward=-883.40 +/- 45.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=929000, episode_reward=-898.60 +/- 33.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=930000, episode_reward=-914.80 +/- 15.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=931000, episode_reward=-905.80 +/- 48.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=932000, episode_reward=-892.60 +/- 54.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=933000, episode_reward=-923.80 +/- 36.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=934000, episode_reward=-925.20 +/- 25.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=935000, episode_reward=-899.20 +/- 48.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=936000, episode_reward=-933.40 +/- 46.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=937000, episode_reward=-922.60 +/- 24.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=938000, episode_reward=-905.00 +/- 24.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=939000, episode_reward=-925.40 +/- 57.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=940000, episode_reward=-912.40 +/- 42.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=941000, episode_reward=-909.40 +/- 24.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=942000, episode_reward=-928.80 +/- 14.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=943000, episode_reward=-900.80 +/- 39.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=944000, episode_reward=-957.40 +/- 55.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=945000, episode_reward=-936.40 +/- 31.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=946000, episode_reward=-914.00 +/- 24.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=947000, episode_reward=-909.40 +/- 25.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=948000, episode_reward=-945.60 +/- 39.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=949000, episode_reward=-920.20 +/- 49.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=950000, episode_reward=-907.40 +/- 52.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=951000, episode_reward=-912.00 +/- 19.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=952000, episode_reward=-925.00 +/- 41.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=953000, episode_reward=-948.40 +/- 24.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=954000, episode_reward=-872.20 +/- 23.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=955000, episode_reward=-934.00 +/- 38.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=956000, episode_reward=-878.80 +/- 45.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=957000, episode_reward=-911.40 +/- 19.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=958000, episode_reward=-961.20 +/- 21.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=959000, episode_reward=-890.20 +/- 44.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=960000, episode_reward=-905.80 +/- 24.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=961000, episode_reward=-903.00 +/- 27.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=962000, episode_reward=-923.20 +/- 41.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=963000, episode_reward=-921.40 +/- 40.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=964000, episode_reward=-904.00 +/- 12.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=965000, episode_reward=-960.80 +/- 22.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=966000, episode_reward=-918.20 +/- 36.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=967000, episode_reward=-892.40 +/- 33.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=968000, episode_reward=-915.20 +/- 30.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=969000, episode_reward=-912.40 +/- 25.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=970000, episode_reward=-903.20 +/- 32.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=971000, episode_reward=-898.80 +/- 42.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=972000, episode_reward=-929.40 +/- 47.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=973000, episode_reward=-935.20 +/- 39.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=974000, episode_reward=-926.00 +/- 20.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=975000, episode_reward=-921.80 +/- 48.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=976000, episode_reward=-907.80 +/- 19.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=977000, episode_reward=-903.60 +/- 38.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=978000, episode_reward=-926.60 +/- 32.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=979000, episode_reward=-919.60 +/- 50.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=980000, episode_reward=-926.00 +/- 51.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=981000, episode_reward=-931.20 +/- 22.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=982000, episode_reward=-910.40 +/- 24.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=983000, episode_reward=-934.80 +/- 21.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=984000, episode_reward=-919.40 +/- 56.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=985000, episode_reward=-903.00 +/- 38.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=986000, episode_reward=-879.80 +/- 45.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=987000, episode_reward=-885.80 +/- 24.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=988000, episode_reward=-909.20 +/- 31.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=989000, episode_reward=-911.20 +/- 23.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=990000, episode_reward=-929.00 +/- 34.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=991000, episode_reward=-938.60 +/- 27.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=992000, episode_reward=-896.60 +/- 31.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=993000, episode_reward=-938.60 +/- 53.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=994000, episode_reward=-932.60 +/- 24.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=995000, episode_reward=-916.40 +/- 44.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=996000, episode_reward=-920.20 +/- 29.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=997000, episode_reward=-933.80 +/- 43.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=998000, episode_reward=-945.00 +/- 36.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=999000, episode_reward=-907.80 +/- 20.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1000000, episode_reward=-921.60 +/- 41.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1001000, episode_reward=-920.00 +/- 37.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1002000, episode_reward=-909.80 +/- 48.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1003000, episode_reward=-892.80 +/- 20.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1004000, episode_reward=-904.60 +/- 24.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1005000, episode_reward=-929.40 +/- 33.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1006000, episode_reward=-905.20 +/- 29.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1007000, episode_reward=-925.40 +/- 39.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1008000, episode_reward=-906.60 +/- 41.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1009000, episode_reward=-915.80 +/- 23.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1010000, episode_reward=-938.00 +/- 63.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1011000, episode_reward=-927.40 +/- 55.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1012000, episode_reward=-912.00 +/- 16.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1013000, episode_reward=-905.60 +/- 51.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1014000, episode_reward=-912.00 +/- 63.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1015000, episode_reward=-892.60 +/- 49.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1016000, episode_reward=-901.60 +/- 27.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1017000, episode_reward=-901.80 +/- 39.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1018000, episode_reward=-911.60 +/- 29.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1019000, episode_reward=-931.00 +/- 54.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1020000, episode_reward=-910.80 +/- 22.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1021000, episode_reward=-907.20 +/- 23.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1022000, episode_reward=-930.40 +/- 63.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1023000, episode_reward=-895.80 +/- 54.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1024000, episode_reward=-937.00 +/- 30.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1025000, episode_reward=-905.00 +/- 46.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1026000, episode_reward=-901.00 +/- 42.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1027000, episode_reward=-890.60 +/- 8.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1028000, episode_reward=-925.80 +/- 50.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1029000, episode_reward=-963.40 +/- 34.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1030000, episode_reward=-943.60 +/- 43.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1031000, episode_reward=-909.80 +/- 36.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1032000, episode_reward=-914.80 +/- 23.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1033000, episode_reward=-906.80 +/- 42.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1034000, episode_reward=-933.80 +/- 17.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1035000, episode_reward=-910.20 +/- 35.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1036000, episode_reward=-899.20 +/- 55.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1037000, episode_reward=-949.20 +/- 37.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1038000, episode_reward=-943.40 +/- 45.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1039000, episode_reward=-953.60 +/- 44.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1040000, episode_reward=-899.40 +/- 42.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1041000, episode_reward=-908.80 +/- 10.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1042000, episode_reward=-888.20 +/- 24.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1043000, episode_reward=-910.60 +/- 42.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1044000, episode_reward=-902.60 +/- 22.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1045000, episode_reward=-937.40 +/- 26.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1046000, episode_reward=-929.60 +/- 18.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1047000, episode_reward=-931.00 +/- 23.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1048000, episode_reward=-898.00 +/- 30.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1049000, episode_reward=-892.80 +/- 15.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1050000, episode_reward=-915.60 +/- 23.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1051000, episode_reward=-907.40 +/- 52.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1052000, episode_reward=-903.80 +/- 46.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1053000, episode_reward=-927.00 +/- 36.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1054000, episode_reward=-924.00 +/- 23.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1055000, episode_reward=-902.40 +/- 29.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1056000, episode_reward=-926.40 +/- 27.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1057000, episode_reward=-905.80 +/- 39.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1058000, episode_reward=-912.60 +/- 19.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1059000, episode_reward=-919.40 +/- 34.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1060000, episode_reward=-899.80 +/- 44.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1061000, episode_reward=-913.40 +/- 35.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1062000, episode_reward=-890.20 +/- 12.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1063000, episode_reward=-909.80 +/- 10.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1064000, episode_reward=-919.80 +/- 24.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1065000, episode_reward=-916.60 +/- 30.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1066000, episode_reward=-898.60 +/- 39.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1067000, episode_reward=-926.80 +/- 35.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1068000, episode_reward=-950.20 +/- 42.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1069000, episode_reward=-921.40 +/- 40.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1070000, episode_reward=-924.60 +/- 25.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1071000, episode_reward=-891.80 +/- 29.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1072000, episode_reward=-919.60 +/- 32.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1073000, episode_reward=-927.20 +/- 37.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1074000, episode_reward=-926.40 +/- 42.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1075000, episode_reward=-890.60 +/- 49.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1076000, episode_reward=-902.20 +/- 59.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1077000, episode_reward=-938.00 +/- 24.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1078000, episode_reward=-894.40 +/- 16.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1079000, episode_reward=-902.80 +/- 51.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1080000, episode_reward=-930.60 +/- 56.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1081000, episode_reward=-922.00 +/- 47.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1082000, episode_reward=-909.80 +/- 51.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1083000, episode_reward=-924.80 +/- 40.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1084000, episode_reward=-917.60 +/- 25.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1085000, episode_reward=-936.20 +/- 7.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1086000, episode_reward=-928.00 +/- 62.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1087000, episode_reward=-923.20 +/- 48.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1088000, episode_reward=-912.80 +/- 21.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1089000, episode_reward=-898.40 +/- 33.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1090000, episode_reward=-899.80 +/- 34.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1091000, episode_reward=-904.00 +/- 27.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1092000, episode_reward=-920.20 +/- 17.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1093000, episode_reward=-922.20 +/- 26.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1094000, episode_reward=-916.80 +/- 7.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1095000, episode_reward=-947.00 +/- 18.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1096000, episode_reward=-945.00 +/- 31.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1097000, episode_reward=-895.40 +/- 51.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1098000, episode_reward=-926.60 +/- 50.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1099000, episode_reward=-919.40 +/- 38.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1100000, episode_reward=-916.80 +/- 38.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1101000, episode_reward=-928.80 +/- 27.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1102000, episode_reward=-904.80 +/- 30.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1103000, episode_reward=-913.00 +/- 46.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1104000, episode_reward=-910.40 +/- 12.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1105000, episode_reward=-941.40 +/- 43.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1106000, episode_reward=-930.60 +/- 39.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1107000, episode_reward=-890.40 +/- 52.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1108000, episode_reward=-892.40 +/- 38.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1109000, episode_reward=-917.60 +/- 51.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1110000, episode_reward=-922.20 +/- 31.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1111000, episode_reward=-930.60 +/- 45.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1112000, episode_reward=-921.40 +/- 39.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1113000, episode_reward=-906.40 +/- 41.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1114000, episode_reward=-899.80 +/- 24.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1115000, episode_reward=-897.80 +/- 24.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1116000, episode_reward=-899.20 +/- 39.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1117000, episode_reward=-934.00 +/- 26.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1118000, episode_reward=-939.40 +/- 32.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1119000, episode_reward=-907.00 +/- 38.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1120000, episode_reward=-924.20 +/- 47.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1121000, episode_reward=-914.20 +/- 55.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1122000, episode_reward=-902.20 +/- 47.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1123000, episode_reward=-926.40 +/- 42.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1124000, episode_reward=-917.80 +/- 32.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1125000, episode_reward=-896.80 +/- 33.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1126000, episode_reward=-933.40 +/- 29.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1127000, episode_reward=-914.20 +/- 14.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1128000, episode_reward=-907.20 +/- 36.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1129000, episode_reward=-915.20 +/- 30.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1130000, episode_reward=-891.80 +/- 31.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1131000, episode_reward=-905.80 +/- 27.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1132000, episode_reward=-902.80 +/- 21.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1133000, episode_reward=-893.00 +/- 28.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1134000, episode_reward=-930.40 +/- 36.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1135000, episode_reward=-899.00 +/- 16.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1136000, episode_reward=-885.40 +/- 37.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1137000, episode_reward=-901.40 +/- 42.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1138000, episode_reward=-901.00 +/- 26.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1139000, episode_reward=-912.80 +/- 66.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1140000, episode_reward=-947.40 +/- 28.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1141000, episode_reward=-907.40 +/- 11.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1142000, episode_reward=-926.60 +/- 23.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1143000, episode_reward=-918.40 +/- 23.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1144000, episode_reward=-900.20 +/- 39.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1145000, episode_reward=-957.00 +/- 47.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1146000, episode_reward=-905.80 +/- 31.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1147000, episode_reward=-913.80 +/- 34.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1148000, episode_reward=-907.00 +/- 57.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1149000, episode_reward=-950.20 +/- 51.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1150000, episode_reward=-918.20 +/- 54.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1151000, episode_reward=-907.00 +/- 39.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1152000, episode_reward=-888.00 +/- 27.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1153000, episode_reward=-901.40 +/- 28.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1154000, episode_reward=-906.20 +/- 34.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1155000, episode_reward=-925.60 +/- 20.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1156000, episode_reward=-894.20 +/- 10.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1157000, episode_reward=-909.20 +/- 17.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1158000, episode_reward=-917.00 +/- 25.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1159000, episode_reward=-932.80 +/- 20.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1160000, episode_reward=-911.80 +/- 37.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1161000, episode_reward=-887.00 +/- 54.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1162000, episode_reward=-899.80 +/- 12.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1163000, episode_reward=-916.20 +/- 46.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1164000, episode_reward=-869.80 +/- 61.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1165000, episode_reward=-887.40 +/- 47.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1166000, episode_reward=-926.60 +/- 41.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1167000, episode_reward=-904.80 +/- 11.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1168000, episode_reward=-911.00 +/- 43.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1169000, episode_reward=-917.40 +/- 56.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1170000, episode_reward=-911.80 +/- 29.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1171000, episode_reward=-915.20 +/- 25.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1172000, episode_reward=-937.80 +/- 31.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1173000, episode_reward=-953.40 +/- 21.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1174000, episode_reward=-907.60 +/- 27.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1175000, episode_reward=-940.60 +/- 36.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1176000, episode_reward=-894.20 +/- 26.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1177000, episode_reward=-934.80 +/- 48.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1178000, episode_reward=-915.00 +/- 24.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1179000, episode_reward=-891.20 +/- 20.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1180000, episode_reward=-907.60 +/- 23.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1181000, episode_reward=-886.20 +/- 55.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1182000, episode_reward=-910.20 +/- 30.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1183000, episode_reward=-907.60 +/- 24.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1184000, episode_reward=-914.40 +/- 18.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1185000, episode_reward=-891.40 +/- 21.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1186000, episode_reward=-917.40 +/- 33.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1187000, episode_reward=-898.20 +/- 21.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1188000, episode_reward=-892.20 +/- 27.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1189000, episode_reward=-897.40 +/- 27.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1190000, episode_reward=-874.20 +/- 25.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1191000, episode_reward=-896.40 +/- 45.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1192000, episode_reward=-906.40 +/- 44.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1193000, episode_reward=-901.80 +/- 61.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1194000, episode_reward=-876.60 +/- 26.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1195000, episode_reward=-931.00 +/- 56.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1196000, episode_reward=-913.20 +/- 32.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1197000, episode_reward=-887.00 +/- 55.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1198000, episode_reward=-911.40 +/- 45.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1199000, episode_reward=-891.40 +/- 31.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1200000, episode_reward=-898.80 +/- 32.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1201000, episode_reward=-913.80 +/- 45.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1202000, episode_reward=-900.60 +/- 28.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1203000, episode_reward=-918.20 +/- 44.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1204000, episode_reward=-918.80 +/- 35.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1205000, episode_reward=-927.40 +/- 36.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1206000, episode_reward=-916.20 +/- 47.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1207000, episode_reward=-878.60 +/- 10.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1208000, episode_reward=-936.60 +/- 18.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1209000, episode_reward=-907.40 +/- 33.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1210000, episode_reward=-901.00 +/- 29.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1211000, episode_reward=-940.80 +/- 58.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1212000, episode_reward=-876.00 +/- 9.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1213000, episode_reward=-929.00 +/- 62.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1214000, episode_reward=-933.20 +/- 39.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1215000, episode_reward=-908.80 +/- 30.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1216000, episode_reward=-877.40 +/- 47.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1217000, episode_reward=-914.80 +/- 40.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1218000, episode_reward=-939.80 +/- 14.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1219000, episode_reward=-917.20 +/- 39.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1220000, episode_reward=-920.40 +/- 26.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1221000, episode_reward=-923.40 +/- 26.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1222000, episode_reward=-907.20 +/- 26.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1223000, episode_reward=-907.60 +/- 13.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1224000, episode_reward=-900.60 +/- 19.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1225000, episode_reward=-889.80 +/- 57.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1226000, episode_reward=-897.20 +/- 58.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1227000, episode_reward=-892.00 +/- 27.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1228000, episode_reward=-905.00 +/- 17.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1229000, episode_reward=-940.00 +/- 46.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1230000, episode_reward=-904.40 +/- 22.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1231000, episode_reward=-917.20 +/- 7.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1232000, episode_reward=-944.00 +/- 30.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1233000, episode_reward=-922.20 +/- 12.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1234000, episode_reward=-886.20 +/- 30.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1235000, episode_reward=-929.40 +/- 18.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1236000, episode_reward=-902.60 +/- 40.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1237000, episode_reward=-925.20 +/- 28.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1238000, episode_reward=-900.40 +/- 19.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1239000, episode_reward=-920.80 +/- 36.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1240000, episode_reward=-926.60 +/- 34.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1241000, episode_reward=-951.40 +/- 15.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1242000, episode_reward=-898.40 +/- 22.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1243000, episode_reward=-894.80 +/- 16.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1244000, episode_reward=-883.80 +/- 17.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1245000, episode_reward=-902.00 +/- 24.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1246000, episode_reward=-891.80 +/- 37.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1247000, episode_reward=-920.00 +/- 25.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1248000, episode_reward=-919.40 +/- 23.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1249000, episode_reward=-935.00 +/- 31.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1250000, episode_reward=-897.20 +/- 21.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1251000, episode_reward=-875.80 +/- 37.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1252000, episode_reward=-913.40 +/- 43.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1253000, episode_reward=-947.00 +/- 12.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1254000, episode_reward=-913.40 +/- 61.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1255000, episode_reward=-913.00 +/- 39.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1256000, episode_reward=-910.40 +/- 47.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1257000, episode_reward=-905.00 +/- 23.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1258000, episode_reward=-890.80 +/- 27.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1259000, episode_reward=-924.00 +/- 10.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1260000, episode_reward=-908.00 +/- 74.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1261000, episode_reward=-917.40 +/- 29.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1262000, episode_reward=-945.80 +/- 27.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1263000, episode_reward=-893.20 +/- 35.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1264000, episode_reward=-882.80 +/- 33.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1265000, episode_reward=-937.20 +/- 44.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1266000, episode_reward=-892.60 +/- 44.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1267000, episode_reward=-896.60 +/- 4.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1268000, episode_reward=-882.20 +/- 28.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1269000, episode_reward=-878.60 +/- 11.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1270000, episode_reward=-910.40 +/- 36.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1271000, episode_reward=-926.20 +/- 44.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1272000, episode_reward=-922.20 +/- 17.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1273000, episode_reward=-910.40 +/- 38.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1274000, episode_reward=-908.00 +/- 38.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1275000, episode_reward=-916.60 +/- 28.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1276000, episode_reward=-938.20 +/- 28.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1277000, episode_reward=-911.60 +/- 21.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1278000, episode_reward=-898.80 +/- 21.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1279000, episode_reward=-898.80 +/- 18.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1280000, episode_reward=-892.60 +/- 48.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1281000, episode_reward=-880.40 +/- 40.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1282000, episode_reward=-871.40 +/- 17.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1283000, episode_reward=-892.40 +/- 33.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1284000, episode_reward=-912.20 +/- 49.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1285000, episode_reward=-920.40 +/- 63.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1286000, episode_reward=-879.80 +/- 31.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1287000, episode_reward=-932.80 +/- 41.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1288000, episode_reward=-930.40 +/- 40.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1289000, episode_reward=-920.40 +/- 20.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1290000, episode_reward=-917.60 +/- 31.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1291000, episode_reward=-931.20 +/- 30.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1292000, episode_reward=-946.60 +/- 63.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1293000, episode_reward=-887.60 +/- 59.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1294000, episode_reward=-959.80 +/- 11.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1295000, episode_reward=-896.80 +/- 44.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1296000, episode_reward=-927.60 +/- 24.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1297000, episode_reward=-907.00 +/- 25.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1298000, episode_reward=-913.80 +/- 25.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1299000, episode_reward=-921.80 +/- 34.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1300000, episode_reward=-897.20 +/- 30.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1301000, episode_reward=-950.60 +/- 44.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1302000, episode_reward=-910.60 +/- 42.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1303000, episode_reward=-936.00 +/- 33.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1304000, episode_reward=-911.60 +/- 42.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1305000, episode_reward=-878.20 +/- 49.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1306000, episode_reward=-930.40 +/- 17.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1307000, episode_reward=-922.40 +/- 34.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1308000, episode_reward=-894.00 +/- 17.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1309000, episode_reward=-887.40 +/- 31.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1310000, episode_reward=-918.80 +/- 20.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1311000, episode_reward=-907.40 +/- 35.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1312000, episode_reward=-925.00 +/- 33.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1313000, episode_reward=-887.60 +/- 18.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1314000, episode_reward=-933.40 +/- 40.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1315000, episode_reward=-915.20 +/- 33.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1316000, episode_reward=-886.00 +/- 25.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1317000, episode_reward=-906.00 +/- 37.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1318000, episode_reward=-902.60 +/- 21.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1319000, episode_reward=-888.40 +/- 56.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1320000, episode_reward=-915.00 +/- 69.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1321000, episode_reward=-935.00 +/- 35.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1322000, episode_reward=-880.80 +/- 32.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1323000, episode_reward=-890.60 +/- 51.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1324000, episode_reward=-903.00 +/- 46.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1325000, episode_reward=-876.00 +/- 42.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1326000, episode_reward=-920.80 +/- 35.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1327000, episode_reward=-921.20 +/- 12.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1328000, episode_reward=-902.40 +/- 49.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1329000, episode_reward=-927.00 +/- 30.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1330000, episode_reward=-878.60 +/- 46.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1331000, episode_reward=-891.60 +/- 34.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1332000, episode_reward=-926.60 +/- 49.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1333000, episode_reward=-903.60 +/- 28.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1334000, episode_reward=-910.80 +/- 36.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1335000, episode_reward=-911.00 +/- 55.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1336000, episode_reward=-927.80 +/- 23.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1337000, episode_reward=-908.60 +/- 47.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1338000, episode_reward=-924.80 +/- 42.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1339000, episode_reward=-903.80 +/- 46.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1340000, episode_reward=-912.00 +/- 26.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1341000, episode_reward=-907.00 +/- 12.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1342000, episode_reward=-897.40 +/- 32.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1343000, episode_reward=-883.80 +/- 42.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1344000, episode_reward=-917.40 +/- 25.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1345000, episode_reward=-909.60 +/- 38.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1346000, episode_reward=-908.40 +/- 29.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1347000, episode_reward=-880.80 +/- 22.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1348000, episode_reward=-914.60 +/- 17.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1349000, episode_reward=-906.60 +/- 47.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1350000, episode_reward=-896.80 +/- 46.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1351000, episode_reward=-956.00 +/- 44.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1352000, episode_reward=-880.20 +/- 28.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1353000, episode_reward=-928.80 +/- 21.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1354000, episode_reward=-898.00 +/- 76.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1355000, episode_reward=-927.00 +/- 13.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1356000, episode_reward=-920.00 +/- 28.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1357000, episode_reward=-926.20 +/- 37.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1358000, episode_reward=-911.00 +/- 29.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1359000, episode_reward=-893.60 +/- 34.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1360000, episode_reward=-914.00 +/- 36.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1361000, episode_reward=-883.20 +/- 27.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1362000, episode_reward=-912.00 +/- 32.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1363000, episode_reward=-924.80 +/- 26.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1364000, episode_reward=-912.20 +/- 58.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1365000, episode_reward=-921.20 +/- 42.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1366000, episode_reward=-910.00 +/- 38.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1367000, episode_reward=-910.40 +/- 26.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1368000, episode_reward=-900.80 +/- 34.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1369000, episode_reward=-920.60 +/- 27.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1370000, episode_reward=-930.60 +/- 40.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1371000, episode_reward=-933.20 +/- 40.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1372000, episode_reward=-895.20 +/- 30.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1373000, episode_reward=-907.80 +/- 22.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1374000, episode_reward=-895.20 +/- 53.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1375000, episode_reward=-903.20 +/- 32.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1376000, episode_reward=-919.60 +/- 19.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1377000, episode_reward=-949.80 +/- 32.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1378000, episode_reward=-930.20 +/- 39.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1379000, episode_reward=-919.60 +/- 20.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1380000, episode_reward=-929.00 +/- 38.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1381000, episode_reward=-927.40 +/- 38.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1382000, episode_reward=-915.60 +/- 31.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1383000, episode_reward=-894.00 +/- 30.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1384000, episode_reward=-900.40 +/- 33.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1385000, episode_reward=-910.20 +/- 25.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1386000, episode_reward=-917.40 +/- 47.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1387000, episode_reward=-910.80 +/- 33.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1388000, episode_reward=-923.60 +/- 18.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1389000, episode_reward=-929.00 +/- 33.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1390000, episode_reward=-866.60 +/- 21.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1391000, episode_reward=-913.20 +/- 29.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1392000, episode_reward=-920.60 +/- 38.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1393000, episode_reward=-923.00 +/- 59.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1394000, episode_reward=-908.00 +/- 36.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1395000, episode_reward=-899.20 +/- 28.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1396000, episode_reward=-907.80 +/- 33.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1397000, episode_reward=-899.80 +/- 25.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1398000, episode_reward=-909.00 +/- 52.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1399000, episode_reward=-924.60 +/- 14.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1400000, episode_reward=-908.20 +/- 30.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1401000, episode_reward=-910.20 +/- 45.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1402000, episode_reward=-929.00 +/- 61.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1403000, episode_reward=-914.00 +/- 23.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1404000, episode_reward=-880.80 +/- 46.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1405000, episode_reward=-908.20 +/- 56.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1406000, episode_reward=-903.80 +/- 43.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1407000, episode_reward=-888.00 +/- 29.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1408000, episode_reward=-905.80 +/- 30.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1409000, episode_reward=-897.20 +/- 20.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1410000, episode_reward=-933.60 +/- 61.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1411000, episode_reward=-930.80 +/- 23.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1412000, episode_reward=-897.00 +/- 25.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1413000, episode_reward=-918.60 +/- 15.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1414000, episode_reward=-922.60 +/- 16.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1415000, episode_reward=-937.00 +/- 25.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1416000, episode_reward=-901.00 +/- 63.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1417000, episode_reward=-904.40 +/- 26.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1418000, episode_reward=-920.80 +/- 38.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1419000, episode_reward=-933.80 +/- 33.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1420000, episode_reward=-891.00 +/- 27.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1421000, episode_reward=-916.80 +/- 38.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1422000, episode_reward=-872.60 +/- 21.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1423000, episode_reward=-918.80 +/- 35.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1424000, episode_reward=-920.80 +/- 35.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1425000, episode_reward=-897.20 +/- 53.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1426000, episode_reward=-897.20 +/- 34.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1427000, episode_reward=-902.80 +/- 25.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1428000, episode_reward=-916.80 +/- 42.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1429000, episode_reward=-903.80 +/- 32.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1430000, episode_reward=-888.20 +/- 40.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1431000, episode_reward=-920.20 +/- 48.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1432000, episode_reward=-913.00 +/- 34.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1433000, episode_reward=-892.80 +/- 28.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1434000, episode_reward=-911.20 +/- 34.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1435000, episode_reward=-921.60 +/- 22.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1436000, episode_reward=-904.80 +/- 31.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1437000, episode_reward=-892.60 +/- 50.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1438000, episode_reward=-917.60 +/- 23.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1439000, episode_reward=-890.00 +/- 31.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1440000, episode_reward=-910.80 +/- 39.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1441000, episode_reward=-911.40 +/- 43.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1442000, episode_reward=-866.60 +/- 45.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1443000, episode_reward=-949.20 +/- 34.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1444000, episode_reward=-883.80 +/- 28.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1445000, episode_reward=-913.20 +/- 14.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1446000, episode_reward=-896.80 +/- 39.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1447000, episode_reward=-937.20 +/- 54.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1448000, episode_reward=-912.00 +/- 30.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1449000, episode_reward=-924.80 +/- 25.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1450000, episode_reward=-908.60 +/- 35.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1451000, episode_reward=-896.60 +/- 33.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1452000, episode_reward=-917.40 +/- 18.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1453000, episode_reward=-901.20 +/- 43.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1454000, episode_reward=-871.20 +/- 20.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1455000, episode_reward=-940.00 +/- 21.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1456000, episode_reward=-911.40 +/- 31.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1457000, episode_reward=-924.00 +/- 22.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1458000, episode_reward=-915.40 +/- 33.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1459000, episode_reward=-936.80 +/- 37.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1460000, episode_reward=-899.00 +/- 10.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1461000, episode_reward=-906.80 +/- 22.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1462000, episode_reward=-894.20 +/- 7.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1463000, episode_reward=-925.60 +/- 14.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1464000, episode_reward=-887.80 +/- 35.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1465000, episode_reward=-919.00 +/- 17.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1466000, episode_reward=-905.80 +/- 30.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1467000, episode_reward=-919.40 +/- 44.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1468000, episode_reward=-889.20 +/- 37.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1469000, episode_reward=-900.20 +/- 16.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1470000, episode_reward=-913.80 +/- 25.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1471000, episode_reward=-924.80 +/- 23.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1472000, episode_reward=-926.60 +/- 28.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1473000, episode_reward=-918.00 +/- 35.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1474000, episode_reward=-922.40 +/- 68.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1475000, episode_reward=-871.80 +/- 34.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1476000, episode_reward=-926.00 +/- 24.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1477000, episode_reward=-922.20 +/- 18.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1478000, episode_reward=-906.80 +/- 41.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1479000, episode_reward=-896.40 +/- 7.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1480000, episode_reward=-921.20 +/- 51.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1481000, episode_reward=-937.80 +/- 43.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1482000, episode_reward=-938.40 +/- 29.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1483000, episode_reward=-907.80 +/- 58.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1484000, episode_reward=-916.60 +/- 27.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1485000, episode_reward=-903.80 +/- 48.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1486000, episode_reward=-925.60 +/- 17.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1487000, episode_reward=-882.40 +/- 47.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1488000, episode_reward=-921.00 +/- 20.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1489000, episode_reward=-927.00 +/- 55.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1490000, episode_reward=-875.80 +/- 48.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1491000, episode_reward=-903.80 +/- 47.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1492000, episode_reward=-910.20 +/- 18.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1493000, episode_reward=-895.80 +/- 31.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1494000, episode_reward=-903.60 +/- 39.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1495000, episode_reward=-898.20 +/- 44.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1496000, episode_reward=-920.40 +/- 21.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1497000, episode_reward=-917.80 +/- 66.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1498000, episode_reward=-889.60 +/- 47.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1499000, episode_reward=-888.60 +/- 52.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1500000, episode_reward=-890.60 +/- 29.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1501000, episode_reward=-899.80 +/- 23.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1502000, episode_reward=-914.60 +/- 38.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1503000, episode_reward=-882.40 +/- 53.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1504000, episode_reward=-909.80 +/- 29.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1505000, episode_reward=-893.60 +/- 27.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1506000, episode_reward=-916.80 +/- 21.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1507000, episode_reward=-915.80 +/- 44.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1508000, episode_reward=-891.40 +/- 28.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1509000, episode_reward=-937.40 +/- 36.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1510000, episode_reward=-882.40 +/- 51.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1511000, episode_reward=-881.80 +/- 57.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1512000, episode_reward=-904.40 +/- 37.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1513000, episode_reward=-895.40 +/- 14.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1514000, episode_reward=-890.80 +/- 37.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1515000, episode_reward=-945.20 +/- 27.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1516000, episode_reward=-913.20 +/- 26.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1517000, episode_reward=-946.20 +/- 20.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1518000, episode_reward=-893.60 +/- 30.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1519000, episode_reward=-892.00 +/- 32.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1520000, episode_reward=-901.20 +/- 14.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1521000, episode_reward=-899.40 +/- 30.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1522000, episode_reward=-914.20 +/- 39.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1523000, episode_reward=-939.80 +/- 38.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1524000, episode_reward=-896.80 +/- 24.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1525000, episode_reward=-885.80 +/- 16.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1526000, episode_reward=-937.80 +/- 33.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1527000, episode_reward=-913.00 +/- 24.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1528000, episode_reward=-900.80 +/- 21.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1529000, episode_reward=-873.20 +/- 8.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1530000, episode_reward=-924.20 +/- 62.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1531000, episode_reward=-909.60 +/- 26.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1532000, episode_reward=-923.00 +/- 44.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1533000, episode_reward=-869.60 +/- 14.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1534000, episode_reward=-918.80 +/- 51.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1535000, episode_reward=-893.40 +/- 25.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1536000, episode_reward=-887.00 +/- 22.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1537000, episode_reward=-923.60 +/- 30.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1538000, episode_reward=-929.00 +/- 33.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1539000, episode_reward=-915.80 +/- 16.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1540000, episode_reward=-907.40 +/- 25.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1541000, episode_reward=-927.60 +/- 41.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1542000, episode_reward=-923.80 +/- 41.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1543000, episode_reward=-881.00 +/- 41.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1544000, episode_reward=-891.40 +/- 44.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1545000, episode_reward=-916.40 +/- 31.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1546000, episode_reward=-915.20 +/- 50.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1547000, episode_reward=-919.20 +/- 30.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1548000, episode_reward=-875.80 +/- 53.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1549000, episode_reward=-923.60 +/- 30.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1550000, episode_reward=-909.00 +/- 32.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1551000, episode_reward=-899.80 +/- 35.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1552000, episode_reward=-920.60 +/- 28.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1553000, episode_reward=-961.80 +/- 35.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1554000, episode_reward=-924.60 +/- 48.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1555000, episode_reward=-929.40 +/- 29.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1556000, episode_reward=-897.60 +/- 30.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1557000, episode_reward=-893.80 +/- 22.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1558000, episode_reward=-901.00 +/- 22.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1559000, episode_reward=-901.00 +/- 22.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1560000, episode_reward=-888.40 +/- 38.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1561000, episode_reward=-898.20 +/- 45.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1562000, episode_reward=-897.40 +/- 40.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1563000, episode_reward=-904.20 +/- 44.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1564000, episode_reward=-941.60 +/- 34.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1565000, episode_reward=-917.80 +/- 34.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1566000, episode_reward=-911.60 +/- 20.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1567000, episode_reward=-911.80 +/- 16.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1568000, episode_reward=-898.20 +/- 29.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1569000, episode_reward=-909.20 +/- 34.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1570000, episode_reward=-898.20 +/- 44.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1571000, episode_reward=-929.80 +/- 12.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1572000, episode_reward=-918.80 +/- 31.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1573000, episode_reward=-933.00 +/- 36.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1574000, episode_reward=-884.60 +/- 35.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1575000, episode_reward=-931.20 +/- 31.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1576000, episode_reward=-896.80 +/- 20.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1577000, episode_reward=-897.40 +/- 37.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1578000, episode_reward=-907.00 +/- 55.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1579000, episode_reward=-911.20 +/- 11.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1580000, episode_reward=-910.00 +/- 24.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1581000, episode_reward=-914.20 +/- 15.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1582000, episode_reward=-854.20 +/- 23.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1583000, episode_reward=-897.00 +/- 33.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1584000, episode_reward=-948.80 +/- 30.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1585000, episode_reward=-907.40 +/- 30.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1586000, episode_reward=-905.00 +/- 47.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1587000, episode_reward=-909.80 +/- 50.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1588000, episode_reward=-914.00 +/- 32.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1589000, episode_reward=-897.20 +/- 22.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1590000, episode_reward=-914.80 +/- 47.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1591000, episode_reward=-944.20 +/- 25.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1592000, episode_reward=-923.80 +/- 14.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1593000, episode_reward=-904.60 +/- 60.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1594000, episode_reward=-930.80 +/- 57.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1595000, episode_reward=-912.40 +/- 33.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1596000, episode_reward=-889.20 +/- 53.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1597000, episode_reward=-926.40 +/- 37.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1598000, episode_reward=-904.60 +/- 29.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1599000, episode_reward=-869.20 +/- 35.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1600000, episode_reward=-871.00 +/- 44.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1601000, episode_reward=-907.20 +/- 30.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1602000, episode_reward=-880.80 +/- 25.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1603000, episode_reward=-891.80 +/- 45.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1604000, episode_reward=-896.80 +/- 39.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1605000, episode_reward=-913.60 +/- 36.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1606000, episode_reward=-940.00 +/- 17.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1607000, episode_reward=-888.80 +/- 18.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1608000, episode_reward=-935.60 +/- 44.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1609000, episode_reward=-921.60 +/- 12.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1610000, episode_reward=-931.20 +/- 34.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1611000, episode_reward=-909.60 +/- 13.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1612000, episode_reward=-899.60 +/- 22.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1613000, episode_reward=-899.40 +/- 26.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1614000, episode_reward=-906.20 +/- 35.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1615000, episode_reward=-929.40 +/- 41.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1616000, episode_reward=-902.20 +/- 50.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1617000, episode_reward=-906.00 +/- 34.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1618000, episode_reward=-934.60 +/- 32.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1619000, episode_reward=-918.80 +/- 29.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1620000, episode_reward=-901.00 +/- 38.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1621000, episode_reward=-928.00 +/- 42.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1622000, episode_reward=-895.60 +/- 21.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1623000, episode_reward=-910.40 +/- 52.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1624000, episode_reward=-921.40 +/- 44.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1625000, episode_reward=-910.60 +/- 43.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1626000, episode_reward=-895.80 +/- 21.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1627000, episode_reward=-898.60 +/- 54.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1628000, episode_reward=-868.00 +/- 27.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1629000, episode_reward=-914.20 +/- 30.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1630000, episode_reward=-896.40 +/- 35.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1631000, episode_reward=-892.00 +/- 64.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1632000, episode_reward=-913.20 +/- 42.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1633000, episode_reward=-882.40 +/- 36.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1634000, episode_reward=-905.20 +/- 34.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1635000, episode_reward=-899.60 +/- 32.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1636000, episode_reward=-894.40 +/- 23.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1637000, episode_reward=-879.80 +/- 30.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1638000, episode_reward=-894.40 +/- 51.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1639000, episode_reward=-879.40 +/- 45.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1640000, episode_reward=-867.60 +/- 38.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1641000, episode_reward=-881.40 +/- 20.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1642000, episode_reward=-920.20 +/- 36.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1643000, episode_reward=-907.60 +/- 36.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1644000, episode_reward=-877.00 +/- 37.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1645000, episode_reward=-932.20 +/- 20.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1646000, episode_reward=-875.20 +/- 57.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1647000, episode_reward=-913.60 +/- 44.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1648000, episode_reward=-899.40 +/- 27.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1649000, episode_reward=-897.60 +/- 18.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1650000, episode_reward=-911.20 +/- 39.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1651000, episode_reward=-887.80 +/- 22.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1652000, episode_reward=-911.40 +/- 24.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1653000, episode_reward=-907.40 +/- 36.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1654000, episode_reward=-916.40 +/- 34.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1655000, episode_reward=-896.60 +/- 43.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1656000, episode_reward=-924.00 +/- 31.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1657000, episode_reward=-874.40 +/- 40.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1658000, episode_reward=-925.80 +/- 51.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1659000, episode_reward=-901.40 +/- 35.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1660000, episode_reward=-913.40 +/- 24.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1661000, episode_reward=-899.40 +/- 17.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1662000, episode_reward=-920.60 +/- 42.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1663000, episode_reward=-896.00 +/- 31.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1664000, episode_reward=-899.60 +/- 43.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1665000, episode_reward=-908.00 +/- 33.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1666000, episode_reward=-886.80 +/- 31.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1667000, episode_reward=-934.60 +/- 30.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1668000, episode_reward=-915.20 +/- 35.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1669000, episode_reward=-922.80 +/- 12.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1670000, episode_reward=-919.00 +/- 34.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1671000, episode_reward=-918.40 +/- 26.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1672000, episode_reward=-899.40 +/- 13.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1673000, episode_reward=-887.40 +/- 16.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1674000, episode_reward=-938.40 +/- 29.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1675000, episode_reward=-922.80 +/- 52.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1676000, episode_reward=-886.60 +/- 23.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1677000, episode_reward=-882.00 +/- 36.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1678000, episode_reward=-907.00 +/- 18.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1679000, episode_reward=-897.80 +/- 22.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1680000, episode_reward=-923.40 +/- 46.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1681000, episode_reward=-907.40 +/- 51.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1682000, episode_reward=-890.80 +/- 38.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1683000, episode_reward=-908.60 +/- 47.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1684000, episode_reward=-917.60 +/- 40.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1685000, episode_reward=-892.20 +/- 57.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1686000, episode_reward=-917.20 +/- 24.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1687000, episode_reward=-904.40 +/- 46.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1688000, episode_reward=-914.80 +/- 24.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1689000, episode_reward=-887.60 +/- 26.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1690000, episode_reward=-873.80 +/- 53.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1691000, episode_reward=-936.40 +/- 18.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1692000, episode_reward=-907.00 +/- 16.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1693000, episode_reward=-894.80 +/- 30.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1694000, episode_reward=-881.60 +/- 40.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1695000, episode_reward=-914.00 +/- 33.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1696000, episode_reward=-901.80 +/- 38.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1697000, episode_reward=-918.20 +/- 61.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1698000, episode_reward=-886.00 +/- 25.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1699000, episode_reward=-890.20 +/- 21.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1700000, episode_reward=-909.40 +/- 35.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1701000, episode_reward=-898.20 +/- 41.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1702000, episode_reward=-907.60 +/- 29.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1703000, episode_reward=-896.00 +/- 33.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1704000, episode_reward=-883.80 +/- 29.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1705000, episode_reward=-914.60 +/- 28.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1706000, episode_reward=-919.80 +/- 33.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1707000, episode_reward=-942.80 +/- 54.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1708000, episode_reward=-888.40 +/- 44.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1709000, episode_reward=-928.20 +/- 26.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1710000, episode_reward=-879.20 +/- 63.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1711000, episode_reward=-917.40 +/- 35.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1712000, episode_reward=-908.60 +/- 33.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1713000, episode_reward=-887.80 +/- 27.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1714000, episode_reward=-903.00 +/- 33.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1715000, episode_reward=-927.20 +/- 37.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1716000, episode_reward=-947.60 +/- 20.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1717000, episode_reward=-930.00 +/- 47.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1718000, episode_reward=-914.80 +/- 32.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1719000, episode_reward=-933.00 +/- 28.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1720000, episode_reward=-920.60 +/- 49.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1721000, episode_reward=-925.00 +/- 24.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1722000, episode_reward=-914.20 +/- 37.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1723000, episode_reward=-917.00 +/- 8.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1724000, episode_reward=-918.00 +/- 28.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1725000, episode_reward=-912.60 +/- 39.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1726000, episode_reward=-904.00 +/- 16.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1727000, episode_reward=-920.20 +/- 40.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1728000, episode_reward=-896.40 +/- 58.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1729000, episode_reward=-923.40 +/- 29.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1730000, episode_reward=-879.20 +/- 33.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1731000, episode_reward=-902.80 +/- 50.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1732000, episode_reward=-910.20 +/- 24.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1733000, episode_reward=-915.40 +/- 26.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1734000, episode_reward=-890.80 +/- 44.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1735000, episode_reward=-932.20 +/- 41.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1736000, episode_reward=-918.60 +/- 54.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1737000, episode_reward=-915.00 +/- 30.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1738000, episode_reward=-918.80 +/- 34.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1739000, episode_reward=-909.40 +/- 37.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1740000, episode_reward=-885.20 +/- 37.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1741000, episode_reward=-923.60 +/- 29.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1742000, episode_reward=-908.40 +/- 40.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1743000, episode_reward=-896.00 +/- 33.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1744000, episode_reward=-902.80 +/- 32.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1745000, episode_reward=-928.40 +/- 27.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1746000, episode_reward=-912.00 +/- 37.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1747000, episode_reward=-885.60 +/- 35.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1748000, episode_reward=-914.00 +/- 27.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1749000, episode_reward=-891.20 +/- 33.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1750000, episode_reward=-886.20 +/- 36.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1751000, episode_reward=-906.20 +/- 20.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1752000, episode_reward=-941.00 +/- 28.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1753000, episode_reward=-902.00 +/- 38.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1754000, episode_reward=-904.60 +/- 33.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1755000, episode_reward=-907.40 +/- 16.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1756000, episode_reward=-942.00 +/- 22.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1757000, episode_reward=-916.60 +/- 38.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1758000, episode_reward=-961.60 +/- 58.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1759000, episode_reward=-918.00 +/- 16.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1760000, episode_reward=-903.60 +/- 18.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1761000, episode_reward=-900.00 +/- 37.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1762000, episode_reward=-907.80 +/- 37.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1763000, episode_reward=-880.20 +/- 62.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1764000, episode_reward=-907.00 +/- 7.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1765000, episode_reward=-904.60 +/- 33.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1766000, episode_reward=-901.00 +/- 24.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1767000, episode_reward=-872.80 +/- 22.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1768000, episode_reward=-920.00 +/- 26.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1769000, episode_reward=-910.60 +/- 35.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1770000, episode_reward=-920.80 +/- 16.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1771000, episode_reward=-913.00 +/- 39.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1772000, episode_reward=-912.60 +/- 41.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1773000, episode_reward=-910.60 +/- 21.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1774000, episode_reward=-896.60 +/- 31.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1775000, episode_reward=-923.40 +/- 43.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1776000, episode_reward=-919.20 +/- 34.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1777000, episode_reward=-935.20 +/- 22.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1778000, episode_reward=-880.20 +/- 41.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1779000, episode_reward=-916.00 +/- 38.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1780000, episode_reward=-916.40 +/- 28.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1781000, episode_reward=-894.60 +/- 16.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1782000, episode_reward=-905.60 +/- 32.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1783000, episode_reward=-902.60 +/- 49.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1784000, episode_reward=-866.60 +/- 44.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1785000, episode_reward=-907.80 +/- 43.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1786000, episode_reward=-909.00 +/- 28.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1787000, episode_reward=-923.20 +/- 39.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1788000, episode_reward=-920.60 +/- 12.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1789000, episode_reward=-908.20 +/- 51.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1790000, episode_reward=-913.40 +/- 30.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1791000, episode_reward=-925.00 +/- 42.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1792000, episode_reward=-910.80 +/- 23.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1793000, episode_reward=-892.80 +/- 17.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1794000, episode_reward=-925.40 +/- 30.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1795000, episode_reward=-886.40 +/- 27.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1796000, episode_reward=-893.00 +/- 31.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1797000, episode_reward=-882.60 +/- 29.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1798000, episode_reward=-886.00 +/- 50.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1799000, episode_reward=-895.40 +/- 54.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1800000, episode_reward=-909.20 +/- 25.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1801000, episode_reward=-904.40 +/- 50.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1802000, episode_reward=-916.40 +/- 15.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1803000, episode_reward=-879.80 +/- 50.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1804000, episode_reward=-925.20 +/- 5.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1805000, episode_reward=-880.00 +/- 34.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1806000, episode_reward=-930.60 +/- 12.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1807000, episode_reward=-892.80 +/- 25.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1808000, episode_reward=-883.20 +/- 31.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1809000, episode_reward=-910.40 +/- 29.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1810000, episode_reward=-912.40 +/- 20.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1811000, episode_reward=-911.80 +/- 26.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1812000, episode_reward=-936.60 +/- 26.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1813000, episode_reward=-898.00 +/- 43.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1814000, episode_reward=-914.00 +/- 57.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1815000, episode_reward=-937.20 +/- 22.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1816000, episode_reward=-935.80 +/- 18.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1817000, episode_reward=-947.80 +/- 28.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1818000, episode_reward=-918.60 +/- 17.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1819000, episode_reward=-895.80 +/- 63.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1820000, episode_reward=-937.00 +/- 49.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1821000, episode_reward=-932.00 +/- 55.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1822000, episode_reward=-904.60 +/- 18.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1823000, episode_reward=-937.20 +/- 37.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1824000, episode_reward=-916.40 +/- 35.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1825000, episode_reward=-889.00 +/- 24.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1826000, episode_reward=-919.20 +/- 34.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1827000, episode_reward=-907.20 +/- 29.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1828000, episode_reward=-922.00 +/- 18.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1829000, episode_reward=-895.40 +/- 44.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1830000, episode_reward=-883.00 +/- 28.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1831000, episode_reward=-927.00 +/- 29.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1832000, episode_reward=-882.00 +/- 33.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1833000, episode_reward=-917.00 +/- 48.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1834000, episode_reward=-893.20 +/- 37.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1835000, episode_reward=-867.60 +/- 44.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1836000, episode_reward=-914.40 +/- 26.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1837000, episode_reward=-910.80 +/- 29.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1838000, episode_reward=-878.00 +/- 39.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1839000, episode_reward=-906.20 +/- 31.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1840000, episode_reward=-889.20 +/- 19.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1841000, episode_reward=-904.60 +/- 21.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1842000, episode_reward=-913.60 +/- 31.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1843000, episode_reward=-902.60 +/- 15.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1844000, episode_reward=-901.00 +/- 42.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1845000, episode_reward=-858.20 +/- 29.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1846000, episode_reward=-910.60 +/- 32.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1847000, episode_reward=-905.40 +/- 27.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1848000, episode_reward=-917.60 +/- 25.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1849000, episode_reward=-903.00 +/- 18.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1850000, episode_reward=-904.80 +/- 34.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1851000, episode_reward=-901.80 +/- 32.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1852000, episode_reward=-895.60 +/- 15.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1853000, episode_reward=-919.20 +/- 50.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1854000, episode_reward=-926.60 +/- 49.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1855000, episode_reward=-863.60 +/- 21.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1856000, episode_reward=-861.60 +/- 23.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1857000, episode_reward=-910.20 +/- 52.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1858000, episode_reward=-868.60 +/- 38.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1859000, episode_reward=-949.40 +/- 50.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1860000, episode_reward=-929.60 +/- 49.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1861000, episode_reward=-897.20 +/- 36.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1862000, episode_reward=-909.80 +/- 45.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1863000, episode_reward=-901.80 +/- 53.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1864000, episode_reward=-940.60 +/- 54.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1865000, episode_reward=-895.60 +/- 41.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1866000, episode_reward=-910.20 +/- 28.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1867000, episode_reward=-929.60 +/- 21.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1868000, episode_reward=-927.20 +/- 24.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1869000, episode_reward=-931.00 +/- 37.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1870000, episode_reward=-921.60 +/- 32.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1871000, episode_reward=-933.20 +/- 21.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1872000, episode_reward=-873.80 +/- 34.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1873000, episode_reward=-928.60 +/- 29.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1874000, episode_reward=-919.40 +/- 25.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1875000, episode_reward=-877.60 +/- 35.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1876000, episode_reward=-911.40 +/- 19.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1877000, episode_reward=-909.80 +/- 22.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1878000, episode_reward=-872.00 +/- 24.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1879000, episode_reward=-907.00 +/- 45.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1880000, episode_reward=-944.60 +/- 56.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1881000, episode_reward=-891.20 +/- 27.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1882000, episode_reward=-913.00 +/- 49.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1883000, episode_reward=-925.20 +/- 45.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1884000, episode_reward=-866.40 +/- 24.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1885000, episode_reward=-908.60 +/- 26.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1886000, episode_reward=-935.80 +/- 26.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1887000, episode_reward=-910.80 +/- 37.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1888000, episode_reward=-917.00 +/- 58.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1889000, episode_reward=-920.80 +/- 35.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1890000, episode_reward=-919.60 +/- 28.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1891000, episode_reward=-880.20 +/- 48.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1892000, episode_reward=-904.40 +/- 32.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1893000, episode_reward=-900.60 +/- 45.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1894000, episode_reward=-903.60 +/- 27.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1895000, episode_reward=-887.00 +/- 43.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1896000, episode_reward=-894.00 +/- 38.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1897000, episode_reward=-907.40 +/- 20.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1898000, episode_reward=-896.40 +/- 33.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1899000, episode_reward=-886.40 +/- 43.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1900000, episode_reward=-902.80 +/- 15.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1901000, episode_reward=-878.60 +/- 62.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1902000, episode_reward=-902.60 +/- 35.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1903000, episode_reward=-896.20 +/- 33.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1904000, episode_reward=-901.00 +/- 35.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1905000, episode_reward=-901.60 +/- 17.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1906000, episode_reward=-916.00 +/- 20.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1907000, episode_reward=-921.40 +/- 53.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1908000, episode_reward=-917.80 +/- 31.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1909000, episode_reward=-895.20 +/- 58.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1910000, episode_reward=-894.00 +/- 27.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1911000, episode_reward=-914.20 +/- 13.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1912000, episode_reward=-882.40 +/- 65.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1913000, episode_reward=-890.40 +/- 39.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1914000, episode_reward=-879.20 +/- 20.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1915000, episode_reward=-913.80 +/- 40.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1916000, episode_reward=-894.20 +/- 9.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1917000, episode_reward=-928.80 +/- 39.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1918000, episode_reward=-897.20 +/- 34.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1919000, episode_reward=-897.00 +/- 23.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1920000, episode_reward=-904.20 +/- 17.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1921000, episode_reward=-879.40 +/- 29.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1922000, episode_reward=-877.20 +/- 18.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1923000, episode_reward=-900.40 +/- 74.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1924000, episode_reward=-895.00 +/- 35.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1925000, episode_reward=-885.80 +/- 8.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1926000, episode_reward=-921.00 +/- 48.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1927000, episode_reward=-905.60 +/- 45.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1928000, episode_reward=-903.00 +/- 18.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1929000, episode_reward=-912.80 +/- 48.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1930000, episode_reward=-871.60 +/- 29.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1931000, episode_reward=-926.60 +/- 33.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1932000, episode_reward=-909.20 +/- 66.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1933000, episode_reward=-909.20 +/- 9.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1934000, episode_reward=-868.20 +/- 47.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1935000, episode_reward=-923.00 +/- 18.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1936000, episode_reward=-898.60 +/- 24.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1937000, episode_reward=-912.00 +/- 21.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1938000, episode_reward=-865.80 +/- 34.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1939000, episode_reward=-927.20 +/- 27.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1940000, episode_reward=-890.60 +/- 26.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1941000, episode_reward=-915.80 +/- 44.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1942000, episode_reward=-894.80 +/- 32.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1943000, episode_reward=-930.00 +/- 35.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1944000, episode_reward=-901.80 +/- 26.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1945000, episode_reward=-882.80 +/- 19.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1946000, episode_reward=-913.00 +/- 75.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1947000, episode_reward=-867.60 +/- 47.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1948000, episode_reward=-908.60 +/- 26.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1949000, episode_reward=-930.60 +/- 41.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1950000, episode_reward=-898.20 +/- 27.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1951000, episode_reward=-906.00 +/- 55.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1952000, episode_reward=-905.00 +/- 25.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1953000, episode_reward=-930.40 +/- 46.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1954000, episode_reward=-912.60 +/- 23.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1955000, episode_reward=-916.20 +/- 32.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1956000, episode_reward=-926.60 +/- 34.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1957000, episode_reward=-896.80 +/- 30.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1958000, episode_reward=-920.20 +/- 32.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1959000, episode_reward=-882.80 +/- 13.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1960000, episode_reward=-909.00 +/- 36.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1961000, episode_reward=-917.20 +/- 39.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1962000, episode_reward=-916.80 +/- 22.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1963000, episode_reward=-910.20 +/- 20.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1964000, episode_reward=-871.20 +/- 20.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1965000, episode_reward=-895.00 +/- 33.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1966000, episode_reward=-887.80 +/- 43.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1967000, episode_reward=-916.40 +/- 38.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1968000, episode_reward=-887.00 +/- 36.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1969000, episode_reward=-871.00 +/- 34.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1970000, episode_reward=-933.60 +/- 24.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1971000, episode_reward=-896.20 +/- 21.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1972000, episode_reward=-910.60 +/- 48.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1973000, episode_reward=-910.00 +/- 47.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1974000, episode_reward=-912.00 +/- 16.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1975000, episode_reward=-924.40 +/- 39.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1976000, episode_reward=-881.20 +/- 26.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1977000, episode_reward=-876.80 +/- 41.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1978000, episode_reward=-906.80 +/- 44.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1979000, episode_reward=-906.20 +/- 27.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1980000, episode_reward=-877.00 +/- 19.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1981000, episode_reward=-864.60 +/- 31.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1982000, episode_reward=-945.40 +/- 24.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1983000, episode_reward=-910.20 +/- 41.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1984000, episode_reward=-930.60 +/- 25.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1985000, episode_reward=-915.20 +/- 32.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1986000, episode_reward=-906.60 +/- 16.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1987000, episode_reward=-891.20 +/- 16.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1988000, episode_reward=-892.80 +/- 30.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1989000, episode_reward=-892.60 +/- 30.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1990000, episode_reward=-892.60 +/- 17.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1991000, episode_reward=-910.00 +/- 22.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1992000, episode_reward=-887.80 +/- 45.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1993000, episode_reward=-905.20 +/- 33.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1994000, episode_reward=-926.60 +/- 38.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1995000, episode_reward=-910.40 +/- 41.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1996000, episode_reward=-881.80 +/- 19.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1997000, episode_reward=-919.00 +/- 16.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1998000, episode_reward=-883.80 +/- 28.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1999000, episode_reward=-913.40 +/- 40.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2000000, episode_reward=-915.80 +/- 31.01\n",
      "Episode length: 100.00 +/- 0.00\n"
     ]
    }
   ],
   "source": [
    "### REINFORCEMENT LEARNING IIIa1 ###\n",
    "### TRAIN REACTIVE MODEL ###\n",
    "import gym\n",
    "import stable_baselines3 as sb\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "import pickle\n",
    "\n",
    "# Initiate environment\n",
    "env = gym.make('Production-v0', reactive_mode = True)\n",
    "# Callback for best model\n",
    "best_callback = EvalCallback(env, best_model_save_path='./callback/',\n",
    "                             log_path='./callback/', eval_freq=1000,\n",
    "                             deterministic=True, render=False)\n",
    "\n",
    "model = sb.PPO('MlpPolicy', env, tensorboard_log=\"./tensorboard/\")\n",
    "model.learn(total_timesteps=2e6, tb_log_name=\"PPO_REACT_model\", callback = best_callback)\n",
    "model.save(\"PPO_REACT_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average number of reactive maintenance interventions per episode is:  0.0\n",
      "The average number of preventive maintenance interventions per episode is:  0.0\n",
      "The average mean time between failure per episode is:  0.0\n",
      "The average sum of inventory per episode is:  27.901\n",
      "The average sum of spare parts inventory per episode is:  0.0\n",
      "The average reward per episode is:  -908.089\n",
      "The average upper bound per episode is:  644.052\n"
     ]
    }
   ],
   "source": [
    "### REINFORCEMENT LEARNING IIIa2 ###\n",
    "### EVALUATE REACTIVE MODEL ###\n",
    "\n",
    "import pandas as pd\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3 import PPO\n",
    "import numpy as np\n",
    "\n",
    "env = gym.make('Production-v0', reactive_mode = True)\n",
    "# Best Model\n",
    "model = PPO.load('./callback/best_PPO_reactive', env = env)\n",
    "# Initilaize Reward\n",
    "result_df = pd.DataFrame([[0, 0, 0, 0, 0, 0, 0]], columns=['RM', 'PM', 'MTBF', 'Inventory', 'Spare Parts Inventory', 'Reward', 'Upper'])\n",
    "# Set iterations\n",
    "iterations = 1000\n",
    "for i in range(iterations):\n",
    "    # Initialize episode\n",
    "    store = []\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    store.append([0, obs[0], env.breakdown, obs[2], obs[3], 0, done, obs[1]])\n",
    "    # Compute one episode\n",
    "    while not done:\n",
    "        # Get best action for state\n",
    "        action, _state = model.predict(obs, deterministic=True)\n",
    "        # Compute next state\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        # Store results of this episode\n",
    "        store.append([action, obs[0], env.breakdown, obs[2], obs[3], reward, done, obs[1]])\n",
    "    eps_df = pd.DataFrame(store, columns=['action', 'health', 'breakdown', 'inventory', 'sp_inventory', 'reward', 'done', 'next_order'])\n",
    "    # Calculate nr. of reactive maintenance interventions by counting health 'resets' and substracting PM actions\n",
    "    result_df.iloc[0]['RM'] = result_df.iloc[0]['RM'] + sum(eps_df['breakdown']==True)\n",
    "    # Calculate nr. of preventive maintenance interventions\n",
    "    result_df.iloc[0]['PM'] = result_df.iloc[0]['PM'] + sum(eps_df['action']==10)\n",
    "    # Calculate mean time between failures\n",
    "    # Cut df after last breakdown\n",
    "    eps_df_trim = eps_df.iloc[:(np.where(eps_df['breakdown'].eq(True), eps_df.index, 0).max()+1)]\n",
    "    # Calculate MTBF by dividing periods where machine is running / breakdowns\n",
    "    #result_df.iloc[0]['MTBF'] = result_df.iloc[0]['MTBF'] + (len(eps_df_trim) -\n",
    "    #    sum(eps_df_trim['breakdown'] == True)) / sum(eps_df_trim['breakdown'] == True)\n",
    "    # Calculate inventory\n",
    "    result_df.iloc[0]['Inventory'] = result_df.iloc[0]['Inventory'] + sum(eps_df['inventory'])\n",
    "    # Calculate spare parts inventory per period\n",
    "    result_df.iloc[0]['Spare Parts Inventory'] = result_df.iloc[0]['Spare Parts Inventory'] + sum(eps_df['sp_inventory'])\n",
    "    # Calculate reward\n",
    "    result_df.iloc[0]['Reward'] = result_df.iloc[0]['Reward'] + sum(eps_df['reward'])\n",
    "    # Calculate reward with no costs and fulfillment of all orders\n",
    "    result_df.iloc[0]['Upper'] = result_df.iloc[0]['Upper'] + sum(eps_df.iloc[:-1]['next_order']) * env.order_r\n",
    "\n",
    "print(\"The average number of reactive maintenance interventions per episode is: \", result_df.iloc[0]['RM']/iterations)\n",
    "print(\"The average number of preventive maintenance interventions per episode is: \", result_df.iloc[0]['PM']/iterations)\n",
    "print(\"The average mean time between failure per episode is: \", result_df.iloc[0]['MTBF']/iterations)\n",
    "print(\"The average sum of inventory per episode is: \", result_df.iloc[0]['Inventory']/iterations)\n",
    "print(\"The average sum of spare parts inventory per episode is: \", result_df.iloc[0]['Spare Parts Inventory']/iterations)\n",
    "print(\"The average reward per episode is: \", result_df.iloc[0]['Reward']/iterations)\n",
    "print(\"The average upper bound per episode is: \", result_df.iloc[0]['Upper']/iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21.5"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "### REINFORCEMENT LEARNING IIIb1 ###\n",
    "### TRAIN TIME-BASED PREVENTIVE MODEL ###\n",
    "import gym\n",
    "import stable_baselines3 as sb\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "import pickle\n",
    "\n",
    "# Initiate environment\n",
    "env = gym.make('Production-v0', reactive_mode = True)\n",
    "# Callback for best model\n",
    "best_callback = EvalCallback(env, best_model_save_path='./callback/',\n",
    "                             log_path='./callback/', eval_freq=1000,\n",
    "                             deterministic=True, render=False)\n",
    "\n",
    "model = sb.DQN('MlpPolicy', env, tensorboard_log=\"./tensorboard/\")\n",
    "model.learn(total_timesteps=2e6, tb_log_name=\"DQN_TIME_model\", callback = best_callback)\n",
    "model.save(\"DQN_TIME_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true\n"
     ]
    }
   ],
   "source": [
    "### REINFORCEMENT LEARNING IIIb2 ###\n",
    "### EVALUATE TIME-BASED PREVENTIVE MODEL ###\n",
    "test = 1\n",
    "test2 = 0\n",
    "if test or test2: print(\"true\")\n",
    "else: print(\"false\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "### REINFORCEMENT LEARNING IV ###\n",
    "### VISUALIZE STATE-ACTION ###\n",
    "import numpy as np\n",
    "state_action = []\n",
    "\n",
    "# Define observation grid\n",
    "grid_health = np.arange(0.0, 1.01, 0.01)\n",
    "grid_order = range(0, 5)\n",
    "grid_inventory = range(0, 10)\n",
    "grid_sp_inventory = [0, 1]\n",
    "\n",
    "# Loop through grid and store best action for each state\n",
    "for hlt in grid_health:\n",
    "    for ord in grid_order:\n",
    "        for inv in grid_inventory:\n",
    "            for sin in grid_sp_inventory:\n",
    "                # Predict\n",
    "                action, _state = model.predict((hlt, ord, inv, sin), deterministic=True)\n",
    "                state_action.append([hlt, ord, inv, sin, action])\n",
    "\n",
    "state_action_df = pd.DataFrame(state_action, columns=['health', 'order', 'inventory', 'sp_inventory', 'action'])\n",
    "state_action_df.to_excel(\"visuals/state_action.xlsx\") "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6f69c5940b32a5cbabe45c9825076a627c6cdb9ede58cf4d0fa74ca6057ffe74"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
