{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=1000, episode_reward=57.80 +/- 15.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=55.20 +/- 20.86\n",
      "Episode length: 100.00 +/- 0.00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x1e020102190>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### REINFORCEMENT LEARNING I ###\n",
    "### TRAIN, SAVE, EVALUATE MODEL ###\n",
    "\n",
    "import gym\n",
    "import stable_baselines3 as sb\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "\n",
    "total_timesteps = 1e2\n",
    "env = gym.make('Steel-v0')\n",
    "# Callback for best model\n",
    "best_callback = EvalCallback(env, best_model_save_path='./steel/callback/model',\n",
    "                        log_path='./steel/callback/model', eval_freq=1000,\n",
    "                        deterministic=True, render=False)\n",
    "model = sb.PPO('MlpPolicy', env, tensorboard_log=\"./steel/tensorboard/\")\n",
    "model.learn(total_timesteps=total_timesteps, tb_log_name='model', callback = best_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average number of reactive maintenance interventions per episode is:  4.0\n",
      "The average number of preventive maintenance interventions per episode is:  1.0\n",
      "The average sum of non-faulty bars per episode is:  57.0\n",
      "The average reward per episode is:  -24.0\n"
     ]
    }
   ],
   "source": [
    "### REINFORCEMENT LEARNING II ###\n",
    "### TRY AND EVALUATE MY MODEL ###\n",
    "import pandas as pd\n",
    "import stable_baselines3 as sb\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "env = gym.make('Steel-v0')\n",
    "model = sb.PPO.load('./steel/callback/model/best_model', env = env)\n",
    "# Initilaize Reward\n",
    "result_df = pd.DataFrame(np.nan, index=range(0,100), columns=['RM', 'PM', 'Quality', 'Reward'])\n",
    "# Set iterations\n",
    "iterations = 1\n",
    "for i in range(iterations):\n",
    "    # Initialize episode\n",
    "    store = []\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    store.append([0, obs[0], env.breakdown, obs[1], 0, done])\n",
    "    # Compute one episode\n",
    "    while not done:\n",
    "        # Get best action for state\n",
    "        action, _state = model.predict(obs, deterministic=True)\n",
    "        # Compute next state\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        # Store results of this episode\n",
    "        store.append([action, obs[0], env.breakdown, obs[1], reward, done])\n",
    "    eps_df = pd.DataFrame(store, columns=['action', 'health', 'breakdown', 'quality', 'reward', 'done'])\n",
    "    # Calculate nr. of reactive maintenance interventions by counting health 'resets' and substracting PM actions\n",
    "    result_df.iloc[i]['RM'] = sum(eps_df['breakdown']==True)\n",
    "    # Calculate nr. of preventive maintenance interventions\n",
    "    result_df.iloc[i]['PM'] = sum(eps_df['action']==0)\n",
    "    # Calculate quality\n",
    "    result_df.iloc[i]['Quality'] = sum(eps_df['quality'])\n",
    "    # Calculate reward\n",
    "    result_df.iloc[i]['Reward'] = sum(eps_df['reward'])\n",
    "\n",
    "\n",
    "print(\"The average number of reactive maintenance interventions per episode is: \", result_df['RM'].mean())\n",
    "print(\"The average number of preventive maintenance interventions per episode is: \", result_df['PM'].mean())\n",
    "print(\"The average sum of non-faulty bars per episode is: \", result_df['Quality'].mean())\n",
    "print(\"The average reward per episode is: \", result_df['Reward'].mean())\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e534c7aeb4ab4dadd94ef2c1ee513be3be8c6be7a55d75e4860b1507ce941996"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('PxM-RL')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
