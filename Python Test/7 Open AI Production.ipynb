{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_mc(env, num_episodes):\n",
    "    '''\n",
    "    observation_space[0] is the 18 possible player values. (3 through 20)\n",
    "    observation_space[1] is the 10 possible dealer upcards. (2 through 11)\n",
    "\n",
    "    Combining these together yields all possible states.\n",
    "\n",
    "    Multiplying this with hit/stand yields all possible state/action pairs.\n",
    "\n",
    "    This is the Q map.\n",
    "    '''\n",
    "    Q = np.zeros([env.observation_space[0].n * env.observation_space[1].n, env.action_space.n], dtype=np.float16)\n",
    "\n",
    "\n",
    "    # This map contains the probability distributions for each action (hit or stand) given a state.\n",
    "    # The state (combo of player hand value and dealer upcard value) index in this array yields a 2-element array\n",
    "    # The 0th index of this 2-element array refers to the probability of \"hit\", and the 1st index is the probability of \"stand\"\n",
    "    prob = np.zeros([env.observation_space[0].n * env.observation_space[1].n, env.action_space.n], dtype=np.float16) + 0.5\n",
    "\n",
    "    # The learning rate. Very small to avoid making quick, large changes in our policy.\n",
    "    alpha = 0.001\n",
    "\n",
    "    epsilon = 1\n",
    "    \n",
    "    # The rate by which epsilon will decay over time.\n",
    "    # Since the probability we take the option with the highest Q-value is 1-epsilon + probability,\n",
    "    # this decay will make sure we are the taking the better option more often in the longrun.\n",
    "    # This allows the algorithm to explore in the early stages, and exploit in the later stages.\n",
    "    decay = 0.9999\n",
    "    \n",
    "    # The lowest value that epsilon can go to.\n",
    "    # Although the decay seems slow, it actually grows exponentially, and this is magnified when\n",
    "    # running thousands of episodes.\n",
    "    epsilon_min = 0.9\n",
    "\n",
    "    # may have to be tweaked later.\n",
    "    gamma = 0.8\n",
    "    \n",
    "    for _ in range(num_episodes):\n",
    "        episode = play_game(env, Q, prob)\n",
    "        \n",
    "        epsilon = max(epsilon * decay, epsilon_min)\n",
    "        \n",
    "        Q = update_Q(env, episode, Q, alpha, gamma)\n",
    "        prob = update_prob(env, episode, Q, prob, epsilon)\n",
    "        \n",
    "    return Q, prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_game(env, Q, prob):\n",
    "    # Can contain numerous state->action->reward tuples because a round of \n",
    "    # Blackjack is not always resolved in one turn.\n",
    "    # However, there will be no state that has a player hand value that exceeds 20, since only initial\n",
    "    # states BEFORE actions are made are used when storing state->action->reward tuples.\n",
    "    episode = []\n",
    "    \n",
    "    state = env.reset()\n",
    "    \n",
    "    while done == False:\n",
    "        if state[0] == 19: #Player was dealt Blackjack, player_value already subtracted by 2 to get state[0]\n",
    "            # don't do any episode analysis for this episode. This is a useless episode.\n",
    "            next_state, reward, done, info = env.step(1) # doesn't matter what action is taken.\n",
    "        else:\n",
    "            # Get the index in Q that corresponds to the current state\n",
    "            Q_state_index = get_Q_state_index(state)\n",
    "            \n",
    "            # Use the index to get the possible actions, and use np.argmax()\n",
    "            # to get the index of the action that has the highest current Q\n",
    "            # value. Index 0 is hit, index 1 is stand.\n",
    "            best_action = np.argmax(Q[Q_state_index])\n",
    "            \n",
    "            # Go to the prob table to retrieve the probability of this action.\n",
    "            # This uses the same Q_state_index used for finding the state index\n",
    "            # of the Q-array.\n",
    "            prob_of_best_action = get_prob_of_best_action(env, state, Q, prob)\n",
    "\n",
    "            action_to_take = None\n",
    "\n",
    "            if random.uniform(0,1) < prob_of_best_action: # Take the best action\n",
    "                action_to_take = best_action\n",
    "            else: # Take the other action\n",
    "                action_to_take = 1 if best_action == 0 else 0\n",
    "            \n",
    "            # The agent does the action, and we get the next state, the rewards,\n",
    "            # and whether the game is now done.\n",
    "            next_state, reward, done, info = env.step(action_to_take)\n",
    "            \n",
    "            # We now have a state->action->reward sequence we can log\n",
    "            # in `episode`\n",
    "            episode.append((state, action_to_take, reward))\n",
    "            \n",
    "            # update the state for the next decision made by the agent.\n",
    "            state = next_state\n",
    "        \n",
    "    return episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_Q(env, episode, Q, alpha, gamma):\n",
    "    '''\n",
    "    THIS IS WHERE THE ALGORITHM HINGES ON BEING FIRST VISIT OR EVERY VISIT.\n",
    "    I AM GOING TO USE FIRST-VISIT, AND HERE'S WHY.\n",
    "    \n",
    "    If you want first-visit, you need to use the cumulative reward of the entire\n",
    "    episode when updating a Q-value for ALL of the state/action pairs in the\n",
    "    episode, even the first state/action pair. In this algorithm, an episode\n",
    "    is a round of Blackjack. Although the bulk of the reward may come from the\n",
    "    2nd or 3rd decision, deciding to hit on the 1st decision is what enabled\n",
    "    the future situations to even occur, so it is important to include the\n",
    "    entire cumulative reward. We can reduce the impact of the rewards of the\n",
    "    future decisions by lowering gamma, which will lower the G value for our\n",
    "    early state/action pair in which we hit and did not get any immediate rewards.\n",
    "    This will make our agent consider future rewards, and not just look at \n",
    "    each state in isolation despite having hit previously.\n",
    "     \n",
    "    If you want Every-Visit MC, do not use the cumulative rewards when updating Q-values,\n",
    "    and just use the immediate reward in this episode for each state/action pair.\n",
    "    '''\n",
    "    step = 0\n",
    "    for state, action, reward in episode:\n",
    "        # calculate the cumulative reward of taking this action in this state.\n",
    "        # Start from the immediate rewards, and use all the rewards from the\n",
    "        # subsequent states. Do not use rewards from previous states.\n",
    "        total_reward = 0\n",
    "        gamma_exp = 0\n",
    "        for curr_step in range(step, len(episode)):\n",
    "            curr_reward = episode[curr_step][2]\n",
    "            total_reward += (gamma ** gamma_exp) * curr_reward\n",
    "            gamma_exp += 1\n",
    "        \n",
    "        # Update the Q-value\n",
    "        Q_state_index = get_Q_state_index(state)\n",
    "        curr_Q_value = Q[Q_state_index][action]\n",
    "        Q[Q_state_index][action] = curr_Q_value + alpha * (total_reward - curr_Q_value)\n",
    "        \n",
    "        # update step to start further down the episode next time.\n",
    "        step += 1\n",
    "        \n",
    "        \n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_prob(env, episode, Q, prob, epsilon):\n",
    "    for state, action, reward in episode:\n",
    "        # Update the probabilities of the actions that can be taken given the current\n",
    "        # state. The goal is that the new update in Q has changed what the best action\n",
    "        # is, and epsilon will be used to create a small increase in the probability\n",
    "        # that the new, better action is chosen.\n",
    "        prob = update_prob_of_best_action(env, state, Q, prob, epsilon)\n",
    "        \n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given a state, derive the corresponding index in the Q-array.\n",
    "# The state is a player hand value + dealer upcard pair,\n",
    "# so a \"hashing\" formula must be used to allocate the\n",
    "# indices of the Q-array properly.\n",
    "def get_Q_state_index(state):\n",
    "    # the player value is already subtracted by 1 in the env when it returns the state.\n",
    "    # subtract by 1 again to fit with the array indexing that starts at 0\n",
    "    initial_player_value = state[0] - 1\n",
    "    # the upcard value is already subtracted by 1 in the env when it returns the state.\n",
    "    # dealer_upcard will be subtracted by 1 to fit with the array indexing that starts at 0\n",
    "    dealer_upcard = state[1] - 1\n",
    "\n",
    "    return (env.observation_space[1].n * (initial_player_value)) + (dealer_upcard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prob_of_best_action(env, state, Q, prob):\n",
    "    # Use the mapping function to figure out which index of Q corresponds to \n",
    "    # the player hand value + dealer upcard value that defines each state.\n",
    "    Q_state_index = get_Q_state_index(state)\n",
    "    \n",
    "    # Use this index in the Q 2-D array to get a 2-element array that yield\n",
    "    # the current Q-values for hitting (index 0) and standing (index 1) in this state.\n",
    "    # Use the np.argmax() function to find the index of the action that yields the\n",
    "    # rewards i.e. the best action we are looking for.\n",
    "    best_action = np.argmax(Q[Q_state_index])\n",
    "    \n",
    "    # Retrieve the probability of the best action using the \n",
    "    # state/action pair as indices for the `prob` array,\n",
    "    # which stores the probability of taking an action (hit or stand)\n",
    "    # for a given state/action pair.\n",
    "    return prob[Q_state_index][best_action]\n",
    "    \n",
    "def update_prob_of_best_action(env, state, Q, prob, epsilon):\n",
    "\n",
    "    Q_state_index = get_Q_state_index(state)\n",
    "    \n",
    "    best_action = np.argmax(Q[Q_state_index])\n",
    "    \n",
    "    # Slightly alter the probability of this best action being taken by using epsilon\n",
    "    # Epsilon starts at 1.0, and slowly decays over time.\n",
    "    # Therefore, as per the equation below, the AI agent will use the probability listed \n",
    "    # for the best action in the `prob` array during the beginning of the algorithm.\n",
    "    # As time goes on, the likelihood that the best action is taken is increased from\n",
    "    # what is listed in the `prob` array.\n",
    "    # This allows for exploration of other moves in the beginning of the algorithm,\n",
    "    # but exploitation later for a greater reward.\n",
    "    #prob[Q_state_index][best_action] = prob[Q_state_index][best_action] + ((1 - epsilon) * (1 - prob[Q_state_index][best_action]))\n",
    "    prob[Q_state_index][best_action] = min(1, prob[Q_state_index][best_action] + 1 - epsilon)\n",
    "    \n",
    "    other_action = 1 if best_action == 0 else 0\n",
    "    prob[Q_state_index][other_action] = 1 - prob[Q_state_index][best_action]\n",
    "    \n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gym' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_17412/1295286521.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0menv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Production-v1'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mstart_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mnew_Q\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_prob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrun_mc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1000000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'gym' is not defined"
     ]
    }
   ],
   "source": [
    "import time\n",
    "env = gym.make('Production-v1')\n",
    "\n",
    "start_time = time.time()\n",
    "new_Q, new_prob = run_mc(env, 1000000)\n",
    "end_time = time.time()\n",
    "\n",
    "print(\"Total Time for Learning: \" + str(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'loader'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_14392/1009684758.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m spec = importlib.util.spec_from_file_location(\"prod_gym\",\n\u001b[0;32m      3\u001b[0m     \"C:/Users/w_kevi02/sciebo/PhD/01 Prescriptive Maintenance/30 RQ3 - Digital Twin/PxM & PPC (R)/RL-PxM/Python Test/prod_gym\")\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mfoo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodule_from_spec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mspec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexec_module\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfoo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36mmodule_from_spec\u001b[1;34m(spec)\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'loader'"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make('Production-v0')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ProductionEnv' object has no attribute 'done'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_17684/1616882387.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#env.reset()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m#env.step(3)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'ProductionEnv' object has no attribute 'done'"
     ]
    }
   ],
   "source": [
    "#print(env.action_space.sample)\n",
    "#print(env.action_space)\n",
    "#env.action_space[1].contains(6)\n",
    "#print(env.action_space[1].sample)\n",
    "#env.reset()\n",
    "env.step(3)\n",
    "### TODO Implement RL Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "not  0"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6f69c5940b32a5cbabe45c9825076a627c6cdb9ede58cf4d0fa74ca6057ffe74"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
