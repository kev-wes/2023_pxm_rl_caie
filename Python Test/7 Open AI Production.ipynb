{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to ./tensorboard/DQN_3\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 100       |\n",
      "|    ep_rew_mean      | -1.59e+03 |\n",
      "|    exploration_rate | 0.62      |\n",
      "| time/               |           |\n",
      "|    episodes         | 4         |\n",
      "|    fps              | 2691      |\n",
      "|    time_elapsed     | 0         |\n",
      "|    total_timesteps  | 400       |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 100       |\n",
      "|    ep_rew_mean      | -1.73e+03 |\n",
      "|    exploration_rate | 0.24      |\n",
      "| time/               |           |\n",
      "|    episodes         | 8         |\n",
      "|    fps              | 2728      |\n",
      "|    time_elapsed     | 0         |\n",
      "|    total_timesteps  | 800       |\n",
      "-----------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\w_kevi02\\Anaconda3\\lib\\site-packages\\stable_baselines3\\common\\evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=1000, episode_reward=-1186.20 +/- 139.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 100       |\n",
      "|    mean_reward      | -1.19e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.051     |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 1000      |\n",
      "-----------------------------------\n",
      "New best mean reward!\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 100       |\n",
      "|    ep_rew_mean      | -1.82e+03 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 12        |\n",
      "|    fps              | 1874      |\n",
      "|    time_elapsed     | 0         |\n",
      "|    total_timesteps  | 1200      |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 100       |\n",
      "|    ep_rew_mean      | -1.85e+03 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 16        |\n",
      "|    fps              | 2008      |\n",
      "|    time_elapsed     | 0         |\n",
      "|    total_timesteps  | 1600      |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=2000, episode_reward=-1299.40 +/- 18.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 100      |\n",
      "|    mean_reward      | -1.3e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 2000     |\n",
      "----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 100       |\n",
      "|    ep_rew_mean      | -1.93e+03 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 20        |\n",
      "|    fps              | 1628      |\n",
      "|    time_elapsed     | 1         |\n",
      "|    total_timesteps  | 2000      |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 100       |\n",
      "|    ep_rew_mean      | -1.93e+03 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 24        |\n",
      "|    fps              | 1715      |\n",
      "|    time_elapsed     | 1         |\n",
      "|    total_timesteps  | 2400      |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 100       |\n",
      "|    ep_rew_mean      | -1.97e+03 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 28        |\n",
      "|    fps              | 1706      |\n",
      "|    time_elapsed     | 1         |\n",
      "|    total_timesteps  | 2800      |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=3000, episode_reward=-1289.80 +/- 35.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 100       |\n",
      "|    mean_reward      | -1.29e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 3000      |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 100       |\n",
      "|    ep_rew_mean      | -1.95e+03 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 32        |\n",
      "|    fps              | 1494      |\n",
      "|    time_elapsed     | 2         |\n",
      "|    total_timesteps  | 3200      |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 100       |\n",
      "|    ep_rew_mean      | -1.97e+03 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 36        |\n",
      "|    fps              | 1587      |\n",
      "|    time_elapsed     | 2         |\n",
      "|    total_timesteps  | 3600      |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=4000, episode_reward=-1217.60 +/- 180.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 100       |\n",
      "|    mean_reward      | -1.22e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 4000      |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 100       |\n",
      "|    ep_rew_mean      | -1.96e+03 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 40        |\n",
      "|    fps              | 1535      |\n",
      "|    time_elapsed     | 2         |\n",
      "|    total_timesteps  | 4000      |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 100       |\n",
      "|    ep_rew_mean      | -1.97e+03 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 44        |\n",
      "|    fps              | 1619      |\n",
      "|    time_elapsed     | 2         |\n",
      "|    total_timesteps  | 4400      |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 100       |\n",
      "|    ep_rew_mean      | -1.98e+03 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 48        |\n",
      "|    fps              | 1685      |\n",
      "|    time_elapsed     | 2         |\n",
      "|    total_timesteps  | 4800      |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=5000, episode_reward=-1283.60 +/- 57.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 100       |\n",
      "|    mean_reward      | -1.28e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 5000      |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 100      |\n",
      "|    ep_rew_mean      | -2e+03   |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 52       |\n",
      "|    fps              | 1643     |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 5200     |\n",
      "----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 100       |\n",
      "|    ep_rew_mean      | -2.01e+03 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 56        |\n",
      "|    fps              | 1700      |\n",
      "|    time_elapsed     | 3         |\n",
      "|    total_timesteps  | 5600      |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=6000, episode_reward=-1195.40 +/- 76.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 100      |\n",
      "|    mean_reward      | -1.2e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 6000     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 100      |\n",
      "|    ep_rew_mean      | -2e+03   |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 60       |\n",
      "|    fps              | 1656     |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 6000     |\n",
      "----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 100       |\n",
      "|    ep_rew_mean      | -2.01e+03 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 64        |\n",
      "|    fps              | 1716      |\n",
      "|    time_elapsed     | 3         |\n",
      "|    total_timesteps  | 6400      |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 100       |\n",
      "|    ep_rew_mean      | -2.01e+03 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 68        |\n",
      "|    fps              | 1773      |\n",
      "|    time_elapsed     | 3         |\n",
      "|    total_timesteps  | 6800      |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=7000, episode_reward=-1273.20 +/- 34.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 100       |\n",
      "|    mean_reward      | -1.27e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 7000      |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 100       |\n",
      "|    ep_rew_mean      | -2.02e+03 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 72        |\n",
      "|    fps              | 1730      |\n",
      "|    time_elapsed     | 4         |\n",
      "|    total_timesteps  | 7200      |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 100      |\n",
      "|    ep_rew_mean      | -2e+03   |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 76       |\n",
      "|    fps              | 1781     |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 7600     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=8000, episode_reward=-1303.20 +/- 59.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 100      |\n",
      "|    mean_reward      | -1.3e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 8000     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 100      |\n",
      "|    ep_rew_mean      | -2e+03   |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 80       |\n",
      "|    fps              | 1738     |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 8000     |\n",
      "----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 100       |\n",
      "|    ep_rew_mean      | -1.99e+03 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 84        |\n",
      "|    fps              | 1784      |\n",
      "|    time_elapsed     | 4         |\n",
      "|    total_timesteps  | 8400      |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 100      |\n",
      "|    ep_rew_mean      | -2e+03   |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 88       |\n",
      "|    fps              | 1828     |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 8800     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=9000, episode_reward=-1276.40 +/- 70.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 100       |\n",
      "|    mean_reward      | -1.28e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 9000      |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 100      |\n",
      "|    ep_rew_mean      | -2e+03   |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 92       |\n",
      "|    fps              | 1795     |\n",
      "|    time_elapsed     | 5        |\n",
      "|    total_timesteps  | 9200     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 100      |\n",
      "|    ep_rew_mean      | -2e+03   |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 96       |\n",
      "|    fps              | 1834     |\n",
      "|    time_elapsed     | 5        |\n",
      "|    total_timesteps  | 9600     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=-1241.20 +/- 109.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 100       |\n",
      "|    mean_reward      | -1.24e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 10000     |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 100      |\n",
      "|    ep_rew_mean      | -2e+03   |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 100      |\n",
      "|    fps              | 1799     |\n",
      "|    time_elapsed     | 5        |\n",
      "|    total_timesteps  | 10000    |\n",
      "----------------------------------\n"
     ]
    }
   ],
   "source": [
    "### TRAIN, SAVE, EVALUATE MODEL ###\n",
    "\n",
    "import gym\n",
    "import stable_baselines3 as sb\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "env = gym.make('Production-v0')\n",
    "# Callback for best model\n",
    "best_callback = EvalCallback(env, best_model_save_path='./callback/',\n",
    "                             log_path='./callback/', eval_freq=1000,\n",
    "                             deterministic=True, render=False)\n",
    "\n",
    "model = sb.DQN('MlpPolicy', env, tensorboard_log=\"./tensorboard/\", gamma = 0.99, learning_rate=0.01)\n",
    "model.learn(total_timesteps=1e6, tb_log_name=\"DQN\", callback = best_callback)\n",
    "model.save(\"DQN_1_model\")\n",
    "\n",
    "# Evaluate the agent\n",
    "mean_reward, std_reward = evaluate_policy(model, model.get_env(), n_eval_episodes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(-1245.3, 53.79042665753824)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### LOAD MODEL ###\n",
    "import gym\n",
    "import stable_baselines3 as sb\n",
    "from stable_baselines3 import DQN\n",
    "\n",
    "env = gym.make('Production-v0')\n",
    "# Best Model\n",
    "model = DQN.load('./callback/best_model', env = env)\n",
    "# Last Model\n",
    "#model = DQN.load('DQN_1_model', env = env)\n",
    "\n",
    "# Evaluate the agent\n",
    "evaluate_policy(model, model.get_env(), n_eval_episodes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cumulative reward is:  -1232\n"
     ]
    }
   ],
   "source": [
    "### TRY MODEL ###\n",
    "import pandas as pd\n",
    "\n",
    "store = []\n",
    "obs = env.reset()\n",
    "done = False\n",
    "store.append([0, obs[0], obs[2], 0, done, obs[1]])\n",
    "while not done:\n",
    "    action, _state = model.predict(obs, deterministic=True)\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    store.append([action, obs[0], obs[2], reward, done, obs[1]])\n",
    "\n",
    "store_df = pd.DataFrame(store, columns=['action', 'RUL', 'inventory', 'reward', 'done', 'next_order'])\n",
    "print(\"Cumulative reward is: \", sum(store_df['reward']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6f69c5940b32a5cbabe45c9825076a627c6cdb9ede58cf4d0fa74ca6057ffe74"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
